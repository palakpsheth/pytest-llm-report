{
  "run_meta": {
    "aggregation_policy": null,
    "collect_only": false,
    "collected_count": 623,
    "deselected_count": 0,
    "duration": 119.303826,
    "end_time": "2026-01-21T07:36:48.914055+00:00",
    "exit_code": 0,
    "git_dirty": false,
    "git_sha": "b1f3c49618f01bb762e8fa4ff37082e2ac4ba601",
    "interrupted": false,
    "is_aggregated": true,
    "llm_annotations_count": 621,
    "llm_annotations_enabled": true,
    "llm_annotations_errors": 1,
    "llm_context_mode": "minimal",
    "llm_model": "llama3.2:1b",
    "llm_provider": "ollama",
    "llm_total_input_tokens": 135999,
    "llm_total_output_tokens": 73657,
    "llm_total_tokens": 209656,
    "platform": "Linux-6.11.0-1018-azure-x86_64-with-glibc2.39",
    "plugin_git_dirty": true,
    "plugin_git_sha": "a03dbe622cdc018f89b74731aed91adf1a582867",
    "plugin_version": "0.2.1",
    "pytest_version": "9.0.2",
    "python_version": "3.12.12",
    "repo_git_dirty": false,
    "repo_git_sha": "b1f3c49618f01bb762e8fa4ff37082e2ac4ba601",
    "repo_version": "0.2.0",
    "rerun_count": 0,
    "run_count": 1,
    "run_id": "21201087766-py3.12",
    "selected_count": 623,
    "source_reports": [],
    "start_time": "2026-01-21T07:34:49.610229+00:00"
  },
  "schema_version": "1.1.0",
  "sha256": "df16a24f132e18d9c608265086c35206dbe67762fd709167f71068603d380854",
  "source_coverage": [
    {
      "coverage_percent": 100.0,
      "covered": 2,
      "covered_ranges": "2-3",
      "file_path": "src/pytest_llm_report/_git_info.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 2
    },
    {
      "coverage_percent": 95.04,
      "covered": 115,
      "covered_ranges": "13, 15-19, 21, 36, 39, 45, 47, 53-54, 56-58, 60, 62-65, 70, 74-75, 78-81, 85, 88-90, 94, 104, 110, 113-115, 117-121, 123-124, 129, 131-132, 134-135, 138-139, 145-147, 149, 152, 155, 158, 160, 162, 176, 178, 182, 184, 186, 196, 198-202, 204-205, 208, 210, 219, 231, 233-247, 249, 251, 259-260, 262-263, 265, 267-269, 273, 276-277, 279-280, 283, 285-286, 288, 290-291, 295",
      "file_path": "src/pytest_llm_report/aggregation.py",
      "missed": 6,
      "missed_ranges": "67, 91-92, 111, 206, 217",
      "statements": 121
    },
    {
      "coverage_percent": 93.62,
      "covered": 44,
      "covered_ranges": "13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153",
      "file_path": "src/pytest_llm_report/cache.py",
      "missed": 3,
      "missed_ranges": "64-65, 130",
      "statements": 47
    },
    {
      "coverage_percent": 99.1,
      "covered": 110,
      "covered_ranges": "19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140-141, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285",
      "file_path": "src/pytest_llm_report/collector.py",
      "missed": 1,
      "missed_ranges": "239",
      "statements": 111
    },
    {
      "coverage_percent": 94.34,
      "covered": 50,
      "covered_ranges": "13-15, 18, 27, 29-31, 33, 35-36, 38-41, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 72, 81-82, 86, 88-90, 93, 96, 108, 111, 124, 126-127, 129-130, 133, 135",
      "file_path": "src/pytest_llm_report/context_util.py",
      "missed": 3,
      "missed_ranges": "53, 83-84",
      "statements": 53
    },
    {
      "coverage_percent": 95.56,
      "covered": 129,
      "covered_ranges": "13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127-128, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-250, 252-254, 257, 259-260, 263-264, 271, 273-274, 276-279, 281-283, 285, 299-300, 302, 308",
      "file_path": "src/pytest_llm_report/coverage_map.py",
      "missed": 6,
      "missed_ranges": "62, 123, 125, 157, 221, 251",
      "statements": 135
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 73, 77-79, 83, 132, 142",
      "file_path": "src/pytest_llm_report/errors.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 3,
      "covered_ranges": "4-5, 7",
      "file_path": "src/pytest_llm_report/llm/__init__.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 3
    },
    {
      "coverage_percent": 86.36,
      "covered": 133,
      "covered_ranges": "4, 6-10, 12-15, 21-22, 25-30, 33, 47-48, 50-52, 56, 58-59, 65, 67-68, 70, 73-74, 76, 84, 86-90, 95-96, 98-99, 106-107, 112-113, 116, 121-126, 130, 132, 134, 137, 144, 156, 181-182, 184, 186, 188-189, 199, 211, 213-216, 221-223, 226, 249-252, 254-255, 260, 262, 264-267, 269-270, 277-279, 281, 283-284, 289-290, 292-293, 298-301, 303, 306, 329-332, 334, 336, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381",
      "file_path": "src/pytest_llm_report/llm/annotator.py",
      "missed": 21,
      "missed_ranges": "77-81, 160-168, 173, 286-287, 345, 364-365, 371",
      "statements": 154
    },
    {
      "coverage_percent": 95.42,
      "covered": 125,
      "covered_ranges": "13, 15-18, 20, 30, 33, 47, 50, 53, 59, 65-66, 68, 87-88, 96, 101, 103, 105, 128, 134-135, 137-138, 149, 155, 157, 163, 165, 174, 176, 185-186, 188, 191-198, 200, 202, 212, 214-217, 219-222, 224, 232, 243, 245, 247, 264, 266-267, 270-272, 274-275, 277, 279, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314, 316, 325-326, 329-331, 333-334, 337-339, 342-347, 351, 353, 359-360, 363-364, 367-369, 372, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404, 406",
      "file_path": "src/pytest_llm_report/llm/base.py",
      "missed": 6,
      "missed_ranges": "91-92, 230, 284, 292, 296",
      "statements": 131
    },
    {
      "coverage_percent": 95.56,
      "covered": 86,
      "covered_ranges": "8, 10-13, 20, 23-24, 27-29, 31-32, 34, 36-37, 39, 44, 53-55, 58, 67-68, 70, 73, 92-93, 95, 97, 103-106, 108-110, 112, 122-123, 126-128, 136, 139, 156-157, 160, 162, 164-167, 170-176, 181-185, 187-188, 190, 192-194, 196-197, 203-206, 209-210, 213-214, 216-218, 222, 224",
      "file_path": "src/pytest_llm_report/llm/batching.py",
      "missed": 4,
      "missed_ranges": "158, 207, 211, 220",
      "statements": 90
    },
    {
      "coverage_percent": 97.85,
      "covered": 318,
      "covered_ranges": "7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-89, 91-97, 99-114, 121-122, 125, 128, 134-135, 137-141, 143-144, 146, 164-166, 173-175, 178, 181-182, 184, 186-189, 191-192, 198-206, 208-210, 212-213, 215, 218, 221-230, 232-233, 235-237, 239-243, 246-247, 249-252, 254-255, 259, 261, 263, 268, 272-276, 279-281, 283, 288-293, 295, 299-305, 308-309, 311-312, 318-319, 322, 326, 332-333, 335, 339-343, 345-349, 352-353, 358-359, 366-367, 369, 383, 385-386, 390, 410, 413-415, 418-422, 424-427, 432, 434-435, 437, 441-444, 446, 449-463, 469, 471-473, 475-478, 480, 486, 488-491, 493, 495, 497-498, 502-508, 511, 514-516, 518-521, 523-528, 534, 537, 539-543, 547-548, 550-559, 562-564, 567-570, 574",
      "file_path": "src/pytest_llm_report/llm/gemini.py",
      "missed": 7,
      "missed_ranges": "115-117, 298, 310, 313-314",
      "statements": 325
    },
    {
      "coverage_percent": 98.7,
      "covered": 76,
      "covered_ranges": "8, 10, 12-13, 21, 31, 37-38, 41-42, 44, 51, 60-62, 64, 82-83, 89, 92, 95-96, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 122, 124-126, 129-130, 132, 135, 137, 139, 141-142, 144, 148, 170, 182-183, 186-188, 190, 192-193, 196-198, 204, 206, 211, 213, 215, 221-222, 224, 227-231, 234, 236, 242-243, 245",
      "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
      "missed": 1,
      "missed_ranges": "207",
      "statements": 77
    },
    {
      "coverage_percent": 100.0,
      "covered": 13,
      "covered_ranges": "8, 10, 12-13, 20, 26, 32, 34, 51, 53, 59, 61, 67",
      "file_path": "src/pytest_llm_report/llm/noop.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 13
    },
    {
      "coverage_percent": 98.61,
      "covered": 71,
      "covered_ranges": "7, 9, 11-12, 18, 24, 42-43, 49, 52-53, 55, 58, 60-61, 63-67, 70, 74-77, 83, 85-86, 92, 94, 96-98, 100-101, 103, 107, 113-114, 116-118, 122, 128, 130, 138, 140, 142-144, 149-150, 156, 158, 160-162, 165-167, 172-173, 178, 180, 190, 192-193, 204, 209, 211-212",
      "file_path": "src/pytest_llm_report/llm/ollama.py",
      "missed": 1,
      "missed_ranges": "90",
      "statements": 72
    },
    {
      "coverage_percent": 97.22,
      "covered": 35,
      "covered_ranges": "8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130",
      "file_path": "src/pytest_llm_report/llm/schemas.py",
      "missed": 1,
      "missed_ranges": "39",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 71,
      "covered_ranges": "7, 9-14, 17, 20, 23-24, 36-39, 41-43, 47, 59-60, 63-66, 69-72, 74, 83, 85-88, 90-91, 93, 101-103, 107-109, 111, 113-116, 120, 132-136, 139-140, 143-145, 148-150, 153-156, 158, 160-162",
      "file_path": "src/pytest_llm_report/llm/token_refresh.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 71
    },
    {
      "coverage_percent": 93.94,
      "covered": 31,
      "covered_ranges": "4, 6, 9, 20, 23, 42-43, 46-47, 51-53, 55-56, 66, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90, 93-94, 96, 98",
      "file_path": "src/pytest_llm_report/llm/utils.py",
      "missed": 2,
      "missed_ranges": "48, 78",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 253,
      "covered_ranges": "17-18, 20, 23, 26-27, 36-38, 40, 42, 49-50, 59-61, 63, 65, 72-73, 86-92, 94, 96, 107-108, 120-126, 128, 130, 135-143, 146-147, 169-185, 187-188, 190, 192, 194, 201-224, 227-228, 236-237, 239, 241, 247-248, 257-259, 261, 263, 270-271, 280-282, 284, 286, 290-292, 295-296, 333-362, 364-372, 374, 376, 394-417, 419-437, 440-441, 455-463, 465, 467, 477-479, 482-483, 500-510, 512, 518, 520, 526-540",
      "file_path": "src/pytest_llm_report/models.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 253
    },
    {
      "coverage_percent": 78.73,
      "covered": 211,
      "covered_ranges": "122, 170, 199, 202-204, 209-211, 217-219, 225-227, 233-235, 241-242, 245-254, 257-259, 265-267, 271-274, 276, 284, 293, 308, 311-312, 320-325, 327, 332-337, 340-345, 348-349, 352-353, 356-357, 360-369, 372-375, 378-393, 396-397, 400-405, 408-409, 412-413, 416-421, 426-427, 430-431, 436-439, 444-447, 449, 451, 453, 460-461, 463-464, 466-467, 470-475, 479, 482-495, 498, 502-503, 507, 510, 514-515, 519-520, 524, 527, 531, 534-536, 540-541, 545-546, 550, 553, 557, 560, 564-565, 569, 572-574, 578, 581-584, 587, 591-592, 596, 599-608, 611, 613",
      "file_path": "src/pytest_llm_report/options.py",
      "missed": 57,
      "missed_ranges": "13-15, 21-22, 98-102, 105-107, 110-115, 118-121, 138-139, 142-149, 152-155, 158-160, 163-166, 169, 180-184, 187-188, 191, 193, 278, 287, 296",
      "statements": 268
    },
    {
      "coverage_percent": 86.81,
      "covered": 158,
      "covered_ranges": "41, 44, 50, 56, 62, 68, 74, 81, 90, 96, 102, 108, 114, 122, 128, 134, 142, 148, 155, 161, 169, 176, 185, 192, 199, 208, 215, 223, 229, 235, 241, 247, 254, 260, 268, 274, 283, 289, 297, 304, 311, 328, 332, 336, 342-343, 346-347, 349, 351, 354-356, 362-363, 371-372, 399-400, 403-404, 407, 410-411, 413-414, 417-418, 420, 422-426, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 476-477, 485-487, 491-494, 497, 499, 502-507, 509, 512-514, 516-521, 523, 534-535, 558-559, 562-563, 566-568, 579-580, 583, 586-587, 590-592, 602-603, 606-608, 619-620, 623, 626, 628-629",
      "file_path": "src/pytest_llm_report/plugin.py",
      "missed": 24,
      "missed_ranges": "13, 15-18, 20-21, 23, 29-32, 35, 319, 377, 481-482, 488, 548-549, 571, 595, 611-612",
      "statements": 182
    },
    {
      "coverage_percent": 97.27,
      "covered": 107,
      "covered_ranges": "13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 114, 116, 118, 139-140, 142-144, 147, 152-153, 155-157, 159-161, 163-164, 166-167, 170-171, 173, 177, 180, 189, 192-194, 196-197, 201, 203, 216-217, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249, 251, 268, 275, 284-287",
      "file_path": "src/pytest_llm_report/prompts.py",
      "missed": 3,
      "missed_ranges": "80, 185, 233",
      "statements": 110
    },
    {
      "coverage_percent": 90.77,
      "covered": 59,
      "covered_ranges": "13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 140-142, 147, 155-157, 159, 172-177, 191, 210-211, 224, 267, 269, 285",
      "file_path": "src/pytest_llm_report/render.py",
      "missed": 6,
      "missed_ranges": "148-149, 212, 217-218, 222",
      "statements": 65
    },
    {
      "coverage_percent": 98.2,
      "covered": 164,
      "covered_ranges": "13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 113, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 310, 319, 321-322, 324-335, 337, 339, 347, 350-352, 355-356, 359-361, 364, 367, 375, 383, 385-386, 389, 392, 395, 398, 406, 408-409, 415, 417, 419, 421-432, 439, 441-442, 444-446, 454-458, 460, 462, 465, 468-469, 471, 477-481, 487-488, 495, 502, 504, 506-508, 510, 513-514, 516, 522-523",
      "file_path": "src/pytest_llm_report/report_writer.py",
      "missed": 3,
      "missed_ranges": "135-137",
      "statements": 167
    },
    {
      "coverage_percent": 97.06,
      "covered": 33,
      "covered_ranges": "11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-65, 67, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123",
      "file_path": "src/pytest_llm_report/util/fs.py",
      "missed": 1,
      "missed_ranges": "40",
      "statements": 34
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121",
      "file_path": "src/pytest_llm_report/util/hashing.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 33,
      "covered_ranges": "12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95",
      "file_path": "src/pytest_llm_report/util/ranges.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 16,
      "covered_ranges": "4, 6, 9, 15, 18, 27, 30, 39-44, 46-48",
      "file_path": "src/pytest_llm_report/util/time.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 16
    }
  ],
  "summary": {
    "coverage_total_percent": 93.04,
    "error": 0,
    "failed": 0,
    "passed": 623,
    "skipped": 0,
    "total": 623,
    "total_duration": 116.03446427899951,
    "xfailed": 0,
    "xpassed": 0
  },
  "tests": [
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007482199999913064,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "..."
        ],
        "scenario": "...",
        "token_usage": {
          "completion_tokens": 25,
          "prompt_tokens": 118,
          "total_tokens": 143
        },
        "why_needed": "..."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 185-186, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000767352000025312,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert provider._estimate_test_complexity returns 0 for an empty string', 'expected': 0, 'got': 0}",
          "{'name': 'assert provider._estimate_test_complexity returns 0 for None', 'expected': 0, 'got': 0}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 136,
          "total_tokens": 275
        },
        "why_needed": "The test is necessary because it checks the behavior of the `Config` class when given an empty source."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020817110000166394,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'complexity score', 'value': 'low'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 115,
          "total_tokens": 199
        },
        "why_needed": "The test is necessary to ensure that the complexity estimation algorithm can accurately handle simple tests with low complexity."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000716995000004772,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"The prompt tier should be one of [None, 'standard', 'advanced']\", 'expected_value': 'invalid'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestConfigValidation::test_invalid_prompt_tier",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 126,
          "total_tokens": 218
        },
        "why_needed": "To ensure that the prompt tier is valid and raises an error when it's invalid."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000713213999972595,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected error count for minimal tier', 'expected_value': 0, 'actual_value': 1}",
          "{'name': 'Expected error count for standard tier', 'expected_value': 0, 'actual_value': 1}",
          "{'name': 'Expected error count for auto tier', 'expected_value': 0, 'actual_value': 1}"
        ],
        "scenario": "Valid prompt tiers should pass validation",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 142,
          "total_tokens": 291
        },
        "why_needed": "To ensure that the `prompt_tier` field in the configuration is correctly validated and does not cause any errors."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_valid_prompt_tiers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-220, 222, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006935999999768683,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "..."
        ],
        "scenario": "...",
        "token_usage": {
          "completion_tokens": 25,
          "prompt_tokens": 122,
          "total_tokens": 147
        },
        "why_needed": "..."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007386009999663656,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'selected_prompt_type', 'expected_value': 'MINIMAL_SYSTEM_PROMPT'}",
          "{'name': 'minimal_system_prompt', 'expected_value': {'prompt_type': 'minimal', 'prompt_text': 'def test_simple(): assert 1 == 1'}}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 155,
          "total_tokens": 291
        },
        "why_needed": "To ensure that the auto-tier feature can use minimal prompts for simple tests, which reduces the overhead of prompt generation and improves test performance."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 212, 214-215, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000689211000008072,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config', 'value': 'Config override to minimal should always use minimal prompt.'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 122,
          "total_tokens": 214
        },
        "why_needed": "To ensure that the minimal prompt is always used when possible, even in cases where a more advanced tier is required."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 212, 214, 216-217, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006915600000070299,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': 'STANDARD_SYSTEM_PROMPT'}"
        ],
        "scenario": "Config override to standard should always use standard prompt.",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 148,
          "total_tokens": 225
        },
        "why_needed": "Because the config override is not necessary in this case, as the standard prompt is already used."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_standard_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 71,
          "line_ranges": "53, 56-57, 60, 62-64, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001935335999974086,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregated report should contain both retained tests.",
          "The number of tests in the aggregated report should match the expected number (2).",
          "Each retained test should have a unique ID.",
          "No duplicate tests should be included in the aggregated report.",
          "All test cases should be included in the aggregated report, even if they are not retained.",
          "The aggregate function should correctly handle all policy for multiple test cases.",
          "The aggregate function should not include any redundant or duplicate data."
        ],
        "scenario": "Verify that the aggregate function correctly handles all policy for multiple test cases.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 364,
          "total_tokens": 528
        },
        "why_needed": "This test prevents regression when using 'all' aggregation policy, which may cause duplicate tests to be included in the report."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 8,
          "line_ranges": "53, 56-58, 110, 113-115"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003471380999997109,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert aggregator is None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 104,
          "total_tokens": 181
        },
        "why_needed": "To test that the aggregate function correctly handles a directory that does not exist."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 79,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 204-205, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0034480649999864,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The outcome of the aggregate should match the outcome of the report with the latest timestamp.",
          "There should only be one test in the result.",
          "The outcome of the test should be 'passed'.",
          "The run meta should indicate that the aggregation was performed on multiple runs.",
          "The number of passed tests should equal the total number of runs.",
          "The summary should contain exactly one passed test."
        ],
        "scenario": "Test that the latest policy is selected when aggregating reports with different times.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 477,
          "total_tokens": 621
        },
        "why_needed": "This test prevents a regression where the latest policy might not be chosen for aggregated reports with different times."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 3,
          "line_ranges": "45, 53-54"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007813670000018647,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'agg is None after aggregate call', 'expected': 'None'}"
        ],
        "scenario": "tests/test_aggregation.py",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 110,
          "total_tokens": 177
        },
        "why_needed": "The test ensures that an aggregator can be created without a specified directory configuration."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "53, 56-58, 110, 113-114, 117-118, 184"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001215616999957092,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate()` method should return `None` when called with an aggregator that has no reports and no files to aggregate.",
          "The `aggregate()` method should not throw any errors or raise exceptions in this scenario.",
          "The `aggregate()` method should behave as expected without throwing any errors or raising exceptions due to missing reports or empty file paths."
        ],
        "scenario": "Test that `aggregate` returns `None` when no reports exist and there are no files to aggregate.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 201,
          "total_tokens": 345
        },
        "why_needed": "Prevents a potential bug where the function `aggregate` throws an error or raises an exception due to missing reports or empty file paths."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 87,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138-141, 145-147, 149-150, 152-153, 155, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 40,
          "line_ranges": "42-45, 65-68, 130-133, 135-137, 139, 141-143, 190, 194-199, 201, 203, 205, 207, 210-214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022645330000159447,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "coverage: Verify coverage was properly deserialized from JSON.",
          "llm_annotation: Verify LLM annotation was properly deserialized from JSON.",
          "token_usage: Verify token usage serialization is correct (this was the CI bug fix)."
        ],
        "scenario": "Test that coverage and LLM annotations are properly deserialized and can be re-serialized.",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 1002,
          "total_tokens": 1100
        },
        "why_needed": "Prevents regression in core functionality"
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 67,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 162-169, 171-173, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001627320000011423,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'source_coverage' key in the aggregated report contains a list of SourceCoverageEntry objects.",
          "Each SourceCoverageEntry object has the required attributes: 'file_path', 'statements', 'missed', 'covered', 'coverage_percent', 'covered_ranges', and 'missed_ranges'.",
          "The 'file_path' attribute is set to the correct file path in the report.",
          "The 'statements', 'missed', 'covered', 'coverage_percent', 'covered_ranges', and 'missed_ranges' attributes are correctly populated with values from the report.",
          "The 'coverage_percent' value is a valid percentage between 0 and 100.",
          "The 'covered_ranges' attribute contains ranges in the correct format (e.g., '1-5, 7-11').",
          "The 'missed_ranges' attribute also contains ranges in the correct format (e.g., '6, 12')."
        ],
        "scenario": "Source coverage summary should be deserialized.",
        "token_usage": {
          "completion_tokens": 243,
          "prompt_tokens": 395,
          "total_tokens": 638
        },
        "why_needed": "This test prevents a bug where the source coverage is not correctly deserialized from the aggregated report."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 19,
          "line_ranges": "259-260, 262-263, 265, 267-271, 273, 276-277, 279-280, 283, 285-286, 288"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003088868000020284,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_load_coverage_from_source()` method should return `None` when `llm_coverage_source` is `None`.",
          "The `_load_coverage_from_source()` method should raise a `UserWarning` when trying to load coverage data from an invalid source file.",
          "The `_load_coverage_from_source()` method should mock the `coverage.Coverage` class and its methods to return the expected values.",
          "The `_load_coverage_from_source()` method should verify that it calls the correct functions with the expected arguments.",
          "The `_load_coverage_from_source()` method should verify that the coverage data is correctly loaded into the aggregator's internal state.",
          "The `_load_coverage_from_source()` method should return a valid `SourceCoverageEntry` object when successful loading occurs.",
          "The `SourceCoverageEntry` class should be mocked to return the correct values for its attributes."
        ],
        "scenario": "Test loading coverage from configured source file when option is not set.",
        "token_usage": {
          "completion_tokens": 242,
          "prompt_tokens": 584,
          "total_tokens": 826
        },
        "why_needed": "This test prevents a bug where the aggregator fails to load coverage data when the `llm_coverage_source` option is not provided."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 17,
          "line_ranges": "231, 233-247, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007677010000293194,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of passed, failed, and skipped tests should match the actual counts in the latest summary.",
          "The number of xfailed and xpassed tests should be equal to their actual counts in the latest summary.",
          "The error status should remain unchanged even after adding new tests or removing skipped tests.",
          "The coverage percentage should be preserved from the latest summary.",
          "The total duration of all tests should increase by 5 seconds (1.0 + 4.0 = 5.0) to reflect the updated count in the latest summary."
        ],
        "scenario": "Test that the recalculate_summary function correctly updates the latest summary with test results.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 473,
          "total_tokens": 654
        },
        "why_needed": "This test prevents regression where the recalculate_summary function fails to update the latest summary even after adding new tests or removing skipped tests."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_recalculate_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 72,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123-124, 129, 131-132, 162-167, 169, 171-173, 176, 178-180, 182, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0028202380000266203,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` function should skip the 'invalid.json' file and only count the 'valid.json' file in the aggregation result.",
          "The `aggregate` function should raise a warning when it encounters an invalid JSON file, indicating that it's being skipped.",
          "The `aggregate` function should not include any reports from files with missing fields (e.g., 'missing_fields.json') in its final count of valid reports.",
          "The test should be able to reproduce the issue by creating a temporary directory and writing both an invalid JSON file and a valid JSON file inside it.",
          "The `aggregate` function should correctly handle cases where the input is not a valid JSON report, without raising any errors or unexpected behavior."
        ],
        "scenario": "Test that skipping an invalid JSON file prevents the aggregation from counting it as a valid report.",
        "token_usage": {
          "completion_tokens": 212,
          "prompt_tokens": 352,
          "total_tokens": 564
        },
        "why_needed": "This test verifies that the `aggregate` function correctly handles cases where the input is not a valid JSON report."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_skips_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "45, 231, 233-239, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007705969999847184,
      "file_path": "tests/test_aggregation_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total duration of all passed tests should be equal to the latest summary's total duration.",
          "The number of passed tests should match the latest summary's passed count.",
          "The total duration of failed tests should remain unchanged.",
          "The coverage total percent should still reflect the latest summary's coverage.",
          "The total duration of all tests should increase by 1 second (3.0 seconds) compared to the previous test result."
        ],
        "scenario": "The test verifies that the aggregator correctly recalculates the summary when new tests are added and the latest summary is provided.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 299,
          "total_tokens": 456
        },
        "why_needed": "This test prevents regression in coverage calculation when new tests with different durations are added to the aggregation process."
      },
      "nodeid": "tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-91, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001606749000018226,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_type': 'MockProvider'}",
          "{'name': 'mock_cache', 'expected_type': 'MockCache'}",
          "{'name': 'mock_assembler', 'expected_type': 'MockAssembler'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 112,
          "total_tokens": 228
        },
        "why_needed": "To test the batch optimization message generation and verification."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 50,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-128, 130, 134, 156, 181-182, 184, 211, 213-219, 221, 223"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009902199999487493,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_cache.get_cached_progress_reporting_result().should_return_json', 'description': 'Mocked cache should return the expected JSON result when the annotator has completed the task.'}",
          "{'name': 'mock_provider.get_progress_reporting_status().should_return_success', 'description': 'Mocked provider should return a successful status when the annotator completes the task.'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 101,
          "total_tokens": 248
        },
        "why_needed": "To ensure that the progress reporting is cached correctly and not lost in case of a provider or assembler failure."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 95,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-124, 130, 132, 134, 137-141, 144-151, 156, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016939529999717706,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 'Mocked provider is used', 'actual_value': 'Mocked provider is used'}",
          "{'name': 'mock_cache', 'expected_value': 'Mocked cache is used', 'actual_value': 'Mocked cache is used'}",
          "{'name': 'mock_assembler', 'expected_value': 'Mocked assembler is used', 'actual_value': 'Mocked assembler is used'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
        "token_usage": {
          "completion_tokens": 165,
          "prompt_tokens": 102,
          "total_tokens": 267
        },
        "why_needed": "To ensure that cached tests are skipped correctly and not executed unnecessarily."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 90,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221, 223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003091353000002073,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider should not raise an exception when called concurrently', 'expected_result': 'None'}",
          "{'name': 'Mock cache should not raise an exception when accessed concurrently', 'expected_result': 'None'}",
          "{'name': 'Mock assembler should not raise an exception when called concurrently', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_annotator.py",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 98,
          "total_tokens": 226
        },
        "why_needed": "To ensure that annotators can be annotated concurrently without causing any issues."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221-223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0024838310000063757,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'pytest-capsys', 'expected_output': 'Capture the output of the annotation process and verify it contains an error message.', 'actual_output': 'Capture the output of the annotation process and verify it contains an error message.'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 116,
          "total_tokens": 227
        },
        "why_needed": "To test that concurrent annotation handles failures correctly."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0019190829999615744,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider should be called with a valid task ID', 'expected_result': 1, 'actual_result': 0}",
          "{'name': 'Mock cache should not raise an exception when getting a valid task ID', 'expected_result': 1, 'actual_result': 0}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 96,
          "total_tokens": 225
        },
        "why_needed": "To ensure that the annotator correctly reports progress during the annotation process."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014932089999888376,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider is called with correct arguments', 'expected_calls': [], 'asserted_calls': [], 'message': 'Expected mock provider to be called with correct arguments, but got unexpected ones.'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 101,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the annotator correctly displays progress messages during reporting."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 91,
          "line_ranges": "47, 50-51, 58-59, 65, 67-68, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016000299999632261,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'mocking', 'expected_mocking': ['mock_provider', 'mock_cache', 'mock_assembler']}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 104,
          "total_tokens": 199
        },
        "why_needed": "This test ensures that the annotator respects the opt-out and limit settings when performing annotations."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-257, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017054959999995845,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider.get_rate_limit() returns a valid rate limit value', 'expected_value': 10, 'actual_value': 5}",
          "{'name': 'mock_cache.get_result() does not raise an exception when the result is already cached', 'expected_exception': 'Cache miss', 'actual_exception': 'None'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 112,
          "total_tokens": 245
        },
        "why_needed": "Respects rate limit for annotating test data."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 12.001883538000016,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected_output', 'description': 'The expected output of the sequential annotation process.'}",
          "{'name': 'output_length', 'description': 'The length of the output string after sequential annotation.'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 98,
          "total_tokens": 205
        },
        "why_needed": "To ensure that sequential annotation works correctly and produces the expected output."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 24.002082914000027,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocking the provider', 'expected_output': 'Mocked provider object'}",
          "{'name': 'Mocking the cache', 'expected_output': 'Mocked cache object'}",
          "{'name': 'Mocking the assembler', 'expected_output': 'Mocked assembler object'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 105,
          "total_tokens": 240
        },
        "why_needed": "Error tracking for sequential annotation is needed to ensure that errors are properly reported and handled."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 2,
          "line_ranges": "47-48"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007390370000166513,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_type': 'bool', 'actual_type': 'str'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 108,
          "total_tokens": 187
        },
        "why_needed": "The test should be skipped when the LLM (Language Model) is not enabled."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "47, 50-54, 56"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008484020000310011,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 'MockProvider'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 101,
          "total_tokens": 180
        },
        "why_needed": "The annotator should skip the annotation process if the provider is unavailable."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 359-360"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007837879999783581,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'JSONDecodeError', 'expected_error_message': 'Failed to parse LLM response as JSON'}"
        ],
        "scenario": "Test Base Parse Response Malformed JSON After Extract",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 152,
          "total_tokens": 237
        },
        "why_needed": "To ensure that the `extract_json_from_response` function handles malformed JSON correctly and raises a meaningful error."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342-346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009069179999983135,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.scenario == \"123\"",
          "assert annotation.why_needed == \"['list']\"",
          "assert annotation.key_assertions == ['a']"
        ],
        "scenario": "Test that the `test_base_parse_response_non_string_fields` function handles non-string fields correctly, specifically verifying if it correctly identifies the expected key assertion.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 269,
          "total_tokens": 386
        },
        "why_needed": "This test prevents a potential bug where the function incorrectly assumes all fields are strings and fails to identify the correct list of keys."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007230229999777293,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider_type', 'expected_value': 'GeminiProvider'}",
          "{'name': 'provider_class', 'expected_value': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 104,
          "total_tokens": 215
        },
        "why_needed": "To ensure that the `get_gemini_provider` function returns an instance of `GeminiProvider` when a Gemini provider is requested."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001873470999953497,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected exception message', 'value': 'Unknown LLM provider: invalid'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 106,
          "total_tokens": 186
        },
        "why_needed": "To test that a ValueError is raised when an unknown LLM provider is specified."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007288870000365932,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider_type', 'expected': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 109,
          "total_tokens": 195
        },
        "why_needed": "To ensure the `get_litellm_provider` function returns a valid instance of `LiteLLMProvider`."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007674920000226848,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of NoopProvider', 'expected_result': 'NoopProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 104,
          "total_tokens": 184
        },
        "why_needed": "To test the functionality of getting a provider without any configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010442650000186404,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of OllamaProvider', 'expected_type': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 108,
          "total_tokens": 193
        },
        "why_needed": "To ensure the Ollama provider can be retrieved and used correctly."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 134-135, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007873470000276939,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method of the provider is called and its result is checked to be `True`.",
          "The `is_available()` method of the provider is called again with the same arguments and its result is checked to be `True`.",
          "The value of `provider.checks` after calling `_check_availability()` is checked to be 1."
        ],
        "scenario": "Test that the `is_available()` method returns a boolean indicating availability.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 280,
          "total_tokens": 417
        },
        "why_needed": "This test prevents a potential regression where the `is_available()` method does not return a boolean value when it should."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007069520000300145,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"provider.get_model_name() == 'test-model'\", 'expected_value': 'test-model'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestLlmProviderDefaults",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 114,
          "total_tokens": 195
        },
        "why_needed": "To ensure that the model name defaults to the configuration if no custom value is provided."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 155"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007122360000266781,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.get_rate_limits() should return None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_base_maximal.py",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 108,
          "total_tokens": 183
        },
        "why_needed": "To ensure that the LLM provider defaults to using rate limits when they are not explicitly set."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 174"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000721979999980249,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_local() is False', 'expected_value': 'False'}"
        ],
        "scenario": "tests/test_base_maximal.py",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 105,
          "total_tokens": 179
        },
        "why_needed": "To ensure that the LLM default settings are correctly set to false when using a non-local configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 35,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 192-194, 196-200, 203-206, 209-210, 213-214, 216-218, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007351670000161903,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'src/module.py' file is present in the prompt.",
          "The 'def helper()' function is included in the prompt.",
          "Context files should be added to the prompt for a given function."
        ],
        "scenario": "Verify that context files are included in the batch prompt for a given function.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 261,
          "total_tokens": 364
        },
        "why_needed": "This test prevents regression where context files are not added to the prompt, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_context_files_included",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "34, 39-40, 156-157, 160, 162, 164-168, 170-177, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007288269999889962,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Test Group: test.py::test_add[*]' assertion should be present in the prompt.",
          "The 'Parameterizations (2 variants)' assertion should be present in the prompt.",
          "The '[1+1=2]' assertion should be present in the prompt.",
          "The '[0+0=0]' assertion should be present in the prompt.",
          "The 'ONE annotation' assertion should be present in the prompt."
        ],
        "scenario": "Test the parametrized batch prompt functionality.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 330,
          "total_tokens": 475
        },
        "why_needed": "This test prevents regression by ensuring that all variants of a test are included in a parametrized batch prompt."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_parametrized_batch_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 15,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000724931000036122,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Test: test.py::test_foo should appear in the prompt.",
          "```python should appear in the prompt.",
          "source should be included in the prompt.",
          "Parameterizations should not appear in the prompt."
        ],
        "scenario": "The single_test_prompt test verifies that a normal batched request can be generated.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 269,
          "total_tokens": 376
        },
        "why_needed": "This test prevents a potential bug where the test prompt is not correctly formatted or does not include necessary information."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_single_test_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007124200000134806,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `source` variable is assigned a string containing test function code.",
          "Two calls to `_compute_source_hash(source)` return the same hash value.",
          "The length of the returned hash value is 32 bytes (as expected)."
        ],
        "scenario": "Verify that the same source code produces the same hash value.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 220,
          "total_tokens": 330
        },
        "why_needed": "Prevents a bug where different versions of the test function produce different hashes, potentially leading to inconsistent results or incorrect analysis."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006899350000253435,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'hash1 != hash2', 'description': 'The first computed source hash should be different from the second one'}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 127,
          "total_tokens": 219
        },
        "why_needed": "To ensure that different sources produce different hashes, which is a requirement for batching to work correctly."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006845020000127988,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _compute_source_hash() returns an empty string for an empty input', 'expected_result': '', 'actual_result': ''}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 94,
          "total_tokens": 184
        },
        "why_needed": "The current implementation of compute_source_hash() does not handle an empty source correctly."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007145040000295921,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.validate() returns an error', 'description': 'The `validate()` method of the config object should return a list of errors.'}",
          "{'name': \"any('batch_max_tests' in e for e in errors)\", 'description': \"At least one 'batch_max_tests' key-value pair should be present in the error messages.\"}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 126,
          "total_tokens": 278
        },
        "why_needed": "This test is needed because the `batch_max_tests` configuration option must be at least 1 to ensure that the batch size can be set correctly."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007153299999913543,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'context_line_padding', 'actual_value': -1}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 126,
          "total_tokens": 205
        },
        "why_needed": "Context line padding must be non-negative."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007432840000092256,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config validation errors', 'description': 'The `validate()` method should return a list of error messages for the given configuration.'}",
          "{'name': 'invalid context compression mode', 'description': 'The `context_compression` parameter should be an invalid value and raise an error.'}"
        ],
        "scenario": "TestConfigValidation test_invalid_context_compression",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 122,
          "total_tokens": 247
        },
        "why_needed": "To ensure that the `context_compression` parameter is validated correctly and raises an error when it's invalid."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007337700000107361,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"Context compression mode should be None or 'lines'\", 'expected_value': \"None|'lines'\"}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 133,
          "total_tokens": 200
        },
        "why_needed": "To ensure that valid context compression modes pass validation."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_valid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006772070000238273,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'striped base node id', 'expected': 'test.py::test', 'actual': '_get_base_nodeid('}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 109,
          "total_tokens": 199
        },
        "why_needed": "This test ensures that complex params are fully stripped from the base node id."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007085659999575,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'node_id assertion', 'expected_value': 'tests/test_foo.py::test_add', 'actual_value': '_get_base_nodeid('}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 133,
          "total_tokens": 234
        },
        "why_needed": "To ensure that the `parametrized_nodeid` function correctly strips parameters from node IDs."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007129869999857874,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'nodeid is unchanged', 'expected_value': 'tests/test_foo.py::test_bar', 'actual_value': '_get_base_nodeid('}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 123,
          "total_tokens": 226
        },
        "why_needed": "This test is needed because the `_get_base_nodeid` function does not handle nodeids without parameters correctly."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "53-54, 67-68, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000749692999988838,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of each batch should match the expected number of tests (2, 2, 1).",
          "Each batch should contain exactly two tests.",
          "A batch with only one test is not considered a valid batch.",
          "The total number of tests across all batches should be equal to the specified maximum size (3)."
        ],
        "scenario": "Large groups of tests should be split into batches with a maximum size.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 364,
          "total_tokens": 497
        },
        "why_needed": "This test prevents regression when the batch size is set to a value that would result in too many tests being processed at once."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batch_max_size_respected",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 6,
          "line_ranges": "92-93, 95, 97-99"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000734920000013517,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Number of batches should be equal to number of tests', 'expected_value': 2, 'actual_value': 1}"
        ],
        "scenario": "Test Group Tests For Batching",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 170,
          "total_tokens": 249
        },
        "why_needed": "This test is needed because the batching parameterized tests are disabled."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batching_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 27,
          "line_ranges": "34, 39-40, 53-54, 67, 70, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007456589999605967,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of batches is equal to 1.",
          "Each batch contains exactly 3 tests.",
          "Each batch is a parameterized test.",
          "The base node ID of each batch is 'test.py::test_add'.",
          "All tests in the first batch are parametrized."
        ],
        "scenario": "Test parametrized tests should be grouped together.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 346,
          "total_tokens": 470
        },
        "why_needed": "This test prevents regression due to the lack of grouping of parametrized tests, which can lead to unexpected behavior when running tests in parallel."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_parametrized_tests_grouped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007511069999850406,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Each test is assigned to a separate batch with no grouping.",
          "There are two batches in total, each containing one test.",
          "The first batch contains only one test ('test_a') and the second batch also contains only one test ('test_b')."
        ],
        "scenario": "Test 'test_single_tests_no_grouping' verifies that single tests are batched individually without grouping.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 278,
          "total_tokens": 396
        },
        "why_needed": "This test prevents regression where multiple tests are grouped together and their individual tests are not batched separately."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_single_tests_no_grouping",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007749150000222471,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'same source produces same hash', 'expected': 'hash_source(source)', 'actual': 'hash_source(source)'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_consistent_hash",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 107,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the cache is consistent across multiple tests."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006679749999989326,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'different hash', 'expected': 'different hash', 'actual': 'same hash'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 108,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the cache is working correctly when using different source code."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007084289999852444,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 16, 'actual_value': {'hash': '486d8f5e4b2ae6c99a9c43d3a1f29f44'}}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_hash_length",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 100,
          "total_tokens": 196
        },
        "why_needed": "To ensure the hash length is correct and consistent across different inputs."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_hash_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 26,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011286099999665566,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `clear` method correctly removes all cache entries.",
          "Ensure that after clearing the cache, no existing entries are still present.",
          "Test that the `get` method returns `None` for any non-existent keys.",
          "Verify that the cache size is updated correctly before and after clearing.",
          "Check if the test raises an error when trying to access a non-existent key."
        ],
        "scenario": "Test the `clear` method of LlmCache to ensure it removes all existing entries.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 283,
          "total_tokens": 433
        },
        "why_needed": "This test prevents a potential bug where the cache might not be cleared properly, leading to unexpected behavior or incorrect results in subsequent tests."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_clear",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 11,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008277680000219334,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cache should be empty when annotation has error', 'expected_value': [], 'actual_value': ['abc123']}"
        ],
        "scenario": "tests/test_cache.py",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 157,
          "total_tokens": 231
        },
        "why_needed": "To ensure that LLMCache does not cache errors in annotations."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 9,
          "line_ranges": "39-41, 53, 55-56, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010628169999904458,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'None', 'actual_value': 'result'}"
        ],
        "scenario": "tests/test_cache.py::TestLlmCache::test_get_missing",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 128,
          "total_tokens": 200
        },
        "why_needed": "To test that the get method returns None for missing entries in the cache."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_get_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 28,
          "line_ranges": "39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010809979999635289,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check that the annotation is stored correctly in the cache.",
          "Verify that the annotation's confidence value is preserved during retrieval.",
          "Ensure that the retrieved annotation matches the original annotation."
        ],
        "scenario": "Test that annotations can be set and retrieved from the cache.",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 286,
          "total_tokens": 379
        },
        "why_needed": "Prevents bypass attacks by ensuring that LLMCache stores and retrieves annotations in a consistent manner."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_set_and_get",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007054229999994277,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'nodeid', 'expected_value': 'test_bad.py'}",
          "{'name': 'message', 'expected_value': 'SyntaxError'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 124,
          "total_tokens": 218
        },
        "why_needed": "The test is checking if the collection errors have a correct structure."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007216449999987162,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert get_collection_errors is a list', 'expected_value': [], 'actual_value': 'get_collection_errors'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 114,
          "total_tokens": 208
        },
        "why_needed": "To ensure that the get_collection_errors method returns an empty list when the collection is initially empty."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007161169999676531,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'llm_context_override', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 136,
          "total_tokens": 202
        },
        "why_needed": "Default llm_context_override should be None."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007052849999809041,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'llm_opt_out is not equal to False', 'expected_value': False, 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 136,
          "total_tokens": 226
        },
        "why_needed": "The default value of llm_opt_out should be False."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000698435000003883,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.capture_failed_output', 'expected_value': True, 'actual_value': 'True'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 104,
          "total_tokens": 183
        },
        "why_needed": "The output capture feature is not disabled by default."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000702499999988504,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert capture_output_max_chars is equal to 4000', 'expected_value': 4000, 'message': 'The default value of `capture_output_max_chars` is 4000.'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 108,
          "total_tokens": 235
        },
        "why_needed": "The default value of `capture_output_max_chars` is 4000. This is necessary to ensure that the output does not exceed this limit, which could cause issues with downstream processing."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007152559999781261,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected failure', 'value': 'xfail'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 206,
          "total_tokens": 286
        },
        "why_needed": "To ensure that xfail failures are correctly recorded as xfailed in the test results."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 26,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006977620000157003,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_name': \"result.outcome == 'xpassed'\", 'expected_value': 'xpassed'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 205,
          "total_tokens": 286
        },
        "why_needed": "xfail passes should be recorded as xpassed."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007330949999868608,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `results` attribute of the `collector` object is set to an empty dictionary.",
          "The `collection_errors` attribute of the `collector` object is an empty list.",
          "The `collected_count` attribute of the `collector` object is set to 0."
        ],
        "scenario": "Test that the `TestCollector` class initializes correctly and returns default values for results, collection errors, and collected count.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 205,
          "total_tokens": 336
        },
        "why_needed": "This test prevents a potential bug where the collector does not initialize with empty results, leading to incorrect assertions in subsequent tests."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_create_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007404739999969934,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'nodeids are in expected order', 'expected': ['a_test.py::test_a', 'z_test.py::test_z'], 'actual': [False]}"
        ],
        "scenario": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 227,
          "total_tokens": 321
        },
        "why_needed": "To ensure that the results are sorted by nodeid."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007525789999931476,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collected_count` attribute should be set to 3 (number of collected items).",
          "The `deselected_count` attribute should be set to 1 (number of deselected items)."
        ],
        "scenario": "Verify that the `handle_collection_finish` method correctly tracks collected and deselected items.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 256,
          "total_tokens": 366
        },
        "why_needed": "This test prevents a potential bug where the count of collected items is not updated correctly after calling `handle_collection_finish`."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_handle_collection_finish",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014645319999999629,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result captured_stdout is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_collector_internals.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 211,
          "total_tokens": 306
        },
        "why_needed": "To ensure that the test does not capture output when `config.capture_failed_output=False` and the integration is via handle_runtest_logreport."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000851962000012918,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'collector._capture_output', 'expected_result': 'Some error'}"
        ],
        "scenario": "Tests for Collector Internals",
        "token_usage": {
          "completion_tokens": 62,
          "prompt_tokens": 157,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the collector correctly captures stderr output."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008515830000419555,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'captured_stdout', 'expected_value': 'Some output'}"
        ],
        "scenario": "TestCollectorInternals::test_capture_output_stdout",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 157,
          "total_tokens": 221
        },
        "why_needed": "To test that the collector captures stdout correctly."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000980757000036192,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 174,
          "total_tokens": 258
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 35,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002159350999988874,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "item.callspec.id = 'param1' is called and its value is set to 'param1'.",
          "get_closest_marker('llm_opt_out') returns a mock object with the expected behavior.",
          "get_closest_marker('llm_context') returns a mock object with the correct arguments.",
          "get_closest_marker('requirement') returns a mock object with the expected requirements.",
          "item.get_closest_marker('llm_opt_out') is called and its value is set to True.",
          "item.get_closest_marker('llm_context_override') is called and its value is set to 'complete'.",
          "item.get_closest_marker('requirement') returns a mock object with the expected requirements.",
          "result.param_id matches the expected value of 'param1'."
        ],
        "scenario": "Test creates a result with item markers.",
        "token_usage": {
          "completion_tokens": 213,
          "prompt_tokens": 382,
          "total_tokens": 595
        },
        "why_needed": "Prevents regression in case of item markers being used as requirements or context."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013860079999972186,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'Crash report', 'actual_value': 'Crash report'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 165,
          "total_tokens": 243
        },
        "why_needed": "To handle ReprFileLocation causing crash report"
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009648679999827436,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert extract error is correct', 'description': 'The extracted error string should match the expected value'}"
        ],
        "scenario": "tests/test_collector_internals.py::TestCollectorInternals::test_extract_error_string",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 130,
          "total_tokens": 223
        },
        "why_needed": "To ensure the `extract_error` method returns a string that can be used to reconstruct the original error message."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008512059999929988,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _extract_skip_reason returns None for None longrepr', 'expected_value': 'None'}"
        ],
        "scenario": "test_collector_maximal",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 130,
          "total_tokens": 206
        },
        "why_needed": "To ensure the `_extract_skip_reason` method returns None when no longrepr is provided."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008686250000096152,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert _extract_skip_reason returns 'Just skipped'\", 'expected_value': 'Just skipped'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 133,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the `_extract_skip_reason` method returns a string as expected."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008846320000088781,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 164,
          "total_tokens": 256
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 21,
          "line_ranges": "58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008410429999798907,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collection_errors` list should contain exactly one item.",
          "The first item in the `collection_errors` list should have a 'nodeid' of 'test_broken.py'.",
          "The first item in the `collection_errors` list should have a 'message' that matches 'SyntaxError'."
        ],
        "scenario": "When the `handle_collection_report` method is called with a collection report that fails, it should record this failure.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 273,
          "total_tokens": 406
        },
        "why_needed": "This test prevents a potential bug where a collection report fails and does not trigger any error messages or warnings."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 42,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140-141, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238, 261, 264-265, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021462160000282893,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'rerun' attribute of the report should be set to 1 after handling a runtest_rerun event.",
          "The final outcome of the runtest_rerun event should be updated to 'failed'.",
          "The 'rerun_count' and 'final_outcome' attributes of the results dictionary should contain the expected values."
        ],
        "scenario": "Test the TestCollector's handle_runtest_rerun method to ensure it correctly handles reruns and updates the report accordingly.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 281,
          "total_tokens": 424
        },
        "why_needed": "This test prevents a potential regression where the TestCollector does not update the report when a rerun is requested."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008924820000402178,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.outcome == 'error'",
          "res.phase == 'setup'",
          "res.error_message == 'Setup failed'"
        ],
        "scenario": "When the `handle_runtest_setup_failure` test is executed, it should record an error setup in the report.",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 300,
          "total_tokens": 398
        },
        "why_needed": "This test prevents a potential regression where the collector might not correctly handle setup failures and instead incorrectly reports a success or no action."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 38,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012015060000294397,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert res.outcome == 'error'",
          "assert res.phase == 'teardown'",
          "assert res.error_message == 'Cleanup failed'"
        ],
        "scenario": "Test should record error if teardown fails after pass.",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 391,
          "total_tokens": 476
        },
        "why_needed": "To prevent regression in case of teardown failure, where the collector logs an error instead of a success result."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007123789999923247,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"context_compression must be one of 'none', 'gzip', or 'lz4'\", 'expected': ['context_compression', 'invalid']}"
        ],
        "scenario": "Test invalid context compression mode",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 124,
          "total_tokens": 210
        },
        "why_needed": "To ensure that the context compression mode is valid and does not cause any issues during testing."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_invalid_compression_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007075990000089405,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding is not a valid value for this configuration provider.', 'expected_value': -1}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 121,
          "total_tokens": 196
        },
        "why_needed": "Negative padding should fail validation."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007494280000059916,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Context compression should be None or 'lines'\", 'expected_value': 'None'}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 135,
          "total_tokens": 204
        },
        "why_needed": "To ensure that valid compression modes are correctly validated and do not raise any errors."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_valid_compression_modes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007169790000034482,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.validate() returns no errors', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 122,
          "total_tokens": 193
        },
        "why_needed": "Zero padding is a valid configuration option."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007792189999804577,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_compression', 'value': 'lines'}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 119,
          "total_tokens": 190
        },
        "why_needed": "The context compression should be enabled by default."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006939250000073116,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_compression', 'expected_value': 'lines'}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_compression_mode_lines",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 113,
          "total_tokens": 192
        },
        "why_needed": "Lines compression mode is needed to ensure that the test suite can run without any issues."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_mode_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007111120000331539,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_line_padding', 'expected_value': 2, 'actual_value': 0}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 106,
          "total_tokens": 200
        },
        "why_needed": "To ensure that line padding is set to a default value of 2, which is the expected behavior according to the documentation."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0007511910000062016,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The count of '#' characters should be zero for contiguous lines.",
          "# L3:",
          "# L4:",
          "# L5:"
        ],
        "scenario": "Test that contiguous covered lines do not have gap indicators.",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 293,
          "total_tokens": 370
        },
        "why_needed": "Prevents regression where contiguous lines are marked with a gap indicator."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_contiguous_lines_no_gap",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 216-217"
        }
      ],
      "duration": 0.0007050579999940965,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 130,
          "total_tokens": 200
        },
        "why_needed": "Empty coverage should return empty string."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 24,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249"
        }
      ],
      "duration": 0.0007734719999916706,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result contains the expected gap indicator between the two covered lines (# ...).",
          "The result contains the expected range indicator for line L3: # L3:...",
          "The result contains the expected range indicator for line L15: # L15:..."
        ],
        "scenario": "Test Extract Covered Lines: Multiple covered ranges should be extracted with gap indicators.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 274,
          "total_tokens": 385
        },
        "why_needed": "This test prevents a regression where multiple covered lines are not correctly identified with gap indicators."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_multiple_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.000776765000011892,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result should contain '# L2:' and '# L3:' and '# L4:'",
          "The result should include lines 2, 3, and 4 with the correct number of lines padded (1)",
          "The line numbers in the result should match the original covered lines"
        ],
        "scenario": "The test verifies that a single covered line is extracted with the correct number of lines padded.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 302,
          "total_tokens": 422
        },
        "why_needed": "This test prevents a regression where a single line is not extracted due to insufficient padding."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0007431630000382938,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert '# L1:' in result",
          "assert '# L2:' in result",
          "assert '# L3:' in result"
        ],
        "scenario": "Test Extract Covered Lines: Padding should not go beyond file boundaries.",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 288,
          "total_tokens": 374
        },
        "why_needed": "This test prevents a potential bug where padding exceeds the file boundary, potentially causing incorrect results or errors."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_padding_boundary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008500789999743574,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 158,
          "total_tokens": 240
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_context_limits.py::test_no_truncation_needed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 25,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016257039999914014,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "F1 should be full with ~536 chars (134 tokens).",
          "F2 got the extra budget of ~400 chars (480 tokens) instead of ~100 tokens (110 tokens).",
          "F2's content is truncated to ~800 chars (720 tokens) instead of ~440 tokens (110 tokens)."
        ],
        "scenario": "tests/test_context_limits.py::test_smart_distribution verifies that the smart distribution algorithm does not waste tokens when F1 and F2 have different needs.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 773,
          "total_tokens": 918
        },
        "why_needed": "This test prevents regression in the smart distribution logic, where F1 gets more budget than F2 if their needs are unequal."
      },
      "nodeid": "tests/test_context_limits.py::test_smart_distribution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000894923000032577,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "f1 contains 'truncated' in its prompt",
          "f2 contains 'truncated' in its prompt",
          "prompt includes 'Present'",
          "prompt is within budget (~200 tokens total)",
          "prompt has overhead small (<80 tokens per file)"
        ],
        "scenario": "The test verifies that the splitting logic correctly truncates strings and meets budget requirements.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 317,
          "total_tokens": 435
        },
        "why_needed": "This test prevents a potential bug where the splitting logic does not truncate strings, leading to excessive output or incorrect results."
      },
      "nodeid": "tests/test_context_limits.py::test_splitting_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "243, 245, 264, 266, 270-272, 274-275"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009514139999851068,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The prompt should be truncated to fit within 100 tokens minus system prompt overhead (~40-50 tokens) minus header overhead (~20 tokens)",
          "Context should be very small or empty",
          "Prompt contains '[... truncated]' or 'Relevant context' indicating truncation"
        ],
        "scenario": "Test truncation logic when creating large context files.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 397,
          "total_tokens": 513
        },
        "why_needed": "Prevents a potential bug where the test passes even though the context is too long, causing unnecessary computation and potentially incorrect results."
      },
      "nodeid": "tests/test_context_limits.py::test_truncation_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006857010000089758,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The result of collapsing three+ empty lines is 2.', 'expected_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 128,
          "total_tokens": 218
        },
        "why_needed": "Because the current implementation does not handle three or more empty lines correctly."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006896009999763919,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'line1\\n\\nline2', 'actual_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 127,
          "total_tokens": 210
        },
        "why_needed": "To test the functionality of collapsing many empty lines to one blank line."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007047469999861278,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'line1\\n\\nline2', 'actual_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 125,
          "total_tokens": 206
        },
        "why_needed": "Preserves up to 2 consecutive newlines."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000719023999977253,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': 'line1\\nline2\\nline3'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 121,
          "total_tokens": 195
        },
        "why_needed": "Preserve single newlines in collapsed context."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007025319999911517,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'line1\\nline2', 'actual_value': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 137,
          "total_tokens": 227
        },
        "why_needed": "The test is necessary because the current implementation does not correctly handle empty lines in the input source."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 45,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009324490000039987,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected Optimizations to be applied', 'value': 'All optimizations should be applied'}",
          "{'name': 'Optimization Order', 'value': 'The combined optimization process should apply all optimizations in the expected order.'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 96,
          "total_tokens": 211
        },
        "why_needed": "To ensure that the combined optimization process works as expected and does not introduce any unexpected behavior."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 36,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007988669999576814,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring stripping', 'expected': '', 'actual': 'def foo():'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 100,
          "total_tokens": 207
        },
        "why_needed": "The default behavior of `optimize_context` is to strip all code, including docstrings and comments. However, this can be problematic when working with complex codebases where docstrings provide important context."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007138729999951465,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '', 'actual': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 95,
          "total_tokens": 166
        },
        "why_needed": "The function `optimize_context()` should be able to handle an empty source without raising an exception."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007131039999990207,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the optimize_context function should be a string containing only whitespace characters.",
          "The input to the optimize_context function should be a string containing only whitespace characters."
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 115,
          "total_tokens": 210
        },
        "why_needed": "This test is necessary because the current implementation of optimize_context does not handle source code with only whitespace correctly."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 44,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008743000000208667,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'source: def foo():\\n# ...', 'expected': 'def foo():\\n# ...'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 95,
          "total_tokens": 175
        },
        "why_needed": "To optimize the context by removing unnecessary documentation."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 14,
          "line_ranges": "81-82, 86, 88-90, 93, 108, 124, 126, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008204870000554365,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'string', 'expected_value': 'def foo():\\n  # This is a comment\\n  pass', 'actual_value': {'scenario': 'tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only', 'why_needed': 'To optimize the context by removing unnecessary comments.', 'key_assertions': ['def foo():\\n  # This is a comment\\n  pass']}, 'reason': \"The docstring 'This is a comment' should be removed.\"}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 95,
          "total_tokens": 256
        },
        "why_needed": "To optimize the context by removing unnecessary comments."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006885299999908057,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'source_code', 'expected': 'def foo():\\n  pass', 'actual': 'def foo():\\n  # This line will be stripped by the optimizer'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 94,
          "total_tokens": 191
        },
        "why_needed": "The test is failing because the optimizer strips out unnecessary code."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007556979999776559,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': '\\'url = \"http://example.com#anchor\"\\'', 'actual_result': 'url = \"http://example.com#anchor\",'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 134,
          "total_tokens": 234
        },
        "why_needed": "To ensure that comments are stripped from strings containing hash symbols (#) after they appear within string literals."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007178509999903326,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should return the original string without any escaped quotes.', 'expected_result': 's = \"escaped \\\\'}",
          "actual_result\": {u's': u'\\u201c'}  # The actual result is a Unicode escape sequence for a double quote character, not just a literal double quote."
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 133,
          "total_tokens": 253
        },
        "why_needed": "To handle escaped quotes in strings correctly."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008052229999861993,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '\"don\\'t # worry\"', 'actual': '\"don\\'t\\\\# worry\"'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 101,
          "total_tokens": 178
        },
        "why_needed": "To strip quotes from a string containing both single and double quotes."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007616940000048089,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'source_code', 'expected_result': 'def foo():\\n  # This is a comment\\n    pass'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_no_comments",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 91,
          "total_tokens": 175
        },
        "why_needed": "To strip comments from the source code to ensure correct context."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_no_comments",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007412150000050133,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': '\\'url = \"http://example.com#anchor\"\\'', 'actual_result': '\\'url = \"http://example.com#anchor\"\\''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 135,
          "total_tokens": 234
        },
        "why_needed": "To preserve '#' inside double-quoted strings in the context of strip_comments() function."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007480090000058226,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '# is preserved in single-quoted string', 'actual': \"url = 'http://example.com#anchor'\"}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 135,
          "total_tokens": 225
        },
        "why_needed": "To preserve the hash (#) inside single-quoted strings when stripping comments."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007362750000083906,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'x = 1', 'actual_result': 'x = 1'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 119,
          "total_tokens": 195
        },
        "why_needed": "To remove simple end-of-line comments from the source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000763183999993089,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'line': 1, 'column': 4}, 'actual': {'line': 2, 'column': 0}}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 99,
          "total_tokens": 184
        },
        "why_needed": "To strip standalone comments from the test source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 4,
          "line_ranges": "27, 29-31"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007588240000018232,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'strip_docstrings() returns the original source on syntax error.', 'expected_result': 'def foo( unclosed paren', 'actual_result': 'def foo( unclosed paren'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 119,
          "total_tokens": 226
        },
        "why_needed": "The test is checking if the function correctly handles syntax errors in the input source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 30,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008311170000183665,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function should remove all docstrings from the source code.', 'expected_output': 'No docstring remains in the source code.'}",
          "{'assertion': 'The function should return an empty string if there are no docstrings to strip.', 'expected_output': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 95,
          "total_tokens": 226
        },
        "why_needed": "This test is needed because the current implementation of context_util.strip_docstrings() does not handle multiple docstrings correctly."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007794240000293939,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring triple quotes are preserved', 'expected_value': 'def foo():\\n    \"\"\"\\n    This is a triple quoted string.\\n\"\"\"', 'actual_value': 'def foo():\\n    This is a triple quoted string.', 'message': 'Expected docstring triple quotes to be preserved, but were not.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 103,
          "total_tokens": 236
        },
        "why_needed": "Preserve multiline data strings in docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 25,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007763540000382818,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'strip_docstrings', 'expected_value': 'x = \"hello world\"'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 102,
          "total_tokens": 176
        },
        "why_needed": "Preserve regular strings in test output."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 27,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007860759999971378,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The string 'string 1' is preserved in the output.\", 'expected_output': '\\'\"string 1\"\\'', 'actual_output': \"'''string 1'''\"}",
          "{'assertion': \"The string '\"}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 146,
          "total_tokens": 263
        },
        "why_needed": "Preserving strings in structures is a crucial aspect of the context utility."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009249280000176441,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring removal', 'expected': 'The function should remove all docstrings, including those with multiple lines.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 97,
          "total_tokens": 192
        },
        "why_needed": "This test ensures that the `strip_multiline_docstring` function correctly removes multiline docstrings from Python code."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00090076300000419,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring removal', 'expected': 'def foo():'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 106,
          "total_tokens": 188
        },
        "why_needed": "To ensure that context managers are properly stripped of triple double-quoted docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000781275999997888,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'striped source code contains a triple single-quoted docstring', 'expected_result': 'source = \"\"\"\\ndef foo():\\n>>> foo()\"\"\\\\'}",
          "actual_result"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 106,
          "total_tokens": 257
        },
        "why_needed": "The test is necessary because the current implementation does not correctly strip triple single-quoted docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 19,
          "line_ranges": "134-135, 137-141, 143-144, 476, 478, 524-531"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007666209999683815,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty list when the `model` parameter is None.",
          "The function should return an empty list when the `model` parameter is set to 'All'.",
          "The function should not return any models when the `model` parameter is an invalid string (e.g., 'abc')."
        ],
        "scenario": "Test the GeminiProvider's _parse_preferred_models method with edge cases.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 273,
          "total_tokens": 399
        },
        "why_needed": "This test prevents a potential bug where the method returns incorrect results when given an empty or invalid model configuration."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 35,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007862239999667509,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `next_available_in` method should return an error when there are more tokens than available.",
          "The `next_available_in` method should return 0 when both limits have been exceeded.",
          "The rate limiter should not attempt to process requests beyond the available token limit."
        ],
        "scenario": "Verify that the rate limiter correctly handles edge cases where there are more tokens than available.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 273,
          "total_tokens": 400
        },
        "why_needed": "This test prevents a potential bug where the rate limiter fails to handle situations with excessive token usage, leading to incorrect behavior or errors."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 47,
          "line_ranges": "96-103, 130-133, 135, 137-139, 141, 143, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007354709999844999,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'coverage_percent' key of the dictionary returned by `to_dict()` contains the correct value.",
          "The 'error' key of the dictionary returned by `to_dict()` contains the expected error message.",
          "The 'duration' key of the dictionary returned by `to_dict()` contains the correct duration value.",
          "The 'start_time', 'end_time', and 'duration' keys are present in the dictionary with their respective values.",
          "The coverage percentage is calculated correctly based on the number of statements, covered, missed, and coverage percent.",
          "The LLM annotation error message is correctly extracted from the annotation object."
        ],
        "scenario": "Test that the `models_to_dict` method returns accurate coverage percentages for SourceCoverageEntry objects.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 318,
          "total_tokens": 516
        },
        "why_needed": "This test prevents regression in the `models_to_dict` method by ensuring it accurately calculates coverage percentages for SourceCoverageEntry objects."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007465109999884589,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mapper is an instance of CoverageMapper', 'expected_type': 'CoverageMapper', 'actual_type': 'type'}",
          "{'name': 'config is initialized correctly', 'expected_value': 'Config', 'actual_value': 'type'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 109,
          "total_tokens": 227
        },
        "why_needed": "Mapper initialization should be tested to ensure it creates a new instance with the correct configuration."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 3,
          "line_ranges": "44-45, 308"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007331840000119882,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert isinstance(warnings, list)', 'expected_result': 'list'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 110,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the `get_warnings` method returns a list of warnings as expected."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014064950000260978,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `map_coverage()` method should return an empty dictionary when no coverage file exists.",
          "The `map_coverage()` method should not have any warnings when no coverage file is present.",
          "The `map_coverage()` method should correctly handle the absence of a coverage file by returning an empty dictionary."
        ],
        "scenario": "Test that the `map_coverage` method returns an empty dictionary when no coverage file is present.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 277,
          "total_tokens": 396
        },
        "why_needed": "Prevents a potential bug where the test fails due to missing coverage data."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007386690000430463,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_extract_nodeid` of the `CoverageMapper` class correctly extracts node IDs from the given path.",
          "The function `_extract_nodeid` of the `CoverageMapper` class returns the expected node ID for each phase.",
          "The function `_extract_nodeid` of the `CoverageMapper` class handles cases where the input path is empty or contains only one phase.",
          "The function `_extract_nodeid` of the `CoverageMapper` class correctly handles cases where the input path starts with a phase name (e.g., 'test.py::test_foo|')",
          "The function `_extract_nodeid` of the `CoverageMapper` class does not modify any node IDs.",
          "The function `_extract_nodeid` of the `CoverageMapper` class raises an AssertionError if the input path is invalid or malformed."
        ],
        "scenario": "The test verifies that the `CoverageMapper` extracts all phases when the `include_phase` parameter is set to 'all'.",
        "token_usage": {
          "completion_tokens": 243,
          "prompt_tokens": 279,
          "total_tokens": 522
        },
        "why_needed": "This test prevents a regression where the coverage map does not include all phases when `include_phase=all`."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000725635999970109,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'None', 'actual_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 128,
          "total_tokens": 211
        },
        "why_needed": "To handle cases where the context is empty or None, and return None as per the expected result."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 216, 220, 224-225, 228-230"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007097029999840743,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'nodeid is None', 'expected_value': '', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 139,
          "total_tokens": 221
        },
        "why_needed": "To filter out setup phase when include_phase=run."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007339759999922535,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'node ID extraction', 'expected_value': 'test.py::test_foo', 'actual_value': 'test.py::test_foo'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 145,
          "total_tokens": 244
        },
        "why_needed": "To extract the correct node ID from run phase context in coverage reports."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 29,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152, 156, 160-162, 167-170, 199, 202"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0011746739999694,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_data.contexts_by_lineno.side_effect is set to contexts_side_effect with the correct side effect.",
          "call_count[0] is incremented correctly before raising the exception.",
          "the exception is not raised within the first call to contexts_by_lineno",
          "the exception raises an Exception object with the correct message and context number",
          "the exception does not cause any other tests to fail"
        ],
        "scenario": "Test the coverage mapper's behavior when encountering an exception while extracting contexts by line number.",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 332,
          "total_tokens": 473
        },
        "why_needed": "Prevents a regression where the test fails due to an unexpected exception being raised during context extraction."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_contexts_by_lineno_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 7,
          "line_ranges": "44-45, 118, 121-122, 127-128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010289580000062415,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is an empty dictionary', 'expected_result': '{}'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 136,
          "total_tokens": 211
        },
        "why_needed": "To test the case where coverage data has no measured files."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012670319999870117,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"mocked mock_data.measured_files.return_value is equal to ['file.txt', 'data.json']\", 'expected_result': ['file.txt', 'data.json']}",
          "{'assertion': 'mocked mock_data.contexts_by_lineno.return_value is empty', 'expected_result': {}}"
        ],
        "scenario": "Test Extract Contexts",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 154,
          "total_tokens": 265
        },
        "why_needed": "To skip non-Python files from coverage report."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007163699999637174,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'coverage.py is imported correctly', 'expected_result': 'coverage.py should be imported from the Python path.'}",
          "{'name': 'CoverageMapper is created successfully', 'expected_result': 'CoverageMapper should be created with a Config object.'}"
        ],
        "scenario": "TestLoadCoverageData",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 166,
          "total_tokens": 270
        },
        "why_needed": "To test that coverage.py is installed and properly loaded into the environment."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_coverage_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010292750000076012,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The result of _load_coverage_data() is not None.', 'expected_value': 'None'}",
          "{'description': 'Any warnings are present in the mapper.warnings list.', 'expected_value': ['W001']}"
        ],
        "scenario": "TestLoadCoverageData",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 153,
          "total_tokens": 253
        },
        "why_needed": "To test that the function returns None when no .coverage file exists."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012884280000093895,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the `map_source_coverage` function returns an empty list of warnings.",
          "The test verifies that any warning messages contain 'COVERAGE_ANALYSIS_FAILED'.",
          "The test verifies that the `map_source_coverage` function sets the expected warnings to be empty."
        ],
        "scenario": "Test that analysis exception handling prevents adding an empty coverage report.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 286,
          "total_tokens": 394
        },
        "why_needed": "To prevent adding an empty coverage report when analysis2 raises an exception."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_analysis_exception_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 18,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259-261, 273-274, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012692709999555518,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_cov.get_data.return_value is empty list', 'expected': [], 'actual': []}",
          "{'name': 'mock_cov.analysis2.return_value is correct', 'expected': ['empty.py', [], [], [], []], 'actual': ['empty.py', [], [], [], []]}"
        ],
        "scenario": "TestMapSourceCoverage test_empty_statements",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 178,
          "total_tokens": 301
        },
        "why_needed": "To ensure that the function correctly handles a case where there are no statements in the file."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_empty_statements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.001608331999989332,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `covered` count for each file should be greater than or equal to 1.",
          "The `missed` count for each file should be less than or equal to 0.",
          "All test files (`/project/tests/test_foo.py`) should be included in the coverage map."
        ],
        "scenario": "Test that test files are included when omit_tests_from_coverage is False.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 322,
          "total_tokens": 444
        },
        "why_needed": "This test prevents a regression where the coverage map does not include all test files when omitting tests from coverage."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_include_test_files_when_not_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 10,
          "line_ranges": "44-45, 243-244, 246-249, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002159900000037851,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'mock_data.measured_files.return_value should be an empty list', 'expected_result': [], 'message': 'Expected mock_data.measured_files.return_value to be an empty list'}",
          "{'assertion': 'mock_data.contexts_by_lineno.return_value should be an empty dictionary', 'expected_result': {}, 'message': 'Expected mock_data.contexts_by_lineno.return_value to be an empty dictionary'}"
        ],
        "scenario": "Test Map Source Coverage",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 154,
          "total_tokens": 297
        },
        "why_needed": "This test is needed to ensure that non-Python files are skipped from coverage reports."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-255, 257, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012000170000305843,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Test case 1: Test file should be skipped', 'description': 'The test file should not be included in the coverage report.', 'expected_result': [], 'actual_result': []}",
          "{'name': 'Test case 2: Test file is included in the coverage report', 'description': 'The test file should be included in the coverage report.', 'expected_result': ['Expected result'], 'actual_result': ['Actual result']}"
        ],
        "scenario": "Test Map Source Coverage",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 182,
          "total_tokens": 331
        },
        "why_needed": "To ensure that test files are skipped when omit_tests_from_coverage is True."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_test_files_when_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007493319999980486,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mapper should return the same nodeid for any phase.",
          "The mapper should match the expected nodeids for each phase.",
          "No unexpected nodeids are returned for phases other than 'setup', 'run', and 'teardown'."
        ],
        "scenario": "Test that all phases are accepted when configured.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 305,
          "total_tokens": 402
        },
        "why_needed": "Prevents regression in coverage mapping for 'all' phase configuration."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_all_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007506139999691186,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'None', 'actual_value': 'assert mapper._extract_nodeid(\"\") is None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 115,
          "total_tokens": 201
        },
        "why_needed": "The test extracts node ID from an empty string, which should return None."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007205329999919741,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'None', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 114,
          "total_tokens": 180
        },
        "why_needed": "None input returns None."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007165239999835649,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_extract_nodeid` should return the node ID when the phase matches.",
          "The function `_extract_nodeid` should return None when the phase does not match.",
          "The function `_extract_nodeid` should not throw an error or raise an exception when the phase is setup or teardown."
        ],
        "scenario": "Test that run phase is the default filter.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 297,
          "total_tokens": 407
        },
        "why_needed": "Prevents regression where `run` phase is not included in coverage."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_run_phase_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007706380000058743,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_nodeid should return the nodeid of the specified module and phase.",
          "If the phase does not match 'setup', the function should return None for that phase.",
          "If the phase is neither 'setup' nor 'run' (or 'teardown'), the function should return None for those phases as well."
        ],
        "scenario": "Test that setup phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 293,
          "total_tokens": 419
        },
        "why_needed": "Prevents a potential bug where the test would incorrectly filter out nodes in the setup phase due to missing configuration."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_setup_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233-234, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007335829999988164,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') is None"
        ],
        "scenario": "Test that teardown phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 296,
          "total_tokens": 421
        },
        "why_needed": "This test prevents a potential bug where the teardown phase configuration does not filter out nodeids for certain phases."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_teardown_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 6,
          "line_ranges": "44-45, 216, 220, 224, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007163700000205608,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'test_foo.py::test_bar', 'actual': 'test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 136,
          "total_tokens": 229
        },
        "why_needed": "To ensure that the node id is extracted correctly when there are no phase delimiters in the import statement."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 57,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0015413730000091164,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'test_app.py::test_one' in result",
          "assert 'test_app.py::test_two' in result",
          "assert len(result['test_app.py::test_one']) == 1",
          "assert one_cov[0].line_count == 2",
          "# lines 1 and 2"
        ],
        "scenario": "Test that the test_extract_contexts_full_logic function exercises all paths in _extract_contexts.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 413,
          "total_tokens": 541
        },
        "why_needed": "This test prevents regression where a file's coverage is not fully included in the extracted contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012454379999553566,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 174,
          "total_tokens": 282
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007541589999959797,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_nodeid should return 'test.py::test' when called with a valid context (e.g., 'test.py::test|setup').",
          "The function _extract_nodeid should return None when called with an invalid context (e.g., 'test.py::test_no_phase')."
        ],
        "scenario": "Test the coverage mapper's ability to extract node IDs for different phases and contexts.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 323,
          "total_tokens": 445
        },
        "why_needed": "This test prevents regression by ensuring that the coverage mapper correctly identifies missing lines in the code."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009195810000051097,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `mapper._load_coverage_data()` should return `None` when there are no .coverage files.",
          "The function `mapper.warnings` should contain exactly one warning with code 'W001'.",
          "The current implementation of the test does not raise an exception when no coverage files exist, but instead returns None or other unexpected values."
        ],
        "scenario": "Test the case where no coverage files exist.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 276,
          "total_tokens": 411
        },
        "why_needed": "This test prevents a potential bug where the function does not raise an exception when no coverage files are found, but instead returns None or other unexpected values."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 17,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014639479999800642,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_load_coverage_data()` should raise an exception when it encounters a corrupted coverage file.",
          "Any warnings generated by the `CoverageMapper` should contain the message 'Failed to read coverage data'.",
          "The test should fail if any warnings are generated, indicating that the coverage data is corrupt."
        ],
        "scenario": "Test that the `CoverageMapper` raises an error when trying to load corrupted coverage data.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 343,
          "total_tokens": 480
        },
        "why_needed": "To prevent a regression where the test fails due to a corrupt coverage file being loaded, which would cause the `CoverageMapper` to incorrectly report no coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002462285999968117,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock instances of `CoverageData` should have been called with at least two times.",
          "The `update` method of the mock instances should be called.",
          "The number of calls to `update` for each mock instance should be greater than or equal to 1.",
          "Each mock instance's `update` method should be called before any other method is called on it.",
          "The `update` method should not be called multiple times with the same arguments if it was already called once.",
          "If a mock instance's `update` method is called, it should only be called once per test.",
          "If a mock instance's `update` method is called multiple times, it should raise an AssertionError."
        ],
        "scenario": "Test should handle parallel coverage files from xdist and verify that it correctly updates the CoverageData instances.",
        "token_usage": {
          "completion_tokens": 217,
          "prompt_tokens": 378,
          "total_tokens": 595
        },
        "why_needed": "This test prevents regression in handling parallel coverage files, which can lead to incorrect coverage data being reported if not properly updated."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 5,
          "line_ranges": "44-45, 58-60"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001068724999981896,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mapper should return an empty dictionary when _load_coverage_data returns None.",
          "_load_coverage_data() was called with argument None",
          "No coverage data is available for mapping",
          "mapper._load_coverage_data() did not return a non-None value",
          "mapper.map_coverage() should have been called with an empty dictionary"
        ],
        "scenario": "Test that the test_map_coverage_no_data function returns an empty dictionary when _load_coverage_data returns None.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 228,
          "total_tokens": 363
        },
        "why_needed": "Prevents a potential bug where the mapper does not handle the case of no coverage data being loaded."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013059960000418869,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_cov.analysis2.assert_called_once_with(mock_data)",
          "mock_cov.get_data.return_value.measured_files.return_value == ['app.py']",
          "entries == []"
        ],
        "scenario": "The test verifies that the `map_source_coverage` method skips files with errors during analysis.",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 274,
          "total_tokens": 372
        },
        "why_needed": "This test prevents a regression where an error in analysis results in incorrect skipping of source code files."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0015618499999732194,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function mapper.map_source_coverage() returns a list containing one entry with file_path='app.py', statements=3, covered=2, missed=1, and coverage_percent=66.67.",
          "The entries in the returned list should include all possible paths from map_source_coverage configuration.",
          "All files in the source directory should be included in the analysis.",
          "Statements should be counted correctly based on their actual values.",
          "Covered statements should match the number of statements in the file.",
          "Missed statements should be zero or less, not greater than the total number of statements.",
          "Coverage percentage should be between 0 and 100, inclusive."
        ],
        "scenario": "Test should exercise all paths in map_source_coverage to ensure comprehensive coverage of source files.",
        "token_usage": {
          "completion_tokens": 202,
          "prompt_tokens": 345,
          "total_tokens": 547
        },
        "why_needed": "This test prevents regression by ensuring that the CoverageMapper is correctly analyzing all possible paths from the map_source_coverage configuration."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006950759999995171,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned warning has the correct `code` attribute set to `WarningCode.W001_NO_COVERAGE`.",
          "The message of the warning contains the expected string 'No .coverage file found'.",
          "The detail of the warning matches the provided string 'test-detail'."
        ],
        "scenario": "Test the `make_warning` factory function to ensure it correctly returns a WarningCode.W001_NO_COVERAGE instance with the expected message and detail.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 236,
          "total_tokens": 375
        },
        "why_needed": "Prevents a potential bug where an invalid or unknown code is passed to the `make_warning` function, resulting in unexpected behavior or errors."
      },
      "nodeid": "tests/test_errors.py::test_make_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007695460000149978,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'assert WarningCode.W001_NO_COVERAGE.value == \"W001\"', 'description': \"Checks that the assertion correctly sets the warning code to 'W001'\"}",
          "{'message': 'assert WarningCode.W101_LLM_ENABLED.value == \"W101\"', 'description': \"Checks that the assertion correctly sets the warning code to 'W101'\"}",
          "{'message': 'assert WarningCode.W201_OUTPUT_PATH_INVALID.value == \"W201\"', 'description': \"Checks that the assertion correctly sets the warning code to 'W201'\"}",
          "{'message': 'assert WarningCode.W301_INVALID_CONFIG.value == \"W301\"', 'description': \"Checks that the assertion correctly sets the warning code to 'W301'\"}",
          "{'message': 'assert WarningCode.W401_AGGREGATE_DIR_MISSING.value == \"W401\"', 'description': \"Checks that the assertion correctly sets the warning code to 'W401'\"}"
        ],
        "scenario": "Test that warning codes have correct values.",
        "token_usage": {
          "completion_tokens": 256,
          "prompt_tokens": 240,
          "total_tokens": 496
        },
        "why_needed": "Prevents a potential bug where the warning code for W001_NO_COVERAGE is set to an incorrect value, potentially leading to unexpected behavior or errors in the application."
      },
      "nodeid": "tests/test_errors.py::test_warning_code_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006906220000360008,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return a dictionary with the correct warning code and detail.",
          "The function should return the same warning details even when no detail is provided.",
          "The function should handle WarningCode.W001_NO_COVERAGE correctly.",
          "The function should handle WarningCode.W101_LLM_ENABLED correctly.",
          "The function should not raise any errors for valid warnings with missing detail."
        ],
        "scenario": "Test ReportWarning.to_dict() method to ensure it returns the correct warning code and detail.",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 276,
          "total_tokens": 424
        },
        "why_needed": "This test prevents a potential bug where the 'to_dict()' method of ReportWarning class does not return the expected warning details for certain warnings."
      },
      "nodeid": "tests/test_errors.py::test_warning_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006865789999892513,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function make_warning() returns a Warning object with the correct code and message.",
          "The detail attribute of the Warning object is None, indicating no additional information about the warning.",
          "The assertion that w.code == WarningCode.W101_LLM_ENABLED checks if the created warning has the correct code.",
          "The assertions that w.message == WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED] and w.detail is None check if the created warning has the expected message and no additional information.",
          "If unknown code was used, make_warning() should raise an exception or return a Warning object with incorrect attributes."
        ],
        "scenario": "Test Verify creation of warning with known code.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 222,
          "total_tokens": 389
        },
        "why_needed": "Prevents regression in case unknown code is used."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007239049999725466,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'missing_message_for_valid_code', 'expected': 'Unknown warning.', 'actual': 'Unknown warning.'}",
          "{'name': 'restoring_original_message', 'expected': 'WARNING_MESSAGES[missing_code] = old_message', 'actual': 'WARNING_MESSAGES[missing_code] = old_message'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 202,
          "total_tokens": 334
        },
        "why_needed": "To test the behavior of `make_warning` when it encounters an unknown warning code."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006969789999970999,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'code', 'value': 'WarningCode.W301_INVALID_CONFIG'}",
          "{'name': 'detail', 'value': 'Bad value'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 127,
          "total_tokens": 218
        },
        "why_needed": "The test is needed to ensure that the `make_warning` function creates a warning with detail."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007430499999827589,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert isinstance(code.value, str)', 'expected_result': 'True'}",
          "{'name': \"assert code.value.startswith('W')\", 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 109,
          "total_tokens": 211
        },
        "why_needed": "Because the `code.value` attribute is expected to be a string."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006888499999604392,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"data == {'code': 'W001', 'message': 'No coverage'}\", 'expected_result': {'code': 'W001', 'message': 'No coverage'}}"
        ],
        "scenario": "Tests for errors maximal",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 147,
          "total_tokens": 239
        },
        "why_needed": "To test the warning_to_dict method of ReportWarning class without including detail in the dictionary."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007512269999665477,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"data == {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}\", 'expected_result': {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}, 'actual_result': \"data == {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}\"}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 161,
          "total_tokens": 309
        },
        "why_needed": "The test is necessary to ensure the warning data class can be serialized correctly."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0006636850000063532,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_python_file() should return False for non-.py files', 'expected_result': False, 'actual_result': 'foo/bar.txt'}",
          "{'name': 'is_python_file() should return False for non-.py files (2)', 'expected_result': False, 'actual_result': 'foo/bar.pyc'}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 115,
          "total_tokens": 245
        },
        "why_needed": "The test should return False for non-.py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.00069779299997208,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'type', 'expected': 'str', 'actual': 'bool'}",
          "{'name': 'extension', 'expected': '.py', 'actual': 'True'}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_python_file",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 98,
          "total_tokens": 201
        },
        "why_needed": "The function `is_python_file` should be able to identify .py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.0010132180000255175,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': '/tmp/test_dir/subdir/file.py', 'expected_result': '/tmp/test_dir/file.py'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 144,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the `make_relative` function correctly makes a path relative to the test directory."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0006757289999654859,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': 'foo/bar', 'message': \"Expected result to be 'foo/bar'\"}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 107,
          "total_tokens": 198
        },
        "why_needed": "To ensure that the `make_relative` function returns a normalized path when no base is provided."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.000698480999972162,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'foo/bar', 'actual': 'normalize_path('}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 96,
          "total_tokens": 169
        },
        "why_needed": "The function should be able to handle already-normalized paths without raising an error."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007224089999908756,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '/foo/bar', 'actual': 'foo\\\\bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 100,
          "total_tokens": 174
        },
        "why_needed": "Converts backslashes in path strings to forward slashes for correct behavior in Windows."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.000695473000007496,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert normalize path returns correct result', 'expected_result': 'foo/bar', 'actual_result': 'foo/bar/'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 102,
          "total_tokens": 193
        },
        "why_needed": "To ensure that the `normalize_path` function correctly removes trailing slashes from file paths."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.0007489760000112256,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'should skip path', 'condition': \"assert should_skip_path('tests/conftest.py', exclude_patterns=['test*']) is True\"}",
          "{'message': 'should not skip path', 'condition': \"assert should_skip_path('src/module.py', exclude_patterns=['test*']) is False\"}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 126,
          "total_tokens": 252
        },
        "why_needed": "To ensure that custom patterns are correctly excluded from the test directory."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007175640000127714,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should not skip normal path', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 96,
          "total_tokens": 166
        },
        "why_needed": "The test should not be skipped for normal paths."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0006797600000254533,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert should_skip_path returns True for .git/objects/foo', 'value': True}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 99,
          "total_tokens": 186
        },
        "why_needed": "The test is checking if the `should_skip_path` function correctly identifies .git directories as requiring skipping."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0006983069999932923,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('foo/__pycache__/bar.pyc') is True"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 109,
          "total_tokens": 189
        },
        "why_needed": "Because the file is a .pyc file, which is cached by Python and should not be included in the test output."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007077270000195313,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': True, 'actual': 'True'}",
          "{'expected': '.venv/lib/python/site.py', 'actual': '.venv/lib/python/site.py'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 121,
          "total_tokens": 229
        },
        "why_needed": "Because the test case is checking if venv directories are skipped, which can cause issues with testing and environment setup."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007147479999503048,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert is_python_file('module.txt') is False",
          "assert is_python_file('module.pyc') is False",
          "assert is_python_file('module') is False"
        ],
        "scenario": "Verifies that a non-.py file does not match the expected criteria.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 210,
          "total_tokens": 307
        },
        "why_needed": "Prevents regression in cases where a user might mistakenly assume a non-.py file is Python code."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007171910000352,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `is_python_file` correctly identifies .py files and returns True.",
          "It handles paths with spaces in them by converting them to a single space.",
          "It supports relative paths by resolving them to absolute paths.",
          "It ignores non-existent files, returning False for those.",
          "It does not incorrectly identify non-Python file extensions (e.g., .txt).",
          "It correctly identifies Python modules with different casing (e.g., `module.py` and `Module.py`)."
        ],
        "scenario": "Testing if a file is a Python file.",
        "token_usage": {
          "completion_tokens": 155,
          "prompt_tokens": 212,
          "total_tokens": 367
        },
        "why_needed": "Prevents a potential bug where the test might incorrectly identify non-Python files as such."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 12,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63, 65, 67"
        }
      ],
      "duration": 0.001072394000004806,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'project1' should be included in the result.",
          "The 'file.py' should also be included in the result.",
          "The result should be an absolute path (i.e., it should start with a slash).",
          "The relative_to parameter should fail and return None.",
          "The make_relative function should correctly handle cases where the input paths are not relative to each other.",
          "The test should pass even if the input paths have different names or extensions."
        ],
        "scenario": "Test makes sure make_relative function returns a normalized absolute path when the input paths are not relative to each other.",
        "token_usage": {
          "completion_tokens": 171,
          "prompt_tokens": 301,
          "total_tokens": 472
        },
        "why_needed": "This test prevents regression where make_relative function fails to return a normalized absolute path when the input paths are not relative to each other."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_path_not_under_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.0010407660000169017,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '/subdir/file.py', 'actual': 'subdir/file.py'}"
        ],
        "scenario": "Test Make Relative - Success",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 147,
          "total_tokens": 217
        },
        "why_needed": "To test that the `make_relative` function correctly resolves relative paths to file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0006919329999846013,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result is equal to expected result', 'expected_result': 'path/to/file.py'}"
        ],
        "scenario": "test_make_relative_with_none_base",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 116,
          "total_tokens": 193
        },
        "why_needed": "To ensure that the `make_relative` function correctly handles cases where the base path is None."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.000696815000026163,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '/path/to/file.py', 'actual': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 114,
          "total_tokens": 192
        },
        "why_needed": "To ensure that backslashes are correctly converted to forward slashes in file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0008334750000358326,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'path/to/file.py', 'actual_value': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 110,
          "total_tokens": 195
        },
        "why_needed": "To ensure the `normalize_path` function correctly normalizes path objects, including those with nested directories."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007125329999553287,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The normalized path has no trailing slash.', 'expected_result': 'path/to/dir'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 111,
          "total_tokens": 196
        },
        "why_needed": "To ensure that the `pathlib` module correctly handles paths with trailing slashes."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007302060000142774,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Regular path is included in the coverage report', 'expected_result': True}",
          "{'description': 'Regular path should be excluded from the coverage report', 'expected_result': False}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 120,
          "total_tokens": 229
        },
        "why_needed": "To ensure that regular paths are not skipped and should be included in the coverage report."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007065590000365773,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should be True', 'description': 'The function should return True when the path is a .git directory.'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 102,
          "total_tokens": 195
        },
        "why_needed": "The test should skip the .git directory because it contains Git hooks that are not necessary for the current scenario."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0006939390000297863,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should be True', 'description': 'The function should return True for the given path.'}",
          "{'name': '.venv', 'description': '.venv is a file that starts with a skip directory name.'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 124,
          "total_tokens": 240
        },
        "why_needed": "To ensure that paths starting with a skip directory name are properly skipped."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007173039999770481,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should skip __pycache__ directory', 'description': 'The test should return True indicating that the path is skipped.'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 116,
          "total_tokens": 206
        },
        "why_needed": "Because the '__pycache__' directory contains a cached Python file."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0006969269999785865,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "should be skipped"
        ],
        "scenario": "/usr/lib/python3.12/site-packages/pkg/mod.py",
        "token_usage": {
          "completion_tokens": 45,
          "prompt_tokens": 111,
          "total_tokens": 156
        },
        "why_needed": "Because it's a site-package directory."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_site_packages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007163870000113093,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'venv/lib/python3.12/site.py', 'expected': True}",
          "{'path': '.venv/lib/python3.12/site.py', 'expected': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 130,
          "total_tokens": 238
        },
        "why_needed": "Because the 'venv' directory contains a Python package, which should be skipped."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.001544174999992265,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should be true', 'description': 'The function should return True when the path is excluded.', 'expected_value': 'True'}",
          "{'name': 'should be false', 'description': 'The function should return False when the path is not excluded.', 'expected_value': 'False'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 132,
          "total_tokens": 260
        },
        "why_needed": "Custom exclude patterns are used to skip certain files or directories."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 50,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-227, 232-233, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003172316000018327,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'Gemini requests-per-day limit reached' in res.error",
          "assert 'requests per day' in res.error",
          "assert 'limit exceeded' in res.error",
          "assert 'daily limit hit' in res.error",
          "assert 'requests per day exceeded' in res.error",
          "assert 'limit exceeded daily' in res.error",
          "assert 'requests per day limit reached' in res.error"
        ],
        "scenario": "The test verifies that the 'Gemini requests-per-day limit reached' error is thrown when the daily limit for a model is hit.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 367,
          "total_tokens": 524
        },
        "why_needed": "This test prevents regression where the Gemini provider does not correctly handle daily limits being exceeded."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotate_loop_daily_limit_hit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 100,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-236, 239-244, 263-265, 268, 293, 295, 299-303, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002811952999991263,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "MockGenFailure (Line 293)",
          "MockResExhausted (Lines 300, 301)",
          "_GeminiRateLimitExceeded (Lines 300, 301)",
          "assert 'Gemini requests-per-day' in res.error or 'rate limits reached' in res.error"
        ],
        "scenario": "Test that 'GenerationFailure' exception is raised when Gemini generation fails",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 730,
          "total_tokens": 849
        },
        "why_needed": "Prevents regression where 'GenerationFailure' exception is not raised due to internal model exhaustion."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotation_exceptions_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 173,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181-182, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 254-255, 259, 340, 343, 346, 348-356, 358-361, 363-364, 366-367, 435, 437-439, 441-442, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-498, 502-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-564, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.16108308899998747,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "1. Mock imports are set up correctly without causing errors.",
          "2. Context too long error is raised with the correct error message.",
          "3. RPD (Rate Limiting and Parsing) function returns the expected value for requests_per_day.",
          "4. Fallback models are properly fetched and added to the list of available models.",
          "5. Input limits logic works correctly, providing the expected input token limit."
        ],
        "scenario": "Prevents bug in coverage gaps test by ensuring proper rate limiting and annotation logic for various scenarios.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 821,
          "total_tokens": 970
        },
        "why_needed": "This test prevents regression that could cause coverage gaps due to improper rate limiting or annotation logic."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_coverage_gaps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 13,
          "line_ranges": "134-135, 137-141, 143-144, 524-527"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007977039999786939,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected _parse_preferred_models to return an empty list', 'expected_value': [], 'message': 'The function should not parse any preferred models.'}",
          "{'name': 'Expected mock_config.model to be None after setting it to \"ALL\"', 'expected_value': 'None', 'message': 'mock_config.model should be set to None'}"
        ],
        "scenario": "Test parsing preferred models coverage",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 156,
          "total_tokens": 287
        },
        "why_needed": "To test the behavior of GeminiProvider when no model is specified in the configuration."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_parse_preferred_models_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 10,
          "line_ranges": "39-42, 81-82, 84, 87-89"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007147890000283041,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The length of _daily_requests should be 0 after pruning', 'expected_result': 0}"
        ],
        "scenario": "test_prune_daily_requests",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 157,
          "total_tokens": 242
        },
        "why_needed": "To test the behavior of the GeminiRateLimiter when a daily request is pruned after being older than 24 hours."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_prune_daily_requests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 4,
          "line_ranges": "39-42"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016326390000358515,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The loop should finish without returning if `remaining` decreases and `request_tokens <= limit`.",
          "If `request_tokens` is massive, the function should return 0.0 at line 106/108.",
          "The `tokens_used` value should be equal to `limit` when all tokens are used up.",
          "The `remaining` value should decrease by `request_tokens` for each iteration of the loop.",
          "If `remaining + request_tokens <= limit`, the function should not return and wait for the TPM to become available.",
          "The `time.monotonic()` difference between the start and end times of the loop should be less than or equal to 30 seconds.",
          "The total number of tokens used by the function should be equal to `limit` when all tokens are used up."
        ],
        "scenario": "Test verifies that the test_tpm_available_fallback function behaves correctly when a token is requested.",
        "token_usage": {
          "completion_tokens": 233,
          "prompt_tokens": 524,
          "total_tokens": 757
        },
        "why_needed": "This test prevents regression where the function does not wait for the TPM to become available after requesting tokens."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_tpm_available_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 14,
          "line_ranges": "134-135, 137-141, 143-144, 164-165, 167-169"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008391490000008162,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation contains the string 'google-generativeai not installed' in its error message.",
          "The annotation includes the key-value pair 'module': 'google.generativeai'.",
          "The annotation includes the key-value pair 'outcome': 'passed'.",
          "The annotation includes the nodeid 't'."
        ],
        "scenario": "Test annotation when google-generativeai is not installed.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 259,
          "total_tokens": 398
        },
        "why_needed": "This test prevents a potential import error that could occur when trying to annotate an annotation with a module that does not exist (i.e., `google.generativeai` is not installed)."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_import_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0027475740000113547,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The error message should contain 'GEMINI_API_TOKEN is not set'.",
          "The annotation should fail when GEMINI_API_TOKEN is missing.",
          "The annotation should report an error indicating that GEMINI_API_TOKEN is not set.",
          "The test should verify the presence of a GEMINI_API_TOKEN environment variable before annotating the result.",
          "The test should check for the absence of a GEMINI_API_TOKEN environment variable in the annotation string.",
          "The test should report an error when trying to annotate without setting GEMINI_API_TOKEN.",
          "The test should verify that the annotation is not successful with missing GEMINI_API_TOKEN."
        ],
        "scenario": "Test that annotation fails when token is missing and GEMINI_API_TOKEN is not set.",
        "token_usage": {
          "completion_tokens": 207,
          "prompt_tokens": 313,
          "total_tokens": 520
        },
        "why_needed": "Prevents a potential bug where the test annotates an empty result without checking for the presence of a GEMINI_API_TOKEN environment variable."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_no_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 214,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-237, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00510588699995651,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation's `scenario` field matches the expected value 'Recovered Scenario'.",
          "The mock `mock_post.call_count` is set to 2 when the test call fails with a 429 status code, indicating that the provider retries the request.",
          "The mock `mock_get.return_value.json.return_value` is set to a valid JSON response for a successful rate limit retry scenario.",
          "The `test_result` nodeid matches the expected value 'test1'.",
          "The annotation's `error` field is None when the test call succeeds with a 200 status code, indicating that no error occurred.",
          "The mock `_parse_response` returns a valid response for a successful rate limit retry scenario.",
          "The `mock_get.return_value.status_code` matches the expected value '429' when the test call fails with a 429 status code.",
          "The `mock_get.return_value.headers` set to `{"
        ],
        "scenario": "Test that the GeminiProvider correctly annotates a rate limit retry scenario.",
        "token_usage": {
          "completion_tokens": 264,
          "prompt_tokens": 636,
          "total_tokens": 900
        },
        "why_needed": "This test prevents regression in the case where the GeminiProvider is called with a 429 status code due to rate limiting, and then receives a successful response after a retry."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 208,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-568, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.4122526439999774,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The scenario 'Success Scenario' should be returned by _annotate_internal.",
          "_parse_response returns a Mock object with the correct scenario and no error.",
          "The annotation does not contain any error information.",
          "The annotation's scenario matches the expected value.",
          "_build_prompt is called before _parse_response to avoid complex dependencies, but this test does not require it.",
          "The response from _call_gemini has the correct format (text and tokens).",
          "The annotation correctly handles a successful response from _call_gemini with no error."
        ],
        "scenario": "Test that the _annotate_internal method returns a successful annotation with no error when the response from _call_gemini is in the expected format.",
        "token_usage": {
          "completion_tokens": 188,
          "prompt_tokens": 649,
          "total_tokens": 837
        },
        "why_needed": "This test prevents regression where the GeminiProvider does not correctly handle responses from _call_gemini with the expected format."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134-135, 137-141, 143-144, 332-333, 335"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0024889790000202083,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _check_availability() returns False when environment variable GEMINI_API_TOKEN is set.",
          "The function _check_availability() returns True when environment variable GEMINI_API_TOKEN is not set."
        ],
        "scenario": "Verifies that the availability of a Gemini provider is checked correctly when environment variables are set.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 235,
          "total_tokens": 354
        },
        "why_needed": "To prevent a potential bug where the availability of a Gemini provider is not checked in certain environments, such as when environment variables like GEMINI_API_TOKEN are set."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_availability",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 111,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-237, 239-244, 263-265, 268, 272-276, 279-281, 283-286, 288-292, 318-320, 322-323, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 60.00336882100004,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'm1' model should be marked as exhausted after a daily limit is exceeded.",
          "The 'm1' model should be marked as available again after a retry-after cleanup is triggered.",
          "The cooldown for the 'm1' model should exceed the time it took to reach the daily limit.",
          "The cooldown for the 'm1' model should not be reset immediately after a retry-after cleanup is triggered.",
          "Mock calls to '_ensure_models_and_limits' and '_call_gemini' should result in the correct side effects being simulated."
        ],
        "scenario": "Verify that the GeminiProvider class correctly annotates retry exceptions for models with high request rates.",
        "token_usage": {
          "completion_tokens": 187,
          "prompt_tokens": 651,
          "total_tokens": 838
        },
        "why_needed": "This test prevents a regression where the GeminiProvider class fails to annotate retry exceptions for models with high request rates, potentially leading to incorrect model exhaustion tracking."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_exceptions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 97,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-94, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 254, 259, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0032474690000299233,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `_model_exhausted_at[model]` is set to None before the annotation process starts.",
          "After calling `provider._annotate_internal`, the value of `_model_exhausted_at[model]` becomes False.",
          "If a successful annotation occurs, then the value of `_model_exhausted_at[model]` should be cleared to avoid inconsistent model exhaustion checks."
        ],
        "scenario": "Test that the GeminiProvider correctly clears the `_model_exhausted_at` dictionary after a successful annotation.",
        "token_usage": {
          "completion_tokens": 160,
          "prompt_tokens": 482,
          "total_tokens": 642
        },
        "why_needed": "This test prevents regression where the `GeminiProvider` fails to clear the `_model_exhausted_at` dictionary after a successful annotation, potentially leading to inconsistent model exhaustion checks."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_loop_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-356, 358-361, 363-364, 366-367"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010639249999826461,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Rate limits should be enforced', 'description': 'The function should return a valid rate limit configuration.', 'expected_value': 10, 'actual_value': 10}",
          "{'name': 'Exception is raised when rate limiting is attempted with non-integer value', 'description': 'The function should raise an exception when rate limiting is attempted with a non-integer value.', 'expected_exception': 'RateLimitError'}"
        ],
        "scenario": "TestGeminiProviderDetailed::test_ensure_rate_limits_error",
        "token_usage": {
          "completion_tokens": 160,
          "prompt_tokens": 156,
          "total_tokens": 316
        },
        "why_needed": "This test ensures that the GeminiProvider raises an error when rate limiting is attempted with a non-integer value."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_ensure_rate_limits_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009691120000070441,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'models are empty', 'expected_value': [], 'actual_value': '[]'}",
          "{'name': 'limit_map is empty', 'expected_value': {}, 'actual_value': {}}"
        ],
        "scenario": "TestGeminiProviderDetailed::test_fetch_available_models_error",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 132,
          "total_tokens": 238
        },
        "why_needed": "To test that an exception is raised when fetching available models fails due to a network error."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 476-477, 537, 539-543, 547-548, 550-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015213080000080481,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'm1' model should not be included in the list of available models.",
          "The 'm2' model should not be included in the list of available models.",
          "The 'm3' model should be included in the list of available models.",
          "The 'inputTokenLimit' value for the 'm3' model should match its supported generation methods.",
          "The 'limit_map' dictionary should contain only valid keys ('m1', 'm2', and 'm3')."
        ],
        "scenario": "Test that fetching available models with invalid JSON data prevents a bug.",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 340,
          "total_tokens": 509
        },
        "why_needed": "To prevent a potential issue where the GeminiProvider fetches models from an invalid JSON response, which could lead to incorrect model availability information."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 486, 488-491, 493"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020478440000033515,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock Ensure Function', 'expected_calls': [1], 'message': 'The `mock_ensure` function was called once.'}"
        ],
        "scenario": "TestGeminiProviderDetailed",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 144,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method calls the `mock_ensure` function."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "134-135, 137-141, 143-144, 449-457, 459-460, 463-466"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007428189999814094,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'requests_per_minute', 'expected_value': 'None'}",
          "{'name': 'tokens_per_minute', 'expected_value': 100}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 156,
          "total_tokens": 257
        },
        "why_needed": "To ensure that the Gemini provider can correctly parse rate limits and return the expected configuration."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 11,
          "line_ranges": "39-42, 81-85, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007208199999695353,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of _request_times should be 1 after pruning.",
          "The length of _token_usage should be 1 after pruning.",
          "The first element in _request_times should be equal to the current time minus 10 seconds.",
          "_request_times[0] should be equal to now - 10.0",
          "The first element in _token_usage should be equal to (now - 10.0, 10)."
        ],
        "scenario": "Test the prune logic of the GeminiRateLimiter to ensure it removes old requests within a specified time window.",
        "token_usage": {
          "completion_tokens": 168,
          "prompt_tokens": 323,
          "total_tokens": 491
        },
        "why_needed": "This test prevents a potential issue where old requests are not removed from the limiter's cache, leading to performance degradation or incorrect rate limiting behavior."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_prune_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 6,
          "line_ranges": "39-42, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007080579999865222,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected len(limiter._token_usage) to be 0'}",
          "{'message': 'Actual: {len(limiter._token_usage)}', 'value': 1}"
        ],
        "scenario": "Test that record tokens with a negative value returns an empty list of token usage.",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 127,
          "total_tokens": 223
        },
        "why_needed": "To ensure the rate limiter correctly handles invalid input values, specifically negative token counts."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_record_tokens_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007467350000069928,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'limiter.next_available_in(100) is None', 'description': 'The next available in time should be None after 100 requests'}"
        ],
        "scenario": "Test the rate limiting mechanism",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 129,
          "total_tokens": 220
        },
        "why_needed": "The test ensures that the rate limiting mechanism correctly limits the number of requests to a resource within a certain time frame."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008221809999895413,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method returns 0.0 for the first two requests and 60.0 for the third request.",
          "The limit is correctly reset after the third request.",
          "The rate limiter does not wait for more than 1 minute between requests."
        ],
        "scenario": "Verify that the RPM limit is correctly enforced for the first two requests and that it resets after the third request.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 280,
          "total_tokens": 410
        },
        "why_needed": "This test prevents a potential bug where the rate limiter does not reset properly after the third request, leading to unexpected behavior in subsequent requests."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 100-101, 103-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007336310000027879,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert limiter._seconds_until_tpm_available(now, 0) == 0.0",
          "assert limiter._seconds_until_tpm_available(now, 150) == 0.0",
          "assert wait > 0 and wait <= 60.0 + 1e-9"
        ],
        "scenario": "Verify that the rate limiter correctly returns seconds until TPM available when no tokens are requested or more than limit is exceeded.",
        "token_usage": {
          "completion_tokens": 168,
          "prompt_tokens": 377,
          "total_tokens": 545
        },
        "why_needed": "This test prevents a regression where the rate limiter does not return a correct value for seconds_until_tpm_available when no tokens are requested, and also ensures that it returns a correct value even when more than the limit of tokens is requested but usage is still within the limit."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_seconds_until_tpm_available_branches",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007677829999579444,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `limit_type` attribute of the raised exception is set to 'requests_per_day'.",
          "The `value` attribute of the raised exception contains the string 'requests_per_day'.",
          "The `exc.value.limit_type` attribute matches the expected value 'requests_per_day'.",
          "The `exc.value.limit_type` attribute matches the expected value 'requests_per_day'.",
          "The `exc.value.limit_type` attribute is set to 'requests_per_day' and its value is a string.",
          "The `exc.value.limit_type` attribute is set to 'requests_per_day' and its value is a string.",
          "The `exc.value.limit_type` attribute matches the expected value 'requests_per_day'.",
          "The `exc.value.limit_type` attribute matches the expected value 'requests_per_day'."
        ],
        "scenario": "Verify that the `wait_for_slot` method raises a `GeminiRateLimitExceeded` exception when the daily limit is exceeded.",
        "token_usage": {
          "completion_tokens": 245,
          "prompt_tokens": 263,
          "total_tokens": 508
        },
        "why_needed": "Prevents a potential rate limiter error where the daily limit is exceeded and the user does not receive an error message."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_daily_limit_exceeded",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001461905000041952,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method is called with an argument greater than or equal to 10.0 seconds.",
          "The mock_sleep function is called once with an argument of 10.0 seconds.",
          "The time.monotonic value returned by the mock_next_available_in function is greater than or equal to 10.0 seconds."
        ],
        "scenario": "Test that the wait_for_slot function waits for a sufficient amount of time before returning.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 325,
          "total_tokens": 458
        },
        "why_needed": "This test prevents a potential regression where the rate limiter does not wait long enough for subsequent requests to be processed."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_sleeps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007441010000093229,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config1 and config2 should have different hashes', 'expected_value': 'different hashes'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 119,
          "total_tokens": 198
        },
        "why_needed": "To ensure that different configurations of the same provider produce different hashes."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007048759999861431,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'hash length', 'expected': 16, 'actual': {'type': 'int', 'value': 16}}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 109,
          "total_tokens": 209
        },
        "why_needed": "To ensure the computed hash is short and can be easily stored in a database or used for other purposes where storage space is limited."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 6,
          "line_ranges": "32, 44-48"
        }
      ],
      "duration": 0.0008576500000003762,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'content_hash', 'actual': 'file_hash'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 144,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the computed SHA-256 hash of a file matches its content hash."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "44-48"
        }
      ],
      "duration": 0.0008133679999673404,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected length of hash to be 64 bytes', 'expected': 64, 'actual': 0}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 124,
          "total_tokens": 212
        },
        "why_needed": "To ensure that the `compute_file_sha256` function correctly hashes file contents."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0006781920000094033,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Different keys should produce different signatures.', 'description': 'The HMAC signature for a given input content and key should be unique.'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_different_key",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 125,
          "total_tokens": 203
        },
        "why_needed": "To ensure that different keys produce different signatures."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_different_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007032329999674403,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_length': 64, 'actual_length': 0}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_with_key",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 108,
          "total_tokens": 176
        },
        "why_needed": "To test the computation of HMAC with a key."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_with_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007282590000272648,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'h1': 'hash1', 'h2': 'hash2'}, 'actual': {'h1': 'hash1', 'h2': 'hash1'}}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_consistent",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 115,
          "total_tokens": 215
        },
        "why_needed": "To ensure that the hash function is consistent and produces the same output for the same input."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_consistent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0006469279999805622,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected hash length to be 64. Actual: %r'}",
          "expected_value",
          "actual_value"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_length",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 103,
          "total_tokens": 185
        },
        "why_needed": "The length of the hash is expected to be 64 characters."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.07710326199998008,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'includes pytest package', 'description': \"The 'pytest' package should be present in the dependency snapshot.\"}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 102,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the 'pytest' package is included in the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.07905008400001634,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'snapshot is a dict', 'expected': 'dict', 'actual': 'True'}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 98,
          "total_tokens": 178
        },
        "why_needed": "To ensure that the `get_dependency_snapshot` function returns a dictionary."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "73, 76-77, 80-81"
        }
      ],
      "duration": 0.0008100129999775163,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': \"b'my-secret-key\\n'\", 'actual_value': \"b'...'\"}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 145,
          "total_tokens": 225
        },
        "why_needed": "To test the functionality of loading a HMAC key from a file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 4,
          "line_ranges": "73, 76-78"
        }
      ],
      "duration": 0.0007735570000022562,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected to return None when missing key file exists'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 126,
          "total_tokens": 198
        },
        "why_needed": "The test should return None if the key file does not exist."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 2,
          "line_ranges": "73-74"
        }
      ],
      "duration": 0.0006906959999923856,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'type': 'NoneType', 'message': 'The function should return None.'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 110,
          "total_tokens": 195
        },
        "why_needed": "To test that the function returns None when no key file is configured."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006641840000156662,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.aggregate_dir should be None",
          "config.aggregate_policy should be 'latest'",
          "config.aggregate_include_history should be False"
        ],
        "scenario": "Test aggregation defaults with no aggregate directory specified.",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 201,
          "total_tokens": 280
        },
        "why_needed": "Without an aggregate directory, aggregation may not work as expected or may result in unexpected behavior."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006628169999771671,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.capture_failed_output', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 107,
          "total_tokens": 177
        },
        "why_needed": "The test captures failed output by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006731819999572508,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.llm_context_mode', 'expected_value': 'minimal'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 107,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the context mode is set to 'minimal' by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 4,
          "line_ranges": "123, 171, 284, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007435250000185079,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'LLM is not enabled by default', 'description': 'The LLM should be disabled by default.'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 109,
          "total_tokens": 194
        },
        "why_needed": "The LLM (Language Model) is not enabled by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007020759999818438,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.omit_tests_from_coverage', 'expected_value': True}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 109,
          "total_tokens": 185
        },
        "why_needed": "The test omits all tests from the coverage report by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007854990000168982,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"config.provider == 'none'\", 'expected_result': 'none'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 101,
          "total_tokens": 182
        },
        "why_needed": "To ensure that the provider is set to 'none' by default when no other value is specified."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006935449999900811,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Excluding secret files', 'expected': ['llm_context_exclude_globs'], 'actual': ['secret']}",
          "{'name': 'Excluding .env files', 'expected': ['llm_context_exclude_globs'], 'actual': []}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 132,
          "total_tokens": 259
        },
        "why_needed": "This test is needed because the default configuration includes secret files and environment variables, which are not intended to be exposed."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.006247317000031671,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `nodeid` values in the report are sorted by nodeid.",
          "All `nodeid` values are present in the report.",
          "No duplicates are present in the `nodeid` values.",
          "All `outcome` values are either 'passed' or 'failed'.",
          "The `tests` list contains exactly one test for each `nodeid`.",
          "Each `test_z`, `test_a`, and `test_m` is a valid test function.",
          "The output of the integration gate is deterministic (sorted by nodeid)."
        ],
        "scenario": "Tests the deterministic output of the integration gate.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 313,
          "total_tokens": 477
        },
        "why_needed": "This test prevents regression that could cause non-deterministic output."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.005232868000007329,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests in the report should be zero.",
          "The summary section of the report should contain no data.",
          "The 'total' key in the summary section should have a value of zero.",
          "The test counts are not affected by an empty test suite.",
          "The report does not contain any duplicate test names or IDs.",
          "There are no missing test results in the report.",
          "The test suite is correctly identified as empty in the report."
        ],
        "scenario": "Test that an empty test suite produces a valid report.",
        "token_usage": {
          "completion_tokens": 153,
          "prompt_tokens": 240,
          "total_tokens": 393
        },
        "why_needed": "This test prevents regression where the test suite is empty, ensuring the report is always valid and accurate."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.036926665999999386,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file \"report.html\" exists at the specified path.",
          "The string '<html' is present in the content of 'report.html'.",
          "The string 'test_pass' is present in the content of 'report.html'."
        ],
        "scenario": "The test verifies that the full pipeline generates an HTML report.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 270,
          "total_tokens": 377
        },
        "why_needed": "This test prevents a regression where the HTML report is not generated correctly due to a change in the configuration."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/_git_info.py",
          "line_count": 2,
          "line_ranges": "2-3"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 138,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06228880499998013,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_json` and `report_html` paths should exist in the temporary directory.",
          "The schema version of the report data should match the expected value.",
          "The summary statistics (total, passed, failed, skipped) should be correct."
        ],
        "scenario": "Test that the full pipeline generates a valid JSON report.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 419,
          "total_tokens": 521
        },
        "why_needed": "This test prevents regression in the pipeline where JSON reports are not generated correctly."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007070340000154829,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' field is present in the data.",
          "The 'run_meta' field is present in the data.",
          "The 'summary' field is present in the data.",
          "The 'tests' field is present in the data."
        ],
        "scenario": "Test verifies that the ReportRoot object has required fields.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 250,
          "total_tokens": 361
        },
        "why_needed": "This test prevents a bug where the report root is missing required fields, making it invalid to use as a schema."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006886300000132906,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_aggregated', 'expected_value': 'True'}",
          "{'name': 'run_count', 'expected_value': 0}"
        ],
        "scenario": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 136,
          "total_tokens": 247
        },
        "why_needed": "To ensure that the RunMeta object has an 'is_aggregated' key and a 'run_count' value when it is used in aggregation."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006912040000202069,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be present in the data.",
          "The 'interrupted' field should be present in the data.",
          "The 'collect_only' field should be present in the data.",
          "The 'collected_count' field should be present in the data.",
          "The 'selected_count' field should be present in the data."
        ],
        "scenario": "Test 'RunMeta has run status fields' verifies that the RunMeta object contains the required status fields.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 237,
          "total_tokens": 381
        },
        "why_needed": "This test prevents a potential bug where the RunMeta object is missing certain status fields, potentially leading to incorrect analysis results."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006849929999930282,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'SCHEMA_VERSION', 'value': '1.2.3'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 103,
          "total_tokens": 178
        },
        "why_needed": "The schema version is needed to ensure compatibility with the gate."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006802770000149394,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "nodeid",
          "outcome",
          "duration"
        ],
        "scenario": "Test verifies that the `TestCaseResult` object has required fields.",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 223,
          "total_tokens": 300
        },
        "why_needed": "This test prevents a potential bug where the `TestCaseResult` object is missing some of its required fields, potentially causing incorrect results or errors."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.0015649890000304,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider.annotate` method should raise an exception with the error message.",
          "The `result.error` attribute should be set to a non-None value indicating an error.",
          "The `result` object should contain a `message` key with the error message.",
          "The `error` attribute of the `result.message` string should match the expected error message.",
          "The `provider.annotate` method should not return any result when all retries are exhausted.",
          "The `provider.annotate` method should raise an exception when all retries are exhausted and no result is returned.",
          "The `provider.annotate` method should set the `error` attribute of the `result` object to a non-None value indicating an error."
        ],
        "scenario": "Test verifies that all retries are exhausted when API call fails.",
        "token_usage": {
          "completion_tokens": 205,
          "prompt_tokens": 346,
          "total_tokens": 551
        },
        "why_needed": "Prevents regression where LLMTokenRefreshRetry may not retry if all retries are exhausted."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_all_retries_exhausted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 38,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011362090000375247,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should not return an error with status code 500 (Internal server error).",
          "The annotation should have an error attribute set to the exception object.",
          "The annotation should not call the completion function with any arguments.",
          "The annotation should not raise an exception.",
          "The annotation should be None if the API call fails without raising an exception.",
          "The annotation's status code should match the expected 500 status code.",
          "The annotation's error attribute should have a non-None value."
        ],
        "scenario": "Test that non-401 errors don't force token refresh.",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 367,
          "total_tokens": 536
        },
        "why_needed": "Prevents regression in case of non-401 error, where the LLMTokenRefreshRetry test would fail due to a forced refresh."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_non_401_error_no_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 47,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.0028359149999915,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the retry mechanism works correctly even when the initial API call fails.",
          "The test ensures that the retry process does not get stuck in an infinite loop due to transient errors.",
          "The test verifies that the scenario is updated correctly after a transient error occurs.",
          "The test checks if the error message is properly set and matches the expected format.",
          "The test verifies that the key assertions are met, including 'assert true' as required.",
          "The test ensures that the result object has an 'error' attribute that is None when no error occurred.",
          "The test verifies that the scenario is updated correctly after a transient error occurs."
        ],
        "scenario": "Test that retry succeeds after transient error.",
        "token_usage": {
          "completion_tokens": 187,
          "prompt_tokens": 433,
          "total_tokens": 620
        },
        "why_needed": "To prevent regression in case of transient errors where the API call fails and needs to be retried."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_retry_succeeds_after_transient_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 54,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-188, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 5.960529043000008,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `provider.annotate` should be called twice with an error status code of 401.",
          "The second call to `provider.annotate` should have a valid response containing a new token.",
          "The third check in the test case should not trigger due to successful API call.",
          "The LLMTokenRefreshRetry test case should fail when the API call to refresh the token fails with a 401 status code."
        ],
        "scenario": "Test that 401 error triggers token refresh (lines 123-126).",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 473,
          "total_tokens": 630
        },
        "why_needed": "To ensure the LLMTokenRefreshRetry test case covers a scenario where the API call to refresh the token fails with a 401 status code."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_token_refresh_on_401",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000715723999974216,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected_value': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 131,
          "total_tokens": 220
        },
        "why_needed": "The test is necessary to ensure that the GeminiProvider class is correctly instantiated when the 'gemini' provider is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006867719999945621,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected_value': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 140,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the LiteLLMProvider class is correctly instantiated when a specific provider ('litellm') is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00069048099999236,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected': 'None', 'got': 'NoopProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 115,
          "total_tokens": 228
        },
        "why_needed": "The test is necessary because the LLM returns a `NoopProvider` when the provider is set to 'none'. This is not a realistic scenario, as it would indicate that the LLM has no knowledge of the input data."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006930600000032427,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected_value': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 154,
          "total_tokens": 243
        },
        "why_needed": "To ensure that the OllamaProvider is correctly instantiated when a specific provider ('ollama') is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006890729999895484,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 125,
          "total_tokens": 240
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_unknown_raises",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006707969999979468,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'annotate' method should be present on the provider.",
          "The 'is_available' method should be present on the provider.",
          "The 'get_model_name' method should be present on the provider.",
          "The 'config' attribute should be present on the provider."
        ],
        "scenario": "Test that the NoopProvider class implements all required interface methods.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 232,
          "total_tokens": 351
        },
        "why_needed": "This test prevents a potential bug where the NoopProvider class is not implemented correctly and may cause unexpected behavior or errors."
      },
      "nodeid": "tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007262819999596104,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation is an instance of LlmAnnotation",
          "annotation scenario is an empty string",
          "annotation why_needed is an empty string",
          "annotation key_assertions is an empty list"
        ],
        "scenario": "Test that NoopProvider returns an empty annotation when no nodes are annotated.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 249,
          "total_tokens": 351
        },
        "why_needed": "This test prevents a regression where the annotate method of NoopProvider returns an annotation even if no nodes are annotated."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 67"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006746589999693242,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert provider.get_model_name() == \"\"', 'expected_value': '', 'message': 'Expected get_model_name to return an empty string'}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 114,
          "total_tokens": 211
        },
        "why_needed": "The test is failing because the model name returned by the NoopProvider is not empty."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 59"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007284740000272905,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_available() should return True', 'expected_result': True}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_is_available",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 108,
          "total_tokens": 180
        },
        "why_needed": "The NoopProvider class should always be available."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_is_available",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006467380000003686,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'field_name': 'required', 'expected_value': ['scenario', 'why_needed'], 'actual_value': [0, 1]}"
        ],
        "scenario": "Test the schema requirements",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 115,
          "total_tokens": 200
        },
        "why_needed": "This test ensures that the LLM contract's annotation schema requires 'scenario' and 'why_needed' fields."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006932959999517152,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "checks password",
          "checks username",
          "correctly parses the 'scenario' key",
          "correctly parses the 'why_needed' key",
          "correctly parses the 'confidence' key",
          "has exactly two assertions for the 'key_assertions' list"
        ],
        "scenario": "The test verifies that the `AnnotationSchema.from_dict` method correctly parses a dictionary into an AnnotationSchema instance.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 274,
          "total_tokens": 411
        },
        "why_needed": "This test prevents potential issues where the `AnnotationSchema.from_dict` method fails to parse a dictionary with required keys, potentially leading to incorrect schema creation or other errors."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000677180000025146,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'schema.scenario', 'value': '', 'expected_type': 'str'}",
          "{'name': 'schema.why_needed', 'value': '', 'expected_type': ''}"
        ],
        "scenario": "TestAnnotationSchema.test_schema_handles_empty",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 109,
          "total_tokens": 205
        },
        "why_needed": "This test ensures that the AnnotationSchema class correctly handles empty input scenarios."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000685429999975895,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 182,
          "prompt_tokens": 119,
          "total_tokens": 301
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006489940000165006,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties'],",
          "assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties'],",
          "assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']"
        ],
        "scenario": "The test verifies that the schema has required fields.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 215,
          "total_tokens": 324
        },
        "why_needed": "This test prevents a potential bug where the schema is not properly defined with required fields, potentially leading to invalid or unexpected behavior."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "90-92, 94-96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006688299999950686,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assertion 1",
          "assertion 2",
          "# Ensure 'key_assertions' key exists in the dictionary"
        ],
        "scenario": "Test AnnotationSchema::test_schema_to_dict verifies that the `AnnotationSchema` instance is correctly serialized to a dictionary.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 247,
          "total_tokens": 352
        },
        "why_needed": "This test prevents regression by ensuring that the `AnnotationSchema` instance can be successfully converted to a dictionary, which is necessary for proper data storage and retrieval."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006734819999678621,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected_type': 'NoopProvider', 'actual_type': 'get_provider(config)', 'message': 'Expected get_provider(config) to return NoopProvider, but got '}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 118,
          "total_tokens": 235
        },
        "why_needed": "The test is necessary to ensure that the NoopProvider class can be instantiated correctly when a provider is set to 'none'."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006932979999874078,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The provider is an instance of LLMProvider', 'description': 'We expect the `provider` variable to be an instance of `LLMProvider`. This can be verified by checking if it has a certain attribute or method.'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 117,
          "total_tokens": 267
        },
        "why_needed": "The test is necessary because the `isinstance` function checks if an object is of a certain type. In this case, we want to ensure that the `provider` variable is actually an instance of `LLMProvider`, which is the expected implementation."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006939219999821944,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by NoopProvider is expected to be empty.",
          "No assertion was made in the provided code, indicating that it should return an empty annotation.",
          "The `annotate` method of the `NoopProvider` class is expected to return a `TestCaseResult` object with an empty string as its scenario."
        ],
        "scenario": "The test verifies that NoopProvider returns an empty annotation when the test function does not return any value.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 253,
          "total_tokens": 387
        },
        "why_needed": "This test prevents a regression where the LLM contract incorrectly returns an annotation for a non-executable test function."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006849439999996321,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result has the expected 'scenario' attribute.",
          "The result has the expected 'why_needed' attribute.",
          "The result has the expected 'key_assertions' attribute."
        ],
        "scenario": "Test that the `annotate` method returns a valid LlmAnnotation-like object with the correct attributes.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 263,
          "total_tokens": 366
        },
        "why_needed": "This test prevents regression where the `annotate` method does not return an expected LlmAnnotation-like object."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006822340000098848,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Expected the provider to return a non-empty result for an empty test code.', 'value': 'True'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 145,
          "total_tokens": 230
        },
        "why_needed": "To ensure the contract handles empty code gracefully and returns a valid result."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007166350000034072,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is_not_none', 'expected_value': 'None'}"
        ],
        "scenario": "Provider handles None context gracefully",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 148,
          "total_tokens": 217
        },
        "why_needed": "To ensure the provider can handle cases where `None` is passed as a context."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 15,
          "line_ranges": "65-66, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007298250000076223,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'All providers should have an annotate method.', 'expected_result': 'True'}",
          "{'message': 'Each provider should have a callable annotate method.', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 145,
          "total_tokens": 247
        },
        "why_needed": "To ensure that all providers have an annotate method."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 187,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 263-265, 299, 311-312, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524-525, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008640280000236089,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotate_handles_context', 'description': 'This function is expected to handle the context correctly and not raise an exception.'}",
          "{'name': 'context_size', 'description': 'The size of the context should be within a reasonable limit (e.g., 1024 bytes).'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 98,
          "total_tokens": 219
        },
        "why_needed": "The current implementation of annotate_handles_context may lead to a MemoryError when dealing with large contexts."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-195, 471-473, 497-498, 502-503, 537"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008158629999570621,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation message includes the correct error message for installing the required library.",
          "The annotation message does not include any misleading information about the installation process.",
          "The annotation message provides a clear and concise explanation of what needs to be done to resolve the issue.",
          "The test passes with an empty annotation object if no errors are found during the annotation process.",
          "The test fails with a non-empty annotation object if an error is found during the annotation process.",
          "The annotation object contains the correct information about the required library and its installation instructions.",
          "The annotation object does not contain any unnecessary or misleading information."
        ],
        "scenario": "Test that the LiteLLMProvider annotates a missing dependency correctly.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 270,
          "total_tokens": 451
        },
        "why_needed": "This test prevents the provider from reporting an error when a required library is not installed."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007705519999490207,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation does not throw an error when the `GEMINI_API_TOKEN` environment variable is not set.",
          "The annotation throws an error with a message indicating that the `GEMINI_API_TOKEN` environment variable is not set.",
          "The annotation correctly identifies the absence of an API token and returns it as the error message.",
          "The annotation does not throw any errors when the `GEMINI_API_TOKEN` environment variable is explicitly unset using `unsetenv` or other methods.",
          "The annotation throws a `ResourceExhausted` exception with a mock `GenerationFailure` instance, which is consistent with the expected behavior of the `annotate` method.",
          "The annotation correctly handles cases where the `GEMINI_API_TOKEN` environment variable is not set and returns an appropriate error message."
        ],
        "scenario": "Test that the `annotate` method of the `GeminiProvider` class throws an error when an API token is missing.",
        "token_usage": {
          "completion_tokens": 244,
          "prompt_tokens": 440,
          "total_tokens": 684
        },
        "why_needed": "This test prevents a potential bug where the `annotate` method fails to recognize the absence of an API token, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 220,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-455, 457, 459-460, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008889169999974911,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'json' key in the captured dictionary contains a valid JSON payload with the correct response data.",
          "The 'totalTokenCount' key in the captured dictionary matches the expected value of 123.",
          "The 'candidates' list in the captured dictionary contains at least one record with the correct content and metadata.",
          "The 'usageMetadata' object in the captured dictionary has a valid total token count.",
          "No exceptions are raised when calling `fake_get` or `fake_post` functions.",
          "The rate limits logic is correctly implemented and does not raise any exceptions."
        ],
        "scenario": "Verify that the `annotate` method records tokens on the limiter and rate limits correctly.",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 783,
          "total_tokens": 958
        },
        "why_needed": "Prevents regressions where token usage is not recorded or reported accurately."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009845499999983076,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked requests should be retried', 'description': 'Verify that the LLM is retried when a request exceeds the rate limit'}",
          "{'name': 'Llama model should not be skipped due to rate limit error', 'description': \"Verify that the Llama model is not skipped if it's already been used to generate an answer\"}"
        ],
        "scenario": "Tests for Gemini provider",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 98,
          "total_tokens": 219
        },
        "why_needed": "To handle retries on rate limit errors"
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 210,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526-527, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001012105999961932,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'model rotation', 'expected_result': 'models are rotated daily'}",
          "{'assertion_type': 'cache invalidation', 'expected_result': 'cache is invalidated daily'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 100,
          "total_tokens": 225
        },
        "why_needed": "To ensure that the LLM models are rotated on a daily basis and not cached for an extended period, which could lead to performance issues."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000931003999994573,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mocks', 'expected': ['mocks']}",
          "{'name': 'requests', 'expected': ['annotation_requests']}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 98,
          "total_tokens": 192
        },
        "why_needed": "Skips daily limit due to excessive annotation requests."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 209,
          "line_ranges": "39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009721220000074027,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect"
        ],
        "scenario": "Test that LiteLLM provider annotates a valid response with the correct information.",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 474,
          "total_tokens": 538
        },
        "why_needed": "Prevents regression by ensuring the provider correctly annotates responses from liteLLM."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 222,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001042379000011806,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Provider should be able to recover from exhaustion', 'description': 'After 24 hours, the provider should still be able to serve requests'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 104,
          "total_tokens": 198
        },
        "why_needed": "The LLM provider's model is exhausted after 24 hours. This test ensures the provider can recover and continue serving requests."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 68,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-349, 352-356, 358-361, 363-364, 366-367, 435, 437-439, 441-444, 449-452, 463-466, 476, 478, 497-498, 502-508, 511, 514-516, 518-521, 524-525, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007493250000152329,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"The response should contain a 'no_available_models' key\", 'value': \"{'scenario': ..., 'why_needed': ..., 'key_assertions': [...]}\", 'expected_value': \"{'scenario': ..., 'why_needed': ..., 'key_assertions': [...]\"}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 92,
          "total_tokens": 218
        },
        "why_needed": "To ensure that the `fetch_available_models` method returns an error when there are no available models."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 201,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010569759999725648,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Model list should be empty before refresh', 'value': [], 'expected_value': []}",
          "{'name': 'Model list should contain expected models after refresh', 'value': ['...'], 'expected_value': ['...']}"
        ],
        "scenario": "The model list is refreshed after an interval.",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 96,
          "total_tokens": 202
        },
        "why_needed": "To ensure the model list is updated correctly and consistently across tests."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 50,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009027140000057443,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the provider retries with a new token.",
          "Verify that the provider throws an exception for 401 error.",
          "Verify that the provider captures the captured keys.",
          "Verify that the provider increments call count correctly.",
          "Verify that the provider increments token count correctly.",
          "Verify that the provider returns a successful response after retrying with new token."
        ],
        "scenario": "Test that LiteLLM provider retries on 401 after refreshing token.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 580,
          "total_tokens": 706
        },
        "why_needed": "Reason for retrying on 401 error"
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_401_retry_with_token_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007363099999793121,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The error message is 'boom' which indicates a completion error.",
          "The annotation has an 'error' key with the value 'boom'.",
          "The annotation's error contains the string 'boom'."
        ],
        "scenario": "The test verifies that the LiteLLMProvider annotates completion errors correctly.",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 307,
          "total_tokens": 405
        },
        "why_needed": "This test prevents regression where the provider does not surface completion errors in annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 43,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 206, 211"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007744769999931123,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'response_data' dictionary must be a valid JSON object.",
          "The 'json.dumps(response_data)' expression should return a valid JSON string.",
          "The 'response_data' dictionary must contain exactly one key_assertion value.",
          "The 'key_assertion' value must be a list of strings or a single string.",
          "The 'key_assertions' value must not be empty."
        ],
        "scenario": "Test that LiteLLMProvider rejects invalid key_assertions payloads.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 346,
          "total_tokens": 491
        },
        "why_needed": "This test prevents the provider from silently failing when receiving an invalid key_assertions payload, making it easier to detect and diagnose issues."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 8,
          "line_ranges": "37-38, 41, 82-86"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000711843000033241,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.error == 'litellm not installed. Install with: pip install litellm'",
          "provider.annotate(test, 'def test_case(): assert True')",
          "test_case()",
          "assert True"
        ],
        "scenario": "The LiteLLMProvider should report a missing dependency when trying to annotate a case that depends on it.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 271,
          "total_tokens": 382
        },
        "why_needed": "This test prevents a potential bug where the provider does not handle cases that depend on missing dependencies correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000826926000002004,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect"
        ],
        "scenario": "Test that LiteLLMProvider annotates a successful response with the correct key assertions and confidence level.",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 475,
          "total_tokens": 540
        },
        "why_needed": "Prevents regression by verifying that the provider correctly handles successful responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008024169999885089,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'The `prompt_override` parameter is present in the config.', 'content': 'CUSTOM PROMPT'}",
          "{'message': 'The `prompt_override` value matches the expected custom prompt.', 'content': 'CUSTOM PROMPT'}",
          "{'message': 'The `messages` attribute of the fake completion function contains the correct key assertion.', 'content': 'a'}"
        ],
        "scenario": "Test that LiteLLMProvider uses prompt_override when provided.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 373,
          "total_tokens": 498
        },
        "why_needed": "To prevent a bug where the provider does not override prompts correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009219020000159617,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verifies that the annotation object has a 'token_usage' attribute.",
          "Checks if the 'prompt_tokens', 'completion_tokens', and 'total_tokens' attributes are correctly set.",
          "Asserts that the 'token_usage' attribute is not None.",
          "Ensures that the values of 'prompt_tokens', 'completion_tokens', and 'total_tokens' match the expected values.",
          "Verifies that the total tokens count matches the sum of prompt and completion tokens."
        ],
        "scenario": "Test the LiteLLMProvider's annotate method with token usage.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 426,
          "total_tokens": 575
        },
        "why_needed": "This test prevents regression in handling token usage data from LiteLLM providers."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182-183, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007647429999906308,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `litellm_api_base` is set to `https://proxy.corp.com/v1`.",
          "The `litellm_api_base` attribute is present in the response data.",
          "The `api_base` key is present in the captured dictionary.",
          "The value of `api_base` is equal to `https://proxy.corp.com/v1`.",
          "The `litellm_api_base` attribute is set correctly for the given configuration.",
          "No exception is raised when calling the completion function with the provided arguments."
        ],
        "scenario": "The LiteLLM provider passes the `litellm_api_base` attribute to the completion call of its API.",
        "token_usage": {
          "completion_tokens": 192,
          "prompt_tokens": 387,
          "total_tokens": 579
        },
        "why_needed": "This test prevents a regression where the provider does not set the `litellm_api_base` attribute correctly, causing unexpected behavior in downstream applications."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007782160000147087,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "- The API key is set to 'static-key-placeholder' in the captured dictionary.",
          "- The API key matches the expected value of 'static-key-placeholder'.",
          "- The API key is present in the captured dictionary."
        ],
        "scenario": "The test verifies that the LiteLLM provider passes a static API key to the completion call.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 384,
          "total_tokens": 492
        },
        "why_needed": "This test prevents regression where the API key is not passed through correctly, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 36,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 132-133, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007623079999916627,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `fake_completion` raises a `FakeAuthError` with message '401 Unauthorized'.",
          "The `LiteLLMProvider` instance is configured to use the 'gpt-4o' model, but no token refresh is provided.",
          "The provider returns an authentication error in its annotation."
        ],
        "scenario": "Test that the LiteLLM provider returns an auth error when no refresher is configured.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 338,
          "total_tokens": 463
        },
        "why_needed": "Prevents a bug where the provider does not return an authentication error for cases without token refresh."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_error_without_refresher",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 51,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 129-130, 132-133, 141-142, 170-174, 176-178, 182, 186-188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 31,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.001332625000032,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'error' attribute of the annotation is not None.",
          "The value of 'Authentication failed' in the 'error' attribute contains the string '401 Unauthorized'.",
          "The 'error' attribute contains a string that indicates an authentication failure, such as '401 Unauthorized'."
        ],
        "scenario": "The test verifies that the LiteLLMProvider reports an authentication error when retrying after a second failure.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 419,
          "total_tokens": 540
        },
        "why_needed": "This test prevents regression where the provider fails to report an authentication error on subsequent retries."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_retry_fails_on_second_attempt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000744197000017266,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error is not None",
          "assert 'Context too long for this model' in str(annotation)",
          "assert annotation.json is not None",
          "assert 'scenario' in annotation.json",
          "assert 'why_needed' in annotation.json",
          "assert 'key_assertions' in annotation.json",
          "assert 'error' in annotation.json",
          "assert 'Context too long for this model' in str(annotation.json)"
        ],
        "scenario": "The test verifies that the LiteLLMProvider class correctly handles a context too long error in its _parse_response method.",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 370,
          "total_tokens": 531
        },
        "why_needed": "This test prevents regression where the provider incorrectly raises an AuthenticationError when encountering a response with an invalid JSON structure."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_context_too_long_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227-228, 230-231"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007052870000165967,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is a dictionary', 'expected_value': {'max_tokens': 16384}, 'actual_value': '16384'}"
        ],
        "scenario": "test_get_max_context_tokens_dict_format",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 218,
          "total_tokens": 304
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns a dictionary in the expected format."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_dict_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227, 232-234"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007038040000111323,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'JSON structure', 'expected': {'scenario': '...', 'why_needed': '...', 'key_assertions': ['...']}, 'actual': {'scenario': '...', 'why_needed': '...', 'key_assertions': ['...']}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 101,
          "total_tokens": 211
        },
        "why_needed": "To ensure that the LLM provider returns a JSON response with the expected structure when an error occurs."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 9,
          "line_ranges": "37-38, 41, 221-222, 224, 227-229"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007511160000035488,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 8192, 'actual_value': 0}",
          "{'name': 'provider', 'expected_value': 'LiteLLMProvider', 'actual_value': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 213,
          "total_tokens": 337
        },
        "why_needed": "To ensure that the LiteLLM provider correctly returns the maximum number of context tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 6,
          "line_ranges": "37-38, 41, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006995750000555745,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'module_name': 'litellm'}, 'actual': '__import__('}",
          "message_type_error_message_id_or_key_not_found_0_1: module not found"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 160,
          "total_tokens": 262
        },
        "why_needed": "To ensure the liteLLM provider can detect installed modules."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 41,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008286030000022038,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert captured['api_key'] == 'dynamic-token-789',",
          "assert captured['provider'] == 'litellm',",
          "assert captured['model'] == 'gpt-4o',",
          "assert captured['token_refresh_command'] == 'get-token',",
          "assert captured['token_refresh_interval'] == 3600,",
          "assert captured['expected_token'] == None,",
          "assert captured['actual_token'] == 'dynamic-token-789',"
        ],
        "scenario": "Test: LiteLLM provider uses TokenRefresher for dynamic tokens.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 442,
          "total_tokens": 622
        },
        "why_needed": "The test prevents a scenario where the LLM provider fails to refresh its token due to an interval setting that is too short, causing it to run out of tokens before the next refresh cycle."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_token_refresh_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 42,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008207629999787969,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should not raise an exception when it encounters a transient error.",
          "The provider should retry the operation after a transient error.",
          "The number of retries should be limited to 5.",
          "The provider should only retry if the network error is transient (i.e., not due to other issues).",
          "The provider should not retry indefinitely when it encounters a transient error.",
          "The test should pass even if the provider fails to retry transient errors occasionally."
        ],
        "scenario": "Test that the LiteLLMProvider retries on transient errors.",
        "token_usage": {
          "completion_tokens": 155,
          "prompt_tokens": 426,
          "total_tokens": 581
        },
        "why_needed": "This test prevents a regression where the provider fails to retry transient errors, causing the test to fail intermittently."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_transient_error_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 70,
          "line_ranges": "65-66, 87-89, 97-99, 101, 103, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 243, 245, 264, 266-267, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 27,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 85-86, 92, 138, 140, 142-144, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008326990000000478,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected_exception', 'type': 'Exception'}",
          "{'name': 'message', 'type': 'str'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 103,
          "total_tokens": 186
        },
        "why_needed": "To ensure that the LLM provider correctly handles context length errors during annotation."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 18,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 97-98, 100-101, 103-104"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007333449999578079,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The error message returned by the annotate method is 'Failed after 2 retries. Last error: boom'.",
          "The function `test_case` was not executed successfully.",
          "The last error raised during the annotation attempt is 'boom'."
        ],
        "scenario": "The test verifies that the annotate method of OllamaProvider returns an error message when a call to the annotated function fails.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 347,
          "total_tokens": 471
        },
        "why_needed": "This test prevents regression in case where the annotation fails due to a timeout or other unforeseen reason during the retry process."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "42-46"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007313560000170582,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'httpx not installed. Install with: pip install httpx'",
          "provider.annotate(test, \"def test_case(): assert True\")"
        ],
        "scenario": "The Ollama provider reports missing httpx dependency when annotating a test case.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 268,
          "total_tokens": 365
        },
        "why_needed": "This test prevents a bug where the provider incorrectly assumes that httpx is installed and fails to report an error."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 13,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 96"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007201309999800287,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected_exception', 'type': 'Exception'}",
          "{'name': 'expected_message', 'type': 'RuntimeError'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_runtime_error_immediate_fail",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 99,
          "total_tokens": 195
        },
        "why_needed": "To test the immediate failure of annotating runtime errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_runtime_error_immediate_fail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007921249999753854,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "check status",
          "validate token"
        ],
        "scenario": "Test that annotating a full flow of an Ollama provider with mocked HTTP returns the expected annotations.",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 414,
          "total_tokens": 482
        },
        "why_needed": "Prevents auth bugs by ensuring the correct status and token are returned in case of errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52-53, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008113889999776802,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should have no error.",
          "The captured messages should contain the custom prompt.",
          "The content of the captured message should be 'CUSTOM PROMPT'.",
          "The key assertion 'a' should match the expected value in the captured message."
        ],
        "scenario": "Test that LiteLLMProvider overrides the prompt when provided with a custom prompt.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 373,
          "total_tokens": 486
        },
        "why_needed": "To prevent bugs where LiteLLM provider does not override the prompt and instead uses the default one."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 40,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71, 74-80, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007999360000212619,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation object contains the correct number of prompt tokens, completion tokens, and total tokens.",
          "The token usage is not None.",
          "The prompt tokens are equal to 100.",
          "The completion tokens are equal to 50.",
          "The total tokens are equal to 150."
        ],
        "scenario": "Test LiteLLM provider annotates token usage for a case with a specific configuration and test result.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 426,
          "total_tokens": 561
        },
        "why_needed": "This test prevents regression that would occur if the LiteLLMProvider's annotate method does not correctly handle cases where there is no response or incomplete response."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007542769999986376,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'response' key in the result dictionary should be equal to 'test response'.",
          "The 'url' key in the captured dictionary should contain the string 'http://localhost:11434/api/generate'.",
          "The 'json' key in the captured dictionary should have a 'model' value of 'llama3.2:1b'.",
          "The 'json' key in the captured dictionary should have a 'prompt' value of 'test prompt'.",
          "The 'json' key in the captured dictionary should have a 'system' value of 'system prompt'.",
          "The 'stream' key in the captured dictionary should be False.",
          "The 'timeout' key in the captured dictionary should be equal to 60 seconds."
        ],
        "scenario": "Test Ollama provider makes correct API call to generate response with specified model, prompt, and system prompt.",
        "token_usage": {
          "completion_tokens": 226,
          "prompt_tokens": 470,
          "total_tokens": 696
        },
        "why_needed": "This test prevents regression where the Ollama provider fails to make a successful API call to generate a response."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007352680000280998,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `model` in the captured JSON response should be 'llama3.2'.",
          "The value of `model` in the captured JSON response should not be empty.",
          "The value of `model` in the captured JSON response is a string, not None or an empty string."
        ],
        "scenario": "Test that the Ollama provider uses the default model when not specified.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 344,
          "total_tokens": 464
        },
        "why_needed": "This test prevents a regression where the default model is used instead of the provided model."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 6,
          "line_ranges": "113-114, 116-117, 119-120"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006796090000307231,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider._check_availability() should return False', 'description': \"The provider's check_availability method should return False when the server is not available.\"}"
        ],
        "scenario": "Test Ollama Provider",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 183,
          "total_tokens": 264
        },
        "why_needed": "To test the failure case when the server is unavailable."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007543600000303741,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected False', 'expected_value': False, 'type': 'assertion'}",
          "{'message': 'OllamaProvider._check_availability() returned True instead of False.', 'expected_value': True, 'type': 'exception'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 197,
          "total_tokens": 315
        },
        "why_needed": "To test the Ollama provider's check_availability method with a non-200 status code."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007340330000147333,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method _check_availability() of the OllamaProvider instance should return True.",
          "The status code of the /api/tags endpoint should be 200.",
          "The URL '/api/tags' is present in the request.",
          "No exception is raised when making a GET request to the /api/tags endpoint."
        ],
        "scenario": "Test checks availability of Ollama provider when it returns a successful response.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 296,
          "total_tokens": 419
        },
        "why_needed": "This test prevents regression where the provider fails to return a successful response when available."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000754470999993373,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context length should be an integer', 'expected_value': 100, 'actual_value': 123}",
          "{'name': 'Context length should not exceed the maximum allowed value', 'expected_value': 100, 'actual_value': 150}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 99,
          "total_tokens": 205
        },
        "why_needed": "To ensure the correct behavior of the Ollama provider when retrieving context length from the LLM."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_context_length_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 11,
          "line_ranges": "138, 140, 142-147, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006887449999908313,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected a fallback to use max_context_tokens', 'description': 'The maximum context tokens should be used when an error occurs'}"
        ],
        "scenario": "Tests for OLLAMA provider",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 101,
          "total_tokens": 179
        },
        "why_needed": "To handle errors that may occur during the execution of the model"
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00074470200001997,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 1024, 'actual_value': 0}",
          "{'name': 'context_token_count', 'expected_value': 512, 'actual_value': 0}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 99,
          "total_tokens": 212
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns the correct maximum context tokens for a given model info."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_model_info",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 15,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 158, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001324189000001752,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 32, 'actual_value': 16}",
          "{'name': 'parameter_count', 'expected_value': 2, 'actual_value': 1}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 97,
          "total_tokens": 203
        },
        "why_needed": "To ensure the Ollama provider can correctly retrieve the maximum context tokens from its parameters."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_parameters",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 10,
          "line_ranges": "138, 140, 142-147, 149, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007116129999644727,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The response from the API call is not an object.",
          "The 'max_context_tokens' key in the response is not present or its value is empty."
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 101,
          "total_tokens": 192
        },
        "why_needed": "To ensure that the LLM provider returns a valid response when the API call to get max context tokens fails with a non-200 status code."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_non_200_status",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 1,
          "line_ranges": "128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006947729999637886,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of OllamaProvider', 'expected_value': 'True'}",
          "{'name': 'is_local() method returns True for OllamaProvider instances', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 123,
          "total_tokens": 239
        },
        "why_needed": "To ensure that the Ollama provider always returns `is_local=True`."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "65-66, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006969920000301499,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotation.error', 'value': 'Failed to parse LLM response as JSON'}"
        ],
        "scenario": "Ollama provider reports invalid JSON responses",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 138,
          "total_tokens": 211
        },
        "why_needed": "To ensure the Ollama provider correctly handles and reports invalid JSON responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007234979999566349,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Invalid response', 'message': \"The provided 'key_assertions' value is not a list.\"}"
        ],
        "scenario": "{'id': 1, 'description': 'Test case for Ollama provider with invalid key assertions'}",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 174,
          "total_tokens": 317
        },
        "why_needed": "This test is necessary to ensure the Ollama provider rejects invalid key assertions in its responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007450030000200059,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected JSON format', 'description': 'The extracted JSON should be in the expected format.'}",
          "{'name': 'Expected key assertion', 'description': 'The provider should assert that it has found a specific key in the JSON object.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 127,
          "total_tokens": 250
        },
        "why_needed": "To ensure that the Ollama provider correctly extracts JSON from markdown code fences."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00069779299997208,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected response type to be dict', 'value': 'expected response type is a dictionary'}",
          "{'name': 'Expected keys in response', 'value': \"keys in the response should include 'scenario', 'why_needed', and 'key_assertions'\"}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 128,
          "total_tokens": 255
        },
        "why_needed": "To test the functionality of extracting JSON from plain markdown fences (no language)."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008313330000078167,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert a",
          "assert b"
        ],
        "scenario": "Test Ollama provider parses valid JSON responses and verifies correct key assertions.",
        "token_usage": {
          "completion_tokens": 57,
          "prompt_tokens": 292,
          "total_tokens": 349
        },
        "why_needed": "Prevents bugs that may occur when parsing invalid or malformed JSON responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006962799999996605,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `distribute_token_budget` should return an allocation dictionary with 'small.py' as the key and 10 as its value.",
          "The function `distribute_token_budget` should return an allocation dictionary with 'large.py' as the key and between 30 and 45 (inclusive) as its value.",
          "The total allocated tokens for both files should be less than or equal to the total budget of 60.",
          "The remaining tokens after allocating to 'small.py' should be greater than or equal to the remaining budget of 44.",
          "The allocation of 'large.py' should not exceed the remaining budget of 44."
        ],
        "scenario": "Verify water-fill algorithm satisfies smaller files first.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 396,
          "total_tokens": 589
        },
        "why_needed": "This test prevents a potential bug where the algorithm does not satisfy the constraint of smaller files first, leading to inefficient allocation of tokens."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_constrained",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 2,
          "line_ranges": "42-43"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000684431999957269,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert an empty dictionary is returned when {} is passed to distribute_token_budget(100)', 'expected_output': {}, 'actual_output': 'tests/test_llm_utils.py::test_distribute_token_budget_empty'}",
          "{'name': \"assert an empty dictionary is returned when {'f1': 'c'} is passed to distribute_token_budget(0)\", 'expected_output': {}, 'actual_output': 'tests/test_llm_utils.py::test_distribute_token_budget_empty'}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
        "token_usage": {
          "completion_tokens": 156,
          "prompt_tokens": 115,
          "total_tokens": 271
        },
        "why_needed": "Verify behavior with empty input or no budget."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006815330000335962,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The allocation for `l1.py` should be between 35 and 50 tokens.",
          "The allocation for `l2.py` should also be between 35 and 50 tokens.",
          "The absolute difference in allocations for `l1.py` and `l2.py` should not exceed 1 token."
        ],
        "scenario": "Verify fair sharing when neither fits.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 327,
          "total_tokens": 448
        },
        "why_needed": "Prevents regression in case where both files are too large to fit within the budget, causing unfair distribution of token budgets."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_fair_share",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007069740000247293,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'length of allocations is equal to 3', 'expected_value': 3, 'actual_value': 0}",
          "{'name': 'number of files allocated is less than or equal to max_files', 'expected_value': 3, 'actual_value': 0}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 133,
          "total_tokens": 254
        },
        "why_needed": "Verify the limit of max_files in token budget distribution."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006986779999920145,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of allocations should match the total needed for each file (32 tokens per file).",
          "Each allocation should contain exactly 10 tokens (40 characters in 'f1.py' and 40 characters in 'f2.py')."
        ],
        "scenario": "Verify that all files get full content when the token budget is sufficient.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 332,
          "total_tokens": 446
        },
        "why_needed": "This test prevents a potential regression where the LLM might not be able to handle large budgets without running out of tokens or headers."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_sufficient",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007168209999690589,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert estimate_tokens('') == 1",
          "assert estimate_tokens('a') == 1",
          "assert estimate_tokens('aaaa') == 1",
          "assert estimate_tokens('aaaa' * 10) == 10"
        ],
        "scenario": "Verify the rough token estimation (chars / 4) for an empty string.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 217,
          "total_tokens": 320
        },
        "why_needed": "Prevents a potential division by zero error when estimating tokens."
      },
      "nodeid": "tests/test_llm_utils.py::test_estimate_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "263-266"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007065879999572644,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert d['file_path'] == 'src/foo.py'",
          "assert d['line_ranges'] == '1-3, 5, 10-15'",
          "assert d['line_count'] == 10",
          "The file path is correctly serialized as 'src/foo.py'.",
          "The line ranges are correctly serialized as '1-3, 5, 10-15'.",
          "The line count is correctly serialized as 10.",
          "CoverageEntry object has been successfully converted to a dictionary."
        ],
        "scenario": "The test verifies that a CoverageEntry object can be successfully converted to a dictionary.",
        "token_usage": {
          "completion_tokens": 165,
          "prompt_tokens": 255,
          "total_tokens": 420
        },
        "why_needed": "This test prevents the regression of coverage data not being properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestArtifactEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 3,
          "line_ranges": "241-243"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007084189999773116,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key should contain the string 'src/foo.py'.",
          "The 'line_ranges' key should contain a comma-separated list of strings representing the range of lines covered by the entry. The expected format is '1-3, 5, 10-15'.",
          "The 'line_count' key should contain an integer value equal to the number of lines in the coverage entry.",
          "Each assertion checks that the values returned by `to_dict()` match the expected values."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` returns the expected dictionary structure for a CoverageEntry object.",
        "token_usage": {
          "completion_tokens": 183,
          "prompt_tokens": 255,
          "total_tokens": 438
        },
        "why_needed": "This test prevents a potential bug where the `to_dict()` method of `CoverageEntry` does not return the correct dictionary structure, potentially causing unexpected behavior or errors in downstream code."
      },
      "nodeid": "tests/test_models.py::TestCollectionError::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "65-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006837189999941984,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the serialized dictionary matches the expected value.",
          "The 'line_ranges' key in the serialized dictionary matches the expected format.",
          "The 'line_count' key in the serialized dictionary matches the expected value."
        ],
        "scenario": "The `CoverageEntry` class should serialize correctly to a dictionary.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 255,
          "total_tokens": 370
        },
        "why_needed": "This test prevents a potential bug where the serialization of `CoverageEntry` objects is not accurate, potentially leading to incorrect coverage data being reported."
      },
      "nodeid": "tests/test_models.py::TestCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007119759999909547,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.scenario == \"\" (empty string)",
          "annotation.why_needed == \"\" (empty string) (default value for LlmAnnotation)",
          "annotation.key_assertions == [] (no key assertions are performed on an empty annotation)",
          "assert annotation.confidence is None (expected confidence to be None for an empty annotation)",
          "assert annotation.error is None (expected error to be None for an empty annotation)"
        ],
        "scenario": "An empty annotation should be created with default values.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 212,
          "total_tokens": 351
        },
        "why_needed": "This test prevents a bug where an empty annotation is not properly initialized with default values."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 9,
          "line_ranges": "130-133, 135, 137, 139, 141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007037419999846861,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' key should be present in the dictionary.",
          "The 'why_needed' key should be present in the dictionary.",
          "The 'key_assertions' key should be present in the dictionary.",
          "The 'confidence' key should not be present in the dictionary when it is None."
        ],
        "scenario": "The test verifies that the `LlmAnnotation` object can be serialized into a dictionary with required fields.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 230,
          "total_tokens": 357
        },
        "why_needed": "This test prevents regression by ensuring that the minimal annotation is properly serialized and includes all necessary information."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "130-133, 135-137, 139-141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006923239999991893,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts that the 'scenario' field is present and matches the expected value.",
          "Asserts that the 'confidence' field has a value within the expected range (0.0 to 1.0).",
          "Asserts that the 'context_summary' field contains the expected keys ('mode' and 'bytes') with correct values.",
          "Asserts that all required fields are present in the resulting dictionary.",
          "Asserts that the 'error' field is absent or None, as per the test's expectation."
        ],
        "scenario": "Test that the `to_dict` method returns all required fields for a full annotation.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 284,
          "total_tokens": 450
        },
        "why_needed": "Prevents regression in LLMAnnotation class, where some fields are missing or incomplete."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007329819999881693,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' key should be present in the dictionary with value equal to SCHEMA_VERSION.",
          "The 'tests' key should be an empty list.",
          "The 'warnings' key should not be present in the dictionary.",
          "The 'collection_errors' key should not be present in the dictionary.",
          "If the schema version is missing, a KeyError should be raised when trying to access it.",
          "If the tests are missing, a KeyError should be raised when trying to access them.",
          "If the warnings or collection errors lists are non-empty, an AssertionError should be raised."
        ],
        "scenario": "Test Default Report",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 231,
          "total_tokens": 401
        },
        "why_needed": "Prevents a potential bug where the default report is missing required schema version and empty test lists."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_default_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007115529999737191,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report should contain exactly one collection error.",
          "The first collection error should be for the file \"test_bad.py\".",
          "The node ID of the collection error should match the provided node ID in the ReportRoot constructor."
        ],
        "scenario": "Test ReportRoot::test_report_with_collection_errors verifies that the test reports collection errors properly.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 237,
          "total_tokens": 340
        },
        "why_needed": "This test prevents a regression where collection errors are not reported correctly."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007001810000133446,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Length of warnings list', 'expected': 1, 'actual': 0}",
          "{'name': 'Code in first warning', 'expected': 'W001', 'actual': 'No coverage'}"
        ],
        "scenario": "TestReportRoot::test_report_with_warnings",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 144,
          "total_tokens": 249
        },
        "why_needed": "The test is needed to ensure that the ReportRoot class correctly handles warnings."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 73,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007005200000094192,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'a_test.py::test_a', 'actual_value': 'z_test.py::test_z'}",
          "{'assertion_type': 'contains', 'expected_value': 'm_test.py::test_m', 'actual_value': 'z_test.py::test_z'}"
        ],
        "scenario": "Tests should be sorted by nodeid in output.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 215,
          "total_tokens": 354
        },
        "why_needed": "Because the current implementation does not sort tests by nodeid, which can lead to incorrect test results if multiple tests have the same nodeid."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006556660000001102,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': '/path/to/file'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 131,
          "total_tokens": 223
        },
        "why_needed": "The test is necessary because it checks the `to_dict` method of `ReportWarning` class which might raise a warning if the detail attribute is missing."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006664040000146088,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The warning should be excluded from the dictionary.",
          "The warning message should not contain any details.",
          "The 'detail' key should not exist in the warning dictionary.",
          "The warning code should still match the expected value.",
          "The warning message should remain unchanged."
        ],
        "scenario": "Test 'test_to_dict_without_detail' verifies that a ReportWarning object is created without detail.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 223,
          "total_tokens": 345
        },
        "why_needed": "This test prevents the creation of a ReportWarning object with detail, which could lead to unexpected behavior or errors."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_without_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 39,
          "line_ranges": "286-288, 290-292, 376-392, 394, 397, 399, 402, 405, 407, 409, 411-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007060370000431249,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert d['run_id'] == 'run-123'",
          "assert d['run_group_id'] == 'group-456'",
          "assert d['is_aggregated'] is True",
          "assert d['aggregation_policy'] == 'merge'",
          "assert d['run_count'] == 3",
          "assert len(d['source_reports']) == 2"
        ],
        "scenario": "Verify that RunMeta has aggregation fields.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 343,
          "total_tokens": 471
        },
        "why_needed": "Prevent regression where RunMeta is missing aggregation fields, potentially leading to incorrect aggregation results."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_aggregation_fields_present",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000664294000046084,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_annotations_enabled' key is present in the data dictionary.",
          "The 'llm_provider' key is not present in the data dictionary.",
          "The 'llm_model' key is not present in the data dictionary."
        ],
        "scenario": "Test LLM fields are excluded when annotations are disabled.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 232,
          "total_tokens": 329
        },
        "why_needed": "To prevent regression and ensure consistent behavior when annotations are disabled."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 43,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419-431, 433, 435, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006924469999489702,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "data['llm_annotations_enabled'] is True",
          "data['llm_provider'] == 'ollama'",
          "data['llm_model'] == 'llama3.2:1b'",
          "data['llm_context_mode'] == 'complete'",
          "data['llm_annotations_count'] == 10",
          "data['llm_annotations_errors'] == 2"
        ],
        "scenario": "Verify that LLM traceability fields are included when enabled.",
        "token_usage": {
          "completion_tokens": 160,
          "prompt_tokens": 327,
          "total_tokens": 487
        },
        "why_needed": "This test prevents regression by ensuring that the LLM traceability fields are properly set when running with llm_provider='ollama' and llm_model='llama3.2:1b'."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_traceability_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00068310400001792,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Non-aggregated report should not include source_reports', 'expected_result': {'source_reports': [], 'is_aggregated': False}}"
        ],
        "scenario": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 130,
          "total_tokens": 224
        },
        "why_needed": "The test is necessary because it checks if a non-aggregated report excludes source reports."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 49,
          "line_ranges": "286-288, 290-292, 376-392, 394-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007196910000288881,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that 'git_sha' is set to 'abc1234'.",
          "Verify that 'git_dirty' is True.",
          "Verify that 'repo_version' is set to '1.0.0'.",
          "Verify that 'repo_git_sha' is set to 'abc1234'.",
          "Verify that 'repo_git_dirty' is False.",
          "Verify that 'plugin_git_sha' is set to 'def5678'.",
          "Verify that 'plugin_git_dirty' is False.",
          "Verify that 'config_hash' is set to 'def5678'.",
          "Verify the length of 'source_reports' is 1."
        ],
        "scenario": "Test RunMeta to dict with all optional fields.",
        "token_usage": {
          "completion_tokens": 188,
          "prompt_tokens": 483,
          "total_tokens": 671
        },
        "why_needed": "Prevents regression where RunMeta's optional fields are not properly populated in the dictionary."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006768090000264237,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' attribute of the 'RunMeta' object should be equal to 1.",
          "The 'interrupted' attribute of the 'RunMeta' object should be True.",
          "The 'collect_only' attribute of the 'RunMeta' object should be True.",
          "The 'collected_count' attribute of the 'RunMeta' object should be equal to 10.",
          "The 'selected_count' attribute of the 'RunMeta' object should be equal to 8.",
          "The 'deselected_count' attribute of the 'RunMeta' object should be equal to 2."
        ],
        "scenario": "Test the 'RunMeta' object's run status fields.",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 285,
          "total_tokens": 481
        },
        "why_needed": "This test prevents a potential bug where the 'RunMeta' object is not properly initialized with all required fields, leading to incorrect or missing data in its attributes."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006906449999632969,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"schema_version.split('.').should.have.length.equal.to(3)\", 'message': \"The schema version should have exactly 3 parts (e.g., '1.2.3')\", 'type': 'assertion'}",
          "{'name': 'each part.should.be.a.digit', 'message': 'Each part of the schema version should be a digit (0-9)', 'type': 'assertion'}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 115,
          "total_tokens": 277
        },
        "why_needed": "To ensure the schema version is in semver format, which is a standard way of expressing software compatibility and change history."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008499070000311804,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'ReportRoot.schema_version', 'expected_value': 'SCHEMA_VERSION'}",
          "{'name': 'report.to_dict().schema_version', 'expected_value': 'SCHEMA_VERSION'}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 119,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the ReportRoot includes the schema version in its JSON representation."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "96-103"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007007739999949081,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key should be present and contain the expected value.",
          "The 'line_ranges' key should be present and contain the expected string.",
          "The 'line_count' key should be present and contain the expected integer value.",
          "All assertions should pass for the `CoverageEntry` object to be considered valid."
        ],
        "scenario": "Test coverage entry serialization.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 256,
          "total_tokens": 373
        },
        "why_needed": "This test prevents a bug where the `CoverageEntry` object is not correctly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestSourceCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "286-288, 290, 292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006652700000131517,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in d: The expected field is present in the output dictionary.",
          "assert 'why_needed' in d: The expected field is present in the output dictionary.",
          "assert 'key_assertions' in d: The expected field is present in the output dictionary.",
          "assert 'confidence' not in d: The unexpected field is not present in the output dictionary."
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 229,
          "total_tokens": 372
        },
        "why_needed": "This test prevents a potential bug where the minimal annotation is missing some required fields."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "286-288, 290-292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006536129999972218,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"Expected 'run_id' key in source object\", 'value': 'run-1'}"
        ],
        "scenario": "TestSourceReport to_dict_with_run_id",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 134,
          "total_tokens": 212
        },
        "why_needed": "To ensure SourceReport objects are properly serialized and can be easily converted back into a dictionary."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006783209999525752,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should be equal to 'src/foo.py'.",
          "The 'line_ranges' key in the dictionary should be equal to '1-3, 5, 10-15'.",
          "The 'line_count' key in the dictionary should be equal to 10.",
          "The 'coverage_data' key (if present) should not be missing from the dictionary.",
          "Any additional keys or values in the dictionary should match the expected format.",
          "The 'file_path' value should start with a forward slash '/' and end with a backslash '\\'.",
          "The 'line_ranges' value should contain valid ranges separated by commas (e.g., '1-3, 5, 10-15').",
          "The 'line_count' value should be an integer.",
          "Any other unexpected values in the dictionary should raise an AssertionError."
        ],
        "scenario": "Test the `to_dict` method of `CoverageEntry` class.",
        "token_usage": {
          "completion_tokens": 247,
          "prompt_tokens": 254,
          "total_tokens": 501
        },
        "why_needed": "This test prevents a potential bug where the `to_dict` method does not correctly serialize the coverage entry data."
      },
      "nodeid": "tests/test_models.py::TestSummary::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006850810000287311,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field should be present in the result dictionary.",
          "The 'outcome' field should be present in the result dictionary.",
          "The 'duration' field should be present in the result dictionary.",
          "The 'phase' field should be present in the result dictionary."
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 244,
          "total_tokens": 356
        },
        "why_needed": "This test prevents a regression where the minimal result is missing required fields."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 24,
          "line_ranges": "65-68, 190, 194-199, 201, 203, 205, 207, 210-212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006970549999891773,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `coverage` key should be present and contain exactly one entry.",
          "The `file_path` attribute of the first coverage entry should match the nodeid of the test.",
          "All file paths in the coverage list should be relative to the source directory.",
          "Each coverage entry should have a `line_ranges` attribute with values '1-5' and a `line_count` attribute equal to 5.",
          "The total number of lines covered by code in the coverage list should match the total number of lines in the test file.",
          "All line ranges in the coverage list should be contiguous (i.e., no gaps between them).",
          "No coverage entries should have a `line_ranges` attribute with values that are not '1-5'."
        ],
        "scenario": "Test `test_result_with_coverage` verifies that the `TestCaseResult` object includes a coverage list.",
        "token_usage": {
          "completion_tokens": 226,
          "prompt_tokens": 256,
          "total_tokens": 482
        },
        "why_needed": "This test prevents regressions where the coverage is not included in the result, potentially leading to incorrect reporting of code coverage."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006872469999734676,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert llm_opt_out is True in result dictionary', 'expected': True, 'actual': 'is True'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 145,
          "total_tokens": 242
        },
        "why_needed": "To ensure that the `llm_opt_out` flag is correctly set for test results with LLM opt-out."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203, 205, 207-210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006788930000425353,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'field_name': 'rerun_count', 'expected_value': 2, 'actual_value': \"assert d['rerun_count'] == 2\"}",
          "{'field_name': 'final_outcome', 'expected_value': 'passed', 'actual_value': \"assert d['final_outcome'] == 'passed'\"}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 162,
          "total_tokens": 289
        },
        "why_needed": "The test case result should include rerun fields."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007174149999968904,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Result without reruns excludes rerun fields', 'description': 'The `result` object should not contain the following fields: rerun_count, rerun_message'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 152,
          "total_tokens": 280
        },
        "why_needed": "This test is needed because the `result` object contains fields that are specific to reruns (e.g. 'rerun_count', 'rerun_message') which should be excluded in a result without reruns."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "96-103, 241-243, 263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000740065999991657,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result['llm_opt_out'] == True",
          "assert len(result['coverage']) == 1",
          "assert result['llm_annotation']['scenario'] == 'Tests foo'",
          "assert result['captured_stdout'] == 'stdout content'",
          "assert result['captured_stderr'] == 'stderr content'"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 454,
          "total_tokens": 599
        },
        "why_needed": "Prevents bar because llm_opt_out is True and llm_context_override is set to 'complete'. Without these, the test would fail due to missing coverage."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 59,
          "line_ranges": "263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530-532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007057210000311898,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of 'artifacts' in the result should be 2.",
          "The path of the first artifact ('report.html') should match its expected value.",
          "All artifacts should have their paths included in the result dictionary."
        ],
        "scenario": "Test to_dict includes artifacts when set.",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 264,
          "total_tokens": 363
        },
        "why_needed": "This test prevents a regression where the 'to_dict' method does not include all required artifacts."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_artifacts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007073070000274129,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `result['collection_errors']` is 1 and its first element's 'nodeid' is `broken_test.py`.",
          "The value of `result['collection_errors'][0]` is an instance of `CollectionError` with a non-empty `nodeid` attribute.",
          "The value of `result['collection_errors'][0]['nodeid']` matches the expected value `broken_test.py`."
        ],
        "scenario": "Test to_dict includes collection_errors when set.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 243,
          "total_tokens": 387
        },
        "why_needed": "Prevents a potential bug where the test reports collection errors without checking if they are actually present in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534-536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006997219999789195,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'custom_metadata' key should be present in the result dictionary with the correct values.",
          "The 'project' value of 'custom_metadata' should match the provided value.",
          "The 'environment' value of 'custom_metadata' should match the provided value.",
          "The 'build_number' value of 'custom_metadata' should match the provided value.",
          "The custom metadata dictionary should not be empty after calling to_dict.",
          "The report root object passed to to_dict() should have a 'custom_metadata' attribute with the correct values."
        ],
        "scenario": "Test to_dict includes custom_metadata when set.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 264,
          "total_tokens": 431
        },
        "why_needed": "This test prevents a bug where the custom metadata is not included in the report even though it was provided."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_custom_metadata",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000717486999974426,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'signature123'}"
        ],
        "scenario": "Test to_dict includes hmac_signature when set.",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 128,
          "total_tokens": 221
        },
        "why_needed": "HMAC signature is required for the report to be considered valid."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536-538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007056339999849115,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'value': 'abcdef1234567890'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 131,
          "total_tokens": 214
        },
        "why_needed": "To ensure that the ReportRoot class correctly includes a sha256 value in its dictionary representation."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 63,
          "line_ranges": "96-103, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532-534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007166490000258818,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'source_coverage' key is present in the result dictionary.",
          "The value of 'source_coverage' is a list containing exactly one element.",
          "The first element of 'source_coverage' has the correct 'file_path' attribute.",
          "The 'file_path' attribute matches the expected value 'src/mod.py'.",
          "The coverage percentage and ranges are correctly calculated."
        ],
        "scenario": "Test to_dict includes source_coverage when set.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 282,
          "total_tokens": 408
        },
        "why_needed": "Prevents test failure due to missing source coverage information in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006861170000433958,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"len(result['warnings']) == 1\", 'expected': 1, 'actual': 0}",
          "{'name': \"result['warnings'][0]['code'] == 'W001'\", 'expected': 'W001', 'actual': ''}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 151,
          "total_tokens": 278
        },
        "why_needed": "This test is needed because the `to_dict` method of ReportRoot includes warnings when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 12,
          "line_ranges": "467-475, 477-479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006702889999701256,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'coverage_total_percent', 'expected_value': 85.5, 'actual_value': 85.5}"
        ],
        "scenario": "Test to_dict includes coverage_total_percent when set",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 153,
          "total_tokens": 269
        },
        "why_needed": "The test is needed because it checks if the `to_dict` method of `Summary` class correctly returns the total coverage percentage."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_with_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007849309999983234,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': {'coverage_total_percent': None}}"
        ],
        "scenario": "Test to_dict excludes coverage_total_percent when None.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 131,
          "total_tokens": 252
        },
        "why_needed": "The test is needed because it checks the behavior of `summary.to_dict()` when `coverage_total_percent` is `None`. Without this test, the test might fail due to unexpected behavior in other tests."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_without_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 42,
          "line_ranges": "65-68, 130-133, 135, 137, 139, 141, 143, 190, 194-199, 201-207, 210-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007280070000206251,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "param_id should be 'a-b-c',",
          "param_summary should contain 'a=1, b=2, c=3',",
          "captured_stdout and captured_stderr should match 'stdout content' and 'stderr content respectively',",
          "requirements list should include 'REQ-100',",
          "llm_opt_out should be True,",
          "llm_context_override should be 'complete',",
          "coverage length should be 1 (only one entry),"
        ],
        "scenario": "Test to_dict includes all optional fields when set.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 454,
          "total_tokens": 597
        },
        "why_needed": "Prevents bar regression in coverage analysis."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220-222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006983570000329564,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected': 'captured_stderr', 'actual': 'Error output here'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 149,
          "total_tokens": 236
        },
        "why_needed": "to include captured_stderr in the result dictionary when it is set to True."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218-220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006857129999957579,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': {'captured_stdout': 'Debug output here'}}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 149,
          "total_tokens": 232
        },
        "why_needed": "to cover the scenario where captured stdout is needed for accurate test results"
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203-207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000711238000008052,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 163,
          "total_tokens": 275
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_param_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007032869999648028,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"result['requirements'] == ['REQ-001', 'REQ-002]\", 'expected_value': ['REQ-001', 'REQ-002']}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 151,
          "total_tokens": 256
        },
        "why_needed": "This test is necessary because it checks if the `to_dict` method of `TestCaseResult` includes requirements when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006923219999634966,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `llm_context_exclude_globs` is called and its return value is checked.",
          "The string `*.pyc` is found in the returned list of default exclude globs.",
          "The string `__pycache__/*` is found in the returned list of default exclude globs.",
          "The string `*secret*` is found in the returned list of default exclude globs.",
          "The string `*password*` is found in the returned list of default exclude globs.",
          "The function returns a list with all expected strings.",
          "The function does not return an empty list when no exclude globs are specified.",
          "The function does not raise any exceptions when encountering unknown files or directories."
        ],
        "scenario": "Test the default exclude globs for LLMContext.",
        "token_usage": {
          "completion_tokens": 202,
          "prompt_tokens": 222,
          "total_tokens": 424
        },
        "why_needed": "Prevents a potential bug where default exclude globs are not correctly identified."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007160879999901226,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The pattern '--password' is present in the default redact patterns.",
          "The pattern '--token' is present in the default redact patterns.",
          "The pattern '--api[_-]?key' is present in the default redact patterns."
        ],
        "scenario": "Verifies that default redact patterns include sensitive information.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 228,
          "total_tokens": 333
        },
        "why_needed": "Prevents a potential security vulnerability where sensitive information like passwords and tokens are not properly redacted."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007770240000013473,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.llm_context_mode == 'minimal'",
          "cfg.llm_max_tests == 0",
          "cfg.llm_max_retries == 10",
          "cfg.llm_context_bytes == 32000",
          "cfg.llm_context_file_limit == 10",
          "cfg.llm_requests_per_minute == 5",
          "cfg.llm_timeout_seconds == 30",
          "cfg.llm_cache_ttl_seconds == 86400",
          "cfg.include_phase == 'run'",
          "cfg.aggregate_policy == 'latest'",
          "not cfg.is_llm_enabled() is True"
        ],
        "scenario": "Test that default values are set correctly.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 318,
          "total_tokens": 498
        },
        "why_needed": "Prevents a regression where the default values of Config are not properly initialized."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000695744999973158,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg is an instance of Config', 'expected': 'True'}",
          "{'name': \"cfg.provider == 'none'\", 'expected': 'True'}"
        ],
        "scenario": "test_get_default_config",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 104,
          "total_tokens": 191
        },
        "why_needed": "to ensure that the default configuration is created correctly without any external dependencies."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_get_default_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007386430000337896,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `Config.is_llm_enabled()` should return `False` when the provider is set to `'none'`.",
          "The function `Config.is_llm_enabled()` should return `True` when the provider is set to `'ollama'` or `'litellm'`.",
          "The function `Config.is_llm_enabled()` should return `True` when the provider is set to `'gemini'`."
        ],
        "scenario": "Verifies that the `is_llm_enabled` check returns False for a provider without an LLM.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 263,
          "total_tokens": 413
        },
        "why_needed": "Prevents regression in case the LLM provider is changed to 'none'."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-221, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006971120000116571,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The number of errors returned by the validate() method should be 1.', 'expected_value': 1, 'actual_value': 0}",
          "{'description': \"The first error message should contain 'Invalid aggregate_policy 'random'.\", 'expected_value': \"'Invalid aggregate_policy 'random'\", 'actual_value': ''}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 128,
          "total_tokens": 268
        },
        "why_needed": "To ensure that the `aggregate_policy` parameter is validated correctly and raises an error when it's invalid."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-213, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007042199999887089,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': \"Invalid llm_context_mode 'mega_max'\", 'expected_value': \"Invalid llm_context_mode 'mega_max'\"}"
        ],
        "scenario": "test_validate_invalid_context_mode",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 131,
          "total_tokens": 212
        },
        "why_needed": "to test the validation of an invalid context mode"
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-229, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007212709999748768,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected error message', 'value': \"Invalid include_phase 'lunch_break'\"}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 129,
          "total_tokens": 202
        },
        "why_needed": "To test the validation of an invalid include phase."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 28,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007080940000037117,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'condition': \"Invalid provider 'invalid_provider'\", 'expected_value': \"Invalid provider 'invalid_provider'\"}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 122,
          "total_tokens": 203
        },
        "why_needed": "To test the validation of an invalid provider."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 31,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245-254, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006773429999498148,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.validate() returns an error message indicating that llm_context_bytes must be at least 1000",
          "llm_context_bytes in errors contains the expected string 'llm_context_bytes must be at least 1000'",
          "The number of errors is greater than or equal to 5",
          "llm_context_bytes must be at least 1000 is found in the list of errors",
          "llm_max_tests must be 0 (no limit) or positive is found in the list of errors",
          "llm_requests_per_minute must be at least 1 is found in the list of errors",
          "llm_timeout_seconds must be at least 1 is found in the list of errors",
          "llm_max_retries must be 0 or positive is found in the list of errors"
        ],
        "scenario": "Test validation of numeric constraints.",
        "token_usage": {
          "completion_tokens": 228,
          "prompt_tokens": 329,
          "total_tokens": 557
        },
        "why_needed": "Prevents regression where the llm_context_bytes is set to a value less than 1000, potentially causing issues with LLM context creation."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_numeric_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006748079999852052,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config is empty', 'expected': [], 'actual': []}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_valid_config",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 100,
          "total_tokens": 172
        },
        "why_needed": "To ensure the config is correctly validated and no errors are returned."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003349941999999828,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` attribute of the configuration object is set to `aggr_dir`.",
          "The `aggregate_policy` attribute of the configuration object is set to `merge`.",
          "The `aggregate_run_id` attribute of the configuration object is set to `run-123`.",
          "The `aggregate_group_id` attribute of the configuration object is set to `group-abc`."
        ],
        "scenario": "Test the `load_aggregation_options` function to ensure it correctly loads aggregation options.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 295,
          "total_tokens": 440
        },
        "why_needed": "This test prevents a bug where the aggregation options are not loaded correctly, potentially causing issues with downstream processing."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_aggregation_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0033147670000062135,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg.batch_parametrized_tests', 'expected_value': {'scenario': 'False', 'why_needed': 'disabled batch flag works'}}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 138,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the batch parameter in LLMs is correctly set to False when it's disabled."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0038481280000155493,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_max_retries' attribute in the config should be set to 10 by default.",
          "The 'llm_provider' attribute in the config should be None.",
          "The 'llm_model' attribute in the config should be None.",
          "The 'llm_context_mode' attribute in the config should be None.",
          "The 'llm_prompt_tier' attribute in the config should be None.",
          "The 'llm_batch_parametrized' attribute in the config should be False.",
          "The 'llm_context_compression' attribute in the config should be None.",
          "The 'llm_aggregate_dir' attribute in the config should not exist.",
          "The 'llm_coverage_source' attribute in the config should not exist.",
          "The 'llm_evidence_bundle' attribute in the config should not exist.",
          "The 'llm_report_html', 'llm_report_json', and 'llm_report_pdf' attributes in the config should be None."
        ],
        "scenario": "Test handling when pyproject.toml doesn't exist.",
        "token_usage": {
          "completion_tokens": 268,
          "prompt_tokens": 413,
          "total_tokens": 681
        },
        "why_needed": "This test prevents a regression where the LLM configuration is not properly loaded due to the absence of pyproject.toml."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_config_missing_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607-608, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004000142999984746,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'id': 'assert_cfg_llm_coverage_source', 'expected_value': 'cov_dir'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 126,
          "total_tokens": 195
        },
        "why_needed": "To test the coverage source option."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0034638619999896036,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"cfg.provider == 'none'\", 'expected': 'None', 'message': 'Expected cfg.provider to be None'}",
          "{'name': 'cfg.report_html is None', 'expected': 'None', 'message': 'Expected cfg.report_html to be None'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_defaults",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 116,
          "total_tokens": 232
        },
        "why_needed": "To test the default configuration of the Pytest plugin."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 132,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492-494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004003244999978506,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'CLI options should override pyproject.toml options', 'expected_value': {'option1': 'value1', 'option2': 'value2'}, 'actual_value': {}}"
        ],
        "scenario": "test_load_from_cli_overrides_pyproject",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 134,
          "total_tokens": 224
        },
        "why_needed": "To test that CLI options override pyproject.toml options."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 133,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0046515439999552655,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml', 'expected_content': '```toml\\n[tool.pytest.ini]\\nversion = 6.2.0\\ndirectories = [\\'tests\\', \\'docs\\']\\npytest.ini\\n``\"', 'actual_content': ''}"
        ],
        "scenario": "Test that CLI provider option overrides pyproject.toml.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 130,
          "total_tokens": 242
        },
        "why_needed": "To ensure that the correct configuration is loaded based on the provided CLI provider."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_provider_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494-495, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003359370999987732,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_pytest_config.option.llm_max_retries', 'expected_value': 2, 'actual_value': 0}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 130,
          "total_tokens": 215
        },
        "why_needed": "To test the functionality of loading retries from CLI."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 134,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386-388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005132941999988816,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is not empty', 'expected': {'status': 0, 'message': ''}, 'actual': {'status': 0, 'message': ''}}",
          "{'name': 'pyproject.toml file has the correct structure', 'expected': {'status': 1, 'message': \"The pyproject.toml file should have a 'tool' section with the required keys\"}, 'actual': {'status': 0, 'message': ''}}"
        ],
        "scenario": "test_load_from_pyproject",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 119,
          "total_tokens": 281
        },
        "why_needed": "To test the functionality of the `load_config` function in the `LoadConfig` class."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 88,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470-474, 476-477, 479, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0033004269999992175,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.prompt_tier should be 'minimal'",
          "cfg.batch_parametrized_tests should be False",
          "cfg.context_compression should be 'none'"
        ],
        "scenario": "Test loading token optimization options from CLI.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 264,
          "total_tokens": 361
        },
        "why_needed": "This test prevents a potential bug where the `llm_prompt_tier` and `batch_parametrized_tests` options are not correctly set for token optimization."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_token_optimization_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488, 490-492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004890773999989051,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_dependency_snapshot` option in the configuration file should be set to 'deps.json'.",
          "The value of `llm_dependency_snapshot` in the configuration file should match 'deps.json'.",
          "The `llm_dependency_snapshot` option should have a valid value.",
          "The configuration file should contain the correct dependency snapshot path.",
          "The CLI output should display the expected dependency snapshot path.",
          "The `report_dependency_snapshot` option should be enabled in the CLI configuration.",
          "The `llm_dependency_snapshot` option should be set to 'deps.json' in the CLI configuration."
        ],
        "scenario": "Testing the CLI override for dependency snapshot configuration.",
        "token_usage": {
          "completion_tokens": 187,
          "prompt_tokens": 213,
          "total_tokens": 400
        },
        "why_needed": "This test prevents a potential bug where the `llm_dependency_snapshot` option is not correctly set to 'deps.json' when running the CLI."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_dependency_snapshot",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488-490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005580667000003814,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_evidence_bundle` option in the configuration file should be set to 'bundle.zip'.",
          "The `report_evidence_bundle` value in the configuration file should match 'bundle.zip'."
        ],
        "scenario": "Verify that the test CLI override for evidence bundle sets the correct value.",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 217,
          "total_tokens": 318
        },
        "why_needed": "This test prevents a potential regression where the CLI override is not set to the expected evidence bundle."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_evidence_bundle",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484-486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005159730999992007,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `cfg.report_json` should be 'output.json'.",
          "The `llm_report_json` option in the mock configuration should match the value specified by the test.",
          "The expected value of `cfg.report_json` is not being set correctly in the mock configuration.",
          "The `test_cli_report_json` test case does not prevent a regression where the `llm_report_json` option is not set for CLI override reports.",
          "The `report_json` option in the mock configuration should be updated to 'output.json' after setting it by the test.",
          "The expected value of `cfg.report_json` is being set correctly in the mock configuration.",
          "The `test_cli_report_json` test case prevents a regression where the `llm_report_json` option is not set for CLI override reports."
        ],
        "scenario": "Verify that the `test_cli_report_json` test case sets the `report_json` option to 'output.json' in the mock configuration.",
        "token_usage": {
          "completion_tokens": 246,
          "prompt_tokens": 212,
          "total_tokens": 458
        },
        "why_needed": "This test prevents a regression where the `llm_report_json` option is not set correctly for CLI override reports."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486-488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0049394880000477315,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `llm_report_pdf` in the configuration is set to `output.pdf` after applying the override.",
          "The expected value of `report_pdf` in the configuration is `output.pdf`.",
          "No other values for `llm_report_pdf` are present in the configuration.",
          "The configuration file does not contain any other overrides that would affect the default report location.",
          "The `llm_report_pdf` option is correctly overridden to point to a PDF file.",
          "The test passes without raising an assertion error if no override is applied."
        ],
        "scenario": "Verify that the `llm_report_pdf` option is correctly overridden to point to a PDF file.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 212,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a potential bug where the default report location is not updated when CLI overrides are applied."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_pdf",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-237, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007136059999766076,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"litellm_token_output_format should be either 'json', 'xml' or 'yaml'\", 'expected_value': ['json', 'xml', 'yaml'], 'message': 'Invalid token output format'}"
        ],
        "scenario": "test_validate_invalid_token_output_format",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 130,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the token output format is valid and not causing coverage issues."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_invalid_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241-242, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000703004999991208,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The token refresh interval must be at least 60 seconds.', 'expected_value': 60, 'actual_value': 30}"
        ],
        "scenario": "Test validation when token refresh interval is too short",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 146,
          "total_tokens": 234
        },
        "why_needed": "Token refresh intervals that are too short may cause issues with the application's security and functionality."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_token_refresh_interval_too_short",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006970179999825632,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is None', 'expected_value': [], 'actual_value': 0}"
        ],
        "scenario": "Test validation of valid LiteLLM config",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 142,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the LiteLLM configuration is correctly validated and no errors are returned."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_valid_litellm_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438-440, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014316420000000107,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'aggregate_include_history = [\"...\", \"...\"]', 'actual': 'aggregate_include_history = [...]'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 118,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the aggregate_include_history feature is properly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436-438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014256350000323437,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'pyproject.toml contains the correct keys', 'expected_value': {'aggregate_policy': 'some_value'}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 121,
          "total_tokens": 240
        },
        "why_needed": "To ensure that the aggregate policy is loaded correctly from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 150,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340-346, 348-350, 352-354, 356-357, 360-369, 372-375, 378-392, 396, 400, 402, 404, 408-410, 412-413, 416-422, 426-428, 430-432, 436-440, 444-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021024639999609462,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml should contain all config keys', 'expected': True, 'actual': 'all'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 120,
          "total_tokens": 214
        },
        "why_needed": "To ensure that the 'all' configuration key is loaded in a single pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390-392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014260510000099202,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True'}",
          "{'name': 'cache_dir exists in pyproject.toml', 'expected': '/path/to/cache/dir'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 113,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the cache directory is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388-390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014238089999594195,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'cache_ttl_seconds key exists in pyproject.toml', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 116,
          "total_tokens": 228
        },
        "why_needed": "To ensure the cache TTL seconds are correctly loaded from pyproject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418-420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001406174999999621,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'pyproject.toml was created successfully'}",
          "{'name': 'pyproject.toml content', 'expected': 'The pyproject.toml file contains the necessary information to load capture_failed_output.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 116,
          "total_tokens": 249
        },
        "why_needed": "To ensure that the `capture_failed_output` option in the `pyproject.toml` file is correctly loaded and used when running tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420-422, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014188969999509027,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'max_chars = 1024\\n', 'actual': ''}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 119,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the `capture_output_max_chars` option is correctly loaded from the `pyproject.toml` file and used to set the capture output character limit."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362-364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014093579999894246,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context bytes are written to tmp_path', 'expected_value': \"tmp_path / 'pyproject.toml'\", 'actual_value': \"tmp_path / 'pyproject.toml'\"}",
          "{'name': 'Context bytes are read from tmp_path', 'expected_value': '/tmp/pytest-pyproject-loading-coverage.py', 'actual_value': \"tmp_path / 'pyproject.toml'\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
        "token_usage": {
          "completion_tokens": 155,
          "prompt_tokens": 113,
          "total_tokens": 268
        },
        "why_needed": "To ensure that the context_bytes from pyproject.toml is correctly loaded and used in tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368-369, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001470907999987503,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context exclude globs are not included in coverage reports', 'description': 'The context_exclude_globs setting in pyproject.toml should be excluded from coverage reports.', 'expected_value': False, 'actual_value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 119,
          "total_tokens": 244
        },
        "why_needed": "To ensure that the `context_exclude_globs` setting in `pyproject.toml` is correctly excluded from coverage reports."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364-366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015195560000051955,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'context_file_limit = 100', 'actual': 'context_file_limit = 50'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 116,
          "total_tokens": 213
        },
        "why_needed": "To ensure that the context file limit is correctly applied when loading a Pytest project."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366-368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014550110000186578,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context include globs are being loaded correctly', 'expected_value': ['<path_to_context_include_glob>'], 'actual_value': []}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 119,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the `context_include_globs` setting is correctly loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014114589999962845,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'The HMAC key file should exist in the pyproject.toml file.', 'actual': 'The HMAC key file does not exist or is empty.'}",
          "{'expected': 'The HMAC key file should be loaded correctly from the pyproject.toml file.', 'actual': 'The HMAC key file is not properly formatted or is missing required information.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 118,
          "total_tokens": 259
        },
        "why_needed": "To ensure that the HMAC key file is loaded correctly and used for signing messages."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372-374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015359960000296269,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `include_param_values` option should be present in the `pyproject.toml` file.', 'expected_value': True, 'message': 'Expected to find the `include_param_values` option.'}",
          "{'name': 'The `include_param_values` option should have a valid value.', 'expected_value': 'True', 'message': 'Expected the value to be True.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
        "token_usage": {
          "completion_tokens": 158,
          "prompt_tokens": 116,
          "total_tokens": 274
        },
        "why_needed": "To ensure that the `include_param_values` option is correctly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014004529999738224,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'expected': '/path/to/pyproject.toml', 'actual': 'exists and is not empty'}",
          "{'name': \"pyproject.toml has a 'include' section\", 'expected': '/path/to/pyproject.toml', 'actual': \"has a 'include' section\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 113,
          "total_tokens": 256
        },
        "why_needed": "To ensure that the include phase is properly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426-428, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014293199999997341,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'contents of pyproject.toml', 'actual': 'contents of pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 122,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the `include_pytest_invocation` option is correctly loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430-432, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014412020000236225,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'pyproject.toml contains invocation_redact_patterns key', 'expected_value': 'True'}"
        ],
        "scenario": "test_load_invocation_redact_patterns",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 121,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the `invocation_redact_patterns` are correctly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_invocation_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340-342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016069480000169278,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"The contents of pyproject.toml should contain a section named 'litellm_api_base'.\"}",
          "{'name': 'pyproject.toml module exports', 'expected': ['litellm_api_base']}"
        ],
        "scenario": "test_load_litellm_api_base",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 122,
          "total_tokens": 243
        },
        "why_needed": "To ensure that the `litellm_api_base` module is loaded correctly from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342-344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001533873000028052,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': '/path/to/pyproject.toml', 'expected_content': '...', 'actual_content': ''}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 122,
          "total_tokens": 215
        },
        "why_needed": "To ensure that the litellm API key is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356-357, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015168229999744653,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True', 'actual': 'False'}",
          "{'name': 'pyproject.toml file has correct content', 'expected': \"The pyproject.toml file should contain a section named 'tool litellm' with the following key: 'litellm_token_json_key'.\", 'actual': 'The pyproject.toml file does not have this section or its key is incorrect.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 125,
          "total_tokens": 295
        },
        "why_needed": "To ensure that the litellm token JSON key is correctly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352-354, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014589699999874028,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'File exists', 'expected': 'True', 'actual': 'False'}",
          "{'name': 'pyproject.toml file created', 'expected': 'True', 'actual': 'False'}"
        ],
        "scenario": "Test loading of `litellm_token_output_format` from `pyproject.toml`",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 125,
          "total_tokens": 246
        },
        "why_needed": "To ensure that the `litellm_token_output_format` is correctly loaded and used in the Pytest project."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344-346, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014856800000302428,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"pyproject.toml exists and has a 'tool' section\", 'expected': \"The 'tool' section should exist in the pyproject.toml file.\"}",
          "{'name': \"pyproject.toml has a 'tool.litellm.token_refresh_command' entry\", 'expected': \"The 'tool.litellm.token_refresh_command' entry should exist in the pyproject.toml file.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 125,
          "total_tokens": 288
        },
        "why_needed": "To ensure that the `litellm_token_refresh_command` is properly loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348-350, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002400622999971347,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected value of litellm_token_refresh_interval', 'value': 300, 'expected_type': 'int'}"
        ],
        "scenario": "Loading litellm_token_refresh_interval from pyproject.toml",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 125,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the token is refreshed at the correct interval, we need to verify that the 'litellm_token_refresh_interval' configuration value in the pyproject.toml file matches the expected value."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 73,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 449, 451, 453-456, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014069620000327632,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_malformed_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380-382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001407198000038079,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': \"The 'pyproject.toml' file was found at the specified path.\"}",
          "{'name': 'max_concurrency setting exists in pyproject.toml', 'expected': \"The 'max_concurrency' setting in the 'pyproject.toml' file is a valid integer value.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 116,
          "total_tokens": 267
        },
        "why_needed": "To ensure that the `max_concurrency` setting in the `pyproject.toml` file is correctly loaded and used when running tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378-380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001422588000025371,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Coverage is loaded from pyproject.toml', 'description': 'The coverage should be loaded from the specified file in the pyproject.toml.'}"
        ],
        "scenario": "Loading max_tests from pyproject.toml",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 113,
          "total_tokens": 196
        },
        "why_needed": "To ensure that the coverage is loaded correctly when running tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444-446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001437539999983528,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected_value': 'True'}",
          "{'name': 'metadata_file path is correct', 'expected_value': '/path/to/pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 113,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the metadata file is loaded correctly and contains all necessary information."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336-337, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014183590000129698,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The ollama_host setting should be present in the pyproject.toml file', 'expected_value': 'ollama_host', 'actual_value': 'ollama_host'}",
          "{'name': 'The ollama_host setting should have a valid value', 'expected_value': \"a string value (e.g. 'https://example.com/ollama')\", 'actual_value': 'https://example.com/ollama'}"
        ],
        "scenario": "Test PyprojectLoadingCoverage",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 119,
          "total_tokens": 271
        },
        "why_needed": "To ensure that the ollama_host is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014527149999707945,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'omitting tests from coverage', 'actual': 'omitting tests from coverage'}"
        ],
        "scenario": "PyProjectLoadingCoverage",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 121,
          "total_tokens": 204
        },
        "why_needed": "To ensure that the 'omit_tests_from_coverage' feature flag is correctly applied to tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374-375, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014148760000125549,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'File exists', 'expected': 'True', 'actual': 'False'}",
          "{'name': \"File path contains 'max_chars'\", 'expected': 'True', 'actual': 'False'}"
        ],
        "scenario": "Tests for PyProject loading coverage",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 119,
          "total_tokens": 229
        },
        "why_needed": "To ensure that the `param_value_max_chars` value in `pyproject.toml` is correctly loaded and used."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416-418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014081820000342304,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'Collecting all dependencies', 'actual': 'Collecting only dependencies'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 116,
          "total_tokens": 215
        },
        "why_needed": "To ensure that the 'collect' option in 'report Collect Only' is loaded correctly from pyproject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014290419999838377,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected': 'True'}",
          "{'name': 'timeout_seconds key exists in pyproject.toml', 'expected': 'True'}"
        ],
        "scenario": "Loading timeout_seconds from pyproject.toml",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 113,
          "total_tokens": 209
        },
        "why_needed": "To ensure that the timeout seconds are correctly loaded into the Pytest plugin."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400-402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004023097000015241,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `batch_max_tests` option should be present in the `pyproject.toml` file', 'value': True}",
          "{'name': 'The value of `batch_max_tests` should be a number', 'value': 10}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 117,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the `batch_max_tests` option in the `pyproject.toml` file is correctly loaded and used for token optimization."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 131,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396-398, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0049610819999657,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Pytest test discovery', 'description': 'The pytest test discovery should be able to find and run the specified tests'}",
          "{'name': 'Test execution', 'description': 'The tests should execute successfully and produce the expected output'}"
        ],
        "scenario": "Load batch parametrized tests",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 123,
          "total_tokens": 228
        },
        "why_needed": "To ensure Pytest coverage is accurate for batch parameterized tests"
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402-404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004015275999961432,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected_value': 'True'}",
          "{'name': 'context_compression section exists', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 117,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the context compression feature is properly loaded and used in PyProject files."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404-405, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004848694000031628,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context line padding is applied correctly', 'expected_value': '', 'actual_value': '...'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 117,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the `context_line_padding` option is correctly loaded and applied to the context."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392-393, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004077348000009806,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `prompt_tier` should be present in the `pyproject.toml` file.', 'expected_value': 'prompt_tier', 'actual_value': 'None'}",
          "{'name': 'The `prompt_tier` should have the correct type (dict).', 'expected_value': 'dict', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 117,
          "total_tokens": 267
        },
        "why_needed": "To ensure that the `prompt_tier` is loaded correctly from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006980440000461385,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'batch_max_tests must be at least 1', 'expected_value': 1}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 135,
          "total_tokens": 223
        },
        "why_needed": "The test is necessary to ensure that the batch_max_tests configuration option has a valid value."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007029670000520127,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': 'context_line_padding must be 0 or positive', 'value': 'Negative value for context_line_padding'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 129,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the context line padding is not negative and raises an error when it is."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006709359999490516,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': 'Invalid context_compression', 'value': \"Any error message containing 'Invalid context_compression'\"}"
        ],
        "scenario": "test_validate_invalid_context_compression",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 124,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the validation of context compression is not affected by invalid settings."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007227329999750509,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'type': 'assertion', 'message': 'Invalid prompt_tier', 'expected': ['Invalid prompt_tier']}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 125,
          "total_tokens": 218
        },
        "why_needed": "To ensure that the validation of invalid `prompt_tier` values does not return any error messages."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 124,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378-380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603-605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002613982999946529,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg is an instance of Config', 'expected_type': 'Config'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 119,
          "total_tokens": 192
        },
        "why_needed": "To ensure that the config defaults are correct and safe."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006939280000324288,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytestconfig is not None', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 108,
          "total_tokens": 183
        },
        "why_needed": "This test ensures that markers are present in the plugin configuration."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 91,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.08910232199997381,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should exist at the specified path.",
          "The `report.html` file should exist at the specified path.",
          "Both files should be present in the test directory.",
          "The report generation should work correctly even when run with different output formats (JSON and HTML).",
          "The `pytester.runpytest()` call should successfully generate both JSON and HTML reports."
        ],
        "scenario": "Verify that the test generates both JSON and HTML reports for Pytest.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 279,
          "total_tokens": 419
        },
        "why_needed": "This test prevents a regression where the report generation is only done in JSON format, but not in HTML."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_both_json_and_html_outputs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05651742100002366,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert data['run_meta']['collected_count'] == 3\", 'expected_value': 3}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 198,
          "total_tokens": 281
        },
        "why_needed": "pytest_collection_finish counts items (line 378)"
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 11,
          "line_ranges": "70-71, 73-75, 77, 79, 142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05295675100001063,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `nested` subdirectory should be present in the report.json file.",
          "The `report.json` file should exist in the specified location.",
          "The `makepyfile` command should create the necessary directories and files."
        ],
        "scenario": "Test that output directories are created if missing.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 247,
          "total_tokens": 341
        },
        "why_needed": "Prevents regression where the plugin does not create an output directory."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_creates_nested_directory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 50,
          "line_ranges": "78-79, 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330, 332, 334-335, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05881090700000868,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the report contains an error code of 1, indicating a failure.",
          "The 'error' key in the 'summary' dictionary contains the string 'RuntimeError: Fixture failed'.",
          "The test passes even though the fixture failed, due to the captured error being reported as a false positive."
        ],
        "scenario": "Test that fixture errors are captured in report.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 286,
          "total_tokens": 403
        },
        "why_needed": "Fixture failures are not properly reported and can lead to false positives or missed issues."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_fixture_error_captured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 59,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 114,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.14305184199997711,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the report includes 'passed', 'failed', and 'skipped' outcomes.",
          "The test asserts that there are at least three types of outcomes in the report (passed, failed, and skipped).",
          "The test checks if the report contains all three types of outcomes.",
          "The test ensures that the report does not exclude any outcome by checking for missing values.",
          "The test verifies that the report includes a mix of passing and failing tests.",
          "The test asserts that the report correctly identifies which tests were skipped.",
          "The test checks if the report accurately reflects the number of passed, failed, and skipped tests."
        ],
        "scenario": "Test pytest_runtest_makereport captures all outcomes",
        "token_usage": {
          "completion_tokens": 195,
          "prompt_tokens": 335,
          "total_tokens": 530
        },
        "why_needed": "pytest_runtest_makereport may not capture all outcomes if the test is skipped or fails, leading to incorrect reporting."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_makereport_captures_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 250,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403-404, 558-559, 562-563, 566-568, 579, 583, 602-603, 619-620"
        }
      ],
      "duration": 0.0515267490000042,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'exists', 'expected_result': True}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 150,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the plugin correctly handles cases where no output is specified."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.5540238510000108,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that `pytester.runpytest('--llm-pdf=report.pdf')` exits with code 0 (success) or warning, but definitely runs.",
          "The test checks if collection/hooks run successfully after enabling the plugin via --llm-pdf.",
          "The test verifies that passing only --llm-pdf works to trigger the plugin logic and does not say 'no reports configured'.",
          "The test ensures that the plugin is enabled correctly even when logging a report file name like 'report.pdf'."
        ],
        "scenario": "Test that --llm-pdf option enables the plugin.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 435,
          "total_tokens": 592
        },
        "why_needed": "Prevents regression where the plugin is not enabled due to missing Playwright configuration."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_pdf_option_enables_plugin",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.055505313999958616,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'start_time' key should be present in the run_meta dictionary within the 'report.json' file.",
          "The value of the 'start_time' key should match the actual start time of the pytest session.",
          "If no start time is recorded, the 'start_time' key should still be present and have a default value (e.g., 0)",
          "Any changes to the start time should be reflected in the report.json file immediately after running pytest",
          "The 'start_time' value should not be affected by any external factors that might impact the session timing (e.g., system clock, network latency)",
          "If multiple sessions are run concurrently, the start times should be correctly reported and consistent across all sessions"
        ],
        "scenario": "Test that the pytest_sessionstart records start time is correctly reported in the report.json file.",
        "token_usage": {
          "completion_tokens": 216,
          "prompt_tokens": 276,
          "total_tokens": 492
        },
        "why_needed": "This test prevents a regression where the start time of the session might not be accurately reported in the report.json file."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_session_start_records_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006709799999953248,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 4096,
          "total_tokens": 4234
        },
        "why_needed": ""
      },
      "llm_context_override": "balanced",
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0006644910000090931,
      "file_path": "tests/test_plugin_integration.py",
      "llm_opt_out": true,
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007104520000211778,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'does_not_raise', 'expected_output': [], 'actual_output': []}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 90,
          "total_tokens": 171
        },
        "why_needed": "This test is necessary to ensure that the requirement marker does not cause any errors."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
      "outcome": "passed",
      "phase": "call",
      "requirements": [
        "REQ-001",
        "REQ-002"
      ]
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 136,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.03916552800001227,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that a full report is generated and saved to the specified JSON file.",
          "Verify that the report includes summary data with total count of tests, passed, and failed.",
          "Verify that the HTML report includes references to all test files (test_a.py and test_b.py)."
        ],
        "scenario": "Test the integration of report writer with pytest_llm_report.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 417,
          "total_tokens": 530
        },
        "why_needed": "This test prevents regression that may occur when using the report writer with pytest_llm_report."
      },
      "nodeid": "tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586-587"
        }
      ],
      "duration": 0.001280409999992571,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 151,
          "total_tokens": 283
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 12,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586, 590-592"
        }
      ],
      "duration": 0.0015794850000361293,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_report', 'expected_calls': [1]}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 204,
          "total_tokens": 280
        },
        "why_needed": "The `pytest_collectreport` plugin is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0008280979999994997,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'pytest_collectreport should be called with a session object', 'expected_result': 'No session attribute'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 138,
          "total_tokens": 223
        },
        "why_needed": "To ensure that collectreport skips when session is not available."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0008354450000069846,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_report.session is None', 'expected': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 134,
          "total_tokens": 216
        },
        "why_needed": "To ensure that the collectreport plugin behaves correctly when a Pytest session is None."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0028436289999831388,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'LLM enabled warning should be raised', 'expected_value': True, 'actual_value': 'False'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 143,
          "total_tokens": 235
        },
        "why_needed": "LLM enabled warning is raised when pytest is run with the --llm flag."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 135,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 25,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-358, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0025714930000049208,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected a UsageError to be raised', 'message': 'A UsageError should be raised when the plugin is used with an invalid pyproject.toml file.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 134,
          "total_tokens": 231
        },
        "why_needed": "Validation errors are raised when the plugin is used with invalid configuration files."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 17,
          "line_ranges": "328-330, 332-334, 336-338, 342-343, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011758879999774763,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_config.addinivalue_line.called', 'expected_value': 1}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 170,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the configure function skips on xdist workers as expected."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0029025830000364294,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_load.return_value.mock_cfg == mock_cfg",
          "# Test that load_config returns the correct mock config object"
        ],
        "scenario": "Test fallback to load_config if Config.load is missing.",
        "token_usage": {
          "completion_tokens": 230,
          "prompt_tokens": 747,
          "total_tokens": 977
        },
        "why_needed": "This test prevents a potential bug where the plugin would attempt to load configuration files without calling Config.load, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 122,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002294543999994403,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'pyproject.toml contents'}",
          "{'name': 'load_config function call', 'expected': 'load_config function call'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 140,
          "total_tokens": 250
        },
        "why_needed": "To ensure that CLI options override pyproject.toml options when loading plugin configurations."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 112,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.09681826999997156,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True'}",
          "{'name': 'pyproject.toml is readable', 'expected': 'True'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 136,
          "total_tokens": 237
        },
        "why_needed": "To ensure that the plugin can load configuration from a PyProject file."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 9,
          "line_ranges": "399, 403-404, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012044540000033521,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocked stash.get() with _enabled_key and False returns None, as expected.",
          "Mocked stash.get() with _enabled_key and True does not return None, but instead calls test_terminal_summary() with the provided arguments.",
          "Test_terminal_summary() is called with the provided arguments (0, mock_config) when stash.get(_enabled_key, False) returns False."
        ],
        "scenario": "Test that terminal summary skips when plugin is disabled.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 281,
          "total_tokens": 414
        },
        "why_needed": "Prevents a potential regression where the plugin's terminal summary functionality is not properly handled when it is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "399-400, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009645270000078199,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is None', 'expected': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 164,
          "total_tokens": 250
        },
        "why_needed": "The test is necessary because it checks for the correct behavior of the `pytest_terminal_summary` plugin when skipping terminal summary workers."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 69,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004598039999962111,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_config.option.llm_report_html == 'out.html'",
          "mock_config.option.llm_report_json == None",
          "mock_config.option.llm_report_pdf == None",
          "mock_config.option.llm_evidence_bundle == None",
          "mock_config.option.llm_dependency_snapshot == None",
          "mock_config.option.llm_requests_per_minute == None",
          "mock_config.option.llm_aggregate_dir == None",
          "mock_config.option.llm_aggregate_policy == None",
          "mock_config.option.llm_aggregate_run_id == None",
          "mock_config.option.llm_aggregate_group_id == None",
          "mock_config.option.llm_max_retries == None",
          "mock_config.option.llm_coverage_source == None",
          "mock_config.option.llm_prompt_tier == None",
          "mock_config.option.llm_batch_parametrized == None",
          "mock_config.option.llm_context_compression == None",
          "mock_config.option.llm_context_bytes == None",
          "mock_config.option.llm_context_file_limit == None",
          "mock_config.option.llm_max_tests == None",
          "mock_config.option.llm_max_concurrency == None",
          "mock_config.option.llm_timeout_seconds == None",
          "mock_config.option.llm_capture_failed == None",
          "mock_config.option.llm_ollama_host == None",
          "mock_config.option.llm_litellm_api_base == None",
          "mock_config.option.llm_litellm_api_key == None",
          "mock_config.option.llm_litellm_token_refresh_command == None",
          "mock_config.option.llm_litellm_token_refresh_interval == None",
          "mock_config.option.llm_litellm_token_output_format == None",
          "mock_config.option.llm_litellm_token_json_key == None",
          "mock_config.option.llm_cache_dir == None",
          "mock_config.option.llm_cache_ttl == None"
        ],
        "scenario": "Test config loading from pytest objects (CLI) for maximal plugin functionality.",
        "token_usage": {
          "completion_tokens": 466,
          "prompt_tokens": 639,
          "total_tokens": 1105
        },
        "why_needed": "This test prevents a potential issue where the lllm_report option is not set correctly, potentially leading to incorrect configuration or errors during execution."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::testload_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 7,
          "line_ranges": "558-559, 562-563, 566-568"
        }
      ],
      "duration": 0.0015326810000146907,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_item.config.stash.get.return_value', 'expected_value': 'False'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 220,
          "total_tokens": 304
        },
        "why_needed": "The test is necessary to verify that makereport skips when disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017837230000168347,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_runtest_makereport` function should be called with the correct mock collector instance.",
          "The `mock_collector.handle_runtest_logreport` method should be called with the provided mock report and item.",
          "The `stash_get` method of the mock item should return True when it is set to `_enabled_key` or `_collector_key`.",
          "The `mock_item.config.stash.get` method should not call the collector instance directly.",
          "The `pytest_runtest_makereport` function should yield a point after calling `send` on the mock outcome.",
          "The `handle_runtest_logreport` method of the mock collector instance should be called with the provided mock report and item."
        ],
        "scenario": "Test makereport calls collector when enabled.",
        "token_usage": {
          "completion_tokens": 205,
          "prompt_tokens": 371,
          "total_tokens": 576
        },
        "why_needed": "This test prevents a potential regression where the plugin does not call the collector when makereport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 602-603"
        }
      ],
      "duration": 0.001211463000004187,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_session.config.stash.get.return_value', 'expected': {'False': {}}}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 149,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the plugin correctly handles collection_finish when disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 602, 606-608"
        }
      ],
      "duration": 0.0016416209999761122,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_finish was called once with mock_session.items', 'expected': 1, 'actual': 0}"
        ],
        "scenario": "Test PluginSessionHooks",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 219,
          "total_tokens": 307
        },
        "why_needed": "To test if the `pytest_collection_finish` function calls the `_collector_key` collector when collection_finish is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 619-620"
        }
      ],
      "duration": 0.001272536999977092,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 157,
          "total_tokens": 241
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 11,
          "line_ranges": "558-559, 562, 566-568, 619, 623, 626, 628-629"
        }
      ],
      "duration": 0.0010095690000184732,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert _collector_key in mock_stash",
          "assert _start_time_key in mock_stash"
        ],
        "scenario": "Test that sessionstart initializes collector when enabled.",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 335,
          "total_tokens": 412
        },
        "why_needed": "Prevents a potential bug where the collector is not created when pytest_sessionstart is called with an enabled configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0023681839999767362,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that `--llm-report` and `--llm-coverage-source` are included in the command line arguments.",
          "Check that both options are present in the `args[0]` of each call to `addoption`.",
          "Verify that the correct group name is used for adding options (in this case, `llm-report` and `LLM-enhanced test reports`)."
        ],
        "scenario": "Test pytest_addoption adds expected arguments and verifies specific options.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 293,
          "total_tokens": 430
        },
        "why_needed": "pytest_addoption may not add all required arguments if the plugin is not properly configured."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0023602790000154528,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'parser.addini was not called', 'expected_result': 0, 'actual_result': 1}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 140,
          "total_tokens": 225
        },
        "why_needed": "pytest_addoption no longer adds INI options"
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 53,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0028988169999593083,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option is set to 'out.html' and the test asserts that the Coverage class correctly reports a coverage percentage of 85.5.",
          "The `stash` dictionary contains `_enabled_key` as True and `_config_key` with the Config object, allowing the test to verify the correct stash configuration.",
          "The mock Coverage class is created with the loaded CoverageMapper instance, ensuring that the coverage report is generated correctly.",
          "The `report` method of the mock Coverage class is called once, verifying that it reports a coverage percentage of 85.5.",
          "The `load` method of the mock Coverage class is called once, verifying that it loads the coverage map correctly.",
          "The `report_html` option is set to 'out.html', allowing the test to verify that the report writer writes the correct HTML file.",
          "The stash dictionary contains the required keys for the Config object, ensuring that the stash configuration is valid.",
          "The mock stash instance is created with the specified stash data, allowing the test to verify its correctness."
        ],
        "scenario": "Test coverage percentage calculation logic for terminal summary.",
        "token_usage": {
          "completion_tokens": 269,
          "prompt_tokens": 395,
          "total_tokens": 664
        },
        "why_needed": "Prevents regression in coverage reporting when terminal summary is enabled and coverage map is loaded."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 66,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-486, 491-494, 497, 499, 502-504, 512-514, 516, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002645331999985956,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the test passes when `llm_enabled` is set to True.",
          "Check if the correct configuration is passed to `pytest_terminal_summary`.",
          "Verify that the `llm_enabled` key is correctly added to the stash dictionary.",
          "Ensure that the `get_provider` method returns the correct provider instance.",
          "Verify that the `annotate_tests` method is called with the correct arguments.",
          "Check if the coverage map is properly updated when LLM is enabled.",
          "Verify that the report writer is able to write to the out.html file correctly."
        ],
        "scenario": "Test terminal summary with LLM enabled runs annotations.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 477,
          "total_tokens": 643
        },
        "why_needed": "Prevents regression in terminal summary functionality when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 45,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0018596639999941544,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_terminalreporter.call_args[1][0].stash._enabled_key should return True",
          "mock_terminalreporter.call_args[1][0].stash._config_key should return cfg",
          "mock_config.stash._enabled_key should return False"
        ],
        "scenario": "Test terminal summary creates collector if missing.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 391,
          "total_tokens": 499
        },
        "why_needed": "This test prevents a bug where the plugin does not create a collector even when it is supposed to be missing."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 21,
          "line_ranges": "399, 403, 407, 410-411, 413-414, 417-418, 420, 422-426, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002976097999976446,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The stash created by `pytest_terminal_summary` should support both get() and [] indexing.",
          "The aggregator returned by `Aggregator` should be called once with the correct arguments.",
          "The report writer should write JSON and HTML files correctly.",
          "The aggregate function should return a report object.",
          "The stash should not have any invalid keys when set to an empty stash.",
          "The aggregation function should raise an error if `aggregate_dir` is not a valid directory path.",
          "The terminal reporter should be patched with the correct class and arguments.",
          "The terminal writer should write JSON and HTML files correctly."
        ],
        "scenario": "Test terminal summary with aggregation enabled.",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 441,
          "total_tokens": 620
        },
        "why_needed": "This test prevents regression in the case where `aggregate_dir` is set to a non-existent directory."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 52,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 476-479, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00463137200000574,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `pytest_terminal_summary` should not raise a UserWarning for a valid configuration.",
          "The `load` method of `CoverageMapper` should be called with a valid path.",
          "The `report_writer` should write the coverage report without raising an exception.",
          "The `coverage.Coverage` object should have a valid load method.",
          "The `MagicMock()` instance passed to `pytest_terminal_summary` should not raise a UserWarning.",
          "A valid configuration with a valid path should be loaded successfully.",
          "The coverage map should be created without raising an exception.",
          "The report writer should write the coverage report without raising an exception."
        ],
        "scenario": "Test coverage calculation error when load fails during computation.",
        "token_usage": {
          "completion_tokens": 188,
          "prompt_tokens": 389,
          "total_tokens": 577
        },
        "why_needed": "This test prevents a regression where the coverage calculation fails due to an OSError during load."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 63,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.006080224000015733,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'utils.py' file should be present in the assembled context.",
          "A function named `util()` should be found within the `utils.py` file.",
          "Only the `def util()` line from `utils.py` should be included in the assembled context.",
          "No additional code should be included in the assembled context beyond what is necessary for the test.",
          "The 'utils.py' file should not contain any other functions or variables that are not used by `util()`",
          "The 'utils.py' file should have a line count of 2, as specified in the coverage report"
        ],
        "scenario": "Test the ContextAssembler to assemble a balanced context with a test file and dependency.",
        "token_usage": {
          "completion_tokens": 185,
          "prompt_tokens": 331,
          "total_tokens": 516
        },
        "why_needed": "The test prevents regression when assembling contexts with unbalanced dependencies, ensuring that only necessary code is included."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139-140, 268-272"
        }
      ],
      "duration": 0.0008770919999960824,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'ContextAssembled', 'context_assembled': {'test_1': {}}}"
        ],
        "scenario": "Test the ContextAssembler to assemble a complete context for a test file",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 176,
          "total_tokens": 264
        },
        "why_needed": "To ensure that the ContextAssembler can correctly assemble a complete context for a test file, including all necessary key assertions."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 30,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0009171670000114318,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'test_1' function should be present in the source code of the assembled test file.",
          "The ContextAssembler should return an empty dictionary for the context of the assembled test file.",
          "The test result nodeid should match the path of the test file being tested.",
          "Any additional imports or dependencies required by the test function should be included in the assembly.",
          "No other code should be present in the source code of the assembled test file, except for the 'test_1' function.",
          "The ContextAssembler should return an empty dictionary when no tests are found in the test file.",
          "Any error messages or warnings from the assembler should not affect the correctness of the assembly."
        ],
        "scenario": "Verifies that the ContextAssembler can assemble a minimal context for a test file with a single test function.",
        "token_usage": {
          "completion_tokens": 213,
          "prompt_tokens": 267,
          "total_tokens": 480
        },
        "why_needed": "This test prevents regression when using the minimal context mode, as it ensures that all necessary code is included in the assembly."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 46,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.000987532999999985,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'f1.py' file is present in the assembled context.",
          "The 'f1.py' file contains the truncated content.",
          "The length of the 'f1.py' file does not exceed 40 bytes (20 + truncation message)."
        ],
        "scenario": "Verifies that the ContextAssembler correctly limits LLM context size to prevent truncation of long content.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 335,
          "total_tokens": 459
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler would truncate long code snippets, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 50,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0009808180000163702,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'f1.py' file is present in the assembled context.",
          "The content of 'f1.py' matches the original content.",
          "Context for 'f1.py' does not contain any 'truncated' keyword.",
          "Context for 'f1.py' has a length equal to its original content."
        ],
        "scenario": "Test that 'complete' mode does not truncate long files when context bytes are set to a small value.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 361,
          "total_tokens": 496
        },
        "why_needed": "This test prevents the truncation of long files, which can lead to incorrect results or data loss in certain contexts."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_complete_context_limits_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 26,
          "line_ranges": "33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0008820620000165036,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_get_test_source` returns an empty string when given a non-existent file name.",
          "The function `_get_test_source` correctly includes the nested test name with parameters in its source code output.",
          "The function `_get_test_source` raises an AssertionError when given a file that does not exist or has incorrect path."
        ],
        "scenario": "Verify the ContextAssembler's ability to handle non-existent file names and nested test names with parameters.",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 275,
          "total_tokens": 416
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly handles files that do not exist or have incorrect paths, leading to unexpected behavior in subsequent tests."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 5,
          "line_ranges": "33, 284-287"
        }
      ],
      "duration": 0.0013890120000041861,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts that 'test.pyc' is excluded because it has a .pyc extension",
          "asserts that 'secret/key.txt' is excluded because it does not match any globs in the config",
          "asserts that 'public/readme.md' is included because it matches one of the globs in the config"
        ],
        "scenario": "The test verifies that the ContextAssembler should exclude certain files from being processed by the LLM.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 227,
          "total_tokens": 367
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly excludes files that are not actually protected by the llm_context_exclude_globs configuration."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_should_exclude",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        }
      ],
      "duration": 0.0008577119999699789,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context_files is an empty list.",
          "def test_foo() is present in test_source.",
          "test_file::test_foo is present in the test source.",
          "test_foo.py is present in the test file.",
          "Context files are not generated when assemble function returns no context files."
        ],
        "scenario": "Test assemble minimal mode returns no context files.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 298,
          "total_tokens": 413
        },
        "why_needed": "Prevents regression where assemble function does not return any context files when run in minimal mode."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_minimal_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 62,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0010201119999919683,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The assembler should use balanced mode when the LLM context override is set to 'balanced'.",
          "The coverage entry for module.py should include the line ranges and count of 1.",
          "The test source file (test_bar.py) should be included in the context files.",
          "The context files should contain the modified module file (module.py).",
          "The LLM context override should be respected when assembling a test with an overridden context.",
          "The assembler should report that it is using balanced mode for the assembly of the test.",
          "The coverage entry should include line ranges and count 1, indicating that only one line was executed in module.py."
        ],
        "scenario": "Test assemble respects llm_context_override from test.",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 362,
          "total_tokens": 558
        },
        "why_needed": "This test prevents regression where the assembly of a test with an overridden LLM context is not using the expected balanced mode."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_with_context_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 20,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163-164, 201, 284-286"
        }
      ],
      "duration": 0.0008468330000255264,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file `secret_config.py` should not be included in the balanced context.",
          "No files matching exclude pattern `*secret*` should be included in the balanced context.",
          "The LLM context mode 'balanced' should exclude files with glob patterns like '*secret*' from being included in the context.",
          "The file paths of excluded files should match the exclude patterns exactly (case-sensitive).",
          "No files with glob patterns like '*secret*' should be found in the directory tree of `tmp_path`.",
          "The LLM context mode 'balanced' should not include any files that are not part of a test or an existing file.",
          "The balanced context should exclude all files that match the exclude patterns, including subdirectories and nested directories."
        ],
        "scenario": "Test that the balanced context excludes files matching exclude patterns.",
        "token_usage": {
          "completion_tokens": 225,
          "prompt_tokens": 331,
          "total_tokens": 556
        },
        "why_needed": "This test prevents a regression where the LLM context mode is set to 'balanced' and it includes files with glob patterns like '*secret*', which are excluded by default."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_excludes_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 16,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-161, 201"
        }
      ],
      "duration": 0.0007714070000020001,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context is empty', 'expected_result': {}, 'actual_result': {}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 201,
          "total_tokens": 289
        },
        "why_needed": "To test that the ContextAssembler correctly handles a scenario where a balanced context file does not exist."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.013888298000040322,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the content in the source file does not exceed 120 bytes.",
          "The 'truncated' keyword is present in the content if it exceeds the maximum bytes limit.",
          "The content is truncated by removing some buffer for truncation messages."
        ],
        "scenario": "Test that balanced context respects max bytes limit.",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 405,
          "total_tokens": 511
        },
        "why_needed": "This test prevents a potential memory leak by ensuring the content is truncated when exceeding the maximum bytes limit."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_max_bytes_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 139-140"
        }
      ],
      "duration": 0.0007354690000056507,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equals', 'expected_value': '{}', 'actual_value': ''}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 162,
          "total_tokens": 244
        },
        "why_needed": "To test the balanced context with no coverage returns an empty dict."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 35,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-157, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0009511639999573163,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context should have only one node (either 'file1.py' or 'file2.py')",
          "context should not exceed max_bytes=5 when both files are processed",
          "context should be empty if neither file is processed",
          "context should contain the first file only (truncated) if it's the only file in the context",
          "context should not contain any other nodes if no files are processed"
        ],
        "scenario": "Test that loop exits when max bytes is reached before processing file.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 409,
          "total_tokens": 559
        },
        "why_needed": "Prevents a potential bug where the ContextAssembler exceeds the maximum allowed bytes in a balanced context without reaching the end of any file."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_reaches_max_bytes_before_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0009395740000286423,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'inclusion', 'expected_inclusion': ['module.py'], 'actual_inclusion': ['module.py']}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 211,
          "total_tokens": 301
        },
        "why_needed": "To ensure that the complete context delegates to balanced correctly."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 9,
          "line_ranges": "33, 78-79, 82-83, 86-89"
        }
      ],
      "duration": 0.0008001609999723769,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Result is an empty string', 'expected_value': '', 'actual_value': 'result'}"
        ],
        "scenario": "Tests for _get_test_source with empty nodeid",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 148,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the function returns an empty string when given an empty nodeid."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_empty_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.0008648720000223875,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The test file contains a def statement immediately after the test function', 'expected_result': 'test_stop.py'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 129,
          "total_tokens": 224
        },
        "why_needed": "To ensure that source extraction stops at the next function definition, as intended."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 6,
          "line_ranges": "33, 78-79, 82-84"
        }
      ],
      "duration": 0.0007607180000377411,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result is not None', 'expected_value': '', 'message': 'Expected result to be an empty string'}"
        ],
        "scenario": "tests/test_prompts_coverage.py",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 142,
          "total_tokens": 230
        },
        "why_needed": "To test the edge case where the _get_test_source method returns an empty string when a non-existent file is provided."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.000854769000000033,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'The extracted function should have correct indentation (2 spaces per level).', 'actual': 'The extracted function has incorrect indentation (1 space per level).'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 118,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the _get_test_source function correctly extracts functions with proper indentation, even when they are nested within other functions or blocks of code."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0007254760000137139,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3', 'actual': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 106,
          "total_tokens": 175
        },
        "why_needed": "To ensure that consecutive lines are compressed correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000708541999983936,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3', 'actual': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 107,
          "total_tokens": 178
        },
        "why_needed": "To test the handling of duplicate ranges in the compress_ranges function."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "29-30"
        }
      ],
      "duration": 0.0007390240000404447,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 92,
          "total_tokens": 164
        },
        "why_needed": "The test is necessary because the function `compress_ranges` does not handle an empty input correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0006979519999958939,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3, 5, 10-12, 15', 'actual': '1-3, 5, 10-12, 15'}",
          "{'expected': 'True', 'actual': 'True'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 130,
          "total_tokens": 233
        },
        "why_needed": "To test the functionality of compressing mixed ranges in a list."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0006719429999861859,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1, 3, 5', 'actual': '1, 3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 113,
          "total_tokens": 197
        },
        "why_needed": "To ensure that non-consecutive lines are correctly compressed to a single value."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "29, 33, 35-37, 39, 50, 52, 65-66"
        }
      ],
      "duration": 0.0006730769999876429,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 5, 'actual': '5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_single_line",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 96,
          "total_tokens": 160
        },
        "why_needed": "The single line should not use range notation."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0006901159999870288,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-2', 'actual': '1-2'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 103,
          "total_tokens": 177
        },
        "why_needed": "To ensure that two consecutive lines are correctly compressed into a single range."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0006866100000024744,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '[1, 3]', 'actual': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 110,
          "total_tokens": 194
        },
        "why_needed": "This test is necessary because the current implementation of `compress_ranges` only works with input that is already sorted."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "81-82"
        }
      ],
      "duration": 0.0006765219999920191,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected result', 'value': [], 'description': 'Expected the function to return an empty list when given an empty string.'}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 90,
          "total_tokens": 172
        },
        "why_needed": "The current implementation does not handle empty strings correctly."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 11,
          "line_ranges": "81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0006902419999619269,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3, 5, 10, 11, 12], 'actual': ['1', '2', '3', '5', '10', '11', '12']}",
          "{'expected': [], 'actual': []}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_mixed",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 121,
          "total_tokens": 240
        },
        "why_needed": "The test is necessary because it checks for the expansion of mixed ranges and singles in the `expand_ranges` function."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_mixed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "81, 84-91, 95"
        }
      ],
      "duration": 0.0006848940000168113,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3], 'actual': ['1', '2', '3']}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_range",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 99,
          "total_tokens": 173
        },
        "why_needed": "The range function should expand to a list."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_range",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 27,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007073980000313895,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'original input', 'value': [1, 2, 3, 5, 10, 11, 12, 15]}",
          "{'name': 'compressed input', 'value': []}",
          "{'name': 'expanded input', 'value': [1, 2, 3, 5, 10, 11, 12, 15]}"
        ],
        "scenario": "compress_ranges and expand_ranges should be inverses.",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 134,
          "total_tokens": 295
        },
        "why_needed": "Because the problem statement requires that `compress_ranges` and `expand_ranges` are inverses, meaning they should have a round-trip relationship. This ensures that any input to one function can be uniquely recovered from its output."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 7,
          "line_ranges": "81, 84-87, 93, 95"
        }
      ],
      "duration": 0.0006952079999678062,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': [5], 'actual_value': ['5']}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_single_number",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 95,
          "total_tokens": 172
        },
        "why_needed": "The test is necessary because it checks for a specific scenario where the function expands ranges around a single number."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_single_number",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65, 67"
        }
      ],
      "duration": 0.0006765260000065609,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return '500ms' when given an input of 0.5 seconds.",
          "The function should return '1ms' when given an input of 0.001 seconds.",
          "The function should return '0ms' when given an input of 0.0 seconds."
        ],
        "scenario": "Test the format_duration function with different input values to verify it correctly formats as milliseconds for < 1s.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 211,
          "total_tokens": 342
        },
        "why_needed": "This test prevents a potential regression where the function might not be able to accurately format durations less than 1 second."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65-66"
        }
      ],
      "duration": 0.000767297999971106,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'format_duration(1.23)', 'expected': '1.23s'}",
          "{'name': 'format_duration(60.0)', 'expected': '60.00s'}"
        ],
        "scenario": "tests/test_render.py::TestFormatDuration::test_seconds",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 116,
          "total_tokens": 221
        },
        "why_needed": "The test is necessary because the function format_duration() expects a float value as input, but it receives an integer value in the provided code."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0006950819999929081,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `outcome_to_css_class` should return a valid CSS class for each outcome type.",
          "The function `outcome_to_css_class` should handle all three outcome types ('passed', 'failed', and 'skipped') correctly.",
          "The function `outcome_to_css_class` should map the string 'xfailed' to the correct CSS class ('outcome-xfailed').",
          "The function `outcome_to_css_class` should map the string 'error' to the correct CSS class ('outcome-error')."
        ],
        "scenario": "Test that all outcomes map to CSS classes.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 263,
          "total_tokens": 413
        },
        "why_needed": "Prevents regression in CSS class mapping for different outcome types."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0007003069999882428,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'outcome-unknown', 'actual_value': 'outcome-unknown'}"
        ],
        "scenario": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 102,
          "total_tokens": 180
        },
        "why_needed": "Unknown outcomes are not handled by the `outcome_to_css_class` function."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65-67, 79-85, 87, 121-124, 126-127, 131-132, 155-157, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0007695150000017748,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '<!DOCTYPE html>' header is present in the rendered HTML.",
          "The 'Test Report' title is included in the rendered HTML.",
          "The 'test::passed' and 'test::failed' node IDs are found in the rendered HTML.",
          "The 'PASSED' and 'FAILED' keywords are displayed in the rendered HTML.",
          "The plugin version and repository version are displayed in the rendered HTML.",
          "The '<strong>Plugin:</strong>' and '<strong>Repo:</strong>' tags contain the expected values in the rendered HTML."
        ],
        "scenario": "Test renders basic report with fallback HTML.",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 426,
          "total_tokens": 589
        },
        "why_needed": "Prevents rendering of incomplete or corrupted reports due to plugin or repository issues."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-129, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0007318659999668853,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report root contains a summary with total and passed tests.",
          "The report root contains a summary with total and passed tests.",
          "The report root contains a summary with total and passed tests.",
          "The test renders coverage information in the rendered HTML.",
          "The test renders coverage information in the rendered HTML.",
          "The test renders coverage information in the rendered HTML."
        ],
        "scenario": "Test 'test_renders_coverage' verifies that the test renders coverage information and includes it in the rendered HTML.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 288,
          "total_tokens": 433
        },
        "why_needed": "This test prevents regression by ensuring that the test renders coverage information, which is crucial for tracking code coverage."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 64,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 140-142, 144, 147, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.000742813999977443,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report includes the 'Tests login flow' scenario.",
          "The report includes the 'Prevents auth bypass' why needed message.",
          "The report includes the 'Confidence:' assertion with value '85%' or higher.",
          "The report does not include any annotations without a confidence score of 0.8 or above.",
          "The report does not contain any annotations without a scenario, why needed, or confidence assertion.",
          "The report is rendered correctly and does not contain any errors or warnings.",
          "The report includes all expected nodes in the HTML output.",
          "The report includes all expected assertions in the HTML output."
        ],
        "scenario": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
        "token_usage": {
          "completion_tokens": 199,
          "prompt_tokens": 317,
          "total_tokens": 516
        },
        "why_needed": "This test prevents the rendering of LLM annotations with a confidence score below 0.8, which could cause authentication bypass issues."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 68,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-178, 180-186, 191, 206, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.000745830000028036,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Source Coverage' section should be present in the HTML report.",
          "'src/foo.py'"
        ],
        "scenario": "Verifies that the test renders source coverage summary with required information.",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 331,
          "total_tokens": 406
        },
        "why_needed": "This test prevents a regression where the source coverage summary is missing or incomplete."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 55,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0007443910000120013,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The HTML contains both 'XFailed' and 'XPassed' strings.",
          "The HTML does not contain any other text that might interfere with the display of 'XFailed' and 'XPassed'.",
          "The 'XFailed' string is displayed correctly, even if there are multiple xfailed entries in the report summary.",
          "The 'XPassed' string is also displayed correctly, even if there are multiple xpassed entries in the report summary.",
          "No other text is displayed that might interfere with the display of 'XFailed' and 'XPassed'.",
          "There are no duplicate counts for either 'xfailed' or 'xpassed' in the report summary."
        ],
        "scenario": "Test renders xpass summary for ReportRoot with Summary containing both xfailed and xpassed entries.",
        "token_usage": {
          "completion_tokens": 216,
          "prompt_tokens": 283,
          "total_tokens": 499
        },
        "why_needed": "This test prevents a rendering issue where the 'xfailed' and 'xpassed' counts are not displayed correctly when there is an overlap in the report summary."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0006919819999779975,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'different', 'actual': 'not equal'}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 115,
          "total_tokens": 181
        },
        "why_needed": "To ensure that different content produces different hashes."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007601309999927253,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected hash1 to be equal to hash2', 'expected_result': 'hash1 == hash2'}",
          "{'message': 'Expected length of hash1 to be 64', 'expected_result': 'len(hash1) == 64'}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 129,
          "total_tokens": 233
        },
        "why_needed": "To ensure that an empty bytes object produces consistent hash values."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 72,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307"
        }
      ],
      "duration": 0.004685729000016181,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The duration of the test run should be accurate (60 seconds in this case).",
          "The pytest version should be present in the metadata.",
          "The plugin version and Python version should also be included in the metadata."
        ],
        "scenario": "Test that the build_run_meta method returns the correct duration and version information for a test run.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 318,
          "total_tokens": 423
        },
        "why_needed": "This test prevents regression where the duration of a test run is not accurately reported."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_run_meta",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 19,
          "line_ranges": "156-158, 319, 321-322, 324-335, 337"
        }
      ],
      "duration": 0.0007379669999636462,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of tests should be equal to 6 (all outcome types).",
          "The passed tests should have 1 outcome type ('passed').",
          "The failed tests should have 1 outcome type ('failed').",
          "The skipped tests should have 1 outcome type ('skipped').",
          "The xfailed and xpassed tests should have 1 outcome type each ('xfailed' or 'xpassed').",
          "The error test should have 1 outcome type ('error')."
        ],
        "scenario": "Test verifies the ReportWriter to correctly build a summary of all outcome types.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 336,
          "total_tokens": 493
        },
        "why_needed": "This test prevents regression where the total count of outcomes is incorrect."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 319, 321-322, 324-329, 337"
        }
      ],
      "duration": 0.000735845999997764,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "summary.total should be equal to the number of tests.",
          "summary.passed should be equal to the number of tests that were passed.",
          "summary.failed should be equal to the number of tests that were failed.",
          "summary.skipped should be equal to the number of tests that were skipped."
        ],
        "scenario": "The test verifies that the `total` attribute of the `Summary` object correctly counts all tests.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 283,
          "total_tokens": 406
        },
        "why_needed": "This test prevents a regression where the total count of passed and failed tests is incorrect."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_counts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "156-158"
        }
      ],
      "duration": 0.0006972470000050635,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` attribute of the `writer` object should be set to an instance of `Config`.",
          "The `warnings` attribute of the `writer` object should be an empty list.",
          "The `artifacts` attribute of the `writer` object should be an empty list."
        ],
        "scenario": "Test that the `ReportWriter` class initializes correctly and sets default values for its attributes.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 199,
          "total_tokens": 325
        },
        "why_needed": "This test prevents a potential bug where the `ReportWriter` instance is not properly initialized with the required configuration."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_create_writer",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337"
        }
      ],
      "duration": 0.0040183819999697334,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the report.tests list should be equal to 2.",
          "The total value of report.summary.total should be equal to 2."
        ],
        "scenario": "Test that ReportWriter writes a report with all tests.",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 255,
          "total_tokens": 338
        },
        "why_needed": "To prevent regression in the case where no output paths are specified for the test reports."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.0050265550000290204,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 85.5, 'actual_value': 'report.summary.coverage_total_percent'}"
        ],
        "scenario": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 132,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the ReportWriter class correctly calculates and returns the total coverage percentage from a report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 97,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.004078986000024543,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `report.source_coverage` should be 1.",
          "The file path of `report.source_coverage[0]` should match `"
        ],
        "scenario": "Test ReportWriter::test_write_report_includes_source_coverage verifies that the test writes a report with source coverage summary.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 291,
          "total_tokens": 398
        },
        "why_needed": "This test prevents regression where the test does not include source coverage in the report, potentially misleading users about the code's quality."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 99,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337"
        }
      ],
      "duration": 0.003917505999993409,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report should have a single merged coverage entry for the given test.",
          "The file path of the merged coverage entry should match the expected file path.",
          "All line ranges and counts in the coverage entry should be present."
        ],
        "scenario": "Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 285,
          "total_tokens": 400
        },
        "why_needed": "This test prevents regression where the report does not merge coverage into tests, potentially leading to inaccurate or misleading test results."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513-514, 516-519, 522-523"
        }
      ],
      "duration": 0.005865719000041736,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report file should exist at the specified path.",
          "Any warnings generated by the ReportWriterWithFiles class should have a code of 'W203'."
        ],
        "scenario": "Test that the ReportWriterWithFiles class falls back to direct write if an atomic write fails and a cross-device link occurs.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 276,
          "total_tokens": 379
        },
        "why_needed": "This test prevents regression that would occur when the atomic write operation fails and a cross-device link is attempted."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 128,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.0069038350000028,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Output Directory Existence', 'expected': True, 'actual': 'exists'}"
        ],
        "scenario": "Test Report Writer with Files",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 171,
          "total_tokens": 240
        },
        "why_needed": "The test should create an output directory if it doesn't exist."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 12,
          "line_ranges": "156-158, 477-480, 487-491"
        }
      ],
      "duration": 0.0011552430000278946,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer.warnings` list contains warnings with code 'W201' for any directory creation attempts that fail.",
          "Any directory creation attempt that raises an OSError is captured as a warning with code 'W201'.",
          "The test verifies that the `test_ensure_dir_failure` function correctly identifies and reports directory creation failures."
        ],
        "scenario": "Test verifies that the `test_ensure_dir_failure` test case ensures a directory creation failure is captured as a warning.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 278,
          "total_tokens": 425
        },
        "why_needed": "This test prevents a potential bug where the directory creation fails silently and does not raise an exception, potentially leading to unexpected behavior or errors in downstream code."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.00105493099999876,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_git_info` function should return `None` for both `sha` and `dirty` variables in case of a git command failure.",
          "The test should pass even if the `git not found` exception is raised during the execution of `subprocess.check_output`.",
          "The expected values `None` for `sha` and `dirty` variables should be verified after calling `get_git_info`.",
          "The function should handle git command failures without raising an exception or crashing the test process.",
          "The test should verify that the returned values are consistent with the expected behavior in case of a failure.",
          "The test should cover all possible scenarios where `git not found` is raised during the execution of `subprocess.check_output`."
        ],
        "scenario": "The test verifies that the `get_git_info` function handles git command failures by returning `None` for both `sha` and `dirty` variables.",
        "token_usage": {
          "completion_tokens": 237,
          "prompt_tokens": 231,
          "total_tokens": 468
        },
        "why_needed": "This test prevents a regression where the `get_git_info` function fails to return expected values when it encounters a git command failure."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 120,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.03693266699997366,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'report.html' file should exist at the specified path.",
          "The 'report.html' file should contain the expected text content.",
          "All required assertions ('test1', 'test2') should be present in the 'report.html' file.",
          "The report writer should correctly identify test results as passed, failed, skipped, XFailed, or XPassed.",
          "Errors and warnings should not be included in the 'report.html' file."
        ],
        "scenario": "Test verifies that the report writer creates an HTML file with expected content.",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 366,
          "total_tokens": 518
        },
        "why_needed": "This test prevents regression by ensuring that the report writer correctly generates an HTML file containing all necessary information."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-333, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.03728527399999848,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts that 'XFAILED' and 'XPASSED' are present in the HTML string.",
          "Checks if the text content of the HTML includes both 'XFAILED' and 'XPASSED'.",
          "Verifies that 'xfailed' and 'xpassed' keywords are found in the HTML summary.",
          "Ensures that the report writer correctly identifies xfail outcomes as 'xfailed' or 'xpassed'.",
          "Checks if the report includes all required text content for an xfail outcome.",
          "Verifies that the report does not include any other keywords related to failures.",
          "Ensures that the report is properly formatted and includes only necessary information."
        ],
        "scenario": "The test verifies that the report writer includes xfail outcomes in the HTML summary.",
        "token_usage": {
          "completion_tokens": 200,
          "prompt_tokens": 308,
          "total_tokens": 508
        },
        "why_needed": "This test prevents regressions where the report writer does not include xfail outcomes in the HTML summary."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.00526279500002147,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should be created in the specified path.",
          "At least one artifact should be tracked for the report.",
          "The number of artifacts tracked should be greater than zero.",
          "The file exists at the specified path.",
          "The `ReportWriter` class correctly tracks artifacts.",
          "The `write_report` method creates a new JSON file with the report."
        ],
        "scenario": "Test verifies that a JSON file is created with the report.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 265,
          "total_tokens": 396
        },
        "why_needed": "This test prevents regression where the report writer does not create a JSON file."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-430, 441-442, 444-450, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.04001975599999241,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `write_pdf` method of the `ReportWriter` class should be able to write the contents of the `report.pdf` file to the system.",
          "The `report.pdf` path should be created in the test directory.",
          "Any artifacts generated by the report writer should have a path that matches the expected value for `report.pdf`.",
          "The `write_report` method should raise an exception if Playwright is not available.",
          "If Playwright is available, it should create a PDF file with the specified path and contents."
        ],
        "scenario": "Should create PDF file when Playwright is available.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 478,
          "total_tokens": 645
        },
        "why_needed": "This test prevents regression where the report writer does not create a PDF file even if Playwright is available."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 103,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408-412, 415"
        }
      ],
      "duration": 0.00515441599998212,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'report.pdf' should exist.",
          "Any warnings with code W204_PDF_PLAYWRIGHT_MISSING should be present in the report."
        ],
        "scenario": "Test verifies that a warning is raised when Playwright is missing for PDF output.",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 311,
          "total_tokens": 404
        },
        "why_needed": "This test prevents the 'Warning: Could not find Playwright' message from being displayed when using PDF output."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.0018252620000112074,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is None', 'expected': {'type': 'NoneType', 'value': ''}, 'message': 'Expected get_git_info() to return None for sha parameter'}",
          "{'name': 'assert dirty is None', 'expected': {'type': 'NoneType', 'value': ''}, 'message': 'Expected get_git_info() to return None for dirty parameter'}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 123,
          "total_tokens": 282
        },
        "why_needed": "To test the Report Writer's ability to handle non-git directory paths and return empty values for SHA1 and dirty flags."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 16,
          "line_ranges": "67-74, 76-81, 83-84"
        }
      ],
      "duration": 0.004470722999997179,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is None or isinstance(sha, str)', 'description': 'The test asserts that the returned Git SHA is either None or an instance of str.'}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 159,
          "total_tokens": 268
        },
        "why_needed": "To ensure that the `get_git_info` function returns a valid Git SHA and/or a string representation of the Git repository."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0008812030000058257,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_plugin_git_info()` returns None or a string.",
          "The function `get_plugin_git_info()` does not raise an exception when _git_info import fails.",
          "The assertion `assert sha is None or isinstance(sha, str)` passes even if the fallback via git runtime fails."
        ],
        "scenario": "Test falls back to git runtime when _git_info import fails.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 253,
          "total_tokens": 376
        },
        "why_needed": "To prevent a critical bug where the plugin Git Info cannot be imported and we still rely on it for fallback functionality."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0007009949999883247,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_type': 'str', 'actual_type': 'None'}",
          "{'expected_type': 'str', 'actual_type': 'is None or str'}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 141,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the plugin's Git information can be retrieved and used correctly."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_returns_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.005704288000003999,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Report writer should create a backup report when writing fails', 'description': 'The backup report is created even if the direct write operation fails.'}"
        ],
        "scenario": "Test atomic write fallback",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 181,
          "total_tokens": 260
        },
        "why_needed": "To ensure the ReportWriter can handle unexpected errors during atomic writes."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterAtomicWrite::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 28,
          "line_ranges": "156-158, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.11373272399998768,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer.warnings` list should contain 'W201' warnings for each failed playwright context.",
          "The `writer.warnings` list should contain 'W202' warnings for each successful playwright context.",
          "The `report.pdf` file should be present in the test directory with a PDF extension.",
          "The `report.pdf` file should have a warning code of W201 or W202.",
          "The `report.pdf` file should not be empty.",
          "The `report.pdf` file should be a valid PDF document (not corrupted).",
          "The `writer.write_pdf` method should raise an exception when playwright raises an exception."
        ],
        "scenario": "Test PDF generation when playwright raises exception (lines 424-432).",
        "token_usage": {
          "completion_tokens": 190,
          "prompt_tokens": 356,
          "total_tokens": 546
        },
        "why_needed": "Prevents bug where the report writer fails to generate a PDF due to playwright exceptions."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "156-158, 408-412, 415"
        }
      ],
      "duration": 0.0011097909999762123,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'W204' warning should be present in the output of the `writer.write_pdf()` method.",
          "The PDF file 'report.pdf' should not exist after the test is completed."
        ],
        "scenario": "Test PDF generation when playwright is not installed.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 293,
          "total_tokens": 387
        },
        "why_needed": "Prevents a potential bug where the report writer fails to create a PDF file due to playwright being missing."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 11,
          "line_ranges": "156-158, 455, 460, 462, 465-469"
        }
      ],
      "duration": 0.03316291600003751,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report writer should create a temporary HTML file.",
          "The temporary HTML file should have the correct suffix (.html).",
          "The path to the temporary HTML file should exist and be accessible.",
          "The suffix of the temporary HTML file should match the expected value (HTML)."
        ],
        "scenario": "Test _resolve_pdf_html_source creates temp file when no HTML source exists.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 265,
          "total_tokens": 380
        },
        "why_needed": "Prevents a potential bug where the test fails if there is no HTML source provided."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_creates_temp",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 455-457, 460, 462, 465-469"
        }
      ],
      "duration": 0.03321687199996859,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` configuration should be able to resolve the HTML source even if it's missing.",
          "The resolved path for the HTML file should exist and not be a temporary file.",
          "The test should still pass even when the HTML file is nonexistent."
        ],
        "scenario": "Test _resolve_pdf_html_source when configured HTML doesn't exist.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 270,
          "total_tokens": 378
        },
        "why_needed": "Prevents a bug where the test fails due to an unresolvable HTML source."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_missing_html_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 7,
          "line_ranges": "156-158, 455-458"
        }
      ],
      "duration": 0.0009082150000381262,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path of the resolved PDF is set to the specified HTML file.",
          "The report is not temporary (is_temp=False).",
          "The resolution method uses an existing HTML file instead of creating a new one."
        ],
        "scenario": "Verify that the _resolve_pdf_html_source method uses an existing HTML file as required.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 266,
          "total_tokens": 381
        },
        "why_needed": "This test prevents a potential bug where the report writer does not use an existing HTML file, potentially leading to incorrect or missing information in the report."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_uses_existing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007849410000062562,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert schema.scenario == \"Verify login\",",
          "assert schema.why_needed == \"Catch auth bugs\",",
          "assert schema.key_assertions == [\"assert 200\", \"assert token\"]",
          "# Correctly asserts all required fields"
        ],
        "scenario": "Test that `AnnotationSchema.from_dict` creates a valid annotation from a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 276,
          "total_tokens": 396
        },
        "why_needed": "Prevents regression in the case where an invalid input is provided, causing the test to fail and potentially introducing bugs."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 8,
          "line_ranges": "90-92, 94-98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007267740000429512,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'Verify login' in data['scenario']",
          "assert 'Catch auth bugs' in data['why_needed']",
          "assert all([item in data['key_assertions'] for item in ['assert 200', 'assert token']])",
          "assert data['confidence'] == 0.95"
        ],
        "scenario": "Should convert to dictionary with all fields.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 273,
          "total_tokens": 384
        },
        "why_needed": "Prevent regression in authentication logic, ensuring correct data is returned for login scenarios."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.08600897299999133,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report file exists at the specified path.",
          "The report contains the expected test name.",
          "The report includes all necessary HTML tags."
        ],
        "scenario": "The HTML report is generated correctly.",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 264,
          "total_tokens": 340
        },
        "why_needed": "This test prevents a regression where the report generation fails due to missing dependencies."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 69,
          "line_ranges": "78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-335, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.12304074399997944,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts that the 'Total Tests' label is present and has a count of 6.",
          "asserts that the 'Passed' label has a count of 1.",
          "asserts that the 'Failed' label has a count of 1.",
          "asserts that the 'Skipped' label has a count of 1.",
          "asserts that the 'XFailed' label has a count of 1.",
          "asserts that the 'XPassed' label has a count of 1.",
          "asserts that the 'Errors' and 'Error' labels have counts equal to 1."
        ],
        "scenario": "test_html_summary_counts_all_statuses verifies that the HTML summary counts include all statuses.",
        "token_usage": {
          "completion_tokens": 191,
          "prompt_tokens": 621,
          "total_tokens": 812
        },
        "why_needed": "This test prevents regression where the report does not include all statuses, which can make it difficult to diagnose issues."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 55,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06255763100000422,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "report_path.exists()",
          "data['schema_version']",
          "data['summary']['total'] == 2",
          "data['summary']['passed'] == 1",
          "data['summary']['failed'] == 1"
        ],
        "scenario": "The JSON report is created and its existence and content are verified.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 295,
          "total_tokens": 403
        },
        "why_needed": "This test prevents a potential issue where the report generation process fails to create or write the expected JSON file."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 55,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 43,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 103,
          "line_ranges": "130-133, 135-137, 139, 141, 143, 190, 194-199, 201, 203, 205, 207, 210, 212-214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419-437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-506, 509, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05508509199995615,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test passes without any errors or warnings.",
          "The 'LLM annotations' key is present in the report.",
          "The 'LLM annotations' value is 'True'.",
          "The 'why_needed' value is 'Prevents regressions'.",
          "The 'key_assertions' list contains the expected assertions.",
          "The 'choices' list within the 'payload' dictionary contains a valid LLM annotation.",
          "The 'message' key within each 'choice' dictionary has a valid message.",
          "The 'content' value of each 'message' dictionary is a valid JSON string."
        ],
        "scenario": "Test that LLM annotations are included in the report when a provider is enabled.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 385,
          "total_tokens": 566
        },
        "why_needed": "Prevents regressions by ensuring LLM annotations are present in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 12,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 100,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 44,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-507, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 111,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.08904511400004367,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_pass()` should raise an error with a meaningful message.",
          "The function `pytestLLMReport` should be able to detect and report the error.",
          "The HTML output of the test should contain the error message.",
          "The error message should be in the correct format (e.g. 'LLM errors are surfaced in HTML output.')",
          "The error message should not be a simple string but rather an actual error message from `litellm.completion`.",
          "The function `pytestLLMReport` should be able to handle different types of LLM errors (e.g. syntax errors, semantic errors)."
        ],
        "scenario": "Verify that LLM errors are surfaced in HTML output.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 313,
          "total_tokens": 494
        },
        "why_needed": "Prevent regression where LLM errors are not reported correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05562613000000738,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' marker should be present in the test list.",
          "The 'llm_opt_out' marker should have an associated boolean value of True.",
          "The number of tests with the 'llm_opt_out' marker should be exactly 1.",
          "The first test in the list should have a 'llm_opt_out' marker.",
          "The 'llm_opt_out' marker should not be present for any other tests."
        ],
        "scenario": "Verify that the LLM opt-out marker is correctly recorded and reported.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 290,
          "total_tokens": 449
        },
        "why_needed": "This test prevents a regression where the LLM opt-out marker is not properly recorded or reported, potentially leading to incorrect analysis results."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05457130199999938,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'requirement' keyword is used to mark requirements with specific names (REQ-001, REQ-002).",
          "The report generated by pytester includes all tests that have been marked as requiring a specific requirement.",
          "The test asserts that there is exactly one test that has been marked with both 'requirement' and 'REQ-001'.",
          "The test asserts that the required requirements are included in the list of tests.",
          "The test verifies that the 'REQ-001' and 'REQ-002' requirements are correctly identified in the report."
        ],
        "scenario": "Verify requirement marker is recorded and correctly identified in report.",
        "token_usage": {
          "completion_tokens": 172,
          "prompt_tokens": 307,
          "total_tokens": 479
        },
        "why_needed": "This test prevents a regression where the requirement marker might not be properly recorded or identified in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06019659200001115,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of xfailed tests is correctly reported.",
          "Each xfailed test has an outcome of 'xfailed'.",
          "No other outcomes are recorded in the report for this test suite."
        ],
        "scenario": "Verifies that multiple xfailed tests are recorded in the report.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 317,
          "total_tokens": 420
        },
        "why_needed": "Prevents regression where multiple tests fail due to a common cause, such as a test suite being updated or refactored."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 43,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.052559246999976494,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'skipped' key in the report data should contain a value equal to 1.",
          "The 'summary' section of the report data should contain an entry for 'skipped'.",
          "The number of skipped tests should be exactly 1 when reported in the JSON file.",
          "The 'report.json' file should contain a single key-value pair with the key 'skipped' and value 1."
        ],
        "scenario": "Test that skipped tests are recorded and reported correctly.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 264,
          "total_tokens": 407
        },
        "why_needed": "This test prevents regression in the reporting of skipped tests, ensuring they are accurately counted and displayed in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05801203999999416,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key under 'xfailed' is present and contains a value of 1.",
          "The number of failed tests is exactly 1 as expected by the test.",
          "The xfailed count matches the actual number of failed tests in the report."
        ],
        "scenario": "Verify that xfailed tests are correctly recorded in the report.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 264,
          "total_tokens": 366
        },
        "why_needed": "This test prevents regression where only failed tests are reported."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203-205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05743713999999045,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of successful parameterized tests is 3.",
          "All parameterized tests are recorded separately and have a 'passed' status.",
          "The report.json file contains accurate data about the test runs."
        ],
        "scenario": "Testing parameterized tests to ensure correct reporting.",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 290,
          "total_tokens": 386
        },
        "why_needed": "This test prevents regression in the reporting system by verifying that all parameterized tests are recorded correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.04847117599996409,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'stdout', 'description': 'The test checks if the stdout matches the expected output.'}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 123,
          "total_tokens": 204
        },
        "why_needed": "The test is necessary to ensure the CLI help text includes usage examples."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.04222568300002649,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'LLM markers are not registered. Expected: True', 'expected': True}",
          "{'message': 'LLM context marker is not registered. Expected: True', 'expected': True}",
          "{'message': 'requirement marker is not registered. Expected: True', 'expected': True}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 142,
          "total_tokens": 275
        },
        "why_needed": "The test is necessary to ensure that the LLM markers are registered correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.04874234400000432,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The plugin is registered', 'message': 'Plugin is registered via pytest11'}"
        ],
        "scenario": "TestPluginRegistration",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 118,
          "total_tokens": 182
        },
        "why_needed": "To verify that the plugin is successfully registered with pytest."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09089940500001603,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The presence of special characters in the nodeid is detected.",
          "The HTML report generated by pytester is valid.",
          "The '<html>' tag is present in the report."
        ],
        "scenario": "Test verifies that special characters in nodeid are handled correctly.",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 288,
          "total_tokens": 374
        },
        "why_needed": "This test prevents a potential crash and ensures the HTML is valid."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0006910879999963981,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': '1m 0.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 106,
          "total_tokens": 186
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats a duration of exactly one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0006706600000256913,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert '\u03bcs' in result\", 'expected': '500\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 121,
          "total_tokens": 201
        },
        "why_needed": "To ensure the `format_duration` function correctly formats sub-millisecond durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0006944639999915125,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'ms' in result",
          "assert result == '500.0ms'"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 119,
          "total_tokens": 189
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-second durations as milliseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007147379999992154,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert 'm' in result\", 'expected': \"'m'\", 'actual': '1m 30.5s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_minutes_format",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 124,
          "total_tokens": 218
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats durations over a minute, including displaying minutes and seconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_minutes_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0006754049999813105,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '3m 5.0s', 'actual': '3m 5.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 112,
          "total_tokens": 191
        },
        "why_needed": "To ensure the `format_duration` function correctly formats multiple minutes into a human-readable string."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0006874939999761409,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1.00s', 'actual': '1.0'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_one_second",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 101,
          "total_tokens": 184
        },
        "why_needed": "To ensure the `format_duration` function correctly formats a duration of exactly one second as '1.00s'."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_one_second",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0006854110000062974,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"result contains 's'\", 'expected': 'True'}",
          "{'name': \"result equals '5.50s'\", 'expected': 'True'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_seconds_format",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 110,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats seconds under a minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_seconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007146039999952336,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 1.0, 'actual': '1.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 111,
          "total_tokens": 186
        },
        "why_needed": "To ensure the `format_duration` function correctly formats small millisecond durations."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0006656630000065888,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1\u03bcs', 'actual': '1'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 116,
          "total_tokens": 187
        },
        "why_needed": "To test the functionality of formatting very small durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007348239999487305,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'value': '2024-01-15T10:30:45+00:00'}"
        ],
        "scenario": "Test ISO Format with UTC",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 143,
          "total_tokens": 218
        },
        "why_needed": "To ensure the correct formatting of datetime objects in UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0006799440000122559,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'value': '2024-06-20T14:00:00'}"
        ],
        "scenario": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 136,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the naive datetime format is correct and consistent across different timezones."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.000705949000007422,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The result is a string containing the correct microsecond value.', 'expected_result': '123456'}"
        ],
        "scenario": "Tests time formatting with microseconds",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 133,
          "total_tokens": 203
        },
        "why_needed": "To ensure datetime objects are correctly formatted with microseconds."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_with_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007049150000284499,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result.tzinfo is not None', 'expected_result': \"datetime.timezone('UTC')\", 'actual_result': 'None'}",
          "{'name': 'assert result.tzinfo == UTC', 'expected_result': \"datetime.timezone('UTC')\", 'actual_result': 'None'}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 109,
          "total_tokens": 239
        },
        "why_needed": "To ensure that the `utc_now()` function returns a datetime object with an associated timezone."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007222330000331567,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected JSON structure', 'description': 'The returned JSON should have the following structure: `{'}"
        ],
        "scenario": "Test that the function returns a valid JSON response",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 116,
          "total_tokens": 248
        },
        "why_needed": "The test is necessary to ensure that the function correctly returns a JSON response with the expected structure."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_is_current_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007591109999793844,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'isinstance(result, datetime)', 'expected': True}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_returns_datetime",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 94,
          "total_tokens": 165
        },
        "why_needed": "To ensure that the `utc_now()` function returns a datetime object."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_returns_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007777399999895351,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit 1' status code is expected to be returned by subprocess.CompletedProcess().",
          "The error message 'Authentication failed' is expected to be included in the error message returned by pytest.raises(TokenRefreshError)."
        ],
        "scenario": "Test TokenRefresher raises error on command failure when get-token command fails.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 310,
          "total_tokens": 429
        },
        "why_needed": "This test prevents a bug where the TokenRefresher class does not raise an exception when the get-token command fails, potentially causing unexpected behavior or errors in subsequent operations."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_command_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-109, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007699639999714236,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get_token` method is set to an empty string.",
          "An exception of type `TokenRefreshError` is raised with the message 'empty output'.",
          "The assertion passes if the output is indeed empty."
        ],
        "scenario": "Test that TokenRefresher raises an error when given an empty output.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 297,
          "total_tokens": 407
        },
        "why_needed": "To prevent a potential bug where the TokenRefresher does not raise an error when it encounters an empty output."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_empty_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008126380000135214,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function correctly returns a new token after force refresh.",
          "The function does not return the cached token.",
          "The function increments the call count correctly.",
          "The function sets the `call_count` variable correctly.",
          "The function updates the `stdout` and `stderr` attributes of the subprocess result correctly.",
          "The function returns a CompletedProcess object with an empty stdout and stderr attribute.",
          "The function does not raise any exceptions when force refreshing a token."
        ],
        "scenario": "Test that 'force_refresh' bypasses cache and returns a new token.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 346,
          "total_tokens": 513
        },
        "why_needed": "To ensure the TokenRefresher function behaves as expected when the `refresh_interval` is set to a value other than 3600 (default),"
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007719589999624077,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `json_key` parameter of the `TokenRefresher` constructor is set to `"
        ],
        "scenario": "Verify that the `TokenRefresher` class uses a custom JSON key for token refresh.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 303,
          "total_tokens": 420
        },
        "why_needed": "This test prevents a potential bug where the default JSON key used by the `TokenRefresher` class is not compatible with the expected custom key."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_custom_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008024170000453523,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get-token` command should be a JSON object with a single key-value pair: `token: 'json-token-value'` and an optional `expires_in` value.",
          "The extracted token should have the correct type (string) and value ('json-token-value').",
          "The presence of an `expires_in` value is optional and should be ignored if present.",
          "The JSON output should not contain any other keys or values that are not relevant to the token extraction process.",
          "The JSON output should be a valid JSON string, with no syntax errors or unexpected characters.",
          "The extracted token should have the correct format (e.g., 'Bearer <token_value>') if present in the output.",
          "The `json_key` parameter is set correctly and does not affect the expected output."
        ],
        "scenario": "The test verifies that the `TokenRefresher` extracts a JSON object from the output of the `get-token` command.",
        "token_usage": {
          "completion_tokens": 244,
          "prompt_tokens": 308,
          "total_tokens": 552
        },
        "why_needed": "This test prevents a potential bug where the extracted token is not in the expected JSON format, potentially leading to incorrect usage or errors."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000776373999997304,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get-token` command contains the string 'my-secret-token'.",
          "The output does not contain any non-text characters (e.g., lines starting with '#', etc.).",
          "The extracted token is in the same case as it appears in the original text.",
          "The extracted token has a length of at least 3 characters.",
          "The extracted token contains only alphanumeric characters and underscores.",
          "The extracted token does not contain any whitespace characters.",
          "The output contains no non-ASCII characters (e.g., emojis, etc.).",
          "The output is a single line of text."
        ],
        "scenario": "Test that the `TokenRefresher` extracts the correct text format from the output of the `get-token` command.",
        "token_usage": {
          "completion_tokens": 206,
          "prompt_tokens": 298,
          "total_tokens": 504
        },
        "why_needed": "This test prevents a potential bug where the extracted token is not in the expected text format, potentially leading to incorrect usage or interpretation of the token."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_text_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-134, 149-150"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008043880000059289,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` in `TokenRefresher` should raise a `TokenRefreshError` when given invalid JSON input.",
          "The error message returned by `get_token()` should contain the string 'json'.",
          "The test should fail if the `get_token()` method is called with an invalid JSON argument."
        ],
        "scenario": "Test that `TokenRefresher` raises an error when given invalid JSON input.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 299,
          "total_tokens": 430
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher class incorrectly handles or raises an exception on receiving invalid JSON data."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007455979999804185,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` instance should return two different tokens after a 1-hour refresh.",
          "The function `invalidate()` of the `TokenRefresher` instance should clear its cache and return 2.",
          "The call count variable should be incremented by 2 when calling `invalidate()`.",
          "The returned token from `get_token()` should not be equal to the first token before a refresh.",
          "The returned token from `get_token()` should be different from the second token after a refresh.",
          "The output of `run()` should contain two tokens separated by a space.",
          "The error message from `run()` should be empty."
        ],
        "scenario": "Test TokenRefresher.invalidate() clears cache and returns the expected tokens after a refresh.",
        "token_usage": {
          "completion_tokens": 211,
          "prompt_tokens": 340,
          "total_tokens": 551
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not clear its cache after a refresh, leading to stale token values being returned."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalidate",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139-141, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009727409999982228,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method of the `TokenRefresher` class should raise a `TokenRefreshError` with a message indicating that the 'token' key is not found.",
          "The error message should be in lowercase and contain the word 'not'.",
          "The error message should indicate that the token was not found.",
          "The error message should specify the required JSON key as 'token'."
        ],
        "scenario": "Test that TokenRefresher raises an error when JSON key is missing.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 325,
          "total_tokens": 469
        },
        "why_needed": "To prevent a potential bug where the token refresh fails due to a missing required JSON key."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_missing_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.05193158999998104,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "All threads should get the same token (first one to acquire lock)",
          "The length of the set of results should be equal to 1",
          "The first element of the results list should match 'token-1'",
          "Multiple threads accessing the TokenRefresher instance concurrently should not return different tokens",
          "The output format of the returned token should be consistent across all threads"
        ],
        "scenario": "Testing thread safety of TokenRefresher by verifying that it returns the same token across multiple threads.",
        "token_usage": {
          "completion_tokens": 154,
          "prompt_tokens": 427,
          "total_tokens": 581
        },
        "why_needed": "This test prevents a potential bug where multiple threads accessing the TokenRefresher instance concurrently could result in different tokens being returned, leading to inconsistencies in the data."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_thread_safety",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 16,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008639920000064194,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "A `TimeoutExpired` exception is raised with a message indicating that the command timed out.",
          "The `get_token()` method of the `TokenRefresher` instance raises an exception with a string containing 'timed out'.",
          "The test asserts that the error message contains the word 'timed out' in lowercase."
        ],
        "scenario": "The test verifies that TokenRefresher handles command timeouts correctly.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 279,
          "total_tokens": 406
        },
        "why_needed": "This test prevents a potential bug where the token refresh process times out and fails without providing any meaningful error message."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_timeout_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007765080000012858,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` returns the same token when called with the same arguments.",
          "The `TokenRefresher` instance is only called once, even if it's called multiple times in a short period of time.",
          "No additional output or error messages are produced for repeated calls to `get_token()`."
        ],
        "scenario": "Verify that TokenRefresher caches tokens and doesn't call command again.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 353,
          "total_tokens": 476
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher calls the command multiple times due to caching."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_token_caching",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007365370000229632,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method of the `TokenRefresher` class raises a `TokenRefreshError` when the command fails with no stderr output.",
          "The error message returned by the `get_token` method includes 'exit 1' as part of it.",
          "The error message also explicitly states that there is no error output produced by the command."
        ],
        "scenario": "Test the TokenRefresher's behavior when a command fails with no stderr output.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 322,
          "total_tokens": 459
        },
        "why_needed": "This test prevents regression where the TokenRefresher incorrectly handles commands that do not produce any stderr output."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_command_failure_no_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007563919999711288,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The function should raise a TokenRefreshError with the message 'empty' when given an empty command string.\", 'expected_value': 'empty'}"
        ],
        "scenario": "TokenRefresherEdgeCases",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 151,
          "total_tokens": 226
        },
        "why_needed": "Test handling of empty command string."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_empty_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-88, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007768969999801811,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `TokenRefresher` constructor should raise a `TokenRefreshError` when given an invalid shell syntax in the `command` argument.",
          "The `get_token()` method of the `TokenRefresher` instance should return a `TokenRefreshError` exception when encountering an invalid command string.",
          "The error message returned by the `get_token()` method should contain the phrase 'Invalid command string'.",
          "The test should fail with a `TokenRefreshError` exception when calling `refresher.get_token()`.",
          "The `refresher` instance is expected to be in an invalid state after encountering an invalid command string.",
          "The `refresh_interval` and `output_format` arguments are not affected by the presence of an invalid command string."
        ],
        "scenario": "Test the test_invalid_command_string function to verify it handles an invalid command string (shlex parse error).",
        "token_usage": {
          "completion_tokens": 218,
          "prompt_tokens": 251,
          "total_tokens": 469
        },
        "why_needed": "Prevent a TokenRefreshError when encountering an invalid command string during token refresh."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_invalid_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 27,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-137, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007834769999703894,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should raise a `TokenRefreshError` with an error message indicating that the output is not a JSON object.",
          "The function `get_token()` should assert that the error message contains the string 'Expected JSON object'.",
          "The function `get_token()` should assert that the error message contains the string 'list'."
        ],
        "scenario": "Test that the TokenRefresher raises a TokenRefreshError when receiving non-dict JSON output.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 328,
          "total_tokens": 465
        },
        "why_needed": "This test prevents regression where the TokenRefresher incorrectly handles JSON output as a list instead of a dict."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_not_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008058350000510472,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'token' key in the response should be empty or a string.",
          "The 'token' key in the response should not contain any whitespace characters.",
          "The error message should indicate that the token value is either empty or not a string.",
          "The output of the subprocess command should include an empty or non-string token value.",
          "The output of the subprocess command should not include any whitespace characters in the 'token' key."
        ],
        "scenario": "Test handling when token value is an empty string.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 324,
          "total_tokens": 468
        },
        "why_needed": "Prevents regression where the TokenRefresher incorrectly handles empty or non-string token values."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009580720000030851,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method of the `TokenRefresher` instance raises a `TokenRefreshError` when it encounters an empty string or a non-string token value.",
          "The error message returned by the `get_token` method includes the phrase 'empty or not a string'.",
          "When a non-string token value is passed to the `get_token` method, the `json.dumps` function is used to convert it to JSON, which may result in an empty string if the token value cannot be converted.",
          "The `json_key` parameter of the `TokenRefresher` instance is set to 'token', which means that any non-string token values will be ignored and not returned by the `get_token` method.",
          "The `output_format` parameter of the `TokenRefresher` instance is set to 'json', which indicates that only JSON output should be produced. If a non-string token value is passed, it will not be included in the output.",
          "The `json_key` and `output_format` parameters are used together to ensure that only valid token values are returned by the `get_token` method.",
          "If a non-string token value is encountered, the `TokenRefreshError` exception will be raised with an error message indicating that the token value was empty or not a string.",
          "The test verifies that the `TokenRefresher` instance correctly handles non-string tokens by asserting that it raises a `TokenRefreshError` exception when such a value is passed to the `get_token` method."
        ],
        "scenario": "Test that the TokenRefresher handles non-string token values correctly.",
        "token_usage": {
          "completion_tokens": 385,
          "prompt_tokens": 326,
          "total_tokens": 711
        },
        "why_needed": "Prevents a potential bug where the TokenRefresher incorrectly handles tokens with non-string values, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_not_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 19,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113, 115-118"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008015580000346745,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method raises a `TokenRefreshError` with a message indicating that the command was not found.",
          "The error message includes the string 'Failed to execute'.",
          "The `pytest.raises(TokenRefreshError)` assertion checks if the `TokenRefreshError` is raised correctly."
        ],
        "scenario": "Test the TokenRefresher's handling of OSError when executing a command.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 280,
          "total_tokens": 402
        },
        "why_needed": "Prevents the test from passing if an OSError is raised during execution, which could indicate a bug or regression."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_oserror_on_execution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 4,
          "line_ranges": "132, 153-155"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007044790000350076,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The _parse_output method should raise a TokenRefreshError when given input that has only non-empty lines after the initial strip.",
          "The output wrapper should contain non-whitespace content but only whitespace lines in the parsed output.",
          "The parsing method should correctly handle text with only blank lines after the initial strip and return an error.",
          "The test should fail when given input that meets these conditions to ensure the correct behavior of TokenRefresher.",
          "The output wrapper should contain non-whitespace content but only whitespace lines in the parsed output.",
          "The parsing method should correctly handle text with only blank lines after the initial strip and return an error.",
          "The test should pass when given input that meets these conditions to ensure the correct behavior of TokenRefresher."
        ],
        "scenario": "Test handling when text output has only whitespace lines after initial strip.",
        "token_usage": {
          "completion_tokens": 213,
          "prompt_tokens": 376,
          "total_tokens": 589
        },
        "why_needed": "Prevents TokenRefreshError due to incorrect parsing of text with only blank lines after the initial strip."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_text_only_whitespace_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007336840000107259,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should raise a `TokenRefreshError` when given an empty command string.",
          "The error message should include the word 'empty' and be case-insensitive (e.g., 'empty token'.').",
          "The test assertion should verify that the error message contains the word 'empty'.",
          "The function should not raise a `TokenRefreshError` when given a non-empty command string.",
          "The test assertion should verify that no error is raised for a non-empty command string.",
          "The function should return an empty token (or None) when given a non-empty command string.",
          "The test assertion should verify that the returned token is indeed empty (or None).",
          "The function should handle whitespace-only commands correctly and raise the expected `TokenRefreshError` instance."
        ],
        "scenario": "Test the test_whitespace_only_command to ensure it correctly raises a TokenRefreshError for an empty command string.",
        "token_usage": {
          "completion_tokens": 237,
          "prompt_tokens": 236,
          "total_tokens": 473
        },
        "why_needed": "To prevent a potential bug where the TokenRefresher is unable to handle whitespace-only commands that do not contain any tokens."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_whitespace_only_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 73,
          "line_ranges": "399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-487, 491-494, 497, 499, 502-506, 509, 512-514, 516-521, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.004360141000006479,
      "file_path": "tests/test_token_usage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the total input tokens, total output tokens, and annotations count are correctly calculated for each test.",
          "Verify that the terminal summary is run with the correct information.",
          "Verify that the llm_info dictionary contains the expected values for total_input_tokens, total_output_tokens, and annotations_count."
        ],
        "scenario": "Test token usage aggregation with mock stash and pytest config to prevent early binding issues.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 775,
          "total_tokens": 899
        },
        "why_needed": "This test prevents regression in token usage aggregation when using a mock stash and pytest config to avoid early binding issues."
      },
      "nodeid": "tests/test_token_usage.py::test_token_usage_aggregation",
      "outcome": "passed",
      "phase": "call"
    }
  ]
}