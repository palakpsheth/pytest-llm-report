{
  "run_meta": {
    "aggregation_policy": null,
    "collect_only": false,
    "collected_count": 623,
    "deselected_count": 0,
    "duration": 120.641588,
    "end_time": "2026-01-26T00:14:25.414343+00:00",
    "exit_code": 0,
    "git_dirty": true,
    "git_sha": "3145d53f452fcab2350c3fbbe2bf81eb0b563169",
    "interrupted": false,
    "is_aggregated": true,
    "llm_annotations_count": 620,
    "llm_annotations_enabled": true,
    "llm_annotations_errors": 2,
    "llm_context_mode": "minimal",
    "llm_model": "llama3.2:1b",
    "llm_provider": "ollama",
    "llm_total_input_tokens": 135747,
    "llm_total_output_tokens": 75289,
    "llm_total_tokens": 211036,
    "platform": "Linux-6.11.0-1018-azure-x86_64-with-glibc2.39",
    "plugin_git_dirty": true,
    "plugin_git_sha": "a03dbe622cdc018f89b74731aed91adf1a582867",
    "plugin_version": "0.2.1",
    "pytest_version": "9.0.2",
    "python_version": "3.12.12",
    "repo_git_dirty": true,
    "repo_git_sha": "3145d53f452fcab2350c3fbbe2bf81eb0b563169",
    "repo_version": "0.2.1",
    "rerun_count": 0,
    "run_count": 1,
    "run_id": "21342027602-py3.12",
    "selected_count": 623,
    "source_reports": [],
    "start_time": "2026-01-26T00:12:24.772755+00:00"
  },
  "schema_version": "1.1.0",
  "sha256": "f7b40be44fd5bd9508ebbb4af15195ef60c4a232716852586569b07f276db3da",
  "source_coverage": [
    {
      "coverage_percent": 100.0,
      "covered": 2,
      "covered_ranges": "2-3",
      "file_path": "src/pytest_llm_report/_git_info.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 2
    },
    {
      "coverage_percent": 95.04,
      "covered": 115,
      "covered_ranges": "13, 15-19, 21, 36, 39, 45, 47, 53-54, 56-58, 60, 62-65, 70, 74-75, 78-81, 85, 88-90, 94, 104, 110, 113-115, 117-121, 123-124, 129, 131-132, 134-135, 138-139, 145-147, 149, 152, 155, 158, 160, 162, 176, 178, 182, 184, 186, 196, 198-202, 204-205, 208, 210, 219, 231, 233-247, 249, 251, 259-260, 262-263, 265, 267-269, 273, 276-277, 279-280, 283, 285-286, 288, 290-291, 295",
      "file_path": "src/pytest_llm_report/aggregation.py",
      "missed": 6,
      "missed_ranges": "67, 91-92, 111, 206, 217",
      "statements": 121
    },
    {
      "coverage_percent": 93.62,
      "covered": 44,
      "covered_ranges": "13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153",
      "file_path": "src/pytest_llm_report/cache.py",
      "missed": 3,
      "missed_ranges": "64-65, 130",
      "statements": 47
    },
    {
      "coverage_percent": 99.1,
      "covered": 110,
      "covered_ranges": "19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140-141, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285",
      "file_path": "src/pytest_llm_report/collector.py",
      "missed": 1,
      "missed_ranges": "239",
      "statements": 111
    },
    {
      "coverage_percent": 94.34,
      "covered": 50,
      "covered_ranges": "13-15, 18, 27, 29-31, 33, 35-36, 38-41, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 72, 81-82, 86, 88-90, 93, 96, 108, 111, 124, 126-127, 129-130, 133, 135",
      "file_path": "src/pytest_llm_report/context_util.py",
      "missed": 3,
      "missed_ranges": "53, 83-84",
      "statements": 53
    },
    {
      "coverage_percent": 95.56,
      "covered": 129,
      "covered_ranges": "13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127-128, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-250, 252-254, 257, 259-260, 263-264, 271, 273-274, 276-279, 281-283, 285, 299-300, 302, 308",
      "file_path": "src/pytest_llm_report/coverage_map.py",
      "missed": 6,
      "missed_ranges": "62, 123, 125, 157, 221, 251",
      "statements": 135
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 73, 77-79, 83, 132, 142",
      "file_path": "src/pytest_llm_report/errors.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 3,
      "covered_ranges": "4-5, 7",
      "file_path": "src/pytest_llm_report/llm/__init__.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 3
    },
    {
      "coverage_percent": 86.36,
      "covered": 133,
      "covered_ranges": "4, 6-10, 12-15, 21-22, 25-30, 33, 47-48, 50-52, 56, 58-59, 65, 67-68, 70, 73-74, 76, 84, 86-90, 95-96, 98-99, 106-107, 112-113, 116, 121-126, 130, 132, 134, 137, 144, 156, 181-182, 184, 186, 188-189, 199, 211, 213-216, 221-223, 226, 249-252, 254-255, 260, 262, 264-267, 269-270, 277-279, 281, 283-284, 289-290, 292-293, 298-301, 303, 306, 329-332, 334, 336, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381",
      "file_path": "src/pytest_llm_report/llm/annotator.py",
      "missed": 21,
      "missed_ranges": "77-81, 160-168, 173, 286-287, 345, 364-365, 371",
      "statements": 154
    },
    {
      "coverage_percent": 95.42,
      "covered": 125,
      "covered_ranges": "13, 15-18, 20, 30, 33, 47, 50, 53, 59, 65-66, 68, 87-88, 96, 101, 103, 105, 128, 134-135, 137-138, 149, 155, 157, 163, 165, 174, 176, 185-186, 188, 191-198, 200, 202, 212, 214-217, 219-222, 224, 232, 243, 245, 247, 264, 266-267, 270-272, 274-275, 277, 279, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314, 316, 325-326, 329-331, 333-334, 337-339, 342-347, 351, 353, 359-360, 363-364, 367-369, 372, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404, 406",
      "file_path": "src/pytest_llm_report/llm/base.py",
      "missed": 6,
      "missed_ranges": "91-92, 230, 284, 292, 296",
      "statements": 131
    },
    {
      "coverage_percent": 95.56,
      "covered": 86,
      "covered_ranges": "8, 10-13, 20, 23-24, 27-29, 31-32, 34, 36-37, 39, 44, 53-55, 58, 67-68, 70, 73, 92-93, 95, 97, 103-106, 108-110, 112, 122-123, 126-128, 136, 139, 156-157, 160, 162, 164-167, 170-176, 181-185, 187-188, 190, 192-194, 196-197, 203-206, 209-210, 213-214, 216-218, 222, 224",
      "file_path": "src/pytest_llm_report/llm/batching.py",
      "missed": 4,
      "missed_ranges": "158, 207, 211, 220",
      "statements": 90
    },
    {
      "coverage_percent": 97.85,
      "covered": 318,
      "covered_ranges": "7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-89, 91-97, 99-114, 121-122, 125, 128, 134-135, 137-141, 143-144, 146, 164-166, 173-175, 178, 181-182, 184, 186-189, 191-192, 198-206, 208-210, 212-213, 215, 218, 221-230, 232-233, 235-237, 239-243, 246-247, 249-252, 254-255, 259, 261, 263, 268, 272-276, 279-281, 283, 288-293, 295, 299-305, 308-309, 311-312, 318-319, 322, 326, 332-333, 335, 339-343, 345-349, 352-353, 358-359, 366-367, 369, 383, 385-386, 390, 410, 413-415, 418-422, 424-427, 432, 434-435, 437, 441-444, 446, 449-463, 469, 471-473, 475-478, 480, 486, 488-491, 493, 495, 497-498, 502-508, 511, 514-516, 518-521, 523-528, 534, 537, 539-543, 547-548, 550-559, 562-564, 567-570, 574",
      "file_path": "src/pytest_llm_report/llm/gemini.py",
      "missed": 7,
      "missed_ranges": "115-117, 298, 310, 313-314",
      "statements": 325
    },
    {
      "coverage_percent": 98.7,
      "covered": 76,
      "covered_ranges": "8, 10, 12-13, 21, 31, 37-38, 41-42, 44, 51, 60-62, 64, 82-83, 89, 92, 95-96, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 122, 124-126, 129-130, 132, 135, 137, 139, 141-142, 144, 148, 170, 182-183, 186-188, 190, 192-193, 196-198, 204, 206, 211, 213, 215, 221-222, 224, 227-231, 234, 236, 242-243, 245",
      "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
      "missed": 1,
      "missed_ranges": "207",
      "statements": 77
    },
    {
      "coverage_percent": 100.0,
      "covered": 13,
      "covered_ranges": "8, 10, 12-13, 20, 26, 32, 34, 51, 53, 59, 61, 67",
      "file_path": "src/pytest_llm_report/llm/noop.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 13
    },
    {
      "coverage_percent": 98.61,
      "covered": 71,
      "covered_ranges": "7, 9, 11-12, 18, 24, 42-43, 49, 52-53, 55, 58, 60-61, 63-67, 70, 74-77, 83, 85-86, 92, 94, 96-98, 100-101, 103, 107, 113-114, 116-118, 122, 128, 130, 138, 140, 142-144, 149-150, 156, 158, 160-162, 165-167, 172-173, 178, 180, 190, 192-193, 204, 209, 211-212",
      "file_path": "src/pytest_llm_report/llm/ollama.py",
      "missed": 1,
      "missed_ranges": "90",
      "statements": 72
    },
    {
      "coverage_percent": 97.22,
      "covered": 35,
      "covered_ranges": "8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130",
      "file_path": "src/pytest_llm_report/llm/schemas.py",
      "missed": 1,
      "missed_ranges": "39",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 71,
      "covered_ranges": "7, 9-14, 17, 20, 23-24, 36-39, 41-43, 47, 59-60, 63-66, 69-72, 74, 83, 85-88, 90-91, 93, 101-103, 107-109, 111, 113-116, 120, 132-136, 139-140, 143-145, 148-150, 153-156, 158, 160-162",
      "file_path": "src/pytest_llm_report/llm/token_refresh.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 71
    },
    {
      "coverage_percent": 93.94,
      "covered": 31,
      "covered_ranges": "4, 6, 9, 20, 23, 42-43, 46-47, 51-53, 55-56, 66, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90, 93-94, 96, 98",
      "file_path": "src/pytest_llm_report/llm/utils.py",
      "missed": 2,
      "missed_ranges": "48, 78",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 253,
      "covered_ranges": "17-18, 20, 23, 26-27, 36-38, 40, 42, 49-50, 59-61, 63, 65, 72-73, 86-92, 94, 96, 107-108, 120-126, 128, 130, 135-143, 146-147, 169-185, 187-188, 190, 192, 194, 201-224, 227-228, 236-237, 239, 241, 247-248, 257-259, 261, 263, 270-271, 280-282, 284, 286, 290-292, 295-296, 333-362, 364-372, 374, 376, 394-417, 419-437, 440-441, 455-463, 465, 467, 477-479, 482-483, 500-510, 512, 518, 520, 526-540",
      "file_path": "src/pytest_llm_report/models.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 253
    },
    {
      "coverage_percent": 78.73,
      "covered": 211,
      "covered_ranges": "122, 170, 199, 202-204, 209-211, 217-219, 225-227, 233-235, 241-242, 245-254, 257-259, 265-267, 271-274, 276, 284, 293, 308, 311-312, 320-325, 327, 332-337, 340-345, 348-349, 352-353, 356-357, 360-369, 372-375, 378-393, 396-397, 400-405, 408-409, 412-413, 416-421, 426-427, 430-431, 436-439, 444-447, 449, 451, 453, 460-461, 463-464, 466-467, 470-475, 479, 482-495, 498, 502-503, 507, 510, 514-515, 519-520, 524, 527, 531, 534-536, 540-541, 545-546, 550, 553, 557, 560, 564-565, 569, 572-574, 578, 581-584, 587, 591-592, 596, 599-608, 611, 613",
      "file_path": "src/pytest_llm_report/options.py",
      "missed": 57,
      "missed_ranges": "13-15, 21-22, 98-102, 105-107, 110-115, 118-121, 138-139, 142-149, 152-155, 158-160, 163-166, 169, 180-184, 187-188, 191, 193, 278, 287, 296",
      "statements": 268
    },
    {
      "coverage_percent": 86.81,
      "covered": 158,
      "covered_ranges": "41, 44, 50, 56, 62, 68, 74, 81, 90, 96, 102, 108, 114, 122, 128, 134, 142, 148, 155, 161, 169, 176, 185, 192, 199, 208, 215, 223, 229, 235, 241, 247, 254, 260, 268, 274, 283, 289, 297, 304, 311, 328, 332, 336, 342-343, 346-347, 349, 351, 354-356, 362-363, 371-372, 399-400, 403-404, 407, 410-411, 413-414, 417-418, 420, 422-426, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 476-477, 485-487, 491-494, 497, 499, 502-507, 509, 512-514, 516-521, 523, 534-535, 558-559, 562-563, 566-568, 579-580, 583, 586-587, 590-592, 602-603, 606-608, 619-620, 623, 626, 628-629",
      "file_path": "src/pytest_llm_report/plugin.py",
      "missed": 24,
      "missed_ranges": "13, 15-18, 20-21, 23, 29-32, 35, 319, 377, 481-482, 488, 548-549, 571, 595, 611-612",
      "statements": 182
    },
    {
      "coverage_percent": 97.27,
      "covered": 107,
      "covered_ranges": "13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 114, 116, 118, 139-140, 142-144, 147, 152-153, 155-157, 159-161, 163-164, 166-167, 170-171, 173, 177, 180, 189, 192-194, 196-197, 201, 203, 216-217, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249, 251, 268, 275, 284-287",
      "file_path": "src/pytest_llm_report/prompts.py",
      "missed": 3,
      "missed_ranges": "80, 185, 233",
      "statements": 110
    },
    {
      "coverage_percent": 90.77,
      "covered": 59,
      "covered_ranges": "13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 140-142, 147, 155-157, 159, 172-177, 191, 210-211, 224, 267, 269, 285",
      "file_path": "src/pytest_llm_report/render.py",
      "missed": 6,
      "missed_ranges": "148-149, 212, 217-218, 222",
      "statements": 65
    },
    {
      "coverage_percent": 98.2,
      "covered": 164,
      "covered_ranges": "13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 113, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 310, 319, 321-322, 324-335, 337, 339, 347, 350-352, 355-356, 359-361, 364, 367, 375, 383, 385-386, 389, 392, 395, 398, 406, 408-409, 415, 417, 419, 421-432, 439, 441-442, 444-446, 454-458, 460, 462, 465, 468-469, 471, 477-481, 487-488, 495, 502, 504, 506-508, 510, 513-514, 516, 522-523",
      "file_path": "src/pytest_llm_report/report_writer.py",
      "missed": 3,
      "missed_ranges": "135-137",
      "statements": 167
    },
    {
      "coverage_percent": 97.06,
      "covered": 33,
      "covered_ranges": "11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-65, 67, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123",
      "file_path": "src/pytest_llm_report/util/fs.py",
      "missed": 1,
      "missed_ranges": "40",
      "statements": 34
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121",
      "file_path": "src/pytest_llm_report/util/hashing.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 33,
      "covered_ranges": "12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95",
      "file_path": "src/pytest_llm_report/util/ranges.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 16,
      "covered_ranges": "4, 6, 9, 15, 18, 27, 30, 39-44, 46-48",
      "file_path": "src/pytest_llm_report/util/time.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 16
    }
  ],
  "summary": {
    "coverage_total_percent": 93.04,
    "error": 0,
    "failed": 0,
    "passed": 623,
    "skipped": 0,
    "total": 623,
    "total_duration": 117.03416755599977,
    "xfailed": 0,
    "xpassed": 0
  },
  "tests": [
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008308220000117217,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assertion count', 'value': 5}",
          "{'name': 'mocks used', 'value': 3}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 118,
          "total_tokens": 221
        },
        "why_needed": "This test is needed because it checks for complexity estimation in tests that have multiple assertions and mocks."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 185-186, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008251710000024559,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"assert provider._estimate_test_complexity('') == 0\", 'expected_value': 0, 'message': \"Expected provider._estimate_test_complexity('') to return 0\"}",
          "{'description': 'assert provider._estimate_test_complexity(None) == 0', 'expected_value': 0, 'message': 'Expected provider._estimate_test_complexity(None) to return 0'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 136,
          "total_tokens": 299
        },
        "why_needed": "The test is necessary because it checks the behavior of the `Config` class when given an empty source."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022366379999994024,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'complexity_score', 'value': 'low'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 115,
          "total_tokens": 194
        },
        "why_needed": "This test is needed because it checks if Simple tests have low complexity scores."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008252009999978327,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The 'prompt_tier' field should be present in any error message.\", 'expected_value': 'prompt_tier'}",
          "{'description': \"Any error messages should contain the string 'prompt_tier'.\", 'expected_value': 'prompt_tier'}"
        ],
        "scenario": "Test invalid prompt tier",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 126,
          "total_tokens": 236
        },
        "why_needed": "To test that the 'invalid' prompt tier is considered an error during validation."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008096720000025925,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected errors for invalid prompt tiers', 'description': 'The validation should return an empty list of errors for valid prompt tiers'}"
        ],
        "scenario": "Valid prompt tiers are validated",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 142,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the `prompt_tier` parameter is correctly validated and does not cause any issues."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_valid_prompt_tiers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-220, 222, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007899950000194167,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'standard_prompt_for_complex_tests', 'expected_value': 'standard prompt for complex tests'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 122,
          "total_tokens": 202
        },
        "why_needed": "Auto mode should use standard prompt for complex tests."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008177869999883569,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'selected_prompt_type', 'value': 'MINIMAL_SYSTEM_PROMPT'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 155,
          "total_tokens": 246
        },
        "why_needed": "To ensure that the auto mode selects a minimal prompt for simple tests, which is essential for efficient and effective test execution."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 212, 214-215, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007865990000084366,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'value': 'none'}",
          "{'name': 'prompt_tier', 'value': 'minimal'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 122,
          "total_tokens": 213
        },
        "why_needed": "Config override to minimal should always use minimal prompt."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 212, 214, 216-217, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007652699999880497,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': 'standard_system_prompt'}"
        ],
        "scenario": "Config override to standard should always use standard prompt.",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 148,
          "total_tokens": 225
        },
        "why_needed": "Because the config override is not necessary in this case, and using the standard prompt is a better practice."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_standard_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 71,
          "line_ranges": "53, 56-57, 60, 62-64, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020987100000127157,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregated report should contain both test case information for each run.",
          "The number of tests in the aggregated report should be equal to the total number of runs.",
          "Each test case should have a corresponding run in the aggregated report.",
          "All retained test cases should be included in the aggregated report.",
          "Any missing or incomplete policy should not result in only one test being retained.",
          "The aggregate function should correctly handle all policy when aggregating multiple tests.",
          "The temporary directory created for the aggregation process should be deleted after the test is completed."
        ],
        "scenario": "Test that the aggregate function correctly handles all policy when aggregating multiple test cases",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 364,
          "total_tokens": 544
        },
        "why_needed": "This test prevents regression where an aggregation of multiple tests results in only one test being retained due to a missing or incomplete policy."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 8,
          "line_ranges": "53, 56-58, 110, 113-115"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0038457029999960923,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected behavior', 'description': 'The `aggregate` method should return None when the aggregation directory does not exist.', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 104,
          "total_tokens": 213
        },
        "why_needed": "The test is failing because the `aggregate` method of the Aggregator class does not check if the aggregation directory exists before aggregating data."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 79,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 204-205, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003807422000022598,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `aggregator.aggregate()` should contain only one test with an outcome of 'passed'.",
          "The number of tests in the result should be equal to 1.",
          "The outcome of the first test in the result should be 'passed'.",
          "The `run_meta` object should have a `is_aggregated` attribute set to True.",
          "The `run_meta.run_count` attribute should be equal to 2.",
          "The `summary.passed` attribute should be equal to 1 (i.e., the first test passed).",
          "The `summary.failed` attribute should be equal to 0 (i.e., no tests failed)."
        ],
        "scenario": "Test that the `aggregate` method consistently picks the latest policy for a given test case across different times.",
        "token_usage": {
          "completion_tokens": 205,
          "prompt_tokens": 477,
          "total_tokens": 682
        },
        "why_needed": "This test prevents regression where the latest policy is not picked correctly due to inconsistent timing of reports."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 3,
          "line_ranges": "45, 53-54"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008212039999762055,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'agg', 'type': 'NoneType', 'expected_type': 'NoneType'}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 110,
          "total_tokens": 194
        },
        "why_needed": "The test is necessary because the aggregator requires a directory to aggregate data from."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "53, 56-58, 110, 113-114, 117-118, 184"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013428279999914139,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` method should be called with an empty list of reports.",
          "The `aggregate` method should raise an error when given an empty list of reports.",
          "The `aggregate` method should not call the `is_file()` and `is_dir()` methods on empty lists of reports."
        ],
        "scenario": "The `aggregate` method of the Aggregator class should not be called when there are no reports to aggregate.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 201,
          "total_tokens": 343
        },
        "why_needed": "This test prevents a potential bug where the `aggregate` method is called on an empty list of reports, causing it to return `None` instead of raising an error."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 87,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138-141, 145-147, 149-150, 152-153, 155, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 40,
          "line_ranges": "42-45, 65-68, 130-133, 135-137, 139, 141-143, 190, 194-199, 201, 203, 205, 207, 210-214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002526609000000235,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Coverage was correctly deserialized from the JSON report.",
          "LLM annotation was correctly deserialized with the expected scenario, why needed, and key assertions.",
          "Token usage was correctly serialized back into a JSON object.",
          "The test can be re-serialized to ensure it remains accurate."
        ],
        "scenario": "Test that coverage and LLM annotations are properly deserialized and can be re-serialized.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 1002,
          "total_tokens": 1123
        },
        "why_needed": "Prevents regression in core functionality by ensuring accurate token usage and coverage information is preserved during serialization."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 67,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 162-169, 171-173, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017728919999910886,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `source_coverage` key exists in the `report1` dictionary and has the expected structure.",
          "The `file_path` value of the first `SourceCoverageEntry` is 'src/foo.py'.",
          "The `coverage_percent` value is 83.33%.",
          "The `covered_ranges` value is '1-5, 7-11'.",
          "The `missed_ranges` value is '6, 12'."
        ],
        "scenario": "Test source coverage summary deserialization when aggregate is configured.",
        "token_usage": {
          "completion_tokens": 158,
          "prompt_tokens": 395,
          "total_tokens": 553
        },
        "why_needed": "This test prevents a potential bug where the source coverage summary is not correctly deserialized if the aggregate configuration does not match the report format."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 19,
          "line_ranges": "259-260, 262-263, 265, 267-271, 273, 276-277, 279-280, 283, 285-286, 288"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0034536200000161443,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that aggregator._load_coverage_from_source() returns None when llm_coverage_source is not set.",
          "Verify that aggregator._load_coverage_from_source() raises a UserWarning when llm_coverage_source does not exist.",
          "Verify that aggregator._load_coverage_from_source() correctly loads coverage from the configured source file (mocking coverage.py).",
          "Verify that mock_cov.report() returns 80.0 as expected after calling cov.report().",
          "Verify that mock_mapper.map_source_coverage() correctly maps the coverage data to entries.",
          "Verify that aggregator._load_coverage_from_source() correctly returns a tuple containing one entry when successful loading is achieved."
        ],
        "scenario": "Test loading coverage from configured source file when option is not set.",
        "token_usage": {
          "completion_tokens": 189,
          "prompt_tokens": 584,
          "total_tokens": 773
        },
        "why_needed": "To prevent a potential bug where the test fails due to an incorrect assumption about the configuration."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 17,
          "line_ranges": "231, 233-247, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008504479999942305,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests passed should be equal to the original count.",
          "The number of failed tests should remain unchanged.",
          "The number of skipped tests should remain unchanged.",
          "The number of tests with unknown status (xfailed, xpassed) should remain unchanged.",
          "The error rate should be within a reasonable range (e.g., 0-100%).",
          "The coverage percentage should not decrease below the original value.",
          "The total duration of all tests should increase by at least 5 seconds to reflect the recalculated summary.",
          "The latest summary should have the same node IDs as the original test results."
        ],
        "scenario": "test_recalculate_summary verifies that the aggregator recalculates the latest summary correctly and preserves coverage percentage.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 473,
          "total_tokens": 671
        },
        "why_needed": "This test prevents regression in the aggregation logic, ensuring that the latest summary is calculated accurately and coverage percentage remains preserved."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_recalculate_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 72,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123-124, 129, 131-132, 162-167, 169, 171-173, 176, 178-180, 182, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003192531999985704,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` function should not count an invalid JSON report as a valid one.",
          "The `aggregate` function should only count the valid report (in this case, 'run1') in its run meta.",
          "The test should raise a warning when trying to aggregate a report with missing fields ('missing_fields.json').",
          "The test should assert that the aggregation result is not None and has exactly one run meta entry.",
          "The test should only count the valid report (in this case, 'run1') in its run meta."
        ],
        "scenario": "Test that skipping an invalid JSON report prevents the aggregation from counting it as a valid report.",
        "token_usage": {
          "completion_tokens": 182,
          "prompt_tokens": 352,
          "total_tokens": 534
        },
        "why_needed": "This test ensures that the `aggregate` function correctly handles reports with missing or malformed data, preventing it from incorrectly counting them as valid."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_skips_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "45, 231, 233-239, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008459600000207956,
      "file_path": "tests/test_aggregation_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "summary.total == 2",
          "summary.passed == 1",
          "summary.failed == 1",
          "summary.coverage_total_percent == 88.5",
          "summary.total_duration == 3.0"
        ],
        "scenario": "The test verifies that the aggregator recalculates the summary correctly when new tests are added and the latest summary is updated.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 299,
          "total_tokens": 424
        },
        "why_needed": "This test prevents regression in coverage calculation when new tests are added to the aggregation process, ensuring accuracy of the overall coverage percentage."
      },
      "nodeid": "tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-91, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016679949999911514,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_assembler.send_message', 'expected_output': 'optimized message'}",
          "{'name': 'mock_provider.get_messages', 'expected_output': ['optimized message']}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 112,
          "total_tokens": 209
        },
        "why_needed": "To test the batch optimization message functionality."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 50,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-128, 130, 134, 156, 181-182, 184, 211, 213-219, 221, 223"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010733050000055755,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked cache should be updated with correct data', 'expected_value': {'progress': 50, 'total': 100}, 'actual_value': {'progress': 0, 'total': 0}}",
          "{'name': 'Mocked cache should not overwrite previous progress reports', 'expected_value': {'progress': 50, 'total': 100}, 'actual_value': {'progress': 25, 'total': 75}}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 101,
          "total_tokens": 264
        },
        "why_needed": "To ensure that the progress reporting is cached correctly and not overwritten by subsequent requests."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 95,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-124, 130, 132, 134, 137-141, 144-151, 156, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001711106000016116,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'mocking', 'expected_mock_calls': [], 'actual_mock_calls': [], 'reason': \"The test is testing if the mock objects are being called correctly, which is essential for verifying the correctness of the annotator's behavior.\"}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 102,
          "total_tokens": 236
        },
        "why_needed": "The test is necessary because it checks if cached tests are skipped. This is a critical functionality for ensuring the reliability and efficiency of the annotator."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 90,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221, 223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003177614999998468,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'concurrency', 'expected_result': 'No exceptions should be raised when multiple annotators are concurrently annotating the same dataset.'}",
          "{'assertion_type': 'threading', 'expected_result': 'All annotators should finish annotating their assigned tasks within a reasonable time frame (e.g., 10 seconds).'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 98,
          "total_tokens": 237
        },
        "why_needed": "To ensure that annotators can annotate data in a concurrent manner without causing any issues."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221-223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002379504999993287,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider returns a valid annotation', 'expected_value': 'Annotation returned by mock provider'}",
          "{'name': 'Mock cache does not store annotations', 'expected_value': 'Annotations are not stored in the mock cache'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 116,
          "total_tokens": 244
        },
        "why_needed": "This test is needed because the current implementation of `annotate` may not handle failures correctly when multiple annotators are working concurrently."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003110268999989785,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider should be called with a valid task', 'expected_value': 'mock_provider.task', 'actual_value': 'mock_provider.get_task()'}",
          "{'name': 'mock_cache should not be modified during the annotation process', 'expected_value': 'None', 'actual_value': 'mock_cache'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 96,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the annotator reports progress accurately and consistently throughout the annotation process."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001536900000019159,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_type': 'MockProvider', 'actual_type': 'MockProvider'}",
          "{'name': 'mock_cache', 'expected_type': 'MockCache', 'actual_type': 'MockCache'}",
          "{'name': 'mock_assembler', 'expected_type': 'MockAssembler', 'actual_type': 'MockAssembler'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 101,
          "total_tokens": 247
        },
        "why_needed": "To ensure that the progress messages are returned correctly when annotating reports."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 91,
          "line_ranges": "47, 50-51, 58-59, 65, 67-68, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015850500000169632,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_result': {'opt_out': [1, 2], 'limit': 5}, 'actual_result': {'opt_out': [1, 3], 'limit': 10}}"
        ],
        "scenario": "tests/test_annotator.py",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 104,
          "total_tokens": 206
        },
        "why_needed": "The test is necessary to ensure that the annotator respects the opt-out and limit settings."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-257, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0018230649999964044,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 1, 'actual_value': 2}",
          "{'name': 'mock_cache', 'expected_value': 1000, 'actual_value': 2000}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 112,
          "total_tokens": 225
        },
        "why_needed": "The test respects the rate limit for annotating a single document."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 12.002042482999997,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotator behaves as expected', 'description': 'The annotator should be able to correctly identify and annotate sequential elements in the input data.'}",
          "{'name': 'sequential annotations are preserved', 'description': 'The sequential annotations should be preserved across multiple passes of the annotator, even if the input data is reordered or split into smaller chunks.'}"
        ],
        "scenario": "Sequential annotation",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 98,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the annotator can correctly annotate sequential data."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 24.002152198999994,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider should be called with error message', 'expected_output': 'Mock provider was called with an error message', 'actual_output': 'Mock provider was not called with an error message'}",
          "{'name': 'Mock cache should be cleared after error is reported', 'expected_output': 'Mock cache should be cleared after error is reported', 'actual_output': 'Mock cache remains unchanged'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 105,
          "total_tokens": 264
        },
        "why_needed": "Error tracking in sequential annotation is necessary to ensure that errors are properly reported and handled by the annotator."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 2,
          "line_ranges": "47-48"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008616089999975429,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config', 'value': \"Config(provider='none')\"}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 108,
          "total_tokens": 189
        },
        "why_needed": "The test should be skipped when the LLM (Large Language Model) is not enabled."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "47, 50-54, 56"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009268510000026708,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Provider is not available', 'expected': 'Provider is not available'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 101,
          "total_tokens": 191
        },
        "why_needed": "The test is skipped if the provider is unavailable because it prevents the test from running and potentially causing data loss."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 359-360"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008487149999893973,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Expected error message', 'expected_value': 'Failed to parse LLM response as JSON'}",
          "{'assertion_type': 'Expected error code', 'expected_value': 2}"
        ],
        "scenario": "Test Base Parse Response Malformed JSON After Extract",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 152,
          "total_tokens": 266
        },
        "why_needed": "To ensure that the `_parse_response` method correctly handles malformed JSON responses and raises a `JSONDecodeError` with a meaningful error message."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342-346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00090191499998582,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.scenario == '123'",
          "assert annotation.why_needed == ['list']",
          "assert annotation.key_assertions == ['a']"
        ],
        "scenario": "Verify that the `test_base_parse_response_non_string_fields` test case checks for non-string fields in the response data.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 269,
          "total_tokens": 372
        },
        "why_needed": "This test prevents a potential bug where the function does not handle cases with non-string fields in its response data."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007787140000061754,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider type', 'expected_type': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 104,
          "total_tokens": 187
        },
        "why_needed": "To ensure the `get_gemini_provider` function is correctly creating a `GeminiProvider` instance."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020787230000109957,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'raises', 'message': 'Unknown LLM provider: invalid'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 106,
          "total_tokens": 186
        },
        "why_needed": "To ensure that a ValueError is raised when an unknown LLM provider is specified."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008222760000080598,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider type', 'expected': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 109,
          "total_tokens": 195
        },
        "why_needed": "To ensure the `get_litellm_provider` function returns a valid instance of `LiteLLMProvider`."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007994429999769181,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider type', 'expected_value': 'NoopProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 104,
          "total_tokens": 191
        },
        "why_needed": "The test is necessary to ensure that the `get_provider` function returns a NoopProvider instance when no provider is specified."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007842349999975795,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider_type', 'expected': 'OllamaProvider', 'actual': 'isinstance(provider, OllamaProvider)'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 108,
          "total_tokens": 197
        },
        "why_needed": "To verify the correctness of the OllamaProvider class."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 134-135, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008713779999993676,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider is not raised an exception when checking its availability.",
          "The provider's checks counter is incremented correctly.",
          "The provider returns True for availability checks."
        ],
        "scenario": "Verify that the LlmProvider can be checked for availability without raising an exception.",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 280,
          "total_tokens": 379
        },
        "why_needed": "This test prevents a potential bug where the LlmProvider raises an exception when checking its availability, causing the test to fail."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007689459999937753,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"provider.get_model_name() == 'test-model'\", 'expected_value': 'test-model'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestLlmProviderDefaults",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 114,
          "total_tokens": 200
        },
        "why_needed": "To ensure that the LLM model name is set to the default configuration when no explicit model name is provided."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 155"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007672240000147212,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert get_rate_limits is None', 'description': 'The `get_rate_limits` method should return `None`.'}"
        ],
        "scenario": "tests/test_base_maximal.py",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 108,
          "total_tokens": 195
        },
        "why_needed": "This test ensures that the `get_rate_limits` method returns `None` when no default rate limits are specified."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 174"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000787661999993361,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_local() should return False', 'expected_value': False}"
        ],
        "scenario": "tests/test_base_maximal.py",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 105,
          "total_tokens": 173
        },
        "why_needed": "To ensure the LLM provider defaults to false when in local mode."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 35,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 192-194, 196-200, 203-206, 209-210, 213-214, 216-218, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007892639999909079,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'src/module.py' file is present in the prompt.",
          "The 'def helper()' function is present in the prompt.",
          "Context files should be added to the prompt according to their locations."
        ],
        "scenario": "Verify that context files are included in the batch prompt.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 261,
          "total_tokens": 369
        },
        "why_needed": "This test prevents a potential issue where context files are not added to the prompt, potentially leading to incorrect usage of the `build_batch_prompt` function."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_context_files_included",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "34, 39-40, 156-157, 160, 162, 164-168, 170-177, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007724030000133553,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Test Group: test.py::test_add[*]",
          "Parameterizations (2 variants)",
          "[1+1=2]",
          "[0+0=0]",
          "ONE annotation"
        ],
        "scenario": "Verifies that the parametrized batch prompt includes all required information.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 330,
          "total_tokens": 437
        },
        "why_needed": "This test prevents a potential regression where the prompt is missing or incorrect, making it difficult to understand and use the `build_batch_prompt` function."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_parametrized_batch_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 15,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008245499999759431,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert \"Test: test.py::test_foo\" in prompt",
          "assert \"```python\" in prompt",
          "assert source in prompt",
          "assert 'Parameterizations' not in prompt"
        ],
        "scenario": "Single test should generate normal prompt.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 269,
          "total_tokens": 377
        },
        "why_needed": "This test prevents a regression where the batched prompt is missing 'Test: test.py::test_foo' and incorrectly includes 'Parameterizations'."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_single_test_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007550709999861738,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `source` variable is set to a string containing the test function definition.",
          "Two calls to `_compute_source_hash(source)` return the same hash value.",
          "The length of the returned hash value is 32 bytes as expected.",
          "_compute_source_hash(source) should not modify or alter the input source code.",
          "The `source` variable should be a string containing the test function definition.",
          "The hash value calculated by `_compute_source_hash(source)` should match the original hash value.",
          "If the same source code is used multiple times, the returned hash values should remain consistent."
        ],
        "scenario": "Verify that the same source code produces the same hash value.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 220,
          "total_tokens": 401
        },
        "why_needed": "Prevents a potential bug where different versions of the test function produce different hashes, leading to inconsistent results."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007549200000198653,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'different', 'condition': 'hash1 != hash2'}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 127,
          "total_tokens": 212
        },
        "why_needed": "To ensure that different source code produces different hashes, which is a requirement for batch processing to work correctly."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000741484999991826,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _compute_source_hash() returns an empty string for an empty input', 'expected_result': '', 'actual_result': '_compute_source_hash()'}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 94,
          "total_tokens": 187
        },
        "why_needed": "The current implementation of ComputeSourceHash does not handle an empty source correctly."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008031999999786876,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The error message should contain 'batch_max_tests' in order to trigger this assertion.\", 'expected_value': 'batch_max_tests', 'actual_value': '0'}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 126,
          "total_tokens": 229
        },
        "why_needed": "The test is necessary because the `batch_max_tests` configuration option must be at least 1."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007861779999984719,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding must be >= 0', 'description': 'The context line padding value is negative.'}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 126,
          "total_tokens": 205
        },
        "why_needed": "Context line padding must be non-negative."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008038910000038868,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'value': 'context_compression', 'expected_value': 'invalid'}"
        ],
        "scenario": "Test invalid context compression",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 122,
          "total_tokens": 194
        },
        "why_needed": "To test that an invalid context compression mode raises an error during validation."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007996429999934662,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Context compression is enabled.', 'expected': 'None'}",
          "{'message': 'Context compression is disabled.', 'expected': 'lines'}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 133,
          "total_tokens": 208
        },
        "why_needed": "Valid compression modes should pass."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_valid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007522159999950873,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert stripped params are correct', 'expected': 'test.py::test[a-b-c]', 'actual': '_get_base_nodeid('}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 109,
          "total_tokens": 208
        },
        "why_needed": "This test is necessary because the current implementation of _get_base_nodeid does not fully strip nested parameters."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007856979999871783,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"_get_base_nodeid('tests/test_foo.py::test_add[1+1=2]'') == 'tests/test_foo.py::test_add'\", 'expected_result': 'tests/test_foo.py::test_add'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 133,
          "total_tokens": 257
        },
        "why_needed": "The test is necessary because it checks the behavior of `_get_base_nodeid` when a parameterized node id is passed."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007574349999970309,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected result', 'value': 'tests/test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 123,
          "total_tokens": 214
        },
        "why_needed": "This test is needed because it checks the behavior of _get_base_nodeid when given a simple nodeid without any parameters."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "53-54, 67-68, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008179579999989528,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of tests in each batch is equal to `batch_max_tests`.",
          "Each batch has exactly two tests.",
          "Only one batch contains three tests.",
          "All tests are present in their respective batches."
        ],
        "scenario": "Large groups should be split by batch_max_tests.",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 364,
          "total_tokens": 470
        },
        "why_needed": "This test prevents regression where large groups are not split into batches of the correct size, potentially leading to performance issues or incorrect results."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batch_max_size_respected",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 6,
          "line_ranges": "92-93, 95, 97-99"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007715309999980491,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Number of batches should be equal to number of tests', 'expected_value': 2, 'actual_value': 1}"
        ],
        "scenario": "Test case for batched tests",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 170,
          "total_tokens": 251
        },
        "why_needed": "To ensure that each test is separate and not affected by the batching mechanism."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batching_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 27,
          "line_ranges": "34, 39-40, 53-54, 67, 70, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008048539999947479,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "batches should contain exactly one group with parameterized tests.",
          "each group in batches should have exactly three tests.",
          "the first group in batches should be marked as parametrized and have its base nodeid as 'test.py::test_add'.",
          "all tests within a group should be from the same module (in this case, 'test.py')."
        ],
        "scenario": "Test parametrized tests should be grouped together.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 346,
          "total_tokens": 475
        },
        "why_needed": "This test prevents a potential regression where the grouping of tests for batching is not considered."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_parametrized_tests_grouped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008229770000127701,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Each test is in its own batch.",
          "There are two batches, one for each test.",
          "Each batch has exactly one test.",
          "The first batch contains only one test.",
          "The second batch also contains only one test.",
          "No tests are grouped together and interfere with each other's execution."
        ],
        "scenario": "Single tests should each be their own batch.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 278,
          "total_tokens": 399
        },
        "why_needed": "This test prevents a potential bug where multiple tests are grouped together and potentially interfere with each other's execution."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_single_tests_no_grouping",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008098129999893899,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Same source should produce same hash', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_consistent_hash",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 107,
          "total_tokens": 182
        },
        "why_needed": "To ensure that the cache is consistent across different runs of the test."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007558920000008129,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Hash of input 1 should not match hash of input 2', 'expected_result': 'different'}",
          "{'name': 'Hash of input 3 should match hash of input 4', 'expected_result': 'same'}"
        ],
        "scenario": "Tests for cache functionality",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 108,
          "total_tokens": 213
        },
        "why_needed": "To ensure that the cache is working correctly and producing unique hashes for different inputs."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000789884999988999,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The hash value should be 16 characters long.', 'value': 16}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_hash_length",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 100,
          "total_tokens": 177
        },
        "why_needed": "To ensure the hash value is of a fixed length (16 characters)."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_hash_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 26,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013148060000105488,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The clear method should remove all cache entries.",
          "The get method should return None for non-existent keys.",
          "The count property should be updated correctly after clearing the cache."
        ],
        "scenario": "Test that clearing the cache removes all entries.",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 283,
          "total_tokens": 365
        },
        "why_needed": "Prevents regression in case of large number of cache entries."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_clear",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 11,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008744339999964268,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cache annotation for error', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 157,
          "total_tokens": 230
        },
        "why_needed": "To ensure that LLM annotations with errors are not cached."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 9,
          "line_ranges": "39-41, 53, 55-56, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009239460000003419,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_cache.py::TestLlmCache::test_get_missing",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 128,
          "total_tokens": 198
        },
        "why_needed": "To test that the get method returns None for missing entries."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_get_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 28,
          "line_ranges": "39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001141932999985329,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check if the annotation is stored correctly in the cache.",
          "Check if the retrieved annotation matches the expected value.",
          "Verify that the confidence level of the retrieved annotation is correct."
        ],
        "scenario": "Verify that annotations can be set and retrieved from the cache.",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 286,
          "total_tokens": 379
        },
        "why_needed": "Prevents bypass attacks by ensuring that LLMCache stores and retrieves annotations in a secure manner."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_set_and_get",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008046429999808424,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': {'message': 'nodeid should be a string'}, 'expected_result': 'nodeid should be a string'}",
          "{'assertion': {'message': 'message should be a string'}, 'expected_result': 'message should be a string'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 124,
          "total_tokens": 240
        },
        "why_needed": "The test is checking if the collection errors have the correct structure."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000781710000012481,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert collector.get_collection_errors() == []', 'expected_value': [], 'message': 'Expected get_collection_errors() to return an empty list'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 114,
          "total_tokens": 217
        },
        "why_needed": "To ensure that the `get_collection_errors` method returns an empty list when the collection is initially empty."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007663910000133001,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 136,
          "total_tokens": 210
        },
        "why_needed": "Default llm_context_override should be None."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007689059999904657,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'llm_opt_out is not equal to False', 'expected_value': False, 'actual_value': 'False'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 136,
          "total_tokens": 225
        },
        "why_needed": "The default value of llm_opt_out should be False."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007947839999928874,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Output capture should be enabled by default', 'description': 'The output capture feature should be enabled by default.'}"
        ],
        "scenario": "Tests for Collector Output Capture",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 104,
          "total_tokens": 174
        },
        "why_needed": "Test that output capture is enabled by default."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007845549999956347,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert config.capture_output_max_chars == 4000', 'expected_result': 4000, 'actual_result': 'tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 108,
          "total_tokens": 213
        },
        "why_needed": "The default value for capture output max chars is not being tested."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007679850000101851,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'xfailed', 'actual_value': 'xfail'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 206,
          "total_tokens": 284
        },
        "why_needed": "To ensure that xfail failures are correctly recorded as xfailed."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 26,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007888039999954799,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'type', 'expected_value': 'xpassed'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 205,
          "total_tokens": 280
        },
        "why_needed": "xfail passes should be recorded as xpassed."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007988220000072488,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "collector.results should be an empty dictionary.",
          "collector.collection_errors should be an empty list.",
          "collector.collected_count should be zero.",
          "assert collector.results == {}",
          "assert collector.collection_errors == []",
          "assert collector.collected_count == 0"
        ],
        "scenario": "Test the `create_collector` method of `TestCollector` class.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 205,
          "total_tokens": 326
        },
        "why_needed": "This test prevents a potential bug where the collector does not initialize with empty results, potentially leading to incorrect data being collected."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_create_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008104839999987234,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'nodeids', 'expected': ['a_test.py::test_a', 'z_test.py::test_z'], 'actual': ['a_test.py::test_a', 'z_test.py::test_z']}"
        ],
        "scenario": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 227,
          "total_tokens": 333
        },
        "why_needed": "The test is necessary because it checks if the results are sorted by nodeid."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000814221000013049,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collected_count` attribute should be set to 3 after collecting all items.",
          "The `deselected_count` attribute should be set to 1 after deselecting one item."
        ],
        "scenario": "Test the `handle_collection_finish` method to ensure it correctly tracks collected and deselected counts.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 256,
          "total_tokens": 361
        },
        "why_needed": "This test prevents a potential issue where the collected count is not updated correctly when items are deselected."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_handle_collection_finish",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015309590000072149,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'collector.handle_runtest_logreport', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 211,
          "total_tokens": 301
        },
        "why_needed": "To test that the collector does not capture output when config is disabled (integration via handle_runtest_logreport)"
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00091552999998612,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected captured stderr to be empty', 'expected_value': '', 'actual_value': 'Some error'}"
        ],
        "scenario": "TestCollectorInternals test",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 157,
          "total_tokens": 233
        },
        "why_needed": "To ensure that the `collector._capture_output` method correctly captures stderr output."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000904528999996046,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'captured_stdout', 'expected_value': 'Some output'}"
        ],
        "scenario": "TestCollectorInternals test capture_output_stdout",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 157,
          "total_tokens": 226
        },
        "why_needed": "To verify that the `test_capture_output_stdout` method correctly captures stdout."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010579759999984617,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected captured stdout length', 'value': 10}"
        ],
        "scenario": "tests/test_collector_internals",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 174,
          "total_tokens": 239
        },
        "why_needed": "Truncating output exceeding max chars in TestCollectorInternals test."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 35,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021446460000049683,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "item.get_closest_marker('llm_opt_out') returns MagicMock().",
          "item.get_closest_marker('llm_context_override') returns MagicMock() with 'complete' args.",
          "item.get_closest_marker('requirement') returns MagicMock().",
          "item.get_closest_marker('llm_opt_out') is not None.",
          "item.get_closest_marker('llm_context_override') is not None.",
          "item.get_closest_marker('requirement') is not None.",
          "result.param_id == 'param1' is True.",
          "result.llm_opt_out is True.",
          "result.llm_context_override == 'complete' is True.",
          "result.requirements == ['REQ-1', 'REQ-2'] is True."
        ],
        "scenario": "Test creates a result with item markers.",
        "token_usage": {
          "completion_tokens": 213,
          "prompt_tokens": 382,
          "total_tokens": 595
        },
        "why_needed": "Prevents regression where the collector does not extract item markers correctly when they are present in an item's callspec."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014288389999990159,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 165,
          "total_tokens": 257
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009768539999868153,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _extract_error returns correct error message', 'expected_value': 'Some error occurred'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 130,
          "total_tokens": 220
        },
        "why_needed": "To ensure the `_extract_error` method returns a string that can be used to recreate the original exception."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009220319999769799,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _extract_skip_reason returns None for no longrepr', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 130,
          "total_tokens": 222
        },
        "why_needed": "To ensure the `_extract_skip_reason` method returns `None` when no longrepr is provided."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009283939999988888,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected return value', 'value': 'Just skipped'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 133,
          "total_tokens": 211
        },
        "why_needed": "To ensure the `extract_skip_reason` method returns a string as expected."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009281240000120761,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'string', 'expected_value': 'The test case is using a MagicMock object to mock the report.longrepr attribute, which may not be suitable for all test scenarios. It would be better to use a real report or a more robust mocking library.'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 164,
          "total_tokens": 334
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 21,
          "line_ranges": "58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009243060000017067,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The collector should have recorded one collection error with nodeid 'test_broken.py' and message 'SyntaxError'.",
          "The collected error should be of type 'TestCollectorReportFailure'.",
          "The collector's collection_errors list should contain the expected error object."
        ],
        "scenario": "Test the TestCollector handleCollectionReport function when a collection report fails.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 273,
          "total_tokens": 385
        },
        "why_needed": "This test prevents a regression where the TestCollector does not correctly record and log collection errors."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 42,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140-141, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238, 261, 264-265, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0023221380000109093,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'rerun' attribute of the report should be set to 1 after a runtest call.",
          "The final outcome of the test should still be 'failed' even if the rerun count is 1.",
          "The `results` dictionary in the collector's results object should contain the expected data for the test case."
        ],
        "scenario": "Test the `handle_runtest_rerun` method of TestCollector.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 281,
          "total_tokens": 403
        },
        "why_needed": "This test prevents regression in handling reruns for a specific test case."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000954774000007319,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'setup' event is recorded with an outcome of 'error'.",
          "The phase is set to 'setup'.",
          "The error message is set to 'Setup failed'."
        ],
        "scenario": "TestCollectorHandleRuntestSetupFailure verifies that the test collector correctly handles a setup failure in the runtest log report.",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 300,
          "total_tokens": 406
        },
        "why_needed": "This test prevents regression by ensuring that the test collector correctly records and reports errors during setup."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 38,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00129495899997778,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.outcome == 'error'",
          "res.phase == 'teardown'",
          "res.error_message == 'Cleanup failed'"
        ],
        "scenario": "Test Collector should record error if teardown fails after pass.",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 391,
          "total_tokens": 479
        },
        "why_needed": "To prevent regression in case of a teardown failure, where the test is expected to fail but actually succeeds due to a teardown issue."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007740960000148789,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Context compression must be one of \"none\", \"gzip\", or \"lzma\"', 'expected_value': 'invalid'}"
        ],
        "scenario": "Test invalid context compression mode",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 124,
          "total_tokens": 198
        },
        "why_needed": "To test that an invalid compression mode fails validation."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_invalid_compression_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008147220000012112,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'column': 0, 'line': 1, 'message': 'context_line_padding is not a valid value for this context type.'}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 121,
          "total_tokens": 202
        },
        "why_needed": "Negative padding should fail validation."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008178680000128224,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'context_compression': 'none'}, 'actual': {'context_compression': 'lines'}}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 135,
          "total_tokens": 199
        },
        "why_needed": "To ensure that valid compression modes pass validation."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_valid_compression_modes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007901060000108373,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config', 'type': 'Config', 'value': {'provider': 'none', 'context_line_padding': 0}}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 122,
          "total_tokens": 205
        },
        "why_needed": "Zero padding is a valid configuration option."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008809150000104182,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_compression', 'expected_value': 'lines'}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 119,
          "total_tokens": 193
        },
        "why_needed": "Context compression should be enabled by default ('lines')."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007615020000173445,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context Compression Mode', 'description': 'The context compression mode should be available.', 'expected_value': 'lines'}"
        ],
        "scenario": "Tests for Context Compression",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 113,
          "total_tokens": 190
        },
        "why_needed": "To ensure that the context compression is working correctly and providing the expected behavior."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_mode_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000769547999993847,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_line_padding', 'expected_value': 2, 'actual_value': 0}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 106,
          "total_tokens": 195
        },
        "why_needed": "To ensure that line padding is correctly set to 2 when the context provider is 'none'."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0008210539999993216,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The # ... indicator is present for contiguous lines.",
          "The line numbers L3, L4, and L5 are included in the result.",
          "No gap indicators are added to contiguous lines."
        ],
        "scenario": "Contiguous covered lines should not have gap indicators.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 293,
          "total_tokens": 387
        },
        "why_needed": "Prevents regression where contiguous lines are separated by gaps, potentially misleading the user about coverage."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_contiguous_lines_no_gap",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 216-217"
        }
      ],
      "duration": 0.0007650290000071891,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 130,
          "total_tokens": 209
        },
        "why_needed": "Empty coverage is necessary to test the ContextAssembler's ability to extract covered lines from a test with no actual execution."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 24,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249"
        }
      ],
      "duration": 0.0008343789999969431,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output should contain a '#' character followed by ' ...' to indicate gaps between ranges.",
          "The output should contain '# L3:' to indicate range starting at line 3.",
          "The output should contain '# L15:' to indicate range starting at line 15.",
          "The output should have gap indicators between the ranges of covered lines."
        ],
        "scenario": "Test that multiple covered ranges are extracted with gap indicators.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 274,
          "total_tokens": 399
        },
        "why_needed": "This test prevents regression where the output does not contain gap indicators between ranges of covered lines."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_multiple_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0008595259999992777,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_extract_covered_lines` includes lines 2, 3, and 4 in the result.",
          "Lines 2, 3, and 4 have 1 line padding.",
          "The test asserts that '# L2:', '# L3:', and '# L4:' are present in the result."
        ],
        "scenario": "Single covered line should be extracted with padding.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 302,
          "total_tokens": 420
        },
        "why_needed": "This test prevents a regression where single lines are not extracted correctly due to missing padding."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0007937420000132533,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert '# L1:' in result",
          "assert '# L2:' in result",
          "assert '# L3:' in result"
        ],
        "scenario": "Test Extracted Covered Lines: Padding should not go beyond file boundaries.",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 288,
          "total_tokens": 373
        },
        "why_needed": "This test prevents a bug where padding exceeds the file boundary, potentially causing incorrect coverage metrics."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_padding_boundary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009072949999904267,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The 'prompt' variable should contain the full content of the node.\", 'expected_value': 'short content'}",
          "{'assertion': \"The 'truncated' key in the 'prompt' dictionary should be absent.\", 'expected_value': ''}"
        ],
        "scenario": "tests/test_context_limits.py::test_no_truncation_needed",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 158,
          "total_tokens": 291
        },
        "why_needed": "This test is needed because the current implementation of `provider._build_prompt` truncates the context when it detects that a node's content exceeds the specified limit."
      },
      "nodeid": "tests/test_context_limits.py::test_no_truncation_needed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 25,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001762381999981244,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "F1 should be full (A*160).",
          "F2 should get at least 110 tokens (100+80).",
          "F2's budget should not exceed the total available tokens (220).",
          "The smart split logic uses 40 + 180 = 220 tokens, which is zero waste.",
          "F2 should be truncated to avoid exceeding their fair share of tokens (480 < 536)."
        ],
        "scenario": "test_smart_distribution verifies that the smart distribution logic prevents a regression and optimizes F2's budget.",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 773,
          "total_tokens": 942
        },
        "why_needed": "The test prevents a regression in the smart distribution logic by ensuring F2 gets at least their fair share of tokens, even if it means truncating some content to avoid waste."
      },
      "nodeid": "tests/test_context_limits.py::test_smart_distribution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000950133999992886,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string 'f1' should be truncated in the prompt.",
          "The string 'f2' should be truncated in the prompt.",
          "The keyword 'truncated' should be present in the prompt."
        ],
        "scenario": "The test verifies that the splitting logic correctly truncates strings and meets the expected requirements.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 317,
          "total_tokens": 426
        },
        "why_needed": "This test prevents a potential bug where the splitting logic does not truncate strings, leading to incorrect output or unexpected behavior."
      },
      "nodeid": "tests/test_context_limits.py::test_splitting_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "243, 245, 264, 266, 270-272, 274-275"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000997442999988607,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "len(prompt) should be less than (limit * 5)",
          "[... truncated] should be present in the prompt or 'Relevant context' should not be present",
          "prompt should contain a header indicating truncation"
        ],
        "scenario": "The test verifies that the `provider._build_prompt` function truncates prompts to fit within a specified limit.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 397,
          "total_tokens": 508
        },
        "why_needed": "This test prevents regressions where large context files cause excessive prompting and context is not relevant."
      },
      "nodeid": "tests/test_context_limits.py::test_truncation_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000770929999987402,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'line1\\n\\nline2', 'actual': 'line1\\n\\n'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 128,
          "total_tokens": 225
        },
        "why_needed": "The test is necessary because it checks the functionality of the `collapse_empty_lines` function when there are 3+ empty lines in a source string."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000743859000010616,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The collapsed source code should have only one newline character.', 'expected_value': '\\n\\nline1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 127,
          "total_tokens": 215
        },
        "why_needed": "To test the functionality of collapsing many empty lines to one blank line."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008019280000155504,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'line1\\n\\nline2', 'actual_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 125,
          "total_tokens": 208
        },
        "why_needed": "Preserves up to 2 consecutive newlines in context."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007878719999894201,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The result should be the original source string with no changes.', 'expected_result': 'line1\\nline2\\nline3'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 121,
          "total_tokens": 211
        },
        "why_needed": "To test the functionality of collapsing empty lines in a single newline scenario."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000773355000006859,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'source should be modified to have only one line with no trailing whitespace', 'expected_result': 'line1\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 137,
          "total_tokens": 239
        },
        "why_needed": "The test is necessary because it checks if the `optimize_context` function always collapses empty lines, regardless of the flags used."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 45,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001023040999996283,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'id': 'combined_optimization_process', 'expected_result': 'The combined optimization process should have been applied successfully.'}",
          "{'id': 'optimized_context', 'expected_result': 'The test context has been optimized correctly.'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 96,
          "total_tokens": 206
        },
        "why_needed": "To ensure that the combined optimization process is applied correctly to the test context."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 36,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008905439999864484,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring stripping', 'expected': True, 'actual': False}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 100,
          "total_tokens": 188
        },
        "why_needed": "The default behavior of the `optimize_context` function should strip all docstrings, not just those in comments."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007872209999959523,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'result == \"\"', 'expected_result': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 95,
          "total_tokens": 173
        },
        "why_needed": "This test is needed because the current implementation of `optimize_context` does not handle empty sources correctly."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008682210000188206,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '   \\n\\n   \\n', 'actual_value': '   \\n\\n   \\n'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 115,
          "total_tokens": 206
        },
        "why_needed": "This test is needed because the current implementation does not handle source code with only whitespace characters correctly."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 44,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009599640000033105,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'strip both', 'expected_result': {'docstring': '', 'comment': ''}}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 95,
          "total_tokens": 173
        },
        "why_needed": "To optimize the context by removing unnecessary docstrings and comments."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 14,
          "line_ranges": "81-82, 86, 88-90, 93, 108, 124, 126, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009139470000150141,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'expected_string_length', 'expected_value': 0, 'actual_value': 1}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 95,
          "total_tokens": 184
        },
        "why_needed": "To optimize the context by removing unnecessary comment lines that do not affect the functionality of the code."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007794359999877543,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'source', 'expected': 'def foo():'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 94,
          "total_tokens": 170
        },
        "why_needed": "To ensure that the optimizer can correctly handle cases where neither strip nor optimize is requested."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008203419999972539,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The expected output of the test is compared to the actual output.', 'expected_output': \"'url = \", 'actual_output': '\"http://example.com#anchor\"'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 134,
          "total_tokens": 237
        },
        "why_needed": "To ensure that the function correctly strips comments from strings containing a hash (#) symbol."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008001349999915419,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should not return a string containing an escaped quote.', 'expected_result': '# comment'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 133,
          "total_tokens": 213
        },
        "why_needed": "To ensure that the context utility correctly handles escaped quotes in strings."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008865460000038183,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '\"don\\'t # worry\"', 'actual': '\"don\\'t \\\\# worry\"'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 101,
          "total_tokens": 178
        },
        "why_needed": "To strip quotes from code that contains both single and double quotes."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008262339999873802,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': '.*#.*', 'expected_result': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_no_comments",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 91,
          "total_tokens": 175
        },
        "why_needed": "To strip comments from the source code, ensuring that only relevant information is included in the output."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_no_comments",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00080859999999916,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'strip_comments() removes leading and trailing whitespace from the entire string, including comments.', 'expected_result': '', 'actual_result': 'url = \"http://example.com#anchor\"'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 135,
          "total_tokens": 239
        },
        "why_needed": "Preserves # inside double-quoted strings in source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008347589999857519,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result == 'url = 'http://example.com#anchor',",
          "  # Expected: 'url = 'http://example.com#anchor'"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 135,
          "total_tokens": 231
        },
        "why_needed": "To ensure that comments are stripped from single-quoted strings, preserving the hash character (#) to maintain code readability."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007999540000014349,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'x = 1', 'actual_value': 'strip_comments(source)', 'message': \"Expected strip_comments(source) to return 'x = 1', but got '{}' instead.\"}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 119,
          "total_tokens": 217
        },
        "why_needed": "To remove simple end-of-line comments from the source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008401800000115145,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': \"The line should be stripped of any leading or trailing whitespace and then removed if it's a standalone comment.\", 'actual': 'The line remains unchanged.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 99,
          "total_tokens": 188
        },
        "why_needed": "To strip standalone comments from the test source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 4,
          "line_ranges": "27, 29-31"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008521819999884883,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should return the original source code without any modifications.', 'expected_result': 'def foo( unclosed paren', 'actual_result': 'def foo( unclosed paren'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 119,
          "total_tokens": 231
        },
        "why_needed": "The test is necessary because it checks that the function does not modify the original source code when a syntax error occurs."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 30,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009153299999979936,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'strip_multiple_docstrings', 'expected_result': 'Module docstring.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 95,
          "total_tokens": 181
        },
        "why_needed": "This test is needed because it checks if the context_util module can strip out multiple docstrings from a given string."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008704859999966175,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring triple quotes are preserved', 'expected_value': 'def foo():\\n    \"\"\"\\n    def bar():\\n        pass\\n    \\\\\\n    return \\'Hello, World!\\'\\n\"\"\"', 'actual_value': '\\'\"\"\"def foo():\\n    \"\"\"def bar():\\n        pass\\n    \\\\\\n    return \\'Hello, World!\\'\\n\"\"\"\\''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 103,
          "total_tokens": 254
        },
        "why_needed": "Preserve multiline data strings in docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 25,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008456189999890285,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Regular string is preserved', 'condition': \"source.contains('hello world') == True\", 'expected_result': True}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 102,
          "total_tokens": 185
        },
        "why_needed": "Preserve regular strings in test context"
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 27,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008278359999849272,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001000638999983039,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'remove_all', 'expected_result': 'str', 'actual_result': 'str'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 97,
          "total_tokens": 184
        },
        "why_needed": "To ensure that the context utility function works correctly when dealing with multiline docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009402260000115348,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `strip_triple_double_quoted_docstring` should remove all triple double-quoted docstrings from the given source code."
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 106,
          "total_tokens": 199
        },
        "why_needed": "To ensure that the context manager works correctly, especially when dealing with triple double-quoted docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008671190000200113,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'strip_triple_single_quoted_docstring', 'expected_value': '', 'message': 'Expected to return an empty string after stripping triple single-quoted docstrings.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 106,
          "total_tokens": 213
        },
        "why_needed": "The test is necessary because the current implementation does not correctly strip triple single-quoted docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 19,
          "line_ranges": "134-135, 137-141, 143-144, 476, 478, 524-531"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008285069999942607,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'm1' in models",
          "assert 'm2' in models",
          "assert provider._parse_preferred_models() == []",
          "assert provider._parse_preferred_models() == ['All']"
        ],
        "scenario": "Tests the parsing of preferred models for a Gemini configuration with edge cases.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 273,
          "total_tokens": 391
        },
        "why_needed": "This test prevents regression in case the 'm1' or 'm2' model is not available, as it would cause an error when trying to parse them."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 35,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008678920000022572,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in() method should return an error (0) for both over and under token limits when recording tokens but not requests.",
          "The next_available_in() method should return a valid time in seconds for both over and under token limits when recording tokens but not requests.",
          "The rate limiter should prevent excessive token usage without preventing the request from being processed."
        ],
        "scenario": "Verify that the rate limiter prevents over and under token limits when recording tokens but not requests.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 273,
          "total_tokens": 415
        },
        "why_needed": "This test prevents a potential bug where the rate limiter allows excessive token usage without preventing the request from being processed."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 47,
          "line_ranges": "96-103, 130-133, 135, 137-139, 141, 143, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008138600000222596,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `d['coverage_percent']` should be equal to 50.0.",
          "The value of `ann.to_dict()['error']` should be equal to 'timeout'.",
          "The value of `meta.to_dict()['duration']` should be equal to 1.0."
        ],
        "scenario": "Verify that the `to_dict()` method of `SourceCoverageEntry` and `LlmAnnotation` classes returns the expected values for coverage percent, error message, and duration.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 318,
          "total_tokens": 465
        },
        "why_needed": "This test prevents a regression where the coverage percent is not correctly calculated for `SourceCoverageEntry` instances with multiple statements covered."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000787881999997353,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mapper.config is config', 'expected_value': 'config'}",
          "{'name': 'mapper.warnings == []', 'expected_value': []}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 109,
          "total_tokens": 200
        },
        "why_needed": "To ensure the Mapper class initializes with a valid configuration."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 3,
          "line_ranges": "44-45, 308"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008056249999981446,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Type of warnings is correct', 'expected_type': 'list', 'actual_type': 'list'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 110,
          "total_tokens": 192
        },
        "why_needed": "To ensure the get_warnings method returns a list of warnings as expected."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001542741999998043,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `map_coverage()` method should return a dictionary with all keys set to False (indicating no coverage).",
          "The `map_coverage()` method should have at least one warning (indicating potential issues)."
        ],
        "scenario": "Test that the `map_coverage` function returns an empty dictionary when no coverage file exists.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 277,
          "total_tokens": 379
        },
        "why_needed": "Prevents regression in case of missing coverage files, ensuring accurate coverage reporting."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008223460000067462,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_nodeid` method returns the expected node ID for each phase.",
          "The `_extract_nodeid` method returns the same node ID across all phases.",
          "The `assert` statements check that the extracted node IDs match the expected values."
        ],
        "scenario": "The test verifies that the `CoverageMapper` correctly extracts node IDs for all phases when the `include_phase` parameter is set to 'all'.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 279,
          "total_tokens": 413
        },
        "why_needed": "This test prevents a potential bug where the `CoverageMapper` does not extract node IDs for certain phases, leading to incorrect coverage reports."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007882129999927656,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'assert mapper._extract_nodeid([]) == None', 'expected_result': 'None'}",
          "{'assertion': 'assert mapper._extract_nodeid(None) == None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 128,
          "total_tokens": 239
        },
        "why_needed": "To handle the case when the context is empty."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 216, 220, 224-225, 228-230"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007775720000040565,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is_none', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 139,
          "total_tokens": 215
        },
        "why_needed": "To filter out setup phase when include_phase=run."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008601369999894359,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"nodeid == 'test.py::test_foo'\", 'expected_result': 'test.py::test_foo'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 145,
          "total_tokens": 235
        },
        "why_needed": "To extract the correct node ID from the run phase context."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 29,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152, 156, 160-162, 167-170, 199, 202"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012269210000113162,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock_data.contexts_by_lineno.side_effect is set to raise an Exception when it encounters the first file ('file.py') in the measured_files list.",
          "The test asserts that the result of calling mapper._extract_contexts(mock_data) is an empty dictionary, indicating that the function handled the exception correctly."
        ],
        "scenario": "Test 'test_contexts_by_lineno_exception' verifies that the test_contexts_by_lineno function handles exceptions correctly.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 332,
          "total_tokens": 472
        },
        "why_needed": "The test prevents a potential regression where the test_contexts_by_lineno function fails to handle an exception when accessing contexts for files with multiple lines of code."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_contexts_by_lineno_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 7,
          "line_ranges": "44-45, 118, 121-122, 127-128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010992840000199067,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is a dictionary', 'expected_value': '{}', 'actual_value': 'assert result == {}'}"
        ],
        "scenario": "Test Extract Contexts",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 136,
          "total_tokens": 214
        },
        "why_needed": "When no measured files are present in the coverage data, an empty dictionary should be returned."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022256979999895066,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equals', 'expected_value': {}, 'actual_value': {}}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 154,
          "total_tokens": 230
        },
        "why_needed": "To skip non-Python files from coverage reports."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007857079999951111,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 166,
          "total_tokens": 311
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_coverage_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011073690000102943,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is None', 'expected_value': 'None'}",
          "{'name': \"assert any('W001' in w.code for w in mapper.warnings)\", 'expected_value': 'True'}"
        ],
        "scenario": "TestLoadCoverageData",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 153,
          "total_tokens": 246
        },
        "why_needed": "To test the scenario when no .coverage file exists."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0014168870000048628,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty list when analyzing a source code with no coverage data.",
          "The function should add a warning to the warnings list for each Analysis2 exception it catches.",
          "The function should not add any COVERAGE_ANALYSIS_FAILED warnings in this specific test case."
        ],
        "scenario": "Test that the test_analysis_exception_handling function prevents regression by catching Analysis2 exceptions and adding warnings.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 286,
          "total_tokens": 408
        },
        "why_needed": "This test verifies that the test_analysis_exception_handling function handles analysis2 exceptions correctly, preventing potential regressions."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_analysis_exception_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 18,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259-261, 273-274, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013862590000144337,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': [], 'actual_result': []}"
        ],
        "scenario": "Test handling when file has no statements.",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 178,
          "total_tokens": 253
        },
        "why_needed": "To ensure that the coverage map is correctly handled when a file contains no statements."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_empty_statements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0017753770000012992,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `covered` attribute of the first result should be equal to 2 (i.e., all test files are included).",
          "The `missed` attribute of the first result should be equal to 1 (i.e., only one test file is missed)."
        ],
        "scenario": "Test that test files are included when omit_tests_from_coverage is False.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 322,
          "total_tokens": 445
        },
        "why_needed": "This test prevents regression in case the `omit_tests_from_coverage` configuration flag is set to True without specifying a custom list of test files."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_include_test_files_when_not_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 10,
          "line_ranges": "44-45, 243-244, 246-249, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013769720000027519,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': {}, 'actual_result': {}}"
        ],
        "scenario": "test_skip_non_python_files",
        "token_usage": {
          "completion_tokens": 56,
          "prompt_tokens": 154,
          "total_tokens": 210
        },
        "why_needed": "Skip non-Python files to ensure accurate coverage reporting."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-255, 257, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012663850000080856,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'Test files are skipped when omit_tests_from_coverage is True.', 'actual_value': 'The test files are included in the coverage report.'}"
        ],
        "scenario": "Test that test files are skipped when omit_tests_from_coverage is True.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 182,
          "total_tokens": 308
        },
        "why_needed": "The test is necessary to ensure that test files are correctly skipped from the coverage report when `omit_tests_from_coverage` is set to `True`. This helps prevent false positives and ensures accurate reporting of covered code."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_test_files_when_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008183979999785151,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mapper should return the nodeid for any phase.",
          "The mapper should match the expected nodeids with the given paths.",
          "All phases should be included in the coverage report."
        ],
        "scenario": "Test that all phases are accepted when configured.",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 305,
          "total_tokens": 385
        },
        "why_needed": "Prevents regression in phase filtering functionality."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_all_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007672630000001845,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert mapper._extract_nodeid() returns None for empty string', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 115,
          "total_tokens": 200
        },
        "why_needed": "To test that an empty string does not return a node ID."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008017179999910695,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'mapper._extract_nodeid(None) == None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 114,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the `_extract_nodeid` method handles `None` inputs correctly and returns `None` as expected."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008145519999800399,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|run') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') is None"
        ],
        "scenario": "Test that run phase is the default filter.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 297,
          "total_tokens": 417
        },
        "why_needed": "This test prevents a regression where the default filter does not match the expected node ID."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_run_phase_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008214150000185327,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') is None"
        ],
        "scenario": "Test that setup phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 293,
          "total_tokens": 418
        },
        "why_needed": "Prevents a regression where the test might fail due to incorrect filtering of nodeids in the setup phase."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_setup_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233-234, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000820040999997218,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') is None"
        ],
        "scenario": "Test that teardown phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 296,
          "total_tokens": 421
        },
        "why_needed": "This test prevents a potential bug where the teardown phase is not properly filtered, leading to incorrect coverage reporting."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_teardown_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 6,
          "line_ranges": "44-45, 216, 220, 224, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007822909999788408,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'value', 'expected_value': 'test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 136,
          "total_tokens": 229
        },
        "why_needed": "This test is necessary because the current implementation of `CoverageMapper` does not correctly handle node IDs without phase delimiters."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 57,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0016354960000057872,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'test_app.py::test_one' in result",
          "assert 'test_app.py::test_two' in result",
          "assert len(one_cov) == 1 and one_cov[0].line_count == 2",
          "# Verify app.py is in test_one's coverage"
        ],
        "scenario": "Should exercise all paths in _extract_contexts to ensure full logic coverage.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 413,
          "total_tokens": 531
        },
        "why_needed": "This test prevents regression by verifying that the mapper extracts all necessary contexts for full logic coverage."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013133230000050844,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {}, 'actual': {}}"
        ],
        "scenario": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 174,
          "total_tokens": 247
        },
        "why_needed": "To test the coverage mapper's behavior when there are no test contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008012970000095265,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_nodeid` method should return the expected node ID for each line in the given phase.",
          "If the line is filtered, the method should return `None`.",
          "The method should handle cases where there are no lines in the given phase.",
          "The method should correctly extract node IDs from code without a pipe (|) separating different phases.",
          "The `_extract_nodeid` method should be able to distinguish between missing and filtered lines."
        ],
        "scenario": "The test verifies that the `CoverageMapper` correctly extracts node IDs for a scenario where there are missing lines in the code.",
        "token_usage": {
          "completion_tokens": 171,
          "prompt_tokens": 323,
          "total_tokens": 494
        },
        "why_needed": "This test prevents a potential regression where the coverage map is not accurately reporting the number of covered lines due to missing or filtered code."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001038910000005444,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return None for _load_coverage_data() when no .coverage files are found.",
          "There should be exactly one warning message with code 'W001' in mapper.warnings.",
          "All warnings should have the same code 'W001'."
        ],
        "scenario": "Test that the function correctly handles the case when no coverage files exist.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 276,
          "total_tokens": 389
        },
        "why_needed": "This test prevents a potential bug where the function would silently fail to load coverage data without raising an error."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 17,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015609959999949297,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return None when trying to load coverage data from a corrupt .coverage file.",
          "Any warnings generated by the mapper should contain 'Failed to read coverage data' in their message.",
          "The mapper's warnings should not be None, indicating that no errors were found during loading.",
          "The function should raise an exception when trying to load coverage data from a corrupted or invalid file.",
          "A mock CoverageData instance with a side effect of raising an Exception on read should be used for mocking purposes.",
          "The mocked CoverageData instance's read method should return an Exception object.",
          "Any assertions made within the try block should not raise any AssertionError, indicating that no errors were found during loading."
        ],
        "scenario": "Test that the test_load_coverage_data_read_error function handles errors reading coverage files correctly.",
        "token_usage": {
          "completion_tokens": 211,
          "prompt_tokens": 343,
          "total_tokens": 554
        },
        "why_needed": "This test prevents a potential regression where the CoverageMapper class fails to handle errors when loading coverage data from corrupted or invalid files."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0025764330000015434,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock instances of `CoverageData` returned by `mock_data_cls.side_effect` should have been updated with at least two calls to `update()`.",
          "The `update()` method should be called on the mock instance of `CoverageData` that was created in the test fixture.",
          "The `update()` method should not be called on any other mock instances of `CoverageData` returned by `mock_data_cls.side_effect`.",
          "Any mock instances of `CoverageData` that were not part of the parallel data (i.e., `mock_parallel_data1` and `mock_parallel_data2`) should not have been updated.",
          "The `update()` method should be called on any mock instance of `CoverageData` that was created in the test fixture, even if it is not a part of the parallel data.",
          "Any mock instances of `CoverageData` that were created in the test fixture but are not part of the parallel data should not have been updated.",
          "The `update()` method should be called on any mock instance of `CoverageData` that was created in the test fixture, even if it is a part of the parallel data."
        ],
        "scenario": "Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its data.",
        "token_usage": {
          "completion_tokens": 304,
          "prompt_tokens": 378,
          "total_tokens": 682
        },
        "why_needed": "This test prevents regression where the CoverageMapper does not update its data when loading coverage files from parallel directories."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 5,
          "line_ranges": "44-45, 58-60"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010954260000062277,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_load_coverage_data` method of the `CoverageMapper` class should be called with `None` as its argument when no data is available.",
          "The `map_coverage` method should return an empty dictionary (`{}`) when no data is found.",
          "No exception should be raised if no coverage data is loaded by `_load_coverage_data`.",
          "The `map_coverage` method should not throw any errors or exceptions when there is no data to map."
        ],
        "scenario": "Test that the `map_coverage` method returns an empty dictionary when `_load_coverage_data` returns None.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 228,
          "total_tokens": 395
        },
        "why_needed": "Prevents a potential bug where the `map_coverage` method does not handle cases where there is no coverage data to map."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0014100639999981013,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock_cov.analysis2 side effect is called with an exception 'Analysis failed'.",
          "mock_data.measured_files returns ['app.py'] as expected.",
          "mock_cov.get_data returns the mocked data.",
          "entries is empty after calling mapper.map_source_coverage(mock_cov)."
        ],
        "scenario": "Test that the CoverageMapper handles analysis errors during source coverage analysis.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 274,
          "total_tokens": 392
        },
        "why_needed": "This test prevents a regression where an error in analysis2 causes all files to be skipped without any meaningful output."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.001617871999997078,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function mapper.map_source_coverage returns exactly one entry for 'app.py'.",
          "The file path of the first entry is correct ('app.py').",
          "The number of statements in the first entry is correct (3).",
          "The coverage percentage of the first entry is correct (66.67%)."
        ],
        "scenario": "Verify that the test covers all paths in map_source_coverage with comprehensive coverage.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 345,
          "total_tokens": 468
        },
        "why_needed": "This test prevents regression by ensuring that all possible source files are covered under the given configuration."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007853470000043217,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `make_warning` should return a WarningCode.W001_NO_COVERAGE warning with the specified detail.",
          "The message of the warning should contain 'No .coverage file found'.",
          "The detail of the warning should be set to 'test-detail'."
        ],
        "scenario": "Test the `make_warning` factory function to verify it returns a WarningCode.W001_NO_COVERAGE warning with the specified detail.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 236,
          "total_tokens": 364
        },
        "why_needed": "This test prevents a potential bug where the `make_warning` function does not correctly identify warnings without coverage files."
      },
      "nodeid": "tests/test_errors.py::test_make_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008086109999965174,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Assertion failed: WarningCode.W001_NO_COVERAGE.value == \"W001\"'}",
          "{'message': 'Assertion failed: WarningCode.W101_LLM_ENABLED.value == \"W101\"'}",
          "{'message': 'Assertion failed: WarningCode.W201_OUTPUT_PATH_INVALID.value == \"W201\"'}",
          "{'message': 'Assertion failed: WarningCode.W301_INVALID_CONFIG.value == \"W301\"'}",
          "{'message': 'Assertion failed: WarningCode.W401_AGGREGATE_DIR_MISSING.value == \"W401\"'}"
        ],
        "scenario": "Test that warning codes have correct values.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 240,
          "total_tokens": 407
        },
        "why_needed": "Prevents a potential bug where the warning code values are incorrect, potentially leading to unexpected behavior or errors in the application."
      },
      "nodeid": "tests/test_errors.py::test_warning_code_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007703279999873303,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'code' key should contain the correct value.",
          "The 'message' key should contain the correct value.",
          "The 'detail' key should be present and have the correct value.",
          "The 'code' key should match the expected value of ReportWarning.W001_NO_COVERAGE.",
          "The 'message' key should match the expected value of ReportWarning.W001_NO_COVERAGE.",
          "The 'detail' key should match the expected value of some/path."
        ],
        "scenario": "Test ReportWarning.to_dict() method.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 276,
          "total_tokens": 420
        },
        "why_needed": "Prevents a warning that is not properly formatted in the report."
      },
      "nodeid": "tests/test_errors.py::test_warning_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000771942000000081,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `make_warning` returns an instance of `WarningCode.W101_LLM_ENABLED` with the correct code.",
          "The warning message is set to `WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]`.",
          "The detail attribute is not provided for warnings with code `WarningCode.W101_LLM_ENABLED`."
        ],
        "scenario": "Test verifies that a warning is created with the correct code and message for known code.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 222,
          "total_tokens": 347
        },
        "why_needed": "To prevent a regression where warnings are not correctly generated when using known code."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007834529999968254,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'missing_code', 'expected_type': 'WarningCode', 'actual_type': 'str'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 202,
          "total_tokens": 285
        },
        "why_needed": "To handle unknown WarningCode values that are not part of the enum."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007760400000051959,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'w.code == WarningCode.W301_INVALID_CONFIG', 'expected_result': 'True'}",
          "{'name': 'w.detail == \"Bad value\"', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 127,
          "total_tokens": 229
        },
        "why_needed": "To test the creation of a warning with detail."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00080446299998016,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Enum values should be strings.', 'expected_type': 'str'}",
          "{'message': \"WarningCode.value.startswith('W').\", 'expected_type': 'str'}"
        ],
        "scenario": "Tests failed",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 109,
          "total_tokens": 204
        },
        "why_needed": "The test 'test_codes_are_strings' is expected to pass. However, it has raised a warning."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007847459999936746,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"data == {'code': 'W001', 'message': 'No coverage'}\", 'expected_result': {'code': 'W001', 'message': 'No coverage'}}"
        ],
        "scenario": "Tests for ReportWarning class",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 147,
          "total_tokens": 238
        },
        "why_needed": "To ensure that the warning is correctly serialized to a dictionary without any additional details."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007922800000130792,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'data', 'expected': {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}, 'actual': {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 161,
          "total_tokens": 289
        },
        "why_needed": "To test the serialization of a ReportWarning object with detail."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007798469999897861,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_python_file() should return False for non-.py files', 'description': 'The function should return False for non-.py files', 'expected_result': False}",
          "{'name': 'is_python_file() should return False for non-.pyc files', 'description': 'The function should return False for non-.pyc files', 'expected_result': False}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 115,
          "total_tokens": 259
        },
        "why_needed": "The test is checking if the function correctly identifies non-.py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.000766041999980871,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assertion', 'description': 'The function `is_python_file()` should return True for .py files.'}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_python_file",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 98,
          "total_tokens": 183
        },
        "why_needed": "The function `is_python_file()` should be able to identify .py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.0011254019999853426,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_output': 'subdir/file.py', 'actual_output': 'subdir/file.py'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 144,
          "total_tokens": 223
        },
        "why_needed": "To test the functionality of making a path relative to the current working directory."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.000762464000018781,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'foo/bar', 'actual_value': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 107,
          "total_tokens": 187
        },
        "why_needed": "To ensure that the `make_relative` function returns a normalized path when no base is provided."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007923199999879671,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'foo/bar', 'actual': 'foo/bar'}",
          "{'expected': 'normalized', 'actual': 'already normalized'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 96,
          "total_tokens": 178
        },
        "why_needed": "The current implementation of `normalize_path` does not correctly handle already-normalized paths."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007897349999836933,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '/foo/bar', 'actual_value': 'foo\\\\bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 100,
          "total_tokens": 176
        },
        "why_needed": "To ensure that the `normalize_path` function correctly handles paths with forward slashes."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.00081333899998981,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '/foo/bar/', 'actual_value': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 102,
          "total_tokens": 181
        },
        "why_needed": "To ensure that the `normalize_path` function correctly removes trailing slashes from file paths."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.0008475939999925686,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'custom patterns should be excluded', 'description': \"The path 'tests/conftest.py' should be skipped due to custom patterns.\"}",
          "{'name': 'module.py should not be skipped', 'description': \"The path 'src/module.py' should not be skipped due to custom patterns.\"}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 126,
          "total_tokens": 255
        },
        "why_needed": "This test ensures that the `should_skip_path` function correctly handles custom pattern exclusion."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007965480000109437,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should not skip normal paths', 'expected': True, 'actual': False}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 96,
          "total_tokens": 172
        },
        "why_needed": "The test should be able to pass without skipping any path."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007709500000032676,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert should_skip_path is True for .git/objects/foo', 'expected_result': True, 'message': 'Expected should_skip_path to return True for the path .git/objects/foo'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 99,
          "total_tokens": 205
        },
        "why_needed": "The current implementation of `should_skip_path` does not correctly handle `.git` directories."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007664719999809222,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is True', 'expected_value': True}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 109,
          "total_tokens": 186
        },
        "why_needed": "Because the test case is testing the functionality of skipping __pycache__ directories."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007983909999893513,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Should skip venv directories', 'description': 'The function should return True for venv directories and False otherwise'}",
          "{'name': '.venv/lib/python/site.py', 'description': '.venv is a virtual environment directory, it should be skipped by the function.'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 121,
          "total_tokens": 265
        },
        "why_needed": "Because the test case `should_skip_path` is trying to check if a specific directory should be skipped, but it's actually causing an issue with the venv directories."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0008124780000002829,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `is_python_file` correctly returns False for non-.py files.",
          "The function `is_python_file` correctly returns False for non-.py files with the `.pyc` extension.",
          "The function `is_python_file` does not incorrectly identify a valid Python file as non-Python code."
        ],
        "scenario": "Verifies that a non-.py file does not match the expected behavior.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 210,
          "total_tokens": 331
        },
        "why_needed": "Prevents potential bugs where a non-.py file is incorrectly identified as Python code."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.000775118000007069,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'The function is_python_file() should return True for .py files.', 'description': 'The function should correctly identify .py files.'}",
          "{'message': 'The function is_python_file() should return True for path/to/.py files.', 'description': 'The function should correctly identify path-to/.py files.'}"
        ],
        "scenario": "Testing if a file is a Python file.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 212,
          "total_tokens": 330
        },
        "why_needed": "Prevents a potential bug where a non-Python file is incorrectly identified as such."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 12,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63, 65, 67"
        }
      ],
      "duration": 0.0012301579999984824,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return a normalized absolute path as expected.",
          "The 'project1' should be present in the result.",
          "The 'file.py' should also be present in the result.",
          "The relative_to parameter should not cause make_relative to fail for non-relative paths.",
          "make_relative should correctly normalize the absolute path when the input is not under the base directory."
        ],
        "scenario": "Test makes a relative path not under the base directory when it is not.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 301,
          "total_tokens": 436
        },
        "why_needed": "Prevents regression where make_relative fails to return normalized absolute paths for non-relative paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_path_not_under_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.0010971890000064377,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected output', 'value': 'subdir/file.py'}"
        ],
        "scenario": "Test Make Relative",
        "token_usage": {
          "completion_tokens": 62,
          "prompt_tokens": 147,
          "total_tokens": 209
        },
        "why_needed": "To test the functionality of making a relative path to a file."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0007557920000067497,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'path/to/file.py', 'actual': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 116,
          "total_tokens": 198
        },
        "why_needed": "To ensure that the `make_relative` function correctly handles cases where the base is None."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007712199999900804,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'path/to/file.py', 'actual_result': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 114,
          "total_tokens": 194
        },
        "why_needed": "To ensure that backslashes are correctly converted to forward slashes in file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0009155410000118991,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': {'message': \"Expected normalized path to be 'path/to/file.py'\"}, 'expected_result': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 110,
          "total_tokens": 206
        },
        "why_needed": "To ensure the `normalize_path` function correctly normalizes path objects, specifically when dealing with file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007737550000115334,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '/path/to/dir/', 'actual_value': 'path/to/dir'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 111,
          "total_tokens": 193
        },
        "why_needed": "To ensure that the `normalize_path` function correctly removes trailing slashes from file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.000806335999982366,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"should_skip_path('src/module.py') is False\", 'expected_result': True}",
          "{'assertion': \"should_skip_path('tests/test_foo.py') is False\", 'expected_result': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 120,
          "total_tokens": 227
        },
        "why_needed": "Regular paths are not skipped by default."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007749579999938305,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should skip .git directory', 'expected': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 102,
          "total_tokens": 185
        },
        "why_needed": "The test should skip the .git directory because it contains a Git hook that may be causing issues with the test."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007728440000107639,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should be True for venv path', 'expected_value': True, 'message': 'The function should return True for paths starting with a skip directory name.'}",
          "{'name': 'should be True for .venv path', 'expected_value': True, 'message': 'The function should return True for paths starting with a skip directory name.'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 124,
          "total_tokens": 270
        },
        "why_needed": "To ensure that the function correctly handles paths starting with a skip directory name."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007741860000010092,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should skip the __pycache__ directory."
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
        "token_usage": {
          "completion_tokens": 62,
          "prompt_tokens": 116,
          "total_tokens": 178
        },
        "why_needed": "Because the test module __pycache__ is being tested."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007548199999973804,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_site_package_directory', 'value': 'True'}"
        ],
        "scenario": "/usr/lib/python3.12/site-packages/pkg/mod.py",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 111,
          "total_tokens": 175
        },
        "why_needed": "Because it's a site-package directory."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_site_packages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007906270000148652,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'venv/lib/python3.12/site.py', 'expected_result': True}",
          "{'path': '.venv/lib/python3.12/site.py', 'expected_result': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 130,
          "total_tokens": 242
        },
        "why_needed": "The test is checking if venv directories are skipped by the `should_skip_path` function."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.001696780000003173,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should_skip_path', 'expected_result': True}",
          "{'name': 'assert_path', 'expected_result': False, 'message': 'Expected path to be excluded, but it was not.'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 132,
          "total_tokens": 242
        },
        "why_needed": "Custom exclude patterns are needed to skip certain files that contain sensitive information."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 50,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-227, 232-233, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0034509860000184744,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an error message indicating that the Gemini requests-per-day limit has been reached when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "The function should not attempt to annotate the internal node as 'passed' when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "The function should return an error message indicating that the Gemini requests-per-day limit has been reached when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "The function should not attempt to annotate the internal node as 'passed' when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "The function should return an error message indicating that the Gemini requests-per-day limit has been reached when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "The function should not attempt to annotate the internal node as 'passed' when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "The function should return an error message indicating that the Gemini requests-per-day limit has been reached when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
          "</key_assertions>"
        ],
        "scenario": "Test that the test_annotate_loop_daily_limit_hit function prevents a daily limit hit when the provider is configured with an empty _models list and a mock limiter that returns None for the daily limit.",
        "token_usage": {
          "completion_tokens": 393,
          "prompt_tokens": 367,
          "total_tokens": 760
        },
        "why_needed": "This test prevents a potential issue where the provider would exceed its daily limit of requests per day, potentially causing unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotate_loop_daily_limit_hit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 100,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-236, 239-244, 263-265, 268, 293, 295, 299-303, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0029304340000066986,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking the _call_gemini function with a side effect of raising _GeminiRateLimitExceeded.",
          "Setting the mock_call.side_effect to _GeminiRateLimitExceeded.",
          "Asserting that res.error contains 'requests-per-day' or 'rate limits reached'.",
          "_GeminiRateLimitExceeded is called with the correct arguments ('requests_per_day')",
          "The model is marked as exhausted and its status is updated correctly.",
          "The _model_exhausted_at dictionary contains the key 'm1' with a value set to True."
        ],
        "scenario": "Test that _GeminiRateLimitExceeded is raised when a request exceeds the limit.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 730,
          "total_tokens": 911
        },
        "why_needed": "To prevent regression where a request exceeds the rate limit and causes the model to be exhausted."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotation_exceptions_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 173,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181-182, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 254-255, 259, 340, 343, 346, 348-356, 358-361, 363-364, 366-367, 435, 437-439, 441-442, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-498, 502-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-564, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.16680439499998556,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The _GeminiRateLimiter is correctly configured with a requests_per_minute value of 100.",
          "The _GeminiRateLimiter is annotated with the correct internal nodeid and outcome for each test scenario.",
          "The _parse_rate_limits function returns the correct number of requests per day.",
          "The mock_config.model is set to 'fallback' correctly after patching the provider._fetch_available_models function.",
          "The input limits logic (Flash vs Pro) works as expected, with the correct value being passed to the models.",
          "The _models attribute of the provider is correctly updated with the fallback models after patching the provider._ensure_models_and_limits function."
        ],
        "scenario": "Prevents regression in coverage gaps by ensuring that the rate limiters are correctly configured and annotated for different scenarios.",
        "token_usage": {
          "completion_tokens": 238,
          "prompt_tokens": 821,
          "total_tokens": 1059
        },
        "why_needed": "This test prevents regression in coverage gaps because it ensures that the rate limiters are correctly configured and annotated for different scenarios, such as prompt_override, context too long error, RPD in parse_rate_limits, fallback models, input limits logic (Flash vs Pro)."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_coverage_gaps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 13,
          "line_ranges": "134-135, 137-141, 143-144, 524-527"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000856500000026017,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': [], 'actual_result': ['[]', []]}"
        ],
        "scenario": "TestGeminiProvider",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 156,
          "total_tokens": 229
        },
        "why_needed": "To ensure the GeminiProvider correctly handles cases where the `model` parameter is not provided or is set to 'ALL'."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_parse_preferred_models_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 10,
          "line_ranges": "39-42, 81-82, 84, 87-89"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007995940000000701,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The length of limiter._daily_requests is equal to 0 after calling _prune(time.time())', 'expected_result': 0, 'actual_result': 'True'}"
        ],
        "scenario": "TestGeminiProvider",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 157,
          "total_tokens": 251
        },
        "why_needed": "To ensure the Gemini provider is correctly pruning daily requests that are older than 24 hours."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_prune_daily_requests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 4,
          "line_ranges": "39-42"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007709799999986444,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total remaining tokens should decrease by at least 30 seconds when the last requested token is used up.",
          "The function should return before `remaining` decreases below `tokens_used + 1` if `request_tokens <= limit`.",
          "If `request_tokens` is massive, the function should return immediately and not wait for a sufficient time.",
          "The remaining tokens should start at `tokens_used` when no requests are made.",
          "The function should handle cases where `request_tokens > limit` without returning prematurely.",
          "The test should pass even if `remaining + request_tokens <= limit` is true, indicating that the loop will not return.",
          "The last requested token should be used up before the function returns."
        ],
        "scenario": "Verify that the test_tpm_available_fallback function waits for a sufficient time before allowing token requests.",
        "token_usage": {
          "completion_tokens": 220,
          "prompt_tokens": 524,
          "total_tokens": 744
        },
        "why_needed": "This test prevents regression where the Gemini provider may not wait long enough for token requests to be processed after a previous request was made."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_tpm_available_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 14,
          "line_ranges": "134-135, 137-141, 143-144, 164-165, 167-169"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008959529999970073,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `provider._annotate_internal` should return an error message indicating 'google-generativeai not installed' when the module is flagged as None in sys.modules.",
          "The function `provider._annotate_internal` should include the string 'google-generativeai' in its error message.",
          "The function `provider._annotate_internal` should raise a ValueError with the message 'google-generativeai not installed' when the module is not found in sys.modules.",
          "The function `provider._annotate_internal` should return None as expected when the module is flagged as None in sys.modules.",
          "The function `provider._annotate_internal` should include the nodeid and outcome of the test case in its error message.",
          "The function `provider._annotate_internal` should not raise an exception when the module is installed correctly."
        ],
        "scenario": "Test that the test_annotate_import_error function verifies when google-generativeai is not installed.",
        "token_usage": {
          "completion_tokens": 224,
          "prompt_tokens": 259,
          "total_tokens": 483
        },
        "why_needed": "This test prevents a potential import error caused by missing required dependencies."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_import_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0028682079999953203,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'GEMINI_API_TOKEN' key in the environment should be present.",
          "The error message should contain 'GEMINI_API_TOKEN is not set'.",
          "The annotation should fail with this error message when the token is missing.",
          "The provider should raise an exception instead of throwing a specific error.",
          "The error message should include the full path to the environment variable.",
          "The error message should be more informative than just 'GEMINI_API_TOKEN is not set'.",
          "The test should fail with this error message when the token is missing."
        ],
        "scenario": "Test that annotation fails when token is missing from the environment.",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 313,
          "total_tokens": 491
        },
        "why_needed": "Prevents a potential bug where the Gemini provider throws an error due to an unprovided GEMINI_API_TOKEN."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_no_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 214,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-237, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0050725849999935235,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by the provider matches the expected scenario.",
          "The mock_post call count is correct, indicating that the provider successfully retried the API after the first failure.",
          "The _parse_response method of the provider returns a Mock object with the correct scenario and error status code.",
          "The provider's internal state is updated correctly to reflect the retry attempt.",
          "The annotation does not contain any additional or incorrect information about the source.",
          "No other critical checks are performed by this test, but it ensures that the provider behaves as expected in this specific scenario."
        ],
        "scenario": "Test that the GeminiProvider correctly annotates a rate limit retry scenario.",
        "token_usage": {
          "completion_tokens": 171,
          "prompt_tokens": 636,
          "total_tokens": 807
        },
        "why_needed": "This test prevents regression in the GeminiProvider's ability to handle rate limit retries."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 208,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-568, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.403533699999997,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by _annotate_internal has the correct scenario 'Success Scenario'.",
          "The annotation does not have any error.",
          "The annotation's outcome is correctly set to 'passed'.",
          "_parse_response returns the correct text and tokens when called with a successful response from _call_gemini.",
          "The _build_prompt method is not mocked, which could potentially cause issues if it has complex dependencies.",
          "_annotate_internal calls _parse_response where the expected format is used.",
          "The annotation's error is None when it should be an error.",
          "The annotation's outcome is correctly set to 'passed' even though there was no error."
        ],
        "scenario": "Verify that the _annotate_success method returns a correct annotation when successful.",
        "token_usage": {
          "completion_tokens": 212,
          "prompt_tokens": 649,
          "total_tokens": 861
        },
        "why_needed": "This test prevents regression in the GeminiProvider's _annotate_internal method, which may return an incorrect annotation if the response from _call_gemini is not in the expected format."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134-135, 137-141, 143-144, 332-333, 335"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0026225290000070345,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `GeminiProvider` instance is created with the correct `provider` set to 'gemini' when environment variable 'GEMINI_API_TOKEN' is present.",
          "The `_check_availability()` method returns False when environment variable 'GEMINI_API_TOKEN' is not present.",
          "The `_check_availability()` method returns True when environment variable 'GEMINI_API_TOKEN' is present and correct."
        ],
        "scenario": "Verifies that the GeminiProvider class correctly checks for availability based on environment variables.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 235,
          "total_tokens": 382
        },
        "why_needed": "This test prevents a potential bug where the provider may not be available due to missing or incorrect environment variables."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_availability",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 111,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-237, 239-244, 263-265, 268, 272-276, 279-281, 283-286, 288-292, 318-320, 322-323, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 60.003525427,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the provider's _model_exhausted_at dictionary contains 'm1' after a mock ResourceExhausted exception is raised.",
          "Verify that the provider's _cooldowns dictionary contains 'm1' after a mock ResourceExhausted exception is raised and the cooldown period exceeds 5.5 seconds.",
          "Verify that the provider correctly handles retry after cleanup by checking if the cooldown period for 'm1' has exceeded 5.5 seconds.",
          "Verify that the provider's _rate_limiters are properly set up to avoid network calls when calling _get_rate_limiter -> _ensure_rate_limits.",
          "Verify that the provider raises a MockResourceExhausted exception with the correct message and error code when calling _call_gemini.",
          "Verify that the provider correctly handles model exhaustion by checking if 'm1' is in the _model_exhausted_at dictionary after a mock ResourceExhausted exception is raised.",
          "Verify that the provider's cooldowns are properly set up to handle retry after cleanup.",
          "Verify that the provider does not raise an error when calling _call_gemini with a mock ResourceExhausted exception."
        ],
        "scenario": "Test that the GeminiProvider class correctly handles retry exceptions and model exhaustion when calling _annotate_internal with a mock ResourceExhausted exception.",
        "token_usage": {
          "completion_tokens": 317,
          "prompt_tokens": 651,
          "total_tokens": 968
        },
        "why_needed": "This test prevents regression in the GeminiProvider class, where it may not handle retry exceptions or model exhaustion correctly when calling _annotate_internal."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_exceptions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 97,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-94, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 254, 259, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0034106509999958234,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider._model_exhausted_at[model] = None",
          "provider._models[model] == [model]",
          "provider._rate_limiters[model].next_available_in.return_value == 0",
          "mock_call.return_value[0][0] == 'response'",
          "mock_call.return_value[1][0] == LlmTokenUsage(10, 10, 20)",
          "assert mock_limiter.next_available_in.called_once_with(0)",
          "assert provider._rate_limiters[model].next_available_in.called_once_with(0)",
          "assert provider._model_exhausted_at[model] is None"
        ],
        "scenario": "Test that the GeminiProvider correctly clears _model_exhausted_at when annotating with a successful call to _call_gemini.",
        "token_usage": {
          "completion_tokens": 210,
          "prompt_tokens": 482,
          "total_tokens": 692
        },
        "why_needed": "The test prevents regression where the _model_exhausted_at is not cleared after a successful annotation, potentially leading to incorrect assertions in other tests."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_loop_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-356, 358-361, 363-364, 366-367"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011620600000128434,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected error to be raised', 'description': 'The test should expect an exception to be raised when attempting to limit rate with a non-numeric value.'}"
        ],
        "scenario": "TestGeminiProviderDetailed::test_ensure_rate_limits_error",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 156,
          "total_tokens": 256
        },
        "why_needed": "To test that the ` GeminiProvider` raises an exception when rate limiting is attempted with a non-numeric value."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_ensure_rate_limits_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010338619999856746,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'models', 'expected_value': [], 'type': 'list'}",
          "{'name': 'limit_map', 'expected_value': {}, 'type': 'dict'}"
        ],
        "scenario": "TestGeminiProviderDetailed.test_fetch_available_models_error",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 132,
          "total_tokens": 230
        },
        "why_needed": "To test the error handling of fetching available models when a network error occurs."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 476-477, 537, 539-543, 547-548, 550-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015011339999944084,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'm1' model should not be included in the list of available models.",
          "The 'm2' model should not be included in the list of available models.",
          "The 'm3' model should be included in the list of available models.",
          "The 'inputTokenLimit' field of the 'm3' model should match the expected value.",
          "The 'supportedGenerationMethods' field of the 'm3' model should only contain 'generateContent'.",
          "The 'limitMap' dictionary should not contain the 'm1', 'm2', or 'm3' models."
        ],
        "scenario": "Test that fetching available models with invalid JSON data prevents a bug related to model validation.",
        "token_usage": {
          "completion_tokens": 187,
          "prompt_tokens": 340,
          "total_tokens": 527
        },
        "why_needed": "This test verifies that the GeminiProvider class correctly handles invalid JSON input when fetching available models."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 486, 488-491, 493"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021273029999804294,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_ensure was called once', 'expected_value': 1}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 144,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method of the GeminiProvider class calls the mock function correctly."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "134-135, 137-141, 143-144, 449-457, 459-460, 463-466"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008363919999965219,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.requests_per_minute', 'expected_value': 'None'}",
          "{'name': 'config.tokens_per_minute', 'expected_value': 100}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 156,
          "total_tokens": 259
        },
        "why_needed": "This test ensures that the GeminiProvider can correctly parse rate limits from a JSON configuration."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 11,
          "line_ranges": "39-42, 81-85, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007831129999829045,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `_request_times` should be equal to 1 after pruning.",
          "The length of `_token_usage` should be equal to 1 after pruning.",
          "The value of `now - 10.0` in `_request_times[0]` should match the current time.",
          "_request_times[-1] == now - 61.0",
          "_token_usage[-1][0] == now - 61.0",
          "_token_usage[-1][1] == 10"
        ],
        "scenario": "Verify that the `prune_logic` method correctly removes old requests and updates token usage when a new request is added.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 323,
          "total_tokens": 503
        },
        "why_needed": "This test prevents regression in the `prune_logic` method, which may cause outdated data to be returned for existing requests."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_prune_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 6,
          "line_ranges": "39-42, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007783030000041435,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'len(limiter._token_usage) == 0', 'expected_result': 0, 'actual_result': 1}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_record_tokens_invalid",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 127,
          "total_tokens": 222
        },
        "why_needed": "The test is failing because the rate limiter is not correctly handling invalid token records."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_record_tokens_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007982109999886688,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'limiter.record_request() should not be called when no requests are made', 'expected_result': 'None'}",
          "{'name': 'next_available_in() should return None for 100 requests', 'expected_result': 'None'}"
        ],
        "scenario": "Test that the rate limiter does not exceed the limit when no requests are made.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 129,
          "total_tokens": 262
        },
        "why_needed": "The test ensures that the rate limiter does not allow more requests than allowed by the configuration, which would result in a 'RateLimitExceeded' error."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008463910000102715,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `next_available_in` method returns 0.0 when there are no more available slots.",
          "The `next_available_in` method returns a value between 0 and 60.0 when there are still available slots.",
          "The `record_request` method is called before the third request waits for an available slot."
        ],
        "scenario": "Verify that the rate limiter does not block the third request after two successful requests.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 280,
          "total_tokens": 410
        },
        "why_needed": "This test prevents a potential issue where the third request is blocked due to insufficient available time for subsequent requests."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 100-101, 103-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007883829999855152,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert wait > 0",
          "assert wait <= 60.0 + 1e-9",
          "# Tokens have been exhausted, but usage is still within limit"
        ],
        "scenario": "Verify that the rate limiter correctly handles requests exceeding the limit when there are no tokens available.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 377,
          "total_tokens": 491
        },
        "why_needed": "This test prevents a potential bug where the rate limiter does not properly handle scenarios where there are no tokens available and more than one minute has passed since the last request."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_seconds_until_tpm_available_branches",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008526719999792931,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `wait_for_slot` method raises an instance of `_GeminiRateLimitExceeded` with the correct `limit_type` attribute.",
          "The `limit_type` attribute of the exception raised by `wait_for_slot` is set to `"
        ],
        "scenario": "Verify that the `wait_for_slot` method raises an exception when the daily limit is exceeded.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 263,
          "total_tokens": 389
        },
        "why_needed": "This test prevents a potential bug where the rate limiter does not raise an exception when the daily limit is exceeded, potentially causing unexpected behavior or errors in downstream systems."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_daily_limit_exceeded",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001572827999979154,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "..."
        ],
        "scenario": "Test that the `wait_for_slot` method sleeps for a sufficient amount of time when waiting for an available slot.",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 325,
          "total_tokens": 411
        },
        "why_needed": "This test prevents regression where the rate limiter does not sleep long enough to allow subsequent requests to wait their turn, potentially leading to performance issues or errors."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_sleeps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0008247499999924912,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'compute_config_hash(config1) != compute_config_hash(config2)', 'expected_result': 'different'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 119,
          "total_tokens": 201
        },
        "why_needed": "To ensure that different configurations of the Compute API produce different hashes."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007626740000148402,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'The length of the hash should be equal to 16'}",
          "{'message': 'The hash should not be longer than 16 characters'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 109,
          "total_tokens": 196
        },
        "why_needed": "To ensure the computed hash is short and does not exceed 16 characters."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 6,
          "line_ranges": "32, 44-48"
        }
      ],
      "duration": 0.000886214999979984,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'file_hash', 'expected_value': 'content_hash'}"
        ],
        "scenario": "File hashing consistency",
        "token_usage": {
          "completion_tokens": 60,
          "prompt_tokens": 144,
          "total_tokens": 204
        },
        "why_needed": "To ensure that the hash of a file matches its content."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "44-48"
        }
      ],
      "duration": 0.0009216020000053504,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The computed SHA-256 hash should be 64 bytes long.', 'expected_value': 64, 'actual_value': 0}",
          "{'description': 'The file contents should not be modified during the test.', 'expected_value': 'hello world', 'actual_value': 'hello world'}"
        ],
        "scenario": "Hashing a file",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 124,
          "total_tokens": 238
        },
        "why_needed": "To test the correctness of the hash computation function."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007794160000003103,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'different', 'actual': 'same'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_different_key",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 125,
          "total_tokens": 190
        },
        "why_needed": "To ensure that different keys produce different signatures."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_different_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0008231970000167621,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected length of signature', 'value': 64}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_with_key",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 108,
          "total_tokens": 178
        },
        "why_needed": "To verify the correctness of HMAC computation with a key."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_with_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0008246099999951184,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'hash': '...'}, 'actual': {'hash': '...'}}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_consistent",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 115,
          "total_tokens": 195
        },
        "why_needed": "To ensure that the hash function is consistent and produces the same output for the same input."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_consistent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007502309999836143,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The length of the hash should be equal to 64.', 'expected_result': 64}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_length",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 103,
          "total_tokens": 191
        },
        "why_needed": "To ensure the length of the computed SHA-256 hash is 64 characters (64 hexadecimal digits)."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.08113413099999889,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'includes pytest package', 'description': \"The 'pytest' package should be present in the dependency snapshot.\"}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 102,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the 'pytest' package is included in the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.0818408909999846,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'snapshot is a dict', 'expected': 'dict', 'actual': 'True'}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 98,
          "total_tokens": 180
        },
        "why_needed": "To ensure that the `get_dependency_snapshot` function returns a dictionary as expected."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "73, 76-77, 80-81"
        }
      ],
      "duration": 0.0009212510000224938,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'my-secret-key\\n', 'actual_value': '', 'error_message': ''}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 145,
          "total_tokens": 227
        },
        "why_needed": "To test that the hmac.load function correctly loads a key from a file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 4,
          "line_ranges": "73, 76-78"
        }
      ],
      "duration": 0.0008450490000200261,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected the function to return None when the key file does not exist.'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 126,
          "total_tokens": 202
        },
        "why_needed": "The test should return None if the key file does not exist."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 2,
          "line_ranges": "73-74"
        }
      ],
      "duration": 0.0007942130000060388,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert is None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 110,
          "total_tokens": 184
        },
        "why_needed": "Because the test case requires a valid HMAC key to be loaded."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007804880000037429,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.aggregate_dir is None",
          "config.aggregate_policy == 'latest'",
          "config.aggregate_include_history is False"
        ],
        "scenario": "Verify aggregation configuration defaults.",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 201,
          "total_tokens": 271
        },
        "why_needed": "Prevents a potential bug where aggregation settings are not properly initialized with default values."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007867799999985436,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert config.capture_failed_output is True', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 107,
          "total_tokens": 180
        },
        "why_needed": "The test captures failed output by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000789754999999559,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.llm_context_mode', 'expected_value': 'minimal'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 107,
          "total_tokens": 184
        },
        "why_needed": "To ensure the context mode is set to 'minimal' by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 4,
          "line_ranges": "123, 171, 284, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007538380000084999,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_llm_enabled', 'expected_value': False, 'actual_value': 'not_llm_enabled'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 109,
          "total_tokens": 190
        },
        "why_needed": "LLM is currently disabled by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007874309999920115,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.omit_tests_from_coverage', 'value': 'True'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 109,
          "total_tokens": 189
        },
        "why_needed": "The test is necessary because it checks the default behavior of omitting tests from coverage."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008266340000204764,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.provider == 'none'"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 101,
          "total_tokens": 165
        },
        "why_needed": "The provider is set to 'none' by default, which may not be suitable for all use cases."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008003450000160228,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Exclude secret files by default', 'description': \"The function should exclude 'secret' files and '.env' files from the list of excludes.\"}",
          "{'name': 'Exclude .env files', 'description': \".env files are a common source of sensitive information, so it's essential to exclude them.\"}"
        ],
        "scenario": "Integration test of gate configuration",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 132,
          "total_tokens": 250
        },
        "why_needed": "To ensure that secret files are excluded from the LLM context."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.011443054000011443,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "nodeids are sorted by nodeid",
          "tests are passed",
          "output is deterministic"
        ],
        "scenario": "The test verifies that the deterministic output of the integration gate is correctly reported.",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 313,
          "total_tokens": 397
        },
        "why_needed": "This test prevents a regression where the deterministic output may not be reported correctly due to changes in the test data or configuration."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.010848092999992787,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests should be zero.",
          "The summary section should have no data.",
          "No error messages or warnings should be printed.",
          "The output file should not contain any report content.",
          "The report writer should not raise an exception when writing to a non-existent file.",
          "The test suite should not produce any invalid JSON data."
        ],
        "scenario": "Test that an empty test suite produces a valid report.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 240,
          "total_tokens": 361
        },
        "why_needed": "This test prevents regression in case the test suite is empty."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04523747900000785,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file named `report.html` exists in the temporary directory.",
          "The string '<html>' is present in the content of the `report.html` file.",
          "The string 'test_pass' is present in the content of the `report.html` file."
        ],
        "scenario": "Test that the full pipeline generates an HTML report.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 270,
          "total_tokens": 378
        },
        "why_needed": "This test prevents regression where the pipeline does not generate an HTML report even when all tests pass."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/_git_info.py",
          "line_count": 2,
          "line_ranges": "2-3"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 138,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06654836699999578,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' key is present in the JSON report and has the correct value.",
          "The 'summary' key contains the correct total, passed, failed, and skipped counts.",
          "The 'passed', 'failed', and 'skipped' keys contain the expected values for each type of test.",
          "The number of tests with a status of 'skipped' is equal to the number of tests that were not executed (i.e., 'skipped' tests).",
          "The total count of all tests is 3, as specified in the test results.",
          "The passed count is 1, as specified in the test results.",
          "The failed count is 1, as specified in the test results.",
          "The skipped count is 1, as specified in the test results."
        ],
        "scenario": "Verify that a full pipeline generates a valid JSON report with the correct schema version, summary statistics, and skipped tests.",
        "token_usage": {
          "completion_tokens": 240,
          "prompt_tokens": 419,
          "total_tokens": 659
        },
        "why_needed": "This test prevents regression in the integration gate by ensuring that the full pipeline correctly generates a JSON report with the expected structure and content."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008272549999901457,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "'schema_version' is present in `data`",
          "'run_meta' is present in `data`",
          "'summary' is present in `data`",
          "'tests' is present in `data`"
        ],
        "scenario": "Tests ReportRoot has required fields.",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 250,
          "total_tokens": 342
        },
        "why_needed": "This test ensures that the report root contains all necessary fields for a valid schema."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008004849999849739,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_aggregated', 'expected_result': True}",
          "{'name': 'run_count', 'expected_result': 0}"
        ],
        "scenario": "{'description': 'RunMeta has aggregation fields.', 'expected_result': {'schema': {'type': 'object', 'properties': {'is_aggregated': {'type': 'boolean'}, 'run_count': {'type': 'integer'}}}, 'assertions': [{'key_assertion': ['is_aggregated'], 'expected_result': True}, {'key_assertion': ['run_count'], 'expected_result': 0}]}}",
        "token_usage": {
          "completion_tokens": 246,
          "prompt_tokens": 136,
          "total_tokens": 382
        },
        "why_needed": "The test is necessary because the RunMeta object does not have an 'aggregation_fields' property. The presence of this property in the schema indicates that aggregation fields are required for a RunMeta object to be valid."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009351260000016737,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be present in the data.",
          "The 'interrupted' field should be present in the data.",
          "The 'collect_only' field should be present in the data.",
          "The 'collected_count' field should be present in the data.",
          "The 'selected_count' field should be present in the data."
        ],
        "scenario": "Test 'RunMeta has run status fields' verifies that the RunMeta object contains status fields.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 237,
          "total_tokens": 372
        },
        "why_needed": "This test prevents a potential regression where the RunMeta object is missing required status fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007782829999882779,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'SCHEMA_VERSION', 'type': 'string'}",
          "{'name': '.', 'type': 'boolean'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 103,
          "total_tokens": 190
        },
        "why_needed": "The schema version is defined to ensure compatibility with gate APIs."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007796959999950559,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field should be present in the TestCaseResult object.",
          "The 'outcome' field should be present in the TestCaseResult object.",
          "The 'duration' field should be present in the TestCaseResult object."
        ],
        "scenario": "Test 'test_case_has_required_fields' verifies that the TestCaseResult object has required fields.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 223,
          "total_tokens": 339
        },
        "why_needed": "This test prevents a potential bug where a TestCaseResult object is created without all necessary fields (nodeid, outcome, duration)."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.0017179429999885,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `result` variable should be `None`.",
          "The `error` attribute of the `result` object should not be `None`.",
          "The `test_source` field of the `result` object should be an empty string.",
          "The `context_files` dictionary of the `result` object should be an empty dictionary.",
          "The `provider.annotate()` method should raise an exception when API calls fail."
        ],
        "scenario": "Test that all retries are exhausted when API calls fail.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 346,
          "total_tokens": 486
        },
        "why_needed": "Prevents regression where LiteLLMProvider fails to retry after exhausting all retries."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_all_retries_exhausted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 38,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001224446999998463,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The API call should fail with a 500 status code.",
          "The annotation should contain an error message.",
          "The annotation should not have any retries set.",
          "The number of retries should be less than or equal to the maximum retries (1 in this case).",
          "No force refresh is triggered when the API call fails with a 500 status code.",
          "The test source does not contain any function that triggers token refresh.",
          "The context files are empty, which means no external dependencies are being refreshed."
        ],
        "scenario": "Test that non-401 errors don't force token refresh.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 367,
          "total_tokens": 526
        },
        "why_needed": "Prevents regression in case of non-401 error without forcing token refresh."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_non_401_error_no_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 47,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.003156037999986,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The API call is mocked to fail twice, then succeed with a successful response.",
          "The test scenario is asserted as 'test scenario'.",
          "The error message is None after the retry attempt.",
          "The LLM completes successfully with a valid response."
        ],
        "scenario": "Test that retry succeeds after transient error.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 433,
          "total_tokens": 535
        },
        "why_needed": "To ensure the LLM can recover from transient errors and still complete successfully."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_retry_succeeds_after_transient_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 54,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-188, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.399022752999997,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the LLM is retried after a 401 error has been encountered.",
          "The test verifies that the LLM will not be retried if the API call succeeds first.",
          "The test verifies that the LLMTokenRefreshRetry test suite catches and reports the retry attempt.",
          "The test verifies that the LLMTokenRefreshRetry test suite ensures the token is refreshed after a 401 error has been encountered.",
          "The test verifies that the LLMTokenRefreshRetry test suite prevents regression in API call handling.",
          "The test verifies that the LLMTokenRefreshRetry test suite handles 401 errors correctly and does not retry without a valid token.",
          "The test verifies that the LLMTokenRefreshRetry test suite reports the retry attempt to the user or logs an error message."
        ],
        "scenario": "Test that 401 error triggers token refresh when API call fails first, then succeeds.",
        "token_usage": {
          "completion_tokens": 229,
          "prompt_tokens": 473,
          "total_tokens": 702
        },
        "why_needed": "To ensure the LLMTokenRefreshRetry test suite covers cases where the API call fails before a retry attempt."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_token_refresh_on_401",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008312829999965743,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 131,
          "total_tokens": 211
        },
        "why_needed": "The test is necessary because the Gemini model requires a specific provider to be used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007941540000047098,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected_value': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 140,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the LiteLLMProvider class is correctly instantiated when a specific provider is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008060360000001765,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider should be NoneType', 'description': 'The provider returned by the GetProvider method should be NoneType.'}",
          "{'name': 'provider should not be an instance of LLMProvider', 'description': 'The provider returned by the GetProvider method should not be an instance of LLMProvider.'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 115,
          "total_tokens": 253
        },
        "why_needed": "This test is necessary because the LLM's GetProvider method returns a NoopProvider when the provider is None."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007854479999878095,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected_value': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 154,
          "total_tokens": 234
        },
        "why_needed": "To ensure that the OllamaProvider is correctly returned when a specific provider is specified."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000824190000003,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 125,
          "total_tokens": 205
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_unknown_raises",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007895239999982095,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider should have annotate method",
          "provider should have is_available method",
          "provider should have get_model_name method",
          "provider should have config attribute"
        ],
        "scenario": "Test that NoopProvider implements LlmProvider contract.",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 232,
          "total_tokens": 322
        },
        "why_needed": "Prevents a potential bug where the NoopProvider does not implement all required methods of LlmProvider."
      },
      "nodeid": "tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008437059999835128,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation is of type LlmAnnotation",
          "annotation scenario is an empty string",
          "annotation why_needed is an empty string",
          "annotation key_assertions are an empty list"
        ],
        "scenario": "The test verifies that the NoopProvider returns an empty annotation when no annotation is specified.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 249,
          "total_tokens": 352
        },
        "why_needed": "This test prevents a regression where the NoopProvider does not return any annotation for a function with no annotations."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 67"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008097230000032596,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert get_model_name() == ''\", 'expected_result': '', 'actual_result': 'NoopProvider.get_model_name()'}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 114,
          "total_tokens": 214
        },
        "why_needed": "This test is needed because the model name is not being returned correctly when an empty string is passed to get_model_name."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 59"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008721590000106971,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_available()', 'expected_value': True, 'actual_value': 'True'}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_is_available",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 108,
          "total_tokens": 182
        },
        "why_needed": "The LLM is not available."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_is_available",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000779546000018172,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'required', 'description': \"The field 'scenario' must be present in the annotation.\", 'type': 'assertion'}",
          "{'name': 'why_needed', 'description': 'The schema does not allow optional fields, and required fields are present.', 'type': 'assertion'}"
        ],
        "scenario": "This test is designed to ensure that the `ANNOTATION_JSON_SCHEMA` correctly requires certain fields.",
        "token_usage": {
          "completion_tokens": 160,
          "prompt_tokens": 115,
          "total_tokens": 275
        },
        "why_needed": "The purpose of this test is to verify that the schema does not allow optional fields and that required fields are present."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007964579999963917,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "checks password",
          "checks username"
        ],
        "scenario": "Test that AnnotationSchema.from_dict parses a dictionary correctly.",
        "token_usage": {
          "completion_tokens": 55,
          "prompt_tokens": 274,
          "total_tokens": 329
        },
        "why_needed": "Prevents incorrect parsing of user data from a dict."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007804669999984526,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'schema.scenario', 'value': '', 'expected_value': ''}",
          "{'name': 'schema.why_needed', 'value': '', 'expected_value': ''}"
        ],
        "scenario": "This test checks if the AnnotationSchema can handle an empty input.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 109,
          "total_tokens": 217
        },
        "why_needed": "The test is necessary because the AnnotationSchema requires a non-empty string for the scenario and why-neededs fields."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007985809999979665,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"schema.scenario should be equal to 'Partial only'\", 'expected_result': 'Partial only'}",
          "{'assertion': 'schema.why_needed should be empty', 'expected_result': ''}"
        ],
        "scenario": "Test case for testing AnnotationSchema",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 119,
          "total_tokens": 217
        },
        "why_needed": "This test is necessary to ensure the AnnotationSchema handles partial input correctly."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007485680000058892,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert isinstance(ANNOTATION_JSON_SCHEMA, dict)",
          "assert len(ANNOTATION_JSON_SCHEMA) > 0",
          "assert all(key in ANNOTATION_JSON_SCHEMA for key in ['scenario', 'why_needed', 'key_assertions'])"
        ],
        "scenario": "The test verifies that the schema has required fields.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 215,
          "total_tokens": 372
        },
        "why_needed": "This test prevents a bug where the schema is missing required fields, potentially leading to errors or inconsistencies."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "90-92, 94-96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007549100000119324,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assertion 1: The 'scenario' key in the data dictionary matches the expected value.",
          "assertion 2: The 'why_needed' key in the data dictionary matches the expected value.",
          "assertion 3: The 'key_assertions' list in the data dictionary contains all expected assertions."
        ],
        "scenario": "TestAnnotationSchema::test_schema_to_dict verifies that the AnnotationSchema instance correctly serializes to a dictionary.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 247,
          "total_tokens": 376
        },
        "why_needed": "This test prevents regression by ensuring that the AnnotationSchema instance can be serialized and deserialized correctly."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008057150000126967,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected': 'None', 'got': 'None'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 118,
          "total_tokens": 204
        },
        "why_needed": "The test is necessary to ensure that the factory returns a NoopProvider for provider='none'."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008309419999932288,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'isinstance(provider, LlmProvider)', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 117,
          "total_tokens": 201
        },
        "why_needed": "To ensure that the NoopProvider class correctly implements the LlmProvider interface."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007985219999966375,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result.scenario == \"\" (empty string)",
          "assert result.why_needed == \"\" (empty string)",
          "assert result.key_assertions == [] (no key assertions performed)"
        ],
        "scenario": "The NoopProvider returns an empty annotation when the test function does not have any annotations.",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 253,
          "total_tokens": 357
        },
        "why_needed": "This test prevents a regression where the NoopProvider incorrectly returns an empty annotation for tests with no annotations."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008201630000144178,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `result` attribute is present and has the correct name (`scenario`, `why_needed`, and `key_assertions`) on the `TestCaseResult` object.",
          "The `result` attribute has the correct type (e.g., a dictionary or an instance of `TestCaseResult`)",
          "The `outcome` attribute is set to `"
        ],
        "scenario": "Verify that the `annotate` method returns a `TestCaseResult` object with the correct attributes.",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 263,
          "total_tokens": 401
        },
        "why_needed": "This test prevents regression where the `annotate` method does not return an expected `TestCaseResult` object."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008122870000022431,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The provider should return a valid TestCaseResult object even when the test has an empty code.', 'expected_result': \"TestCaseResult(nodeid='test::nodeid', outcome='passed')\", 'actual_result': {'nodeid': 'test::nodeid', 'outcome': 'passed'}}"
        ],
        "scenario": "Provider handles empty code gracefully",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 145,
          "total_tokens": 256
        },
        "why_needed": "Test case to ensure the provider can handle scenarios with empty code."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008280060000060985,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The annotate method should return a non-None result even when the test case has no outcome.', 'expected_result': 'True'}"
        ],
        "scenario": "Provider handles None context gracefully",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 148,
          "total_tokens": 226
        },
        "why_needed": "To ensure the provider can handle None context without throwing an error."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 15,
          "line_ranges": "65-66, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009209309999960169,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"has attribute 'annotate'\", 'expected_result': 'True'}",
          "{'name': 'is callable on provider.annotate', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 145,
          "total_tokens": 241
        },
        "why_needed": "To ensure that all providers have an annotate method."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 187,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 263-265, 299, 311-312, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524-525, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009595230000059018,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'expected_exception', 'message': 'An exception was raised when annotating a context that was too large.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 98,
          "total_tokens": 188
        },
        "why_needed": "Because the annotation process is too resource-intensive for large contexts."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-195, 471-473, 497-498, 502-503, 537"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009018249999996897,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation message contains the correct error message indicating that 'litellm' is missing and how to install it.",
          "The annotation message includes the correct path to install 'litellm'.",
          "The annotation message includes the correct provider name ('litellm') in the error message."
        ],
        "scenario": "Test that the LiteLLMProvider annotates a missing dependency correctly.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 270,
          "total_tokens": 386
        },
        "why_needed": "This test prevents a bug where the provider does not report an error for missing dependencies."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000854365999998663,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `error` attribute of the annotation object should be set to 'GEMINI_API_TOKEN is not set'.",
          "The `annotation.error` attribute should contain the string 'GEMINI_API_TOKEN is not set'.",
          "The `provider.annotate` method should raise an error with the message 'GEMINI_API_TOKEN is not set' when no API token is provided.",
          "The `annotation.error` attribute should be a string containing the message 'GEMINI_API_TOKEN is not set'.",
          "The `annotation.error` attribute should contain the exact phrase 'GEMINI_API_TOKEN is not set'.",
          "The `provider.annotate` method should raise an exception with the specified error message when no API token is provided.",
          "The `annotation.error` attribute should be a string that starts with 'GEMINI_API_TOKEN is not set'.",
          "The `annotation.error` attribute should contain the exact phrase 'GEMINI_API_TOKEN is not set' followed by a colon and a space."
        ],
        "scenario": "Test that the `annotate` method of a GeminiProvider object raises an error when no API token is provided.",
        "token_usage": {
          "completion_tokens": 295,
          "prompt_tokens": 440,
          "total_tokens": 735
        },
        "why_needed": "To prevent a potential bug where the `annotate` method fails to raise an error when an API token is missing, allowing the test case to pass even if the provider is not properly configured."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 220,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-455, 457, 459-460, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010071210000148767,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `.annotate` method of the `GeminiProvider` instance records the specified number and type of tokens.",
          "The `annotate_records_tokens` test verifies that the `annotate` method does not raise an exception when called with a valid input.",
          "The rate limits logic is tested to ensure it runs without error."
        ],
        "scenario": "Verify that the `annotate_records_tokens` test prevents regressions by ensuring tokens are recorded correctly.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 783,
          "total_tokens": 907
        },
        "why_needed": "To prevent regressions caused by a change in how token usage is handled."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010720020000007935,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked API call with rate limit exceeded error', 'expected_output': {'error_code': 429, 'message': 'Rate limit exceeded'}, 'actual_output': {}}",
          "{'name': 'Mocked API call with retry logic executed', 'expected_output': {'retry_count': 1}, 'actual_output': {}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 98,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the LLM provider can correctly handle rate limiting and retry logic."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 210,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526-527, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011316739999926995,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'model_rotation', 'expected_value': 'True'}",
          "{'name': 'daily_limit', 'expected_value': 1}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 100,
          "total_tokens": 200
        },
        "why_needed": "To ensure that the model rotation is applied correctly on a daily limit."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010194040000044424,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The annotate method should not skip any annotations when there are no annotations to skip.', 'expected_result': {}, 'actual_result': {}}",
          "{'name': 'The annotate method should skip all annotations when the daily limit is reached.', 'expected_result': {'annotations_skipped': []}, 'actual_result': {'annotations_skipped': ['annotation_1', 'annotation_2']}}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 98,
          "total_tokens": 249
        },
        "why_needed": "Because the `annotate` method is skipping annotations on a daily limit."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 209,
          "line_ranges": "39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010659010000040325,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect"
        ],
        "scenario": "Test that LiteLLM provider annotates a successful response with the correct key assertions and confidence level.",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 474,
          "total_tokens": 539
        },
        "why_needed": "Prevents regression by ensuring the annotation is accurate for valid responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 222,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011563600000101815,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'model recovered within 24h', 'description': 'The model should be available and functional again after 24 hours.'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 104,
          "total_tokens": 185
        },
        "why_needed": "The LLM provider's model should recover from being exhausted after 24 hours."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 68,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-349, 352-356, 358-361, 363-364, 366-367, 435, 437-439, 441-444, 449-452, 463-466, 476, 478, 497-498, 502-508, 511, 514-516, 518-521, 524-525, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008535349999760911,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected a KeyError to be raised', 'description': 'When no models are available, the `fetch_available_models` method should raise a KeyError.'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 92,
          "total_tokens": 181
        },
        "why_needed": "To ensure that the `fetch_available_models` method raises an error when no models are available."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 201,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00131183999999962,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Model list should be refreshed after interval', 'description': 'The model list should be updated with new data after the specified interval.'}"
        ],
        "scenario": "Tests for LLM providers",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 96,
          "total_tokens": 174
        },
        "why_needed": "To ensure that the model list refreshes after a specified interval."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 50,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010476269999912802,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the provider retries after refreshing the token.",
          "Verify that the first failed attempt is due to a 401 Unauthorized error.",
          "Verify that the second successful attempt is obtained with the refreshed token.",
          "Verify that the captured keys match the expected values (token-1 and token-2).",
          "Verify that the provider correctly sets the `litellm_token_refresh_command` and `litellm_token_refresh_interval` attributes.",
          "Verify that the `fake_completion` function is called with a valid API key for the first attempt, and an error for the second attempt."
        ],
        "scenario": "Test that LiteLLM provider retries on 401 after refreshing token.",
        "token_usage": {
          "completion_tokens": 186,
          "prompt_tokens": 580,
          "total_tokens": 766
        },
        "why_needed": "Reason: The current implementation does not retry when the token is refreshed, causing a failure in test_401_retry_with_token_refresh."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_401_retry_with_token_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008656770000072811,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should contain an error message indicating a completion error.",
          "The error message should contain the string 'boom'.",
          "The annotation should raise a RuntimeError exception when trying to annotate a test with a completion error.",
          "The provider should be able to surface completion errors in annotations by setting the litellm module in sys.modules.",
          "The config should be created with the correct provider (in this case, LiteLLMProvider).",
          "The LiteLLMProvider should create an instance of SimpleNamespace with a completion function set to fake_completion.",
          "The annotation should contain the correct key ('def test_case(): assert True')."
        ],
        "scenario": "The test verifies that the LiteLLMProvider annotates completion errors correctly.",
        "token_usage": {
          "completion_tokens": 188,
          "prompt_tokens": 307,
          "total_tokens": 495
        },
        "why_needed": "This test prevents a regression where the provider does not surface completion errors in annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 43,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 206, 211"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009278429999994842,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'response_data' dictionary should be a list.",
          "The 'response_data' dictionary should contain only string keys.",
          "Invalid response: key_assertions must be a list",
          "The 'response_data' dictionary should not have any non-string keys.",
          "The 'response_data' dictionary should not have any non-string values.",
          "</key_assertions>"
        ],
        "scenario": "Test that LiteLLMProvider rejects invalid key_assertions payloads.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 346,
          "total_tokens": 481
        },
        "why_needed": "To prevent regression and ensure the correct behavior of LiteLLMProvider when receiving invalid key_assertions payloads."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 8,
          "line_ranges": "37-38, 41, 82-86"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008493060000205332,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'litellm not installed. Install with: pip install litellm'",
          "assert annotation.provider is None",
          "# The provider should be None when annotating a missing dependency"
        ],
        "scenario": "The LiteLLMProvider annotates the missing dependency 'litellm' in the test case.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 271,
          "total_tokens": 387
        },
        "why_needed": "This test prevents a potential bug where the provider reports an error due to a missing required library, causing the test to fail or produce incorrect results."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009736879999877601,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect"
        ],
        "scenario": "Test that the LiteLLMProvider annotates a valid response payload successfully.",
        "token_usage": {
          "completion_tokens": 61,
          "prompt_tokens": 475,
          "total_tokens": 536
        },
        "why_needed": "Prevents regressions by ensuring the annotation is correct for successful responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009014329999956772,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'prompt_override' key in the annotation should be set to 'CUSTOM PROMPT'.",
          "The content of the 'messages' field in the captured messages should match the expected value.",
          "The error message should not have any issues or warnings.",
          "The custom prompt should override the default one used by LiteLLMProvider.",
          "The key 'why_needed' in the annotation should contain the expected message.",
          "The key 'key_assertions' in the annotation should contain the expected assertion."
        ],
        "scenario": "Test that LiteLLMProvider overrides the prompt when provided.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 373,
          "total_tokens": 532
        },
        "why_needed": "To ensure that the LiteLLM provider uses the custom prompt instead of the default one."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009913819999951556,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token_usage` attribute of the annotation returned by `provider.annotate(test, 'src')` is not None.",
          "The value of `prompt_tokens` in `annotation.token_usage` is set to 100.",
          "The value of `completion_tokens` in `annotation.token_usage` is set to 50.",
          "The total number of tokens extracted from the response is 150."
        ],
        "scenario": "Test LiteLLM provider to annotate with token usage.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 426,
          "total_tokens": 553
        },
        "why_needed": "Prevents regression in token usage extraction from responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182-183, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009305979999965075,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `api_base` should be 'https://proxy.corp.com/v1'.",
          "The `litellm_api_base` attribute should have been updated with the correct value.",
          "The `api_base` key in the response data should match the expected value.",
          "The `key_assertions` list should contain only one assertion.",
          "The `response_data` dictionary should have a single key-value pair.",
          "The `json.dumps(response_data)` function should return a string representation of the dictionary.",
          "The `FakeLiteLLMResponse` class should create a response object with the expected data.",
          "The `litellm_api_base` attribute of the `FakeLiteLLMResponse` object should be set to 'https://proxy.corp.com/v1'."
        ],
        "scenario": "Test that the LiteLLM provider passes `api_base` to completion call.",
        "token_usage": {
          "completion_tokens": 222,
          "prompt_tokens": 387,
          "total_tokens": 609
        },
        "why_needed": "This test prevents regression in case where `api_base` is not set correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000980221000020265,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the captured API key matches the expected value.",
          "Check if the `litellm_api_key` attribute of the `fake_litellm` object is set to 'static-key-placeholder'.",
          "Verify that the `response_data` dictionary contains the expected keys ('scenario', 'why_needed', and 'key_assertions').",
          "Check if the `response_data` dictionary has a key named 'api_key' with value 'static-key-placeholder'.",
          "Verify that the `FakeLiteLLMResponse` object created by `fake_completion` returns a JSON response with the expected structure.",
          "Check if the captured API key is not set to an empty string or None.",
          "Verify that the `litellm_api_key` attribute of the `config` object has been updated correctly.",
          "Check if the `provider` object created by `LiteLLMProvider` has a valid `litellm_api_key` attribute."
        ],
        "scenario": "The test verifies that the LiteLLMProvider passes a static API key to the completion call.",
        "token_usage": {
          "completion_tokens": 260,
          "prompt_tokens": 384,
          "total_tokens": 644
        },
        "why_needed": "This test prevents regression in passing an API key through the completion call, ensuring consistent behavior with previous tests."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 36,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 132-133, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000979830000005677,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `fake_completion` raises a `FakeAuthError` exception with message '401 Unauthorized'.",
          "The error message returned by the LiteLLMProvider is 'Authentication failed'.",
          "The test case passes if the annotation of the provider contains an error message.",
          "The error message is not None.",
          "The error message contains the string 'Authentication failed'."
        ],
        "scenario": "Test that the LiteLLM provider returns an authentication error when no refresher is configured.",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 338,
          "total_tokens": 479
        },
        "why_needed": "This test prevents a bug where the provider does not raise an exception for authentication errors without token refresh."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_error_without_refresher",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 51,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 129-130, 132-133, 141-142, 170-174, 176-178, 182, 186-188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 31,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.001569589000013,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should raise an AuthenticationError with message 'Authentication failed' when the refresh operation fails.",
          "The provider should set the annotation.error attribute to a string containing 'Authentication failed'.",
          "The provider's completion function should not be called after a second failure.",
          "The provider's authentication error should be raised with a message of 'Authentication failed'.",
          "The provider's auth error should contain the token as an attribute.",
          "The provider's refresh command should return a non-zero exit code when the auth error is raised.",
          "The provider's llm_max_retries attribute should not be exceeded after a second failure.",
          "The test case should fail if the LiteLLMProvider does not raise an AuthenticationError on subsequent retries."
        ],
        "scenario": "Test that the LiteLLMProvider reports an authentication error when retrying after a second failure.",
        "token_usage": {
          "completion_tokens": 218,
          "prompt_tokens": 419,
          "total_tokens": 637
        },
        "why_needed": "To prevent the test from passing if the LiteLLMProvider fails to report an authentication error on subsequent retries."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_retry_fails_on_second_attempt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008101239999973586,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error is not None",
          "assert 'Context too long for this model' in str(annotation)",
          "assert 'scenario' in str(annotation)",
          "assert 'why_needed' in str(annotation)",
          "assert 'key_assertions' in str(annotation)",
          "assert 'error' in str(annotation)",
          "assert 'json.dumps(...)' in str(annotation)"
        ],
        "scenario": "The test verifies that the LiteLLMProvider class handles a context too long error correctly.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 370,
          "total_tokens": 514
        },
        "why_needed": "This test prevents a bug where the provider throws an exception when given an invalid response containing a key assertion with no value."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_context_too_long_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227-228, 230-231"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007862390000070718,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 16384, 'actual_value': 16384}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_dict_format",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 218,
          "total_tokens": 302
        },
        "why_needed": "To ensure that the LiteLLM provider correctly handles dict format from get_max_tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_dict_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227, 232-234"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007605809999802204,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected JSON response', 'value': {'scenario': 'tests/test_llm_providers.py', 'why_needed': 'To ensure the LLM provider returns a valid JSON response when an error occurs.'}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 101,
          "total_tokens": 199
        },
        "why_needed": "To ensure the LLM provider returns a valid JSON response when an error occurs."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 9,
          "line_ranges": "37-38, 41, 221-222, 224, 227-229"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008230480000008811,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected return value', 'value': 8192, 'description': 'The expected return value of get_max_context_tokens should be 8192.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 213,
          "total_tokens": 312
        },
        "why_needed": "To test the get_max_context_tokens method of LiteLLMProvider."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 6,
          "line_ranges": "37-38, 41, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008922170000005281,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'fake_litellm', 'expected_type': 'SimpleNamespace'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 160,
          "total_tokens": 238
        },
        "why_needed": "To ensure the LiteLLM provider can detect installed modules."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 41,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009466079999924659,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the 'api_key' attribute of the case result matches the expected token value.",
          "Check if the 'why_needed' key in the test result contains the correct reason for the bug.",
          "Verify that the 'key_assertions' list in the test result includes the expected assertion.",
          "Check if the captured data from the subprocess is correctly updated with the fake completion function call.",
          "Verify that the subprocess returns a CompletedProcess object with returncode 0 and stdout 'dynamic-token-789'.",
          "Check if the subprocess returns an empty string for stderr.",
          "Verify that the config object passed to the provider has the correct values.",
          "Check if the provider is correctly set up to use the fake LitellmResponse object."
        ],
        "scenario": "Test the LiteLLMProvider's token refresh integration.",
        "token_usage": {
          "completion_tokens": 222,
          "prompt_tokens": 442,
          "total_tokens": 664
        },
        "why_needed": "The test prevents a potential bug where the TokenRefresher is not able to refresh tokens for a long time, causing the LLM provider to fail to authenticate."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_token_refresh_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 42,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009176840000009179,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the provider raises ConnectionError when it encounters a network error.",
          "Verify that the provider retries at least once before raising an exception.",
          "Verify that the provider does not raise an exception for non-network errors (e.g., authentication errors).",
          "Verify that the provider returns a successful response with a specific JSON payload after retrying transient errors.",
          "Verify that the test passes with the correct number of calls (3) when encountering 2 failures and 1 success."
        ],
        "scenario": "The test verifies that the LiteLLMProvider retries transient errors and passes with the correct number of calls.",
        "token_usage": {
          "completion_tokens": 171,
          "prompt_tokens": 426,
          "total_tokens": 597
        },
        "why_needed": "This test prevents a regression where the provider fails to retry on transient errors, potentially leading to unexpected behavior or failures in critical applications."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_transient_error_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 70,
          "line_ranges": "65-66, 87-89, 97-99, 101, 103, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 243, 245, 264, 266-267, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 27,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 85-86, 92, 138, 140, 142-144, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00092990699999973,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Context Length Error', 'expected_result': 'A fallback strategy should be applied to handle context length errors.'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 103,
          "total_tokens": 182
        },
        "why_needed": "To ensure that the LLM provider correctly handles context length errors during annotation."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 18,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 97-98, 100-101, 103-104"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008449979999909374,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should return an error message indicating that the annotation failed due to a call error.",
          "The error message should include the last error raised by Ollama.",
          "The error message should indicate that the annotation was unable to handle the call error within the specified retries.",
          "The error message should not be generic but specific to the call error.",
          "The annotation should raise an exception when a call to Ollama raises an exception.",
          "The annotation should log or report the call error in some way."
        ],
        "scenario": "Test OllamaProvider::test_annotate_handles_call_error verifies that the annotate method handles call errors by returning an appropriate error message.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 347,
          "total_tokens": 540
        },
        "why_needed": "This test prevents a regression where the annotation fails with a generic 'Failed after X retries. Last error: <error>' when a call to Ollama raises an exception."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "42-46"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008256720000190398,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'httpx not installed. Install with: pip install httpx'",
          "assert annotation.nodeid == 'tests/test_sample.py::test_case'",
          "assert annotation.outcome == 'passed'"
        ],
        "scenario": "The Ollama provider should report an error when the httpx dependency is missing.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 268,
          "total_tokens": 373
        },
        "why_needed": "This test prevents a bug where the provider incorrectly reports a missing dependency without providing any useful information."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 13,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 96"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008328059999769266,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'LLM provider should return a JSON response with the expected key assertions', 'value': {'scenario': 'tests/test_llm_providers.py', 'why_needed': 'To ensure that the LLM provider can correctly annotate runtime errors and immediately fail.'}}",
          "{'name': 'JSON response should contain the correct keys', 'value': {'scenario': 'tests/test_llm_providers.py', 'why_needed': 'To ensure that the JSON response is correctly formatted and contains the expected keys.'}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 99,
          "total_tokens": 261
        },
        "why_needed": "To ensure that the LLM provider can correctly annotate runtime errors and immediately fail."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_runtime_error_immediate_fail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008981379999966066,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the response status code is 200 (OK) and contains a JSON object with the expected scenario, why-need, and key assertions.",
          "Check if the token is valid by validating its presence and correctness in the response.",
          "Ensure that the response includes the required 'response' field in the JSON structure.",
          "Verify that the 'scenario', 'why-need', and 'key_assertions' fields are present in the response as expected.",
          "Test that the provider correctly raises an exception for a 401 Unauthorized status code when attempting to login with invalid credentials.",
          "Check if the error message is properly formatted and contains relevant information about the authentication process."
        ],
        "scenario": "Test the Ollama provider's full annotation flow with mocked HTTP responses.",
        "token_usage": {
          "completion_tokens": 190,
          "prompt_tokens": 414,
          "total_tokens": 604
        },
        "why_needed": "Prevents authentication-related bugs in the Ollama provider."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52-53, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008880590000046595,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should not contain any error messages.",
          "The custom prompt should be included in the output messages.",
          "The custom prompt should override the original prompt specified in the config.",
          "The custom prompt should be present in the LiteLLM response.",
          "The custom prompt should have a content that is different from the original prompt.",
          "The custom prompt should not cause any error messages to be generated."
        ],
        "scenario": "Test that LiteLLM provider uses prompt_override when provided.",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 373,
          "total_tokens": 514
        },
        "why_needed": "To ensure the correct behavior of the LiteLLM provider, where it overrides prompts with custom ones."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 40,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71, 74-80, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000892185999987305,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token_usage` attribute is populated with correct values.",
          "The `prompt_tokens` value matches the expected number of tokens (100).",
          "The `completion_tokens` value matches the expected number of tokens (50).",
          "The `total_tokens` value matches the expected total number of tokens (150)."
        ],
        "scenario": "Tests the `annotate` method of `LiteLLMProvider` with token usage data.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 426,
          "total_tokens": 549
        },
        "why_needed": "Prevents regression in handling token usage from LiteLLM responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000850950000000239,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'response' key in the result dictionary should be equal to 'test response'.",
          "The 'url' key in the captured dictionary should match the URL of the API call.",
          "The 'json' key in the captured dictionary should contain the expected JSON data.",
          "The 'model' key in the captured dictionary should be set to the correct model name.",
          "The 'prompt' key in the captured dictionary should be equal to the provided prompt.",
          "The 'system' key in the captured dictionary should be equal to the provided system prompt.",
          "The 'stream' key in the captured dictionary should be False (indicating no stream is being generated).",
          "The 'timeout' key in the captured dictionary should match the expected timeout value."
        ],
        "scenario": "Test Ollama provider makes correct API call when calling OLLAMA successfully.",
        "token_usage": {
          "completion_tokens": 217,
          "prompt_tokens": 470,
          "total_tokens": 687
        },
        "why_needed": "This test prevents regression where OLLAMA fails to make the API call with a valid response."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008323950000033165,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured JSON response contains the expected default model.",
          "The captured JSON response does not contain any custom model specified by the user.",
          "The captured JSON response has a 'model' key with value 'llama3.2'.",
          "The captured JSON response does not have a 'model' key or its value is different from 'llama3.2'."
        ],
        "scenario": "Ollama provider uses default model when not specified.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 344,
          "total_tokens": 489
        },
        "why_needed": "This test prevents a regression where the Ollama provider defaults to the 'llama3.2' model even if no model is provided in the config."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 6,
          "line_ranges": "113-114, 116-117, 119-120"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007751680000183114,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider._check_availability() should return False', 'expected_value': False, 'actual_value': 'True'}"
        ],
        "scenario": "TestOllamaProvider::test_check_availability_failure",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 183,
          "total_tokens": 270
        },
        "why_needed": "The test checks if the Ollama provider correctly returns False when the server is unavailable."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007927599999959511,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider._check_availability() is False', 'expected_value': False, 'message': 'Expected provider._check_availability() to return False'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 197,
          "total_tokens": 301
        },
        "why_needed": "The test checks if the Ollama provider returns False for non-200 status codes."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008221150000053967,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The response status code should be 200 (OK).",
          "The URL '/api/tags' should be present in the request URL.",
          "A valid HTTP response from the server should be received."
        ],
        "scenario": "The test verifies that the Ollama provider checks availability successfully by making a GET request to /api/tags.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 296,
          "total_tokens": 409
        },
        "why_needed": "This test prevents regression in case the API endpoint changes or is down, ensuring the Ollama provider can still function correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008458499999903779,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context_length_key', 'expected_value': 'max_context_tokens'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 99,
          "total_tokens": 175
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns the correct context length key for a given scenario."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_context_length_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 11,
          "line_ranges": "138, 140, 142-147, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008135700000195811,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'The maximum number of context tokens is exceeded. This may be due to various reasons such as insufficient memory, excessive model complexity or incorrect hyperparameter settings.', 'expected_value': 1000}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 101,
          "total_tokens": 193
        },
        "why_needed": "To handle cases where the maximum context tokens are exceeded during training."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008221059999868885,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 32, 'actual_value': 0}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 99,
          "total_tokens": 183
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns the correct number of context tokens for a given model info."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_model_info",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 15,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 158, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015152500000112923,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 32, 'actual_value': 0}",
          "{'name': 'context_token_type', 'expected_value': 'text', 'actual_value': 'token'}"
        ],
        "scenario": "Tests for LLM providers",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 97,
          "total_tokens": 194
        },
        "why_needed": "To ensure the correct number of context tokens is returned from the `get_max_context_tokens_from_parameters` method."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_parameters",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 10,
          "line_ranges": "138, 140, 142-147, 149, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007925200000045152,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'status code', 'expected': 200, 'actual': 'None'}",
          "{'name': 'error message', 'expected': 'Error: max_context_tokens exceeded', 'actual': ''}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_non_200_status",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 101,
          "total_tokens": 241
        },
        "why_needed": "This test is needed because the `get_max_context_tokens` method returns a status of 200 when there are less than 200 context tokens, but it should return an error message instead."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_non_200_status",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 1,
          "line_ranges": "128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000804924000021856,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is not None', 'expected_result': 'True'}",
          "{'name': 'provider.is_local() is True', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 123,
          "total_tokens": 227
        },
        "why_needed": "To ensure the Ollama provider always returns `is_local=True`."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "65-66, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008011760000101731,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotation.error', 'value': 'Failed to parse LLM response as JSON'}"
        ],
        "scenario": "Ollama provider reports invalid JSON responses",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 138,
          "total_tokens": 211
        },
        "why_needed": "To ensure the Ollama provider correctly handles and reports invalid JSON responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008080190000043785,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'test_parse_response_invalid_key_assertions', 'description': 'Test case for Ollama provider when invalid key_assertions are provided in the response data.'}"
        ],
        "scenario": "{'description': 'Test case for Ollama provider when invalid key_assertions are provided in the response data.', 'expected_output': {'error': 'Invalid response: key_assertions must be a list'}}",
        "token_usage": {
          "completion_tokens": 209,
          "prompt_tokens": 174,
          "total_tokens": 383
        },
        "why_needed": "The test is necessary to ensure that the Ollama provider correctly handles invalid key_assertions payloads in its responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008395479999876443,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'JSON is present in response', 'expected': 'The response contains a valid JSON string.'}",
          "{'name': 'JSON is properly formatted', 'expected': 'The JSON string is properly formatted and consistent with the expected format.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 127,
          "total_tokens": 248
        },
        "why_needed": "To ensure that the Ollama provider correctly extracts JSON from markdown code fences."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007723829999974896,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'response is a string', 'expected': 'str', 'actual': 'response'}",
          "{'name': 'response contains an opening brace', 'expected': 'True', 'actual': '<br/>'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 128,
          "total_tokens": 246
        },
        "why_needed": "to test the parsing of JSON responses from plain markdown fences (no language)"
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000957168000013553,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert a is not None",
          "assert b is not None",
          "assert 'scenario' in annotation.scenario",
          "assert 'why_needed' in annotation.why_needed",
          "assert 'key_assertions' in annotation.key_assertions"
        ],
        "scenario": "Test the Ollama provider's ability to parse valid JSON responses.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 292,
          "total_tokens": 405
        },
        "why_needed": "Prevents bugs in the LLM providers by ensuring that they correctly identify and extract relevant information from JSON responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008093419999966045,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total content allocated to `small.py` should be approximately equal to its required content plus overhead.",
          "The total content allocated to `large.py` should be greater than or equal to its required content plus overhead, considering the remaining budget and available files.",
          "Both `small.py` and `large.py` should have their expected allocations within a reasonable range of each other."
        ],
        "scenario": "Verify water-fill algorithm satisfies smaller files first.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 396,
          "total_tokens": 532
        },
        "why_needed": "This test prevents regression where the algorithm does not satisfy the constraint of distributing tokens to smaller files first due to insufficient budget."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_constrained",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 2,
          "line_ranges": "42-43"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007659800000112682,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': {'message': 'Expected distribute_token_budget({}, 100) to return {}'}, 'expected_result': '{}'}",
          "{'assertion': {'message': \"Expected distribute_token_budget({'f1': 'c'}, 0) to return {}\"}, 'expected_result': '{}'}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 115,
          "total_tokens": 245
        },
        "why_needed": "This test ensures that the `distribute_token_budget` function behaves correctly when given an empty input or no budget."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007781839999836393,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `allocations['l1.py']` should be between 35 and 50 (inclusive).",
          "The value of `allocations['l2.py']` should also be between 35 and 50 (inclusive).",
          "The absolute difference between the values of `allocations['l1.py']` and `allocations['l2.py']` should not exceed 1.",
          "Both `allocations['l1.py']` and `allocations['l2.py']` should be roughly equal."
        ],
        "scenario": "Verify fair sharing when neither fits.",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 327,
          "total_tokens": 497
        },
        "why_needed": "Prevents regression where either L1 or L2 file gets more than half of the budget, leading to unfair token distribution."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_fair_share",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007812489999992067,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert len(allocations) is equal to 3', 'expected_value': 3, 'actual_value': 0}",
          "{'name': 'assert allocations contains exactly three files', 'expected_value': 3, 'actual_value': []}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 133,
          "total_tokens": 243
        },
        "why_needed": "Verify max_files limit."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007768709999993462,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of allocations for each file should be equal to the total needed tokens.",
          "Each allocation should contain exactly 10 tokens (i.e., ~40 characters).",
          "All files should have their full content allocated with sufficient token budget."
        ],
        "scenario": "Verify that the `distribute_token_budget` function allocates tokens to files in a sufficient manner.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 332,
          "total_tokens": 448
        },
        "why_needed": "This test prevents a potential bug where the token budget is insufficient, leading to incomplete or corrupted file contents."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_sufficient",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007998339999915061,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert estimate_tokens('') == 1",
          "assert estimate_tokens('a') == 1",
          "assert estimate_tokens('aaaa') == 1",
          "assert estimate_tokens('aaaa' * 10) == 10"
        ],
        "scenario": "Verify the rough token estimation (chars / 4) for an empty string.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 217,
          "total_tokens": 320
        },
        "why_needed": "Prevents a potential division by zero error when estimating tokens."
      },
      "nodeid": "tests/test_llm_utils.py::test_estimate_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "263-266"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008216049999987263,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the expected value.",
          "The 'line_ranges' key in the dictionary should match the expected value.",
          "The 'line_count' key in the dictionary should match the expected value.",
          "The `to_dict()` method of the `CoverageEntry` object returns a dictionary with the correct keys and values."
        ],
        "scenario": "Test coverage entry serialization.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 255,
          "total_tokens": 378
        },
        "why_needed": "This test prevents a bug where the `CoverageEntry` object is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestArtifactEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 3,
          "line_ranges": "241-243"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007700980000038271,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `file_path` key should contain the correct file path.",
          "The `line_ranges` key should contain the correct line ranges in the format 'start-end'.",
          "The `line_count` key should contain the correct number of lines.",
          "Each assertion should match the expected values exactly.",
          "The dictionary structure should be consistent and follow the standard JSON format."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` returns the expected dictionary structure.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 255,
          "total_tokens": 391
        },
        "why_needed": "This test prevents a potential bug where the `CoverageEntry` object is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestCollectionError::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "65-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007781129999955283,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should be equal to 'src/foo.py'.",
          "The 'line_ranges' key in the dictionary should be equal to '1-3, 5, 10-15'.",
          "The 'line_count' key in the dictionary should be equal to 10.",
          "The 'file_path' value is not a string.",
          "The 'line_ranges' value is not a list or tuple of strings.",
          "The 'line_ranges' value contains non-string values (e.g., integers).",
          "The 'line_count' value is not an integer."
        ],
        "scenario": "Test coverage serialization for CoverageEntry.",
        "token_usage": {
          "completion_tokens": 177,
          "prompt_tokens": 255,
          "total_tokens": 432
        },
        "why_needed": "This test prevents a potential bug where the coverage entry is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008088710000038191,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotation.scenario', 'value': ''}",
          "{'name': 'annotation.why_needed', 'value': ''}",
          "{'name': 'annotation.key_assertions', 'value': []}",
          "{'name': 'annotation.confidence', 'value': 'None'}",
          "{'name': 'annotation.error', 'value': 'None'}"
        ],
        "scenario": "An empty annotation should be created with default values.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 212,
          "total_tokens": 336
        },
        "why_needed": "This test prevents a regression where an empty annotation would result in a `NoneType` attribute."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 9,
          "line_ranges": "130-133, 135, 137, 139, 141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007797760000016751,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' field should be present in the dictionary.",
          "The 'why_needed' field should also be present in the dictionary.",
          "The 'key_assertions' field should not include the 'confidence' key, as it is an optional field."
        ],
        "scenario": "The test verifies that the `LlmAnnotation` object can be serialized into a dictionary with required fields.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 230,
          "total_tokens": 347
        },
        "why_needed": "This test prevents regression by ensuring that the minimal annotation is properly serialized without any optional fields."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "130-133, 135-137, 139-141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007876020000026074,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Assert scenario is set correctly",
          "Assert confidence value matches expected",
          "Assert context summary contains required mode and bytes properties",
          "Assert error is None or empty",
          "Assert token presence in response body is verified"
        ],
        "scenario": "Test to dictionary with all fields",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 284,
          "total_tokens": 371
        },
        "why_needed": "Prevents incorrect data representation in API responses"
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000827154000006658,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' field should be set to SCHEMA_VERSION.",
          "The 'tests' field should be an empty list.",
          "The 'warnings' field should not be included in the dictionary.",
          "The 'collection_errors' field should not be included in the dictionary."
        ],
        "scenario": "Test that the default report has a schema version and empty lists.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 231,
          "total_tokens": 342
        },
        "why_needed": "Prevents regression by ensuring the default report is correctly defined with required fields."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_default_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000795926999984431,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `collection_errors` in the report should be 1.",
          "The nodeid of the first error in `collection_errors` should be 'test_bad.py'.",
          "All collection errors in `collection_errors` should have a valid nodeid."
        ],
        "scenario": "Test Report Root with collection errors should be verified.",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 237,
          "total_tokens": 341
        },
        "why_needed": "This test prevents a regression where the report does not include all collection errors."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008264239999959955,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'length', 'expected_value': 1, 'actual_value': 1}",
          "{'assertion_type': 'code', 'expected_value': 'W001', 'actual_value': 'W001'}"
        ],
        "scenario": "TestReportRoot::test_report_with_warnings",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 144,
          "total_tokens": 256
        },
        "why_needed": "The test is necessary to ensure that the ReportWarning class correctly identifies and includes warnings in the report."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 73,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008217950000073415,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"nodeids == ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z']\", 'expected_result': ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z']}"
        ],
        "scenario": "Tests should be sorted by nodeid in output.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 215,
          "total_tokens": 335
        },
        "why_needed": "Because the current implementation does not sort tests by nodeid, which can lead to incorrect test results."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007363459999965016,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'detail', 'expected': '/path/to/file', 'actual': '/path/to/file'}"
        ],
        "scenario": "The `to_dict()` method of the `ReportWarning` class is used to convert a warning object into a dictionary.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 131,
          "total_tokens": 247
        },
        "why_needed": "This test is needed because it checks if the detail attribute of the `ReportWarning` object is correctly populated in the returned dictionary."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007613520000120388,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'detail' key is expected to be present in the dictionary.",
          "The value of 'detail' is not provided in this case.",
          "The presence of 'detail' does not prevent the test from passing."
        ],
        "scenario": "Test to dictionary without detail should exclude it.",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 223,
          "total_tokens": 315
        },
        "why_needed": "This test prevents a warning about missing detailed information in the report."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_without_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 39,
          "line_ranges": "286-288, 290-292, 376-392, 394, 397, 399, 402, 405, 407, 409, 411-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007923299999958999,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert d['run_id'] == 'run-123',",
          "assert d['run_group_id'] == 'group-456',",
          "assert d['is_aggregated'] is True,",
          "assert d['aggregation_policy'] == 'merge',",
          "assert d['run_count'] == 3,",
          "assert len(d['source_reports']) == 2"
        ],
        "scenario": "Test that RunMeta has aggregation fields.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 343,
          "total_tokens": 470
        },
        "why_needed": "Prevents regression where RunMeta is not aggregated by default."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_aggregation_fields_present",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008013469999923473,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_annotations_enabled' key should not be present in the data.",
          "The 'llm_provider' key should not be present in the data.",
          "The 'llm_model' key should not be present in the data."
        ],
        "scenario": "Test that LLM fields are excluded when annotations are disabled.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 232,
          "total_tokens": 337
        },
        "why_needed": "This test prevents regression where LLMs are enabled but annotations are disabled, causing unexpected behavior."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 43,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419-431, 433, 435, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008101929999781987,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "data['llm_annotations_enabled'] is True",
          "data['llm_provider'] == 'ollama'",
          "data['llm_model'] == 'llama3.2:1b'",
          "data['llm_context_mode'] == 'complete'",
          "data['llm_annotations_count'] == 10",
          "data['llm_annotations_errors'] == 2"
        ],
        "scenario": "Verify that LLM traceability fields are included when enabled.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 327,
          "total_tokens": 455
        },
        "why_needed": "Prevents regression in LLM model tracing functionality."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_traceability_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007584870000130195,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'source_reports is not in d', 'expected_result': {'source_reports': []}}",
          "{'name': 'is_aggregated is False', 'expected_result': {'is_aggregated': False}}"
        ],
        "scenario": " tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 130,
          "total_tokens": 241
        },
        "why_needed": "It's necessary to ensure that non-aggregated reports do not include source_reports."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 49,
          "line_ranges": "286-288, 290-292, 376-392, 394-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008143309999866233,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'git_sha' field is set to the expected value.",
          "The 'git_dirty' field is set to True.",
          "The 'repo_version' field matches the expected value.",
          "The 'repo_git_sha' field matches the expected value.",
          "The 'repo_git_dirty' field is False.",
          "The 'plugin_git_sha' field matches the expected value.",
          "The 'plugin_git_dirty' field is False.",
          "The 'config_hash' field matches the expected value.",
          "The length of the data dictionary is 1 as expected."
        ],
        "scenario": "Test RunMeta to dict with all optional fields.",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 483,
          "total_tokens": 658
        },
        "why_needed": "Prevents regression in case of missing or outdated plugin version, as it would lead to incorrect data being populated."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007876309999801379,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be set to 1.",
          "The 'interrupted' field should be True.",
          "The 'collect_only' field should be True.",
          "The 'collected_count' field should be equal to 10.",
          "The 'selected_count' field should be equal to 8.",
          "The 'deselected_count' field should be equal to 2."
        ],
        "scenario": "Test the RunMeta class to ensure it includes all necessary run status fields.",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 285,
          "total_tokens": 433
        },
        "why_needed": "This test prevents a potential bug where the RunMeta object is missing certain critical fields that are required for proper functioning."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007963380000148845,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The number of parts should be 3', 'expected_value': 3, 'message': \"The schema version should have exactly 3 parts (e.g., '1.2.3').\"}",
          "{'name': 'Each part should be a digit', 'expected_value': [0, 1, 2], 'message': 'Each part of the schema version should be a digit (0-9).'}"
        ],
        "scenario": "Test Schema Version Format",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 115,
          "total_tokens": 259
        },
        "why_needed": "To ensure the schema version is in a valid semver format."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009419689999958791,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'ReportRoot.schema_version', 'expected_value': 'SCHEMA_VERSION'}",
          "{'name': 'report.to_dict().schema_version', 'expected_value': 'SCHEMA_VERSION'}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 119,
          "total_tokens": 226
        },
        "why_needed": "This test is necessary because the ReportRoot class does not include a schema version by default."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "96-103"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007755879999820081,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the serialized dictionary should match the original file path.",
          "The 'line_ranges' key in the serialized dictionary should match the expected range string.",
          "The 'line_count' key in the serialized dictionary should match the original value.",
          "All keys in the serialized dictionary should have unique values.",
          "Any missing keys in the serialized dictionary should raise an assertion error."
        ],
        "scenario": "Test coverage entry serialization.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 256,
          "total_tokens": 390
        },
        "why_needed": "This test prevents a bug where the `CoverageEntry` class does not properly serialize its internal data to JSON."
      },
      "nodeid": "tests/test_models.py::TestSourceCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "286-288, 290, 292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007851669999752175,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' field should be present in the dictionary.",
          "The 'why_needed' field should be present in the dictionary.",
          "The 'key_assertions' field should be present in the dictionary.",
          "The 'confidence' field should not be present in the dictionary when it is `None`."
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 229,
          "total_tokens": 357
        },
        "why_needed": "This test prevents a potential bug where the minimal annotation is missing some required fields."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "286-288, 290-292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007382689999815284,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'run-1', 'actual_value': 'run-1'}"
        ],
        "scenario": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 134,
          "total_tokens": 213
        },
        "why_needed": "To ensure that the SourceReport object is correctly serializing its run_id attribute."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007894940000028328,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the expected value.",
          "The 'line_ranges' key in the dictionary should match the expected value.",
          "The 'line_count' key in the dictionary should match the expected value.",
          "The 'start' and 'end' values of the line ranges should be correctly formatted (e.g., '1-3', '5, 10-15')",
          "Any missing or incorrect line ranges should raise an AssertionError",
          "Any invalid or malformed line ranges should raise an AssertionError"
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes the test summary.",
        "token_usage": {
          "completion_tokens": 174,
          "prompt_tokens": 254,
          "total_tokens": 428
        },
        "why_needed": "This test prevents a potential bug where the serialized test summary is not accurate due to incorrect formatting of line ranges."
      },
      "nodeid": "tests/test_models.py::TestSummary::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007769220000000132,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field should match the expected node ID.",
          "The 'outcome' field should be set to 'passed'.",
          "The 'duration' field should be set to 0.0 (indicating no execution time).",
          "The 'phase' field should be set to 'call'."
        ],
        "scenario": "Test that a minimal result has the required fields.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 244,
          "total_tokens": 361
        },
        "why_needed": "This test prevents regression where a minimal result is not provided with all necessary information."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 24,
          "line_ranges": "65-68, 190, 194-199, 201, 203, 205, 207, 210-212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007948150000061105,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'coverage' key should be present in the `result` dictionary.",
          "The 'coverage' key should contain a list of coverage entries.",
          "Each coverage entry should have a 'file_path' attribute set to the expected file path ('src/foo.py').",
          "Each coverage entry should have a 'line_ranges' attribute set to a valid range (e.g., '1-5') and a 'line_count' attribute equal to the actual number of lines in the file.",
          "The list of coverage entries should not be empty.",
          "All file paths in the 'coverage' list should match the expected file path ('src/foo.py')."
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage verifies that the `result` dictionary contains a single 'coverage' key with a list of coverage entries.",
        "token_usage": {
          "completion_tokens": 223,
          "prompt_tokens": 256,
          "total_tokens": 479
        },
        "why_needed": "This test prevents regression by ensuring that the `result` dictionary includes a 'coverage' key, which is necessary for calculating and displaying coverage statistics."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007737949999864213,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert llm_opt_out is True', 'expected_value': True, 'actual_value': 'is True'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 145,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the LLM opt-out flag is correctly set in the test result."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203, 205, 207-210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007857479999984207,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Rerun count', 'value': 2}",
          "{'name': 'Final outcome', 'value': 'passed'}"
        ],
        "scenario": "Test case 'test_result_with_rerun' has been executed.",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 162,
          "total_tokens": 255
        },
        "why_needed": "The test result is not being recorded in the database because reruns are disabled."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007923599999912767,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'field_name': 'rerun_count', 'expected_value': 0, 'actual_value': 'Not applicable'}",
          "{'field_name': 'final_outcome', 'expected_value': 'passed', 'actual_value': 'passed'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 152,
          "total_tokens": 271
        },
        "why_needed": "This test ensures that the `result` dictionary excludes fields related to reruns."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "96-103, 241-243, 263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008260430000177621,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result['param_id'] == 'a-b-c',",
          "assert result['param_summary'] == 'a=1, b=2, c=3',",
          "assert result['captured_stdout'] == 'stdout content',",
          "assert result['captured_stderr'] == 'stderr content',",
          "assert result['requirements'] == ['REQ-100'],",
          "assert result['llm_opt_out'] is True,"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 454,
          "total_tokens": 611
        },
        "why_needed": "Prevents bar because llm_opt_out=True prevents the annotation from being generated for optional fields."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 59,
          "line_ranges": "263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530-532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008122869999738214,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_artifacts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008022990000142727,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of collection_errors is 1.",
          "collection_errors[0].nodeid matches 'broken_test.py'.",
          "The nodeid value contains the string 'SyntaxError'."
        ],
        "scenario": "Test to_dict includes collection_errors when set.",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 243,
          "total_tokens": 335
        },
        "why_needed": "This test prevents a regression where the to_dict method does not include collection_errors in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534-536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000785478000011608,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "custom_metadata['project'] == 'myproject'",
          "custom_metadata['environment'] == 'staging'",
          "custom_metadata['build_number'] == 123",
          "custom_metadata does not contain any other metadata keys.",
          "result['custom_metadata']['project'] is equal to the expected value.",
          "result['custom_metadata']['environment'] is equal to the expected value.",
          "result['custom_metadata']['build_number'] is equal to the expected value.",
          "result does not contain any additional custom metadata keys."
        ],
        "scenario": "Test to_dict includes custom_metadata when set.",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 264,
          "total_tokens": 426
        },
        "why_needed": "Prevents regression in cases where custom metadata is required but not properly handled by the default to_dict method."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_custom_metadata",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007959669999877406,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'signature123', 'actual_value': 'hmac_signature'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 128,
          "total_tokens": 213
        },
        "why_needed": "to ensure that the `report.to_dict()` method includes an HMAC signature when it is set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536-538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008052039999881799,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected value', 'value': 'abcdef1234567890'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 131,
          "total_tokens": 223
        },
        "why_needed": "The test is necessary because the `to_dict` method of the ReportRoot class includes a SHA-256 hash when it is set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 63,
          "line_ranges": "96-103, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532-534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008039919999873746,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'source_coverage' list contains exactly one entry.",
          "The first element of the 'source_coverage' list has the correct file path ('src/mod.py').",
          "All elements in the 'source_coverage' list have the correct keys (file_path, statements, missed, covered, coverage_percent, covered_ranges, missed_ranges)."
        ],
        "scenario": "Test to_dict includes source_coverage when set.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 282,
          "total_tokens": 416
        },
        "why_needed": "Prevents a potential bug where the test fails if the 'source_coverage' key is not present in the report dictionary, potentially leading to incorrect coverage analysis."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000785938000007036,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"len(result['warnings']) == 1\", 'expected_value': 1, 'message': \"Expected len(result['warnings']) to be equal to 1\"}",
          "{'name': \"result['warnings'][0]['code'] == 'W001'\", 'expected_value': 'W001', 'message': \"Expected result['warnings'][0]['code'] to be equal to 'W001'\"}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 151,
          "total_tokens": 308
        },
        "why_needed": "This test is necessary because the `to_dict()` method of ReportRoot includes warnings when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 12,
          "line_ranges": "467-475, 477-479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007847260000062306,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "{'assertion_name': '...', 'expected_value': '...', 'actual_value': '...', 'message': '...'}"
        ],
        "scenario": "...",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 153,
          "total_tokens": 278
        },
        "why_needed": "..."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_with_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008915649999892139,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "..."
        ],
        "scenario": "...",
        "token_usage": {
          "completion_tokens": 267,
          "prompt_tokens": 131,
          "total_tokens": 398
        },
        "why_needed": "..."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_without_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 42,
          "line_ranges": "65-68, 130-133, 135, 137, 139, 141, 143, 190, 194-199, 201-207, 210-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008201919999919483,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'param_id' field should be present and match 'a-b-c'.",
          "The 'param_summary' field should contain the correct values for 'a', 'b', and 'c'.",
          "The 'captured_stdout' field should not be empty.",
          "The 'captured_stderr' field should not be empty.",
          "The 'requirements' list should include 'REQ-100'.",
          "The 'llm_opt_out' field should be True.",
          "The 'llm_context_override' field should match the scenario name.",
          "The number of coverage entries should be 1.",
          "The 'llm_annotation' field should contain the correct scenario name."
        ],
        "scenario": "Test to_dict includes all optional fields when set.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 454,
          "total_tokens": 647
        },
        "why_needed": "This test prevents regression in coverage calculation when llm_opt_out is True."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220-222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007956860000035704,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'Error output here'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 149,
          "total_tokens": 229
        },
        "why_needed": "to include captured_stderr in the JSON response when to_dict is used."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218-220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007761800000025687,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'captured_stdout', 'expected_value': 'Debug output here'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 149,
          "total_tokens": 227
        },
        "why_needed": "The `to_dict` method includes captured stdout when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203-207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008123270000055527,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 163,
          "total_tokens": 237
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_param_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008037710000223797,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `to_dict` method should include the `requirements` key and its value should be a list of strings.",
          "The `requirements` key should be present in the `test_result` dictionary."
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 151,
          "total_tokens": 275
        },
        "why_needed": "The `to_dict` method includes requirements when set. This is necessary because the `requirements` key in the test result dictionary is not a standard JSON key, but rather an attribute of the TestCaseResult class."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007756589999985408,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `Config().llm_context_exclude_globs` returns a list of strings containing the specified globs.",
          "* The glob '*.pyc' is included in the list of default exclude globs.",
          "* The glob '__pycache__/*' is included in the list of default exclude globs.",
          "* The glob '*secret*' is included in the list of default exclude globs.",
          "* The glob '*password*' is included in the list of default exclude globs."
        ],
        "scenario": "Verify that the default exclude globs are correctly set.",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 222,
          "total_tokens": 383
        },
        "why_needed": "This test prevents a potential bug where the default exclude globs are not correctly set, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008112349999862545,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `--password` pattern is present in the default redact patterns.",
          "The `--token` pattern is present in the default redact patterns.",
          "The `--api[_-]?key` pattern is present in the default redact patterns."
        ],
        "scenario": "Tests the default redact patterns configuration.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 228,
          "total_tokens": 333
        },
        "why_needed": "Prevents a potential security vulnerability where sensitive information like passwords and tokens are not properly redacted."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000816754999988234,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.llm_context_mode == 'minimal'",
          "cfg.llm_max_tests == 0",
          "cfg.llm_max_retries == 10",
          "cfg.llm_context_bytes == 32000",
          "cfg.llm_context_file_limit == 10",
          "cfg.llm_requests_per_minute == 5",
          "cfg.llm_timeout_seconds == 30",
          "cfg.llm_cache_ttl_seconds == 86400",
          "cfg.include_phase == 'run'",
          "cfg.aggregate_policy == 'latest'",
          "cfg.is_llm_enabled() is False",
          "cfg.omit_tests_from_coverage is True"
        ],
        "scenario": "Test that default values are set correctly for the test_default_values scenario.",
        "token_usage": {
          "completion_tokens": 209,
          "prompt_tokens": 318,
          "total_tokens": 527
        },
        "why_needed": "This test prevents a potential regression where the default values of the Config class are not set properly, potentially leading to unexpected behavior or errors in the application."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000785407000023497,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg', 'expected_type': 'Config'}",
          "{'name': 'cfg.provider', 'expected_value': 'none'}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_get_default_config",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 104,
          "total_tokens": 188
        },
        "why_needed": "To test the default configuration of the options."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_get_default_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008239900000148737,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `is_llm_enabled` should return False for a provider without LLM (e.g., 'none')",
          "The function `is_llm_enabled` should return True for providers with LLMs (e.g., 'ollama', 'litellm', 'gemini')"
        ],
        "scenario": "Verify the is_llm_enabled check for different providers.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 263,
          "total_tokens": 375
        },
        "why_needed": "Prevents a potential bug where the test fails when using an unsupported provider."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-221, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007916890000103649,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'length of errors', 'value': 1, 'description': 'The number of errors found during validation'}",
          "{'name': 'error message', 'value': \"Invalid aggregate_policy 'random'\", 'description': 'The error message returned by the validation process'}"
        ],
        "scenario": "test_validate_invalid_aggregate_policy",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 128,
          "total_tokens": 238
        },
        "why_needed": "to test the validation of an invalid aggregation policy"
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-213, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008031710000011572,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'length', 'expected_value': 1, 'actual_value': 0}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 131,
          "total_tokens": 216
        },
        "why_needed": "To ensure that the `validate()` method raises an error when a valid context mode is specified."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-229, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008043219999933626,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 129,
          "total_tokens": 216
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 28,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007833129999994526,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'The provider is invalid.', 'expected_value': \"Invalid provider 'invalid_provider'\"}"
        ],
        "scenario": "Tests for configuration options",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 122,
          "total_tokens": 190
        },
        "why_needed": "To ensure that the `Config` class correctly validates an invalid provider."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 31,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245-254, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007778929999915363,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.validate() returns an empty list if all constraints are met",
          "cfg.validate() returns a list of errors with the specified message",
          "The 'llm_context_bytes' constraint is at least 1000 bytes",
          "The 'llm_max_tests' constraint is positive or zero",
          "The 'llm_requests_per_minute' constraint is at least 1",
          "The 'llm_timeout_seconds' constraint is at least 1",
          "The 'llm_max_retries' constraint is positive or zero"
        ],
        "scenario": "Test validation of numeric constraints for TestConfig.",
        "token_usage": {
          "completion_tokens": 184,
          "prompt_tokens": 329,
          "total_tokens": 513
        },
        "why_needed": "This test prevents a potential regression where the default values for LLM context bytes, max tests, requests per minute, timeout seconds, and max retries are not validated against their expected minimum or maximum values."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_numeric_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007663420000199039,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'errors should be empty', 'expected_value': [], 'actual_value': 0}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_valid_config",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 100,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the `validate` method returns an empty list of errors when a valid configuration is passed."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0037498249999998734,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The expected value of `aggregate_dir` is set to `"
        ],
        "scenario": "Test loads aggregation options with correct directory, policy and run ID.",
        "token_usage": {
          "completion_tokens": 205,
          "prompt_tokens": 295,
          "total_tokens": 500
        },
        "why_needed": "This test prevents a bug where the aggregate options are not loaded correctly due to incorrect or missing values in the mock configuration."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_aggregation_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0036825489999898764,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg.batch_parametrized_tests is True', 'expected_value': True, 'actual_value': 'assert cfg.batch_parametrized_tests is True'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 138,
          "total_tokens": 228
        },
        "why_needed": "To test that the disabled batch flag works correctly."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0042256030000089595,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_max_retries' attribute is not set to its default value (10).",
          "The 'llm_report_html', 'llm_report_json', 'llm_report_pdf', 'llm_evidence_bundle', 'llm_dependency_snapshot', 'llm_requests_per_minute', 'llm_aggregate_dir', 'llm_aggregate_policy', 'llm_aggregate_run_id', 'llm_aggregate_group_id' attributes are set to None.",
          "The 'llm_provider' attribute is not set. This flag is required for LLMs.",
          "The 'llm_model' attribute is not set. This flag is required for LLMs.",
          "The 'llm_context_mode' attribute is not set. This flag is required for LLMs.",
          "The 'llm_prompt_tier', 'llm_batch_parametrized', and 'llm_context_compression' attributes are set to None, which may cause issues with the LLM configuration.",
          "}"
        ],
        "scenario": "Test handling when pyproject.toml doesn't exist.",
        "token_usage": {
          "completion_tokens": 260,
          "prompt_tokens": 413,
          "total_tokens": 673
        },
        "why_needed": "Prevents regression in LLM configuration loading without a pyproject.toml file."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_config_missing_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607-608, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004296917000004896,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_pytest_config.option.llm_coverage_source', 'expected_value': 'cov_dir'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 126,
          "total_tokens": 198
        },
        "why_needed": "To test the coverage source option."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0038017619999948238,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"cfg.provider == 'none'\", 'expected_value': 'None'}",
          "{'name': 'cfg.report_html is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_defaults",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 116,
          "total_tokens": 209
        },
        "why_needed": "To test the default configuration when no options are set."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 132,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492-494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00431961900000033,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml overrides CLI options', 'expected_value': {'key': 'value'}, 'actual_value': {'key': 'override value'}}",
          "{'name': 'CLI options override pyproject.toml values', 'expected_value': {'key': 'override value'}, 'actual_value': {'key': 'override value'}}"
        ],
        "scenario": "Test that CLI options override pyproject.toml options.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 134,
          "total_tokens": 265
        },
        "why_needed": "To test the ability of CLI options to override pyproject.toml settings."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 133,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004945538000015404,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml', 'expected_content': '...toml'}",
          "{'name': 'CLI provider option overrides pyproject.toml', 'expected_content': '...overrides pyproject.toml'}"
        ],
        "scenario": "Test that CLI provider option overrides pyproject.toml.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 130,
          "total_tokens": 244
        },
        "why_needed": "To ensure that the CLI provider option can override the default configuration in pyproject.toml, which is used to load dependencies."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_provider_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494-495, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003678261000004568,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_pytest_config.option.llm_max_retries', 'expected_value': 2, 'actual_value': 0}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 130,
          "total_tokens": 216
        },
        "why_needed": "To test the functionality of loading retries from the CLI."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 134,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386-388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005308766000013065,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': True, 'actual': 'True'}"
        ],
        "scenario": "Create a new directory with a pyproject.toml file",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 119,
          "total_tokens": 206
        },
        "why_needed": "To test the functionality of loading values from a pyproject.toml file in an environment where the file does not exist."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 88,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470-474, 476-477, 479, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0036732220000033067,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `prompt_tier` option should be set to 'minimal'.",
          "The `batch_parametrized_tests` option should not be enabled.",
          "The `context_compression` option should be set to 'none'."
        ],
        "scenario": "Test loading token optimization options from CLI.",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 264,
          "total_tokens": 353
        },
        "why_needed": "Prevents regression in token optimization configuration."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_token_optimization_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488, 490-492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005360013000000663,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `cfg.report_dependency_snapshot` should be set to `'deps.json'` after setting the `llm_dependency_snapshot` option.",
          "The function `load_config(mock)` correctly loads the mock configuration and returns a valid configuration object.",
          "The `mock.option.llm_dependency_snapshot` attribute is set to the expected value 'deps.json'.",
          "The `cfg.report_dependency_snapshot` attribute of the loaded configuration object is updated with the correct value 'deps.json'.",
          "The test function does not fail when an option is set for dependency snapshot.",
          "The test function passes without any assertion failures when an option is set for dependency snapshot."
        ],
        "scenario": "Verify that the test_cli_dependency_snapshot function correctly sets the dependency snapshot to 'deps.json' when an option is set.",
        "token_usage": {
          "completion_tokens": 203,
          "prompt_tokens": 213,
          "total_tokens": 416
        },
        "why_needed": "This test prevents a potential regression where the CLI overrides for dependency snapshots are not being properly applied."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_dependency_snapshot",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488-490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.006094644999990351,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `cfg.report_evidence_bundle` should be set to 'bundle.zip' after setting `mock.option.llm_evidence_bundle = 'bundle.zip'`.",
          "The function `load_config(mock)` should return the correct configuration with the updated option value.",
          "The assertion `assert cfg.report_evidence_bundle == 'bundle.zip'` should pass if the configuration is correctly loaded and the option is set to 'bundle.zip'."
        ],
        "scenario": "Testing the `test_cli_evidence_bundle` function to ensure it correctly sets the `llm_evidence_bundle` option to 'bundle.zip'.",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 217,
          "total_tokens": 392
        },
        "why_needed": "This test prevents a potential bug where the `llm_evidence_bundle` option is not set correctly, potentially leading to incorrect evidence bundle reporting."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_evidence_bundle",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484-486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005713582999987921,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `cfg.report_json` should be set to 'output.json' when `mock.option.llm_report_json = \"output.json\"`.",
          "The `load_config(mock)` function should return a valid configuration object with the correct `report_json` option value.",
          "The `assert cfg.report_json == \"output.json\"` statement should pass if the above conditions are met.",
          "If the `mock.option.llm_report_json = \"other.json\"` is executed, the test should fail and report an error message indicating that the `report_json` option was not overridden correctly."
        ],
        "scenario": "Verify that the `test_cli_report_json` test verifies that the `report_json` option is set to 'output.json' when CLI override for report JSON is enabled.",
        "token_usage": {
          "completion_tokens": 207,
          "prompt_tokens": 212,
          "total_tokens": 419
        },
        "why_needed": "This test prevents a bug where the `report_json` option is not correctly overridden in the configuration, potentially leading to incorrect output."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486-488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0057959069999924395,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_report_pdf` option is set to 'output.pdf' when the test runs.",
          "The `report_pdf` value of the configuration object matches 'output.pdf'.",
          "The `llm_report_pdf` option has been successfully overridden in the mock configuration.",
          "The report PDF path can be changed without affecting the test result.",
          "The `llm_report_pdf` option is correctly set even if the config file does not exist.",
          "The `report_pdf` value of the configuration object matches 'output.pdf' when the config file exists.",
          "The mock configuration has been successfully created with the correct report PDF path."
        ],
        "scenario": "Tests the CLI option to generate a report in PDF format.",
        "token_usage": {
          "completion_tokens": 184,
          "prompt_tokens": 212,
          "total_tokens": 396
        },
        "why_needed": "Prevents regression where the test fails due to incorrect report PDF path."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_pdf",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-237, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008419729999786796,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"errors contains 'litellm_token_output_format' key\", 'description': 'The validation should return an error message indicating the invalid token output format.'}"
        ],
        "scenario": "test_validate_invalid_token_output_format",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 130,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the token output format is correctly validated and raises an error when it's invalid."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_invalid_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241-242, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007953659999770935,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'litellm_token_refresh_interval must be at least 60', 'expected_value': 60}"
        ],
        "scenario": "Test validation when token refresh interval is too short",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 146,
          "total_tokens": 232
        },
        "why_needed": "Token refresh intervals should be at least 60 seconds to ensure sufficient time for the token to expire and be refreshed."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_token_refresh_interval_too_short",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008159440000099494,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The validate method returns an empty list of errors.', 'expected_result': [], 'message': 'Expected validate method to return an empty list of errors.'}"
        ],
        "scenario": "Test validation of valid LiteLLM configuration.",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 142,
          "total_tokens": 233
        },
        "why_needed": "To ensure that the LiteLLM provider is correctly configured and validated without any errors."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_valid_litellm_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438-440, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015340850000029604,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'include_history = True', 'actual': 'include_history = False'}"
        ],
        "scenario": "test_load_aggregate_include_history",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 118,
          "total_tokens": 203
        },
        "why_needed": "To ensure that the `aggregate_include_history` option is properly loaded and included in the generated code."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436-438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001504580000016631,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'aggregate_policy_path exists in pyproject.toml', 'expected_value': '/path/to/aggregate/policy.py'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 121,
          "total_tokens": 242
        },
        "why_needed": "To ensure that the aggregate policy can be loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 150,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340-346, 348-350, 352-354, 356-357, 360-369, 372-375, 378-392, 396, 400, 402, 404, 408-410, 412-413, 416-422, 426-428, 430-432, 436-440, 444-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022711530000094626,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'All config keys should be present in the pyproject.toml file', 'expected_value': {'all_keys': ['config', 'keys', 'load', 'pyproject']}, 'actual_value': [False], 'message': 'Expected all config keys to be present, but got {falses}'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 120,
          "total_tokens": 256
        },
        "why_needed": "To ensure that all config keys are loaded when loading the entire pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390-392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014950720000115325,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has a valid cache_dir', 'expected_value': 'path/to/cache/dir'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 113,
          "total_tokens": 202
        },
        "why_needed": "To ensure that the cache directory is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388-390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015182459999891762,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'value': 'True'}",
          "{'name': \"pyproject.toml has a section named 'cache_ttl_seconds'\", 'value': 'True'}"
        ],
        "scenario": "Tests for PyProjectLoadingCoverage",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 116,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the cache TTL seconds are loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418-420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015526709999846844,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected': 'The pyproject.toml file was created successfully.'}",
          "{'name': 'capture_failed_output key exists in pyproject.toml', 'expected': \"The 'capture_failed_output' key was found in the pyproject.toml file.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 116,
          "total_tokens": 256
        },
        "why_needed": "To ensure that the `capture_failed_output` option in `pyproject.toml` is correctly loaded and used for coverage purposes."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420-422, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015193179999926087,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'max_chars = 100', 'actual': 'None'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 119,
          "total_tokens": 218
        },
        "why_needed": "To ensure that the `capture_output_max_chars` option is properly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362-364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015067649999878086,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context_bytes_path', 'expected_value': 'path/to/pyproject.toml', 'actual_value': 'pyproject / pyproject.toml'}",
          "{'name': 'context_bytes_file_mode', 'expected_value': 'r', 'actual_value': 'rb'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 113,
          "total_tokens": 240
        },
        "why_needed": "To ensure that the context bytes are loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368-369, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001517945999978565,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is writable', 'expected': {'status': 'ok', 'message': ''}, 'actual': {'status': 'error', 'message': \"PermissionError: 'tmp_path / ' is not writable\"}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 119,
          "total_tokens": 246
        },
        "why_needed": "To ensure that the `context_exclude_globs` setting in `pyproject.toml` is properly excluded from coverage reports."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364-366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001584390000004987,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected_value': True, 'message': 'Expected pyproject.toml to exist'}",
          "{'name': 'pyproject.toml content is correct', 'expected_value': {'context_file_limit': 100}, 'message': 'Expected context_file_limit to be 100'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 116,
          "total_tokens": 254
        },
        "why_needed": "To ensure that the context file limit is correctly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366-368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015480909999894266,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context include globs are loaded from pyproject.toml', 'description': 'The context include globs should be present in the test environment.', 'expected_value': 'True'}"
        ],
        "scenario": "Tests for `tests/test_options_coverage`",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 119,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the `context_include_globs` setting is correctly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014981680000119013,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and contains hmac_key_file', 'expected': {'hmac_key_file': 'path/to/hmac_key_file'}, 'actual': {'hmac_key_file': 'path/to/hmac_key_file'}}",
          "{'name': 'pyproject.toml does not contain hmac_key_file', 'expected': {}, 'actual': {}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 118,
          "total_tokens": 264
        },
        "why_needed": "To ensure that the hmac_key_file is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372-374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015961809999964771,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `include_param_values` option should be present in the `pyproject.toml` file.', 'expected_value': True}",
          "{'name': 'The value of `include_param_values` should match the expected value.', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 116,
          "total_tokens": 247
        },
        "why_needed": "To ensure that the `include_param_values` option is correctly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015076870000143572,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': '```toml\\n[tool.pyproject]\\ninclude_phase = [\"phase1\", \"phase2\"]\\n``', 'actual': '```toml\\n[tool.pyproject]\\ninclude_phase = []\\n``', 'error_message': \"The 'include_phase' key is missing from the PyProject.toml file.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 113,
          "total_tokens": 263
        },
        "why_needed": "To ensure that the `include_phase` is loaded correctly from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426-428, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015399360000003526,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'include_pytest_invocation = True', 'actual': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 122,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the `include_pytest_invocation` option is correctly loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430-432, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015510270000049786,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'File existence', 'expected_result': 'pyproject.toml exists in the test directory', 'actual_result': 'pyproject.toml does not exist'}"
        ],
        "scenario": "Tests for pyproject.toml coverage",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 121,
          "total_tokens": 212
        },
        "why_needed": "To ensure that the invocation_redact_patterns are correctly loaded and used in tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_invocation_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340-342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016636580000124468,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'litellm_api_base exists in pyproject.toml', 'expected_value': 'True'}",
          "{'name': \"pyproject.toml contains a [tool] section with 'litellm_api_base' as the name\", 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 122,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the litellm_api_base is loaded correctly from pyproject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342-344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016454430000010234,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'litellm_api_key exists in pyproject.toml', 'expected_value': 'litellm_api_key', 'actual_value': 'litellm_api_key'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 122,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the litellm API key is correctly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356-357, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001522864999998319,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'description': 'The pyproject.toml file should exist at the specified path.', 'expected_result': 'True'}",
          "{'name': 'litellm_token_json_key is present in pyproject.toml', 'description': 'The litellm token JSON key should be present in the pyproject.toml file.', 'expected_result': 'True'}"
        ],
        "scenario": "Loading litellm_token_json_key from pyproject.toml",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 125,
          "total_tokens": 276
        },
        "why_needed": "To ensure that the litellm token JSON key is correctly loaded and used in the application."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352-354, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001559823999997434,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"```toml\\noutput_format = 'litellm_token_output_format'\\n``\", 'actual': 'pyproject.toml contents'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 125,
          "total_tokens": 239
        },
        "why_needed": "To ensure that the `litellm` package correctly handles token output formats in PyProject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344-346, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001592214999988073,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'description': 'The `pyproject.toml` file should exist in the test directory.', 'expected_result': 'True'}",
          "{'name': 'litellm_token_refresh_command is present in pyproject.toml', 'description': 'The `litellm_token_refresh_command` should be present in the `pyproject.toml` file.', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 125,
          "total_tokens": 295
        },
        "why_needed": "To ensure that the `litellm_token_refresh_command` is loaded correctly from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348-350, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002595408000019006,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'a string representing the contents of the PyProject.toml file'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 125,
          "total_tokens": 232
        },
        "why_needed": "To ensure that the `litellm_token_refresh_interval` option is properly loaded from the PyProject.toml file, allowing for accurate coverage analysis."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 73,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 449, 451, 453-456, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015306299999906514,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 189,
          "prompt_tokens": 158,
          "total_tokens": 347
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_malformed_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380-382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015039400000205205,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `max_concurrency` setting in `pyproject.toml` should be a non-negative integer.', 'expected_value': 0, 'actual_value': 1}",
          "{'name': 'The `max_concurrency` setting in `pyproject.toml` should not cause any errors when loaded by the project.', 'expected_error': 'max_concurrency must be a non-negative integer', 'actual_error': ''}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 116,
          "total_tokens": 285
        },
        "why_needed": "To ensure that the `max_concurrency` setting in `pyproject.toml` is correctly loaded and used by the project."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378-380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015535120000151892,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"The 'max_tests' setting is present in pyproject.toml\", 'value': 'True'}",
          "{'name': \"The 'max_tests' setting is a boolean value\", 'value': 3}"
        ],
        "scenario": "Testing the ability to load max_tests from pyproject.toml",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 113,
          "total_tokens": 232
        },
        "why_needed": "To ensure that the 'max_tests' setting in the pyproject.toml file can be loaded correctly and used by the tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444-446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015367610000112109,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True', 'actual': 'False'}",
          "{'name': 'metadata_file exists in pyproject.toml', 'expected': 'True', 'actual': 'False'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 113,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the metadata file is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336-337, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015473499999814067,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'ollama_host is defined in pyproject.toml', 'expected_value': 'True'}"
        ],
        "scenario": "test_pyproject_loading_coverage",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 119,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the ollama_host is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015341049999904044,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"The 'omit_tests_from_coverage' setting in pyproject.toml should be set to a boolean value.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 121,
          "total_tokens": 229
        },
        "why_needed": "To ensure that the `omit_tests_from_coverage` option is correctly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374-375, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001501404000009643,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The value of `param_value_max_chars` is a string', 'expected_type': 'str'}",
          "{'name': 'The length of `param_value_max_chars` is less than or equal to 50 characters', 'expected_value': 50}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 119,
          "total_tokens": 262
        },
        "why_needed": "To ensure that the `param_value_max_chars` option is correctly loaded from the `pyproject.toml` file and that its value is being used in the build process."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416-418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015100010000139719,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'Collect only: [\"...\"]', 'actual': 'Collect only: [\"...\"]'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 116,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the `report_collect_only` option is correctly loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015291659999832063,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'timeout_seconds key exists in pyproject.toml', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 113,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the timeout seconds are loaded correctly from pyproject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400-402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004289783000018588,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"The 'batch_max_tests' key in the PyProject.toml file should be set to a non-empty list of test names.\"}",
          "{'name': 'optimized_packages', 'expected': \"The 'optimized_packages' key in the optimized package metadata should contain a list of test names that were batched together.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
        "token_usage": {
          "completion_tokens": 156,
          "prompt_tokens": 117,
          "total_tokens": 273
        },
        "why_needed": "To ensure that the `batch_max_tests` option is correctly loaded from the PyProject.toml file and used to optimize Python packages."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 131,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396-398, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005227724999997463,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'PyProject token is not empty', 'description': 'The PyProject token should be present in the pyproject.toml file.', 'value': 'True'}",
          "{'name': 'PyProject path exists', 'description': 'The PyProject path should exist and point to a valid directory.', 'value': 'True'}"
        ],
        "scenario": "test_load_batch_parametrized_tests",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 123,
          "total_tokens": 248
        },
        "why_needed": "Optimization of PyProject token in batch parameterized tests"
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402-404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004358010000004242,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context compression should be enabled by default', 'value': True, 'expected_value': False}",
          "{'name': 'Context compression should be disabled if --with-optional-removal flag is used', 'value': True, 'expected_value': False}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 117,
          "total_tokens": 244
        },
        "why_needed": "To ensure that the context compression feature in Pytest is properly loaded and used."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404-405, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00526101700000936,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context line padding is correctly loaded from pyproject.toml', 'expected_value': 'context_line_padding', 'actual_value': 'context_line_padding'}",
          "{'name': 'Context line padding is applied to the build output', 'expected_value': ['context_line_padding'], 'actual_value': []}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 117,
          "total_tokens": 252
        },
        "why_needed": "To ensure that the context line padding is correctly loaded and applied in the build process."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392-393, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004420306000014307,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'prompt_tier loading', 'expected': \"The 'prompt_tier' key should be present in the pyproject.toml file.\"}",
          "{'name': 'token_optimization_strategy', 'expected': \"The 'token_optimization_strategy' value should be a string that indicates the optimization strategy to use.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 117,
          "total_tokens": 265
        },
        "why_needed": "To ensure that the `prompt_tier` is correctly loaded from the `pyproject.toml` file and used to determine the token optimization strategy."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000790395999985094,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The validation error message should contain a clear indication that `batch_max_tests` must be at least 1.', 'expected_value': 'batch_max_tests must be at least 1', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 135,
          "total_tokens": 248
        },
        "why_needed": "Because the `batch_max_tests` configuration option is not being used effectively."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000813429000004362,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding must be 0 or positive'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 129,
          "total_tokens": 197
        },
        "why_needed": "Negative context_line_padding is not allowed."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008108140000047115,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'errors', 'type': 'list', 'value': ['Invalid context_compression']}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 124,
          "total_tokens": 212
        },
        "why_needed": "To ensure that the validation of context compression settings does not fail when an invalid value is provided."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000827495999999428,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'Invalid prompt_tier', 'actual': 'Invalid prompt_tier'}",
          "{'expected': 'Invalid prompt_tier', 'actual': 'Invalid prompt_tier'}"
        ],
        "scenario": "test_validate_invalid_prompt_tier",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 125,
          "total_tokens": 224
        },
        "why_needed": "To ensure that the `validate()` method correctly identifies and reports invalid `prompt_tier` values."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 124,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378-380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603-605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0027618089999918993,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg is an instance of Config', 'expected_type': 'Config'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 119,
          "total_tokens": 191
        },
        "why_needed": "To ensure that the plugin configuration has safe defaults."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007794960000069295,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytestconfig is not None', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 108,
          "total_tokens": 182
        },
        "why_needed": "The test checks if markers exist in the plugin configuration."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 91,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.10211230199999477,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The generated report files should exist at `report.json` and `report.html` paths.",
          "The report data should be correctly formatted as either JSON or HTML depending on the configuration.",
          "The test function should produce a valid output that can be parsed by both Pytester's reporting tools.",
          "The plugin's integration with Pytester should not introduce any new bugs or regressions in this scenario.",
          "The generated report files should have the correct file extensions (JSON and HTML) even when using the `--llm-report` option.",
          "The test function should not fail to run due to a missing or incorrect report file path."
        ],
        "scenario": "Test generates both JSON and HTML reports for a test function.",
        "token_usage": {
          "completion_tokens": 186,
          "prompt_tokens": 279,
          "total_tokens": 465
        },
        "why_needed": "This test prevents regression in cases where the plugin is used with both JSON and HTML output formats."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_both_json_and_html_outputs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06618616400001542,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert data['run_meta']['collected_count'] == 3\", 'expected_value': 3, 'message': 'Expected collected count to be 3'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 198,
          "total_tokens": 294
        },
        "why_needed": "pytest_collection_finish counts items (line 378)"
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 11,
          "line_ranges": "70-71, 73-75, 77, 79, 142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06284381000000394,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nested' directory should be created with 'dir' as its parent directory.",
          "The 'report.json' file within the 'nested' directory should exist.",
          "The 'test_pass()' function should be executed successfully.",
          "The 'pytester.path / nested / dir / report.json' path should be an existing directory.",
          "The 'pytester.runpytest(f--llm-report-json={report_path})' command should execute without raising any errors."
        ],
        "scenario": "The test verifies that the `test_pass()` function is executed and a new directory 'nested' with 'dir' as its parent directory is created.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 247,
          "total_tokens": 414
        },
        "why_needed": "This test prevents regression in cases where the plugin integration fails to create output directories."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_creates_nested_directory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 50,
          "line_ranges": "78-79, 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330, 332, 334-335, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06691091700000129,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the report contains an error code of 1.",
          "The 'error' value under the 'summary' key is set to a non-zero value (in this case, 1).",
          "The test fixture raises a RuntimeError and its name is included in the error summary."
        ],
        "scenario": "Test that fixture errors are captured in report.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 286,
          "total_tokens": 399
        },
        "why_needed": "Fixture failures are not properly reported, leading to incorrect error counts and debugging issues."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_fixture_error_captured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 59,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 114,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.17920272399999249,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'passed' outcome should be present in the report.",
          "The 'failed' outcome should be present in the report.",
          "The 'skipped' outcome should be present in the report.",
          "All test names should be included in the report.",
          "The plugin should not miss any test outcomes, including skipped tests.",
          "The plugin should correctly identify and report all test outcomes, regardless of their status (e.g., running, pending)."
        ],
        "scenario": "Test pytest_runtest_makereport captures outcomes to verify that it correctly identifies all test outcomes.",
        "token_usage": {
          "completion_tokens": 165,
          "prompt_tokens": 335,
          "total_tokens": 500
        },
        "why_needed": "pytest_runtest_makereport prevents regression by ensuring that the plugin correctly identifies and reports all test outcomes, including skipped tests."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_makereport_captures_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 250,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403-404, 558-559, 562-563, 566-568, 579, 583, 602-603, 619-620"
        }
      ],
      "duration": 0.061544793000024356,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'No report generated when run without a file path', 'expected': 'The report.json file should not exist'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 150,
          "total_tokens": 238
        },
        "why_needed": "To ensure that the plugin correctly handles cases where no output is specified."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.6285646659999884,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert True, 'Expected test_pass() to be called with no arguments.'",
          "assert result.ret == 0, 'Expected pytester.runpytest('--llm-pdf=report.pdf') to exit with code 0 (success) or warning.'"
        ],
        "scenario": "Test that the `--llm-pdf` option enables the plugin and triggers its logic.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 435,
          "total_tokens": 548
        },
        "why_needed": "Prevents regression in plugin integration where --llm-pdf is used without enabling the plugin."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_pdf_option_enables_plugin",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06516882299999338,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'start_time' key should be present in the run_meta dictionary.",
          "The value of the 'start_time' key should be a valid timestamp.",
          "The 'start_time' value should be greater than or equal to 0.",
          "The start time should not be None.",
          "The start time should be within the expected range (e.g., between 2023-01-01 00:00:00 and 2024-01-01 23:59:59).",
          "The 'start_time' value should match the actual start time of the session recorded by pytest_sessionstart.",
          "The test should fail if the start time is not within the expected range or is None."
        ],
        "scenario": "Test that pytest_sessionstart records start time is verified by Pytester.",
        "token_usage": {
          "completion_tokens": 205,
          "prompt_tokens": 276,
          "total_tokens": 481
        },
        "why_needed": "This test prevents a potential bug where the start time of the session is not recorded correctly."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_session_start_records_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007695170000090457,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 347,
          "prompt_tokens": 4096,
          "total_tokens": 4443
        },
        "why_needed": ""
      },
      "llm_context_override": "balanced",
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007679649999943194,
      "file_path": "tests/test_plugin_integration.py",
      "llm_opt_out": true,
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000805054000011296,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'Requirement marker should not cause errors.', 'expected_result': 'True', 'message': ''}"
        ],
        "scenario": "tests/test_plugin_integration.py",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 90,
          "total_tokens": 178
        },
        "why_needed": "The requirement marker is used to mark requirements as having a plugin. This helps in identifying which requirements require plugins and can be useful for testing purposes."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
      "outcome": "passed",
      "phase": "call",
      "requirements": [
        "REQ-001",
        "REQ-002"
      ]
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 136,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.047696836000000076,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report JSON file exists at `report.json` in the specified temporary directory.",
          "The total count of passed tests is 1 (test_a.py::test_pass) out of 2 (total)",
          "All test names are included in the report HTML, specifically including `test_a.py` and `test_b.py`.",
          "The report HTML file exists at `report.html` in the specified temporary directory.",
          "The report HTML contains a reference to each test name (`test_a.py` and `test_b.py`)."
        ],
        "scenario": "Test the integration of report writer with pytest_llm_report.",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 417,
          "total_tokens": 596
        },
        "why_needed": "This test prevents regression when integrating report writer with pytest_llm_report, as it ensures that all tests are properly formatted and include required information for a full report."
      },
      "nodeid": "tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586-587"
        }
      ],
      "duration": 0.001407137999990482,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 151,
          "total_tokens": 237
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 12,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586, 590-592"
        }
      ],
      "duration": 0.00173572200000649,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_report was called once with mock_report', 'expected': 1}"
        ],
        "scenario": "TestPluginCollectReport",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 204,
          "total_tokens": 272
        },
        "why_needed": "To test the collectreport functionality when it is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0009205489999999372,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_report.session', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 138,
          "total_tokens": 219
        },
        "why_needed": "To ensure that collectreport does not throw an exception when a session is not available."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0009249580000130209,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytest_collectreport() should not be called with a mock report object that has a None session attribute', 'description': 'The pytest_collectreport function should not be called with a mock report object that has a None session attribute.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 134,
          "total_tokens": 249
        },
        "why_needed": "To ensure that the collectreport plugin behaves correctly when a Pytest session is None."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0030683619999933853,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'LLM enabled flag is present in pyproject.toml', 'value': 'True'}",
          "{'name': 'pyproject.toml does not exist or is empty', 'value': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 143,
          "total_tokens": 258
        },
        "why_needed": "LLM enabled warning is raised when pytest is run with the --llm flag."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 135,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 25,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-358, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0027555669999799193,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytest_configure raises UsageError', 'description': 'The pytest_configure function should raise a UsageError if the configuration is invalid.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 134,
          "total_tokens": 221
        },
        "why_needed": "Validation errors are raised when the pytest configuration is invalid."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 17,
          "line_ranges": "328-330, 332-334, 336-338, 342-343, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012835579999830316,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_config.addinivalue_line.called', 'expected_result': True}"
        ],
        "scenario": "TestPluginConfigure::test_pytest_configure_worker_skip",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 170,
          "total_tokens": 242
        },
        "why_needed": "To ensure that the configure function skips on xdist workers correctly."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0031883460000017294,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking Config.load with None returns a mock object.",
          "Mocking load_config with no arguments calls validate() on mock_cfg.",
          "mock_load.assert_called_once() checks that load_config was called once.",
          "mock_cfg.validate.return_value is an empty list.",
          "mock_load.return_value is mock_cfg, which has the correct option values.",
          "Pytest_configure(mock_config) passes mock_config to load_config.",
          "load_config() does not call any other functions or methods on mock_cfg.",
          "The test fails if Config.load is missing and load_config is called without arguments."
        ],
        "scenario": "Test that fallback to load_config is triggered when Config.load is missing.",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 747,
          "total_tokens": 926
        },
        "why_needed": "To prevent regression where Config.load is missing, and the plugin falls back to load_config."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 122,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0024465799999973115,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file creation', 'expected': 'pyproject.toml was created successfully', 'actual': 'pyproject.toml was not created'}",
          "{'name': 'pyproject.toml content', 'expected': 'pyproject.toml content was created with the correct CLI options', 'actual': 'pyproject.toml content was not created with the correct CLI options'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 140,
          "total_tokens": 299
        },
        "why_needed": "To test the plugin's ability to load configuration files with CLI options overriding those in pyproject.toml."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 112,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.12021364499997844,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected': 'True'}",
          "{'name': 'pyproject.toml file is not empty', 'expected': 'True'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 136,
          "total_tokens": 241
        },
        "why_needed": "To ensure that the plugin can load configuration files from the PyPI repository."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 9,
          "line_ranges": "399, 403-404, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013669729999890023,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mocked stash.get() was called once with _enabled_key and False argument.",
          "Mocking stash.get() to return False for enabled is necessary because pytest_terminal_summary() relies on this assertion.",
          "pytest_terminal_summary() should have checked if the plugin is enabled before reporting its terminal summary.",
          "The test verifies that the plugin's terminal summary is skipped when it's disabled.",
          "This test ensures that the plugin's terminal summary is correctly reported as disabled even without worker input.",
          "Without this test, there might be a false positive report of the plugin being enabled when it's not."
        ],
        "scenario": "Test that terminal summary skips when plugin is disabled.",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 281,
          "total_tokens": 460
        },
        "why_needed": "Prevents a regression where the plugin's terminal summary might be incorrectly reported as enabled even though it's not."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "399-400, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010586479999972198,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is_none', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 164,
          "total_tokens": 238
        },
        "why_needed": "To test that terminal summary skips on xdist worker."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 69,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00450930299999186,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_report_html` option should be set to 'out.html'.",
          "The `llm_report_json` option should be set to 'out.json'.",
          "The `llm_report_pdf` option should be set to None.",
          "The `llm_evidence_bundle` option should be set to None.",
          "The `llm_dependency_snapshot` option should be set to None.",
          "The `llm_requests_per_minute` option should be set to None.",
          "The `llm_aggregate_dir` option should be set to None.",
          "The `llm_aggregate_policy` option should be set to None.",
          "The `llm_aggregate_run_id` option should be set to None.",
          "The `llm_aggregate_group_id` option should be set to None.",
          "The `llm_max_retries` option should be set to None.",
          "The `llm_coverage_source` option should be set to None.",
          "The `llm_prompt_tier` option should be set to None.",
          "The `llm_batch_parametrized` option should be set to None.",
          "The `llm_context_compression` option should be set to None.",
          "The `llm_context_bytes` option should be set to None.",
          "The `llm_context_file_limit` option should be set to None.",
          "The `llm_max_tests` option should be set to None.",
          "The `llm_max_concurrency` option should be set to None.",
          "The `llm_timeout_seconds` option should be set to None.",
          "The `llm_capture_failed` option should be set to None.",
          "The `llm_ollama_host` option should be set to None.",
          "The `llm_litellm_api_base` option should be set to None.",
          "The `llm_litellm_api_key` option should be set to None.",
          "The `llm_litellm_token_refresh_command` option should be set to None.",
          "The `llm_litellm_token_refresh_interval` option should be set to None.",
          "The `llm_litellm_token_output_format` option should be set to None.",
          "The `llm_litellm_token_json_key` option should be set to None.",
          "The `llm_cache_dir` option should be set to the value of `tmp_path`."
        ],
        "scenario": "Test config loading from pytest objects (CLI) to ensure the correct value is set for llm_report_html.",
        "token_usage": {
          "completion_tokens": 587,
          "prompt_tokens": 639,
          "total_tokens": 1226
        },
        "why_needed": "This test prevents a potential bug where the correct value for llm_report_html is not being set, potentially leading to incorrect configuration output."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::testload_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 7,
          "line_ranges": "558-559, 562-563, 566-568"
        }
      ],
      "duration": 0.00164003300000104,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'gen.send(mock_outcome).exception', 'expected_type': 'StopIteration'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 220,
          "total_tokens": 306
        },
        "why_needed": "The test is failing because the makereport hookwrapper is not completing successfully."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0019623659999865595,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_runtest_makereport` function should be able to find and call the `mock_collector` instance when it is enabled.",
          "The `mock_collector.handle_runtest_logreport` method should be called with the `mock_report` object as its argument.",
          "The `mock_collector` instance should have a `handle_runtest_logreport` method that takes two arguments: `mock_report` and `mock_item`.",
          "The `mock_collector` instance should be able to handle runtest log reports by calling the `handle_runtest_logreport` method.",
          "The `pytest_runtest_makereport` function should not call any other functions or methods on the `mock_collector` instance when it is enabled.",
          "The `mock_collector` instance should have a `stash_get` method that returns `True` for `_enabled_key` and `mock_collector` instances, and `None` otherwise.",
          "The `stash_get` method should return `False` for `_collector_key` and `None` otherwise.",
          "The `stash_get` method should not raise an exception when called with a key that is neither `_enabled_key` nor `_collector_key`.",
          "</key_assertions>"
        ],
        "scenario": "Test that makereport calls collector when enabled.",
        "token_usage": {
          "completion_tokens": 317,
          "prompt_tokens": 371,
          "total_tokens": 688
        },
        "why_needed": "This test prevents a potential regression where the plugin does not report any errors even if makereport is called."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 602-603"
        }
      ],
      "duration": 0.00133922099999495,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_session.config.stash.get.assert_called_with(_enabled_key, False)', 'description': 'Verify that stash.get was called with _enabled_key and False as arguments.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 149,
          "total_tokens": 252
        },
        "why_needed": "This test is needed because the pytest_collection_finish function should skip collection finish when disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 602, 606-608"
        }
      ],
      "duration": 0.0017874690000212468,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocking pytest_collection_finish with mock_collector', 'expected_result': 1, 'actual_result': 0}",
          "{'name': 'Mocking stash_get with _enabled_key and _collector_key', 'expected_result': ['True', 'mock_collector'], 'actual_result': ['True', 'mock_collector']}"
        ],
        "scenario": "TestPluginSessionHooks",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 219,
          "total_tokens": 353
        },
        "why_needed": "To ensure that the `pytest_collection_finish` function calls the `_collector_key` collector when collection finish is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 619-620"
        }
      ],
      "duration": 0.0013884729999915635,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_session.get', 'expected_calls': [{'_enabled_key': '_enabled_key', 'False': []}]}"
        ],
        "scenario": "TestPluginSessionHooks",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 157,
          "total_tokens": 233
        },
        "why_needed": "To ensure that the plugin correctly handles session start when disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 11,
          "line_ranges": "558-559, 562, 566-568, 619, 623, 626, 628-629"
        }
      ],
      "duration": 0.0011191709999991417,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The _collector_key should be present in the mock_stash dictionary.",
          "The _start_time_key should also be present in the mock_stash dictionary.",
          "The collector should have been created successfully by pytest_sessionstart.",
          "_enabled_key should be set to True in stash_dict.",
          "Config() should have been created with stash_dict.",
          "pytest_sessionstart() should not raise any exceptions when called with a valid stash.",
          "The mock_stash should contain both get() and [] methods.",
          "The mock_stash should contain the _enabled_key and _config_key keys."
        ],
        "scenario": "Test that sessionstart initializes collector when enabled and creates a stash with both get() and [] methods.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 335,
          "total_tokens": 533
        },
        "why_needed": "This test prevents a potential regression where the collector is not created or does not have access to the stash, potentially leading to incorrect data collection or other issues."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00251622100000759,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')",
          "group.addoption.call_args_list[0][0] == '--llm-report'",
          "group.addoption.call_args_list[1][0] == '--llm-coverage-source'"
        ],
        "scenario": "Test pytest_addoption adds expected arguments and verifies specific options.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 293,
          "total_tokens": 410
        },
        "why_needed": "pytest_addoption prevents a potential bug where the plugin does not add all required arguments to the parser."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00260282200000006,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'parser.addini was not called', 'expected_result': 0, 'actual_result': 1}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 140,
          "total_tokens": 225
        },
        "why_needed": "pytest_addoption no longer adds INI options"
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 53,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0032481479999830754,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option should be set to 'out.html' before calling `pytest_terminal_summary()`.",
          "The `CoverageMapper` instance should be created with the correct configuration.",
          "The `Coverage` object should have a report method that returns the coverage percentage correctly.",
          "The `MockStash` instance should be used as expected in the mock configuration.",
          "The `coverage.Coverage` class should be instantiated and returned correctly from the mock.",
          "The `pytest_llm_report.coverage_map.CoverageMapper` class should be patched to return a mock object.",
          "The `pytest_llm_report.report_writer.ReportWriter` class should be patched to return a mock object with a report method that returns 85.5 as expected."
        ],
        "scenario": "Test coverage percentage calculation logic for terminal summary.",
        "token_usage": {
          "completion_tokens": 203,
          "prompt_tokens": 395,
          "total_tokens": 598
        },
        "why_needed": "Prevents regression in coverage reporting when terminal summary is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 66,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-486, 491-494, 497, 499, 502-504, 512-514, 516, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0029953149999926154,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `pytest_terminal_summary_llm_enabled` test function is executed only once.",
          "Check if the correct configuration is passed to `pytest_terminal_summary`.",
          "Verify that the LLM annotator is called with the correct arguments.",
          "Ensure that the provider is correctly retrieved and used.",
          "Verify that the coverage map is not modified during testing.",
          "Confirm that the report writer is properly initialized.",
          "Check if the LLM model name matches the expected value."
        ],
        "scenario": "Test terminal summary with LLM enabled runs annotations.",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 477,
          "total_tokens": 629
        },
        "why_needed": "Prevents regression by ensuring that the plugin is correctly configured when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 45,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00205862600000728,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert mock_config.stash._enabled_key == True",
          "assert mock_config.stash._config_key == cfg",
          "assert mock_terminalreporter.call_args_list[0][1] == [0, {}]",
          "assert mock_mapper.map_coverage.return_value == {}",
          "assert mock_writer_cls.return_value.report_writer.call_args_list[0][1] == [0, {}]",
          "assert stash._enabled_key == True and stash._config_key == cfg"
        ],
        "scenario": "Test terminal summary creates collector if missing.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 391,
          "total_tokens": 548
        },
        "why_needed": "The test prevents a potential bug where the plugin does not create a collector even when it is supposed to be present in the configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 21,
          "line_ranges": "399, 403, 407, 410-411, 413-414, 417-418, 420, 422-426, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.003348926000001029,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregate_dir parameter should be set to '/agg' when aggregation is enabled.",
          "The stash object should support both get() and [] methods.",
          "The aggregator function should return a report when aggregate=True.",
          "The ReportWriter class should write JSON and HTML files correctly when aggregate=True.",
          "The aggregate method of the Aggregator class should be called once when aggregate=True.",
          "The aggregation flag should be set to True in the config object.",
          "The stash object should have an _enabled_key with value True."
        ],
        "scenario": "Test terminal summary with aggregation enabled.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 441,
          "total_tokens": 600
        },
        "why_needed": "This test prevents regression in the case where aggregation is enabled and there are multiple terminals being reported."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 52,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 476-479, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.005359330999993972,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert mock_cov_cls.return_value is None",
          "assert mock_cov.load.side_effect == OSError('Disk full')",
          "assert pytest_terminal_summary(MagicMock(), 0, mock_config).report_html is None",
          "assert mock_config.stash._enabled_key is True",
          "assert mock_config.stash._config_key is cfg",
          "assert _enabled_key in mock_config.stash",
          "assert _config_key in mock_config.stash"
        ],
        "scenario": "Test coverage calculation error when loading coverage map.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 389,
          "total_tokens": 534
        },
        "why_needed": "This test prevents regression where the coverage calculation fails due to an OSError during load."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 63,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0068007530000215866,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'utils.py' file is present in the assembled context.",
          "The 'def util()' function is found in the 'utils.py' file within the assembled context.",
          "The coverage report includes the 'utils.py' file and the 'def util()' function."
        ],
        "scenario": "Tests the ContextAssembler with a balanced context configuration to ensure it correctly includes dependencies and passes coverage tests.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 331,
          "total_tokens": 453
        },
        "why_needed": "This test prevents regression by ensuring that the ContextAssembler correctly assembles a balanced context, including all necessary dependencies."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139-140, 268-272"
        }
      ],
      "duration": 0.0009784080000088125,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'test_1', 'actual_value': 'True'}"
        ],
        "scenario": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 176,
          "total_tokens": 263
        },
        "why_needed": "To test the ContextAssembler's ability to assemble a complete context for a test file."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 30,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0011096919999999955,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'test_1' function is present in the source code of the test file.",
          "The context for the test function is empty.",
          "The test result nodeid matches the expected outcome."
        ],
        "scenario": "Test the ContextAssembler with minimal context mode and a test file.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 267,
          "total_tokens": 361
        },
        "why_needed": "This test prevents regression when using minimal context mode without specifying a repository root."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 46,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0011110359999975117,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'f1.py' file in the test result should contain the original long content.",
          "The 'truncated' message should not appear within the 'f1.py' file in the test result.",
          "The length of the 'f1.py' file in the test result should be less than or equal to 40 bytes (20 bytes + truncation message)."
        ],
        "scenario": "Test the ContextAssembler with balanced context limits to ensure it does not truncate long content within a file.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 335,
          "total_tokens": 482
        },
        "why_needed": "This test prevents bugs that may occur when the ContextAssembler is used with large files, causing the context to be truncated unnecessarily."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 50,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0011066669999877377,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Context is present in the assembled output.",
          "File content is preserved and not truncated.",
          "The 'truncated' assertion is not triggered.",
          "Context does not contain any 'truncated' key.",
          "Context size matches the file content size.",
          "LLM context bytes limit is respected for long files.",
          "Context assembler correctly handles large files in complete mode."
        ],
        "scenario": "Test that 'complete' mode does not truncate long files despite a small llm_context_bytes limit.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 361,
          "total_tokens": 511
        },
        "why_needed": "This test prevents a regression where the LLM context size exceeds the file content size, causing truncation of long files in complete mode."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_complete_context_limits_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 26,
          "line_ranges": "33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0010025619999964874,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_get_test_source` returns an empty string when given a non-existent file path.",
          "The function `_get_test_source` correctly identifies the nested test name with parameters in the provided source code.",
          "The function `_get_test_source` handles nested test names with parameters by including the parameter value in the source code."
        ],
        "scenario": "Verify the correct handling of non-existent files and nested test names with parameters.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 275,
          "total_tokens": 410
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly handles cases where the test file does not exist or has nested test names with parameters."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 5,
          "line_ranges": "33, 284-287"
        }
      ],
      "duration": 0.001513827999986006,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert assembler._should_exclude('test.pyc') is True",
          "assert assembler._should_exclude('secret/key.txt') is True",
          "assert assembler._should_exclude('public/readme.md') is False"
        ],
        "scenario": "The test verifies that the ContextAssembler should exclude certain Python files and directories from being processed.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 227,
          "total_tokens": 341
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly includes certain files or directories in its processing, leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_should_exclude",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        }
      ],
      "duration": 0.0009686700000202109,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context_files == {}",
          "def test_foo()",
          "test_source contains 'def test_foo'"
        ],
        "scenario": "Test assemble minimal mode returns no context files.",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 298,
          "total_tokens": 376
        },
        "why_needed": "To prevent a regression where the assemble function does not generate any context files when run in minimal mode."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_minimal_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 62,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0012112619999982144,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "ContextAssembler should use balanced mode due to override.",
          "ContextAssembler should include module.py in context_files.",
          "ContextAssembler should respect llm_context_override from test.",
          "Test assemble respects llm_context_override from test.",
          "ContextAssembler should not modify test file content when overriding LLM context.",
          "ContextAssembler should preserve original file path and line information when assembling with override mode.",
          "ContextAssembler should use correct mode for assembly (balanced in this case).",
          "ContextAssembler should respect the specified llm_context_override value."
        ],
        "scenario": "Test assemble respects llm_context_override from test.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 362,
          "total_tokens": 528
        },
        "why_needed": "This test prevents regression by ensuring the ContextAssembler uses the correct mode when overriding LLM context."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_with_context_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 20,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163-164, 201, 284-286"
        }
      ],
      "duration": 0.0009578789999977744,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'secret_config.py' should not be included in the balanced context.",
          "The file 'test_foo.py' should not be included in the balanced context.",
          "The LLM context mode is set to 'balanced', and it excludes files matching exclude patterns.",
          "The LLM context include glob pattern '*secret*' is excluded from the balanced context.",
          "The coverage of the test file 'secret_config.py' under the balanced context should be 0%.",
          "The coverage of the test file 'test_foo.py' under the balanced context should be 0%.",
          "The LLM context exclude glob pattern '*secret*' is not included in the output files.",
          "The LLM context include glob pattern '**/*' is excluded from the balanced context.",
          "The LLM context include glob pattern '**/*' excludes files that match exclude patterns."
        ],
        "scenario": "Test 'test_balanced_context_excludes_patterns' verifies that a balanced context excludes files matching exclude patterns.",
        "token_usage": {
          "completion_tokens": 254,
          "prompt_tokens": 331,
          "total_tokens": 585
        },
        "why_needed": "This test prevents regression where the LLM context mode is set to 'balanced', and it includes files in the context that match exclude patterns."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_excludes_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 16,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-161, 201"
        }
      ],
      "duration": 0.0008703859999741326,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'context is empty', 'description': 'The ContextAssembler should return an empty dictionary when no balanced context file exists.'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 201,
          "total_tokens": 297
        },
        "why_needed": "To ensure that the ContextAssembler correctly handles cases where a balanced context file is not found."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.01366705000000934,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The content of the source file is not longer than 120 bytes.",
          "The LLM context is not truncated when the source file exceeds 6000 bytes.",
          "The LLM context is truncated when the source file exceeds 10000 bytes (1000 + 9000).",
          "The LLM context does not exceed the maximum allowed bytes even if it contains a large number of lines.",
          "The LLM context does not contain any 'truncated' messages when the source file exceeds 6000 bytes.",
          "The LLM context is truncated only after the last line of the source file."
        ],
        "scenario": "Test that balanced context respects max bytes limit.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 405,
          "total_tokens": 585
        },
        "why_needed": "This test prevents a potential bug where the LLM context exceeds the maximum allowed bytes, causing truncated content."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_max_bytes_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 139-140"
        }
      ],
      "duration": 0.0007976900000130627,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equals', 'expected_value': '{}', 'actual_value': '{}'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 162,
          "total_tokens": 248
        },
        "why_needed": "To ensure that the ContextAssembler can correctly assemble a balanced context with no coverage."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 35,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-157, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0011106440000219209,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context should have only one node (either 'file1.py' or 'file2.py')",
          "context length should be less than or equal to 1",
          "context should contain both file paths and their respective lines and line counts"
        ],
        "scenario": "Test that loop exits when max bytes is reached before processing file.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 409,
          "total_tokens": 519
        },
        "why_needed": "Prevents a potential memory leak by ensuring the context assembler does not exceed the maximum allowed bytes before processing files."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_reaches_max_bytes_before_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0010505120000061652,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'inclusion', 'expected_values': ['module.py'], 'actual_value': 'module.py'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 211,
          "total_tokens": 298
        },
        "why_needed": "To ensure that complete context delegates to balanced correctly."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 9,
          "line_ranges": "33, 78-79, 82-83, 86-89"
        }
      ],
      "duration": 0.0008928279999906863,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_name': 'result == \"\"', 'expected_result': ''}"
        ],
        "scenario": "Test _get_test_source with empty nodeid returns empty string",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 148,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the ContextAssembler correctly handles an empty node ID in the test source."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_empty_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.0009688700000083372,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The test source extraction should stop at the next function definition.', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 129,
          "total_tokens": 229
        },
        "why_needed": "To ensure that source extraction stops at the next function definition, even if there are multiple definitions in a single file."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 6,
          "line_ranges": "33, 78-79, 82-84"
        }
      ],
      "duration": 0.0008154129999979887,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Test that the function returns an empty string for a non-existent test source file.', 'expected_result': '', 'actual_result': ''}"
        ],
        "scenario": "Edge Case: Test Source File Not Exists",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 142,
          "total_tokens": 232
        },
        "why_needed": "The test assembly function `_get_test_source` should handle cases where the test source file does not exist."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.0009786580000081813,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected output is a string', 'expected_value': 'test_example.py', 'actual_value': {'scenario': 'tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class', 'why_needed': 'To ensure that the _get_test_source function correctly extracts functions with proper indentation, even when they are nested within other code blocks.', 'key_assertions': ['Expected output is a string'], 'value': 'test_example.py'}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
        "token_usage": {
          "completion_tokens": 174,
          "prompt_tokens": 118,
          "total_tokens": 292
        },
        "why_needed": "To ensure that the _get_test_source function correctly extracts functions with proper indentation, even when they are nested within other code blocks."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0008064559999922949,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3', 'actual': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 106,
          "total_tokens": 178
        },
        "why_needed": "To ensure that consecutive lines are compressed into a single range."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000792821000004551,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3', 'actual': '1-2'}",
          "{'expected': '2-4', 'actual': '2-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 107,
          "total_tokens": 200
        },
        "why_needed": "To test the handling of duplicate ranges in the compress_ranges function."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "29-30"
        }
      ],
      "duration": 0.0007743270000162283,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 92,
          "total_tokens": 162
        },
        "why_needed": "Because an empty list is considered a valid input for the `compress_ranges` function."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0008785010000167404,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3, 5, 10-12, 15', 'actual': '1-3, 5, 10-12, 15'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 130,
          "total_tokens": 225
        },
        "why_needed": "To test the functionality of compressing mixed ranges in a list."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007606210000119518,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '1, 3, 5', 'actual_value': '1, 3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 113,
          "total_tokens": 201
        },
        "why_needed": "To ensure that non-consecutive lines are correctly compressed to a single comma-separated value."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "29, 33, 35-37, 39, 50, 52, 65-66"
        }
      ],
      "duration": 0.00078237099998546,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 5, 'actual': '5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_single_line",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 96,
          "total_tokens": 162
        },
        "why_needed": "The single line should be compressed using the range notation."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0008299609999937729,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-2', 'actual': '1-2'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 103,
          "total_tokens": 179
        },
        "why_needed": "The test is necessary because the current implementation does not handle two consecutive lines correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0007806780000123581,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3, 5', 'actual': '1-3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 110,
          "total_tokens": 196
        },
        "why_needed": "The test is necessary to ensure that the `compress_ranges` function can handle unsorted input correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "81-82"
        }
      ],
      "duration": 0.0007850159999804873,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [], 'actual': []}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
        "token_usage": {
          "completion_tokens": 60,
          "prompt_tokens": 90,
          "total_tokens": 150
        },
        "why_needed": "The current implementation does not handle empty strings correctly."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 11,
          "line_ranges": "81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0008030200000064269,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3, 5, 10, 11, 12], 'actual': ['1', '2', '3', '5', '10', '11', '12']}",
          "{'expected': [], 'actual': []}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_mixed",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 121,
          "total_tokens": 234
        },
        "why_needed": "The test is necessary because it checks for the correct expansion of mixed ranges and singles."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_mixed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "81, 84-91, 95"
        }
      ],
      "duration": 0.0007593790000157696,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3], 'actual': ['1', '2', '3']}",
          "{'expected': \"expand_ranges('1-3')\", 'actual': \"['1', '2', '3']\"}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_range",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 99,
          "total_tokens": 198
        },
        "why_needed": "The range function is not correctly expanding the input string."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_range",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 27,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0008015969999917161,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'original == expanded', 'expected': [1, 2, 3, 5, 10, 11, 12, 15], 'message': 'Original and expanded lists must be equal.'}"
        ],
        "scenario": "compress_ranges and expand_ranges should be inverses.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 134,
          "total_tokens": 250
        },
        "why_needed": "This test ensures that the `compress_ranges` and `expand_ranges` functions are inverse operations, meaning they can be used to reconstruct the original list from a compressed representation."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 7,
          "line_ranges": "81, 84-87, 93, 95"
        }
      ],
      "duration": 0.0007890939999981583,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'The output should be an array containing only the single element: 5', 'expected_value': [5]}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_single_number",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 95,
          "total_tokens": 180
        },
        "why_needed": "To ensure that the `expand_ranges` function correctly handles a single number as input."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_single_number",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65, 67"
        }
      ],
      "duration": 0.0007838740000067901,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"durations under 1s are formatted correctly as '0ms'\"}",
          "{'description': \"durations between 1s and 2s are formatted correctly as '100ms'\"}",
          "{'description': \"durations exactly equal to 1 second are formatted correctly as '1000ms'\"}",
          "{'description': \"durations greater than or equal to 2 seconds are formatted correctly as '2000ms'\"}",
          "{'description': \"durations under 0.5s are formatted correctly as '500ms'\"}",
          "{'description': \"durations between 0.5s and 1 second are formatted correctly as '1000ms'\"}",
          "{'description': \"durations exactly equal to 1 second is formatted correctly as '2000ms'\"}",
          "{'description': \"durations greater than or equal to 2 seconds are formatted correctly as '4000ms'}\"}"
        ],
        "scenario": "tests/test_render.py::TestFormatDuration::test_milliseconds verifies that the function correctly formats durations in milliseconds for times less than 1 second.",
        "token_usage": {
          "completion_tokens": 274,
          "prompt_tokens": 211,
          "total_tokens": 485
        },
        "why_needed": "This test prevents a potential bug where the function does not format durations as expected for times less than 1 second, potentially leading to incorrect rendering of time-related content."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65-66"
        }
      ],
      "duration": 0.0008184600000049613,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Expected format to be '1.23s' for duration 1.23 seconds\"}",
          "{'message': \"Expected format to be '60.00s' for duration 60 seconds\"}"
        ],
        "scenario": "tests/test_render.py::TestFormatDuration::test_seconds",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 116,
          "total_tokens": 210
        },
        "why_needed": "To ensure the function `format_duration` correctly formats time durations in seconds."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0007627049999996416,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `outcome_to_css_class` function should map each outcome to a unique CSS class.",
          "The function should handle cases where an outcome is not recognized (e.g., 'xfailed').",
          "The function should preserve the original outcome value when mapping to a CSS class (e.g., 'passed' -> 'outcome-passed')."
        ],
        "scenario": "Test Outcome Mapping to CSS Classes",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 263,
          "total_tokens": 379
        },
        "why_needed": "To ensure that all outcomes are correctly mapped to their corresponding CSS classes."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0008208630000012818,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"outcome_to_css_class('unknown') == 'outcome-unknown'\", 'expected_result': 'outcome-unknown'}"
        ],
        "scenario": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 102,
          "total_tokens": 193
        },
        "why_needed": "The test is necessary because it checks for the default CSS class when an unknown outcome is encountered."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65-67, 79-85, 87, 121-124, 126-127, 131-132, 155-157, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008619099999975788,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The presence of the '<!DOCTYPE html>' header in the rendered HTML.",
          "The inclusion of 'Test Report' in the HTML content.",
          "The presence of 'test::passed' and 'test::failed' node IDs in the HTML.",
          "The correct display of 'PASSED' and 'FAILED' text within the report.",
          "The accurate display of plugin and repository versions in the HTML."
        ],
        "scenario": "The test verifies that a complete HTML document is rendered with the expected report content.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 426,
          "total_tokens": 575
        },
        "why_needed": "This test prevents a potential rendering issue where the report might not be displayed correctly due to missing or incorrect HTML elements."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-129, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008112350000146762,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'src/foo.py' file should be included in the rendered HTML.",
          "The number of lines rendered should match the total number of lines in the file.",
          "All lines in the file should be present in the rendered HTML."
        ],
        "scenario": "Test renders coverage for fallback HTML test.",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 288,
          "total_tokens": 378
        },
        "why_needed": "Prevents regression and ensures accurate coverage reporting."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 64,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 140-142, 144, 147, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008392780000008315,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report includes \"Tests login flow\" in its HTML content.",
          "The report includes \"Prevents auth bypass\" in its HTML content.",
          "The report includes the string 'Confidence:' in its HTML content with a confidence score of '85%'.",
          "The report does not include any LLM annotations without a confidence score."
        ],
        "scenario": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 317,
          "total_tokens": 456
        },
        "why_needed": "This test prevents the rendering of LLM annotations with a low confidence score, which could be misleading and potentially lead to security vulnerabilities."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 68,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-178, 180-186, 191, 206, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008284079999896221,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Source Coverage' section should be present in the rendered HTML.",
          "The 'src/foo.py' file path should be included in the 'Source Coverage' section.",
          "The percentage of covered code (80.0%) should be displayed correctly in the 'Source Coverage' section.",
          "The ranges of missed code ('1-4, 6-8') and missed files ('5, 9-10') should be accurately represented in the 'Source Coverage' section.",
          "The coverage percentage should be calculated correctly based on the actual number of statements (10) and the total number of lines (12).",
          "The covered code ranges should match the expected values ('1-4', '6-8').",
          "The missed code ranges should match the expected values ('5', '9-10').",
          "</source_coverage>",
          "key_assertions[0] = True",
          "# The 'Source Coverage' section should be present in the rendered HTML."
        ],
        "scenario": "Test renders source coverage for fallback HTML.",
        "token_usage": {
          "completion_tokens": 254,
          "prompt_tokens": 331,
          "total_tokens": 585
        },
        "why_needed": "Prevents a regression where the source coverage summary is not displayed correctly when using fallback HTML."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 55,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008378050000032999,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string 'XFailed' should be present in the HTML output.",
          "The string 'XPassed' should be present in the HTML output.",
          "Both 'XFailed' and 'XPassed' strings should be found in the HTML output."
        ],
        "scenario": "Test renders xpass summary for ReportRoot report.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 283,
          "total_tokens": 396
        },
        "why_needed": "This test prevents a regression where the 'xfailed/xpassed' summary is not rendered correctly when there are multiple failed and passed tests."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007925209999939398,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'hash': 'd41d8cd98f00b804d0a131e86038e95'}, 'actual': {'hash': '6f5dbce7c8694edd9f2d0783ba3f1d32'}}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 115,
          "total_tokens": 226
        },
        "why_needed": "To ensure that different content produces different hashes."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.000781891000002588,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Empty bytes should produce consistent hash.', 'expected_result': 'True'}",
          "{'message': 'Hash1 length should be 64 (SHA256 hex length).', 'expected_result': 64}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 129,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the test suite is robust and can handle empty input data."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 72,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307"
        }
      ],
      "duration": 0.00971585799999275,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The duration of the test should be 60.0 seconds.",
          "The pytest version should have a value.",
          "The plugin version should match the current __version__.",
          "The Python version should match the current __python_version__."
        ],
        "scenario": "Test 'Run meta should include version info' verifies that the test report writer correctly includes version information in the build run metadata.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 318,
          "total_tokens": 449
        },
        "why_needed": "This test prevents regression where the test report writer does not include version information in the build run metadata, potentially leading to incorrect reporting or analysis of test results."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_run_meta",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 19,
          "line_ranges": "156-158, 319, 321-322, 324-335, 337"
        }
      ],
      "duration": 0.0008615799999915907,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of outcomes should be equal to 6 (all outcome types).",
          "The number of passed outcomes should be 1.",
          "The number of failed outcomes should be 1.",
          "The number of skipped outcomes should be 1.",
          "The number of xfailed outcomes should be 1.",
          "The number of xpassed outcomes should be 1.",
          "The number of error outcomes should be 1."
        ],
        "scenario": "Test verifies that the `build_summary` method counts all outcome types correctly.",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 336,
          "total_tokens": 488
        },
        "why_needed": "This test prevents a regression where the summary does not include all outcome types, potentially leading to incorrect reporting."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 319, 321-322, 324-329, 337"
        }
      ],
      "duration": 0.0007866500000091037,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of tests should be equal to the sum of passed, failed and skipped tests.",
          "The number of passed tests should be equal to the sum of passed outcomes.",
          "The number of failed tests should be equal to the sum of failed outcomes.",
          "The number of skipped tests should be equal to the sum of skipped outcomes.",
          "All test results should have a valid `nodeid` and an associated `outcome`.",
          "The summary should not contain any invalid or missing data."
        ],
        "scenario": "Test that the `build_summary_counts` method correctly counts outcomes in a test report.",
        "token_usage": {
          "completion_tokens": 165,
          "prompt_tokens": 283,
          "total_tokens": 448
        },
        "why_needed": "This test prevents regression where the total count of passed, failed and skipped tests is not updated correctly."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_counts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "156-158"
        }
      ],
      "duration": 0.0007790650000174537,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` attribute of the `ReportWriter` instance should be equal to the provided `Config` object.",
          "The `warnings` list of the `ReportWriter` instance should be empty.",
          "The `artifacts` list of the `ReportWriter` instance should be empty."
        ],
        "scenario": "Testing the creation of a ReportWriter instance with a valid configuration.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 199,
          "total_tokens": 316
        },
        "why_needed": "This test prevents potential bugs where a new ReportWriter instance is created without properly initializing its configuration."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_create_writer",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337"
        }
      ],
      "duration": 0.009922896000006176,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the report.tests list should be equal to 2 (the number of tests).",
          "The total value of report.summary.total should be equal to 2 (the number of tests)."
        ],
        "scenario": "Test writes a report with all assembled tests.",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 255,
          "total_tokens": 354
        },
        "why_needed": "This test prevents regression where the report does not include all tests, potentially leading to incorrect reporting or missing important information."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.010287806999997429,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert report.summary.coverage_total_percent == 85.5', 'expected_value': 85.5, 'message': 'Expected coverage total percent to be 85.5'}"
        ],
        "scenario": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 132,
          "total_tokens": 240
        },
        "why_needed": "To ensure that the ReportWriter class correctly calculates and returns the total coverage percentage in the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 97,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.01046549200000868,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `report.source_coverage` should be 1.",
          "The file path of the first element in `report.source_coverage` should match 'src/foo.py'.",
          "All elements in `report.source_coverage` should have a valid `file_path` attribute.",
          "Each element in `report.source_coverage` should have a corresponding `covered` value between 0 and 100.",
          "The total coverage percentage of all covered statements should be greater than or equal to the given coverage percent.",
          "All covered ranges should match one of the provided patterns.",
          "All missed ranges should match an empty string.",
          "Each statement in `source_coverage` should have a corresponding `missed` value between 0 and 100.",
          "The total number of statements in `source_coverage` should be greater than or equal to the given number of statements.",
          "All covered ranges should cover at least one statement.",
          "All missed ranges should not contain any statements."
        ],
        "scenario": "Test ReportWriter::test_write_report_includes_source_coverage verifies that the test writes a report with source coverage summary.",
        "token_usage": {
          "completion_tokens": 274,
          "prompt_tokens": 291,
          "total_tokens": 565
        },
        "why_needed": "This test prevents regression where the report does not include source coverage information, which is crucial for debugging and tracking changes in codebase."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 99,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337"
        }
      ],
      "duration": 0.01003674899999396,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report should contain only one coverage entry for the specified test.",
          "The file path of the coverage entry matches the expected file path.",
          "All line ranges and counts in the coverage entry match the expected values."
        ],
        "scenario": "Test ReportWriter::test_write_report_merges_coverage verifies that the test writes a merged coverage report.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 285,
          "total_tokens": 395
        },
        "why_needed": "This test prevents regression where the coverage is not properly merged into tests, potentially leading to inaccurate reporting."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513-514, 516-519, 522-523"
        }
      ],
      "duration": 0.011442003000013301,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file \"report.json\" exists at the expected location.",
          "Any warnings generated by the ReportWriter are marked with code 'W203'.",
          "The `write_report` method does not raise an exception when writing to a non-existent file."
        ],
        "scenario": "Test that the ReportWriterWithFiles class falls back to direct write if atomic write fails and writes warnings.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 276,
          "total_tokens": 409
        },
        "why_needed": "This test prevents a regression where the ReportWriterWithFiles class does not fall back to direct write when an atomic write operation fails, potentially leading to incorrect report generation or data loss."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 128,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.01149168599999939,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The output directory should exist.', 'expected_result': 'True'}",
          "{'assertion': 'The output file should be created.', 'expected_result': 'False'}"
        ],
        "scenario": "Test case 'tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing'",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 171,
          "total_tokens": 280
        },
        "why_needed": "Because the test writer does not create an output directory if it already exists."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 12,
          "line_ranges": "156-158, 477-480, 487-491"
        }
      ],
      "duration": 0.0012983649999966929,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `writer.warnings` is populated with warnings that have a code of 'W201'.",
          "The directory creation fails and an OSError is raised.",
          "The `pathlib.Path.mkdir` mock raises an OSError with the message 'Permission denied' when called on the specified path."
        ],
        "scenario": "Test that a directory creation failure prevents the capture of a warning code 'W201' from ReportWriter._ensure_dir.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 278,
          "total_tokens": 407
        },
        "why_needed": "This test prevents a regression where the report writer does not capture warnings when creating directories with permission issues."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.001212534000018195,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_git_info()` function should not raise an exception when `git` is not found.",
          "The `get_git_info()` function should set both `sha` and `dirty` attributes to `None` in such cases.",
          "The test should be able to run without any issues or errors when the git command fails.",
          "The expected values of `sha` and `dirty` should match the actual output for a successful git command execution.",
          "The function should not raise an exception when it encounters a non-existent git repository.",
          "The function should set both `sha` and `dirty` attributes to `None` even if the git command fails but returns no error message.",
          "The test should be able to handle cases where the git command fails due to other reasons (e.g., network issues, etc.) without crashing or raising an exception."
        ],
        "scenario": "Test 'test_git_info_failure' verifies that the `get_git_info` function handles git command failures gracefully by returning `None` for both SHA and dirty flag values.",
        "token_usage": {
          "completion_tokens": 262,
          "prompt_tokens": 231,
          "total_tokens": 493
        },
        "why_needed": "This test prevents a regression where the `get_git_info` function fails to return expected values when it encounters a git command failure."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 120,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04480169800001477,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'report.html' file should exist in the temporary directory.",
          "The 'report.html' file should contain the expected content.",
          "The 'report.html' file should include the following text: 'test1', 'test2', 'PASSED', 'FAILED', 'Skipped', 'XFailed', and 'XPassed'.",
          "The 'report.html' file should not be empty."
        ],
        "scenario": "Test verifies that the report writer creates an HTML file with expected content.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 366,
          "total_tokens": 515
        },
        "why_needed": "This test prevents a regression where the report writer does not create an HTML file even if there are tests that fail or are skipped."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-333, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04562071800000922,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'XFAILED' keyword is present in the HTML summary.",
          "The 'XFailed' keyword is present in the HTML summary.",
          "The 'XPASSED' keyword is present in the HTML summary.",
          "The 'XPassed' keyword is present in the HTML summary.",
          "All xfail results are included in the report.",
          "No xfail results are excluded from the report.",
          "The report includes a summary of all test outcomes."
        ],
        "scenario": "Test verifies that xfail outcomes are included in the HTML summary.",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 308,
          "total_tokens": 456
        },
        "why_needed": "This test prevents regression where xfail results are not included in the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.01149032399999328,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should be created in the specified path.",
          "At least one artifact should be tracked for the report.",
          "The number of artifacts should be greater than zero.",
          "The `writer.artifacts` list should contain at least one element.",
          "The JSON file should have a valid hash."
        ],
        "scenario": "Test verifies that a JSON file is created with the report.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 265,
          "total_tokens": 383
        },
        "why_needed": "This test prevents regression where the report writer does not create a JSON file."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-430, 441-442, 444-450, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.04915649200000871,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer` object should have created a new PDF file at the specified path.",
          "Any artifacts generated by the test should match the expected path.",
          "The `writer` object should have returned a list of artifact paths that match the expected path.",
          "The `writer` object's `artifacts` attribute should contain any artifacts generated by the test.",
          "The `writer` object's `path` attribute should be set to the expected PDF file path.",
          "Any errors raised during the execution of the `write_pdf` method should have been caught and reported correctly."
        ],
        "scenario": "Test verifies that the `write_pdf` method creates a PDF file when Playwright is available.",
        "token_usage": {
          "completion_tokens": 186,
          "prompt_tokens": 478,
          "total_tokens": 664
        },
        "why_needed": "This test prevents regression where the `report_writer` module does not create a PDF file when Playwright is installed."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 103,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408-412, 415"
        }
      ],
      "duration": 0.01073045399999728,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pdf_path` does not exist after writing the report.",
          "At least one warning has the code `WarningCode.W204_PDF_PLAYWRIGHT_MISSING.value`."
        ],
        "scenario": "Test should warn when Playwright is missing for PDF output.",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 311,
          "total_tokens": 406
        },
        "why_needed": "To prevent a warning about missing Playwright for PDF output, which may indicate an issue with the test environment."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.002501343000005818,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is None', 'expected_result': {'type': 'NoneType', 'message': 'git info from nonexistent path should return None'}}",
          "{'name': 'assert dirty is None', 'expected_result': {'type': 'NoneType', 'message': 'git info from nonexistent path should return None'}}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 123,
          "total_tokens": 262
        },
        "why_needed": "To test that the report writer does not attempt to write to a non-existent Git directory."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 16,
          "line_ranges": "67-74, 76-81, 83-84"
        }
      ],
      "duration": 0.00991011099998218,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result is None or str', 'description': 'The test expects the function to return either None or a string (representing the git SHA).'}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 159,
          "total_tokens": 262
        },
        "why_needed": "To ensure that the `get_git_info` function returns a valid git SHA for a valid repository."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0010103670000205511,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_plugin_git_info()` returns None or a string when the fallback is necessary.",
          "The function `get_plugin_git_info()` does not raise any exceptions when the fallback is necessary.",
          "The function `get_plugin_git_info()` still works via git runtime after the fallback.",
          "The `_git_info` cache is cleared before the fallback occurs.",
          "The `sha` variable is None or a string in the case of a fallback.",
          "The `isinstance(sha, str)` assertion passes when the fallback is necessary.",
          "The function does not raise an exception when the fallback is necessary.",
          "The function still works after clearing the `_git_info` cache."
        ],
        "scenario": "Test falls back to git runtime when _git_info import fails.",
        "token_usage": {
          "completion_tokens": 199,
          "prompt_tokens": 253,
          "total_tokens": 452
        },
        "why_needed": "Prevents a regression where the plugin's Git info cannot be retrieved due to an import failure."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0008166759999994611,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is not None or isinstance(sha, str)', 'description': \"Test that the function returns a non-None value for sha and a string if it's dirty\"}"
        ],
        "scenario": "test_get_plugin_git_info",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 141,
          "total_tokens": 225
        },
        "why_needed": "to ensure plugin git info returns some values"
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_returns_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.011836901000009448,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected report to be written with correct content', 'description': 'The report should contain the expected key assertions.', 'expected_value': {'scenario': 'Test atomic write fallback', 'why_needed': 'To ensure that the ReportWriter can handle unexpected errors during an atomic write operation.'}}"
        ],
        "scenario": "Test atomic write fallback",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 181,
          "total_tokens": 293
        },
        "why_needed": "To ensure that the ReportWriter can handle unexpected errors during an atomic write operation."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterAtomicWrite::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 28,
          "line_ranges": "156-158, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.1225577539999847,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocked playwright context should raise a RuntimeError when browser launch fails.",
          "Writer should have warnings about PDF failure for the mocked playwright context.",
          "Any warning code in the writer should match 'W201'.",
          "The test should fail if no warnings are present in the writer's output.",
          "The test should pass if all warnings are removed or ignored by the writer.",
          "The writer should not raise an exception when the playwright context is successfully launched and PDF generation proceeds normally."
        ],
        "scenario": "Test PDF generation with playwright exception when browser launch fails.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 356,
          "total_tokens": 505
        },
        "why_needed": "Prevents test from passing if playwright raises an exception during PDF generation."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "156-158, 408-412, 415"
        }
      ],
      "duration": 0.0012639309999826764,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'W204' warning should be present in the warnings list.",
          "The PDF file 'report.pdf' should not exist.",
          "The 'ReportWriter' instance does not raise an exception when playwright is not installed."
        ],
        "scenario": "Test PDF generation when playwright is not installed.",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 293,
          "total_tokens": 394
        },
        "why_needed": "Prevents a potential bug where the report writer fails to create a PDF file due to playwright being missing."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 11,
          "line_ranges": "156-158, 455, 460, 462, 465-469"
        }
      ],
      "duration": 0.036530687999999145,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report writer creates a new temporary file with the suffix '.html'.",
          "The path of the created temporary file exists and matches the expected extension.",
          "The path of the temporary file does not contain any non-existent files (i.e., no actual HTML content)."
        ],
        "scenario": "Verify that _resolve_pdf_html_source creates a temporary file when no HTML source is provided.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 265,
          "total_tokens": 380
        },
        "why_needed": "Prevents regression where the test fails due to missing or empty HTML source configuration."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_creates_temp",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 455-457, 460, 462, 465-469"
        }
      ],
      "duration": 0.03667338500000028,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report writer should be able to resolve the HTML source and generate a PDF.",
          "The resolved path should exist and not point to an empty directory.",
          "The test should fail when the file does not exist, indicating a bug in the configuration or reporting logic.",
          "The temporary file created by the test should be deleted after use.",
          "The report writer should handle missing HTML files correctly, falling back to a temporary file if necessary."
        ],
        "scenario": "Verify that the test resolves an HTML source when a missing HTML file exists.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 270,
          "total_tokens": 415
        },
        "why_needed": "Prevents regression in case of missing HTML files, ensuring correct PDF generation."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_missing_html_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 7,
          "line_ranges": "156-158, 455-458"
        }
      ],
      "duration": 0.0009894989999850168,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function writes to the specified HTML path.",
          "The function returns False indicating that is_temp is True.",
          "The function writes to an existing HTML file.",
          "The function checks if the path matches the expected html_path.",
          "The function does not write to a non-existent HTML file.",
          "The function returns True indicating that is_temp is False."
        ],
        "scenario": "The test verifies that the _resolve_pdf_html_source method uses an existing HTML file as its source.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 266,
          "total_tokens": 408
        },
        "why_needed": "This test prevents a potential bug where the method does not find any existing HTML files and therefore cannot resolve the PDF."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_uses_existing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000965944000000718,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert schema.scenario == 'Verify login'",
          "assert schema.why_needed == 'Catch auth bugs'",
          "assert schema.key_assertions == ['assert 200', 'assert token']",
          "assert schema.confidence == 0.95"
        ],
        "scenario": "Test that `AnnotationSchema.from_dict` can create a full annotation from a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 276,
          "total_tokens": 391
        },
        "why_needed": "Prevents regression in case of missing required fields, ensuring the validation logic works correctly."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 8,
          "line_ranges": "90-92, 94-98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008481739999979254,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert data['scenario'] == 'Verify login',",
          "assert data['why_needed'] == 'Catch auth bugs',",
          "assert data['key_assertions'] == ['assert 200', 'assert token'],",
          "assert data['confidence'] == 0.95"
        ],
        "scenario": "test_to_dict_full verifies that the AnnotationSchema can convert to a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 273,
          "total_tokens": 401
        },
        "why_needed": "This test prevents regression bugs in the AnnotationSchema where it may not be able to generate a full dictionary representation of an annotation."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.10018998499998588,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report path exists and contains the expected content.",
          "The report path exists and contains the string 'test_simple' in its content.",
          "The report path is a valid file system path.",
          "The report path does not contain any other HTML tags than '<html' and 'test_simple'.",
          "The report path does not contain any other text except for the expected content.",
          "The test function `test_simple` passes correctly."
        ],
        "scenario": "The HTML report is generated correctly.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 264,
          "total_tokens": 411
        },
        "why_needed": "Prevents a potential issue where the test does not produce an HTML report even if the function `test_simple` passes."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 69,
          "line_ranges": "78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-335, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.14468240200000082,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert True is called for each label in the labels list",
          "assert int(match.group(1)) == expected for match in re.findall(card_pattern, html)",
          "assert int(match.group(1)) == expected for match in re.findall(fallback_pattern, html)",
          "assert_summary(['Total Tests', 'Total'], 6) is not called",
          "assert_summary(['Passed'], 1) is called and asserts True",
          "assert_summary(['Failed'], 1) is called and asserts False",
          "assert_summary(['Skipped'], 1) is called and asserts True",
          "assert_summary(['XFailed'], 1) is called and asserts False",
          "assert_summary(['XPassed'], 1) is called and asserts True",
          "assert_summary(['Errors', 'Error'], 1) is called and asserts True"
        ],
        "scenario": "test_html_summary_counts_all_statuses verifies that HTML summary counts include all statuses.",
        "token_usage": {
          "completion_tokens": 242,
          "prompt_tokens": 621,
          "total_tokens": 863
        },
        "why_needed": "This test prevents regression where the HTML summary counts do not include all statuses, such as when there are multiple failed tests or skipped tests."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 55,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.07311377799999264,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `schema_version` key in the report data should be set to '1.0'.",
          "The `summary` object should contain exactly two keys: `total` and `passed`.",
          "The `summary` object should have a single key-value pair for `failed`, with a value of 1.",
          "The number of tests passed should match the total count in the report.",
          "The number of failed tests should match the failed count in the report.",
          "The JSON data should be well-formed and contain only valid JSON syntax."
        ],
        "scenario": "The JSON report is created and contains the expected schema version, summary statistics, and test counts.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 295,
          "total_tokens": 475
        },
        "why_needed": "This test prevents a regression where the report generation process fails to create a valid JSON file with the required metadata."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 55,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 43,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 103,
          "line_ranges": "130-133, 135-137, 139, 141, 143, 190, 194-199, 201, 203, 205, 207, 210, 212-214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419-437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-506, 509, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.0657591650000029,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_pass()` passes without any errors.",
          "The `pytester.makepyfile` call returns a valid test file.",
          "The `pytester.makeconftest` call creates a conftest that patches litellm.completion before it's imported.",
          "The `pytester.makefile` call creates a pyproject.toml configuration with the [tool.pytest_llm_report] setting.",
          "The `pytester.makepyfile` call returns a valid test file after the patch is applied.",
          "The `pytester.makeconftest` call sets up the conftest to use the patched completion function.",
          "The `pytester.makefile` call creates a pyproject.toml configuration with the [tool.pytest_llm_report] setting.",
          "The `pytester.makepyfile` call returns a valid test file after the patch is applied and the config is set up."
        ],
        "scenario": "Verify that LLM annotations are included in the report when a provider is enabled.",
        "token_usage": {
          "completion_tokens": 246,
          "prompt_tokens": 385,
          "total_tokens": 631
        },
        "why_needed": "Prevent regressions by ensuring LLM annotations are present in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 12,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 100,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 44,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-507, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 111,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.10510564299997327,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_llm_error_is_reported` should raise an error when executed with the given input.",
          "The error message should include the string 'boom'.",
          "The HTML output of the test should contain a table with the error details.",
          "The table columns should be labeled correctly (e.g., 'Error Type', 'Error Message').",
          "The error type column should display the correct value ('LLM Error').",
          "The error message column should display the correct text ('boom').",
          "The test output should include a descriptive title indicating that an LLM error occurred.",
          "The test output should contain a link to the LLM error report (if available)."
        ],
        "scenario": "Test that LLM errors are surfaced in HTML output.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 313,
          "total_tokens": 511
        },
        "why_needed": "This test prevents a regression where LLM errors might not be reported in the expected format."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06333753399999864,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' marker should be present in the test report.",
          "The 'llm_opt_out' marker should be set to True for this test.",
          "There should be only one test associated with the 'llm_opt_out' marker in the report.",
          "The 'llm_opt_out' marker should not be False for this test."
        ],
        "scenario": "Verify that the LLM opt-out marker is correctly recorded in the test report.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 290,
          "total_tokens": 430
        },
        "why_needed": "This test prevents regression where a test might not record the LLM opt-out marker due to a missing or incorrect configuration."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06198474800001463,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest.mark.requirement` decorator should be applied to the `test_with_req` function with both 'REQ-001' and 'REQ-002' requirements.",
          "The `reqs` list in the test data should contain both 'REQ-001' and 'REQ-002'.",
          "The requirement string 'REQ-001' should be present in the `reqs` list of the first test in the report."
        ],
        "scenario": "Test the requirement marker functionality.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 307,
          "total_tokens": 452
        },
        "why_needed": "This test prevents a potential bug where the requirement marker is not recorded correctly, potentially leading to incorrect reporting or analysis of tests."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06880736799999454,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of xfailed tests is correctly reported as 2.",
          "All xfailed tests are included in the report.",
          "Each xfailed test has an outcome of 'xfailed'.",
          "No other outcomes are present in the report.",
          "The report path contains a file named 'report.json' with the correct format.",
          "The JSON data is correctly parsed and loaded into memory."
        ],
        "scenario": "Test that multiple xfailed tests are recorded in the report.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 317,
          "total_tokens": 453
        },
        "why_needed": "This test prevents regression by ensuring that all xfailed tests are properly reported and counted."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 43,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06504906999998639,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `data['summary']['skipped']` should be equal to 1 when the 'skip' marker is applied.",
          "The 'summary' key should exist in the report data and its value should be an integer.",
          "The 'skipped' key should contain a single integer value, which represents the number of skipped tests.",
          "The 'summary' dictionary should have a 'skipped' key with a string value equal to 1.",
          "The 'report.json' file should contain a 'summary' section with a 'skipped' key and its corresponding value.",
          "The 'json.loads()' method should successfully parse the report data from the 'report.json' file."
        ],
        "scenario": "Test that skipping tests prevents the 'skip' marker from being recorded.",
        "token_usage": {
          "completion_tokens": 206,
          "prompt_tokens": 264,
          "total_tokens": 470
        },
        "why_needed": "This test prevents regression in case of a skipped test, ensuring that the expected number of skipped tests is reported correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.0681311639999933,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of xfailed tests is exactly 1 as recorded in the report.json file.",
          "The value of `xfailed` in the `summary` dictionary matches the actual count of xfailed tests.",
          "The test data contains a single failed test with the label 'xfailed'.",
          "The report.json file includes an entry for the xfailed test, indicating its status.",
          "The number of xfailed tests is not affected by changes to the test code or environment.",
          "The test does not fail when run without pytester or any other dependencies.",
          "The test can be run multiple times with different inputs and still produce the same report.json file."
        ],
        "scenario": "Test that xfailed tests are recorded and reported correctly.",
        "token_usage": {
          "completion_tokens": 192,
          "prompt_tokens": 264,
          "total_tokens": 456
        },
        "why_needed": "This test prevents regression in the reporting of failed tests, ensuring accurate tracking of xfailed tests."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203-205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06556057399998849,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'pytester.makepyfile' function creates a pytest file with parameterized tests.",
          "The 'pytester.runpytest' function runs the tests and reports their results in a JSON format.",
          "The 'json.loads' function parses the report JSON and verifies that it contains the correct information.",
          "The test asserts that the total number of tests is 3 (since there are three input values) and that all tests pass (since all assertions pass).",
          "The test also asserts that the passed count matches the expected value (three passes)."
        ],
        "scenario": "Test Parametrized Tests: Verify that parameterized tests are recorded separately and their results are reported correctly.",
        "token_usage": {
          "completion_tokens": 192,
          "prompt_tokens": 290,
          "total_tokens": 482
        },
        "why_needed": "This test prevents regression by ensuring that the same test is run multiple times with different inputs, which can lead to false negatives if the test is not properly configured."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.05470381800000723,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_name': 'CLI help text includes usage examples', 'expected_result': 'The CLI help text should include usage examples for the plugin registration feature.', 'actual_result': 'True'}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 123,
          "total_tokens": 229
        },
        "why_needed": "This test is necessary to ensure that the CLI help text includes usage examples for the plugin registration feature."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.04923075199999971,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Markers are registered', 'description': 'LLM markers should be present in the test suite.', 'expected_result': 'True'}",
          "{'name': 'Markers match expected lines', 'description': 'The output of `pytest --markers` should contain the expected marker lines.', 'expected_result': 'True'}"
        ],
        "scenario": "TestPluginRegistration",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 142,
          "total_tokens": 257
        },
        "why_needed": "To verify that LLM markers are registered."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.05566283599998201,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': '--llm-report*', 'expected_value': ''}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 118,
          "total_tokens": 195
        },
        "why_needed": "To verify that the plugin is registered correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.1065505910000013,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The presence of special characters in the nodeid does not cause Pytest to crash.",
          "The HTML report generated by Pytest is valid and contains the '<html' tag.",
          "The 'report.html' file exists after running the test.",
          "The content of the 'report.html' file includes the '<html' tag.",
          "The presence of special characters in nodeids does not prevent Pytest from generating a report with a valid HTML structure."
        ],
        "scenario": "Verify that special characters in nodeid do not cause Pytest to crash or produce invalid HTML reports.",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 288,
          "total_tokens": 450
        },
        "why_needed": "This test prevents a potential regression where special characters in nodeids might cause Pytest to fail or produce corrupted report files."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007964669999864782,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '1m 0.0s', 'actual_value': '1m 0.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 106,
          "total_tokens": 190
        },
        "why_needed": "To ensure the `format_duration` function correctly formats durations in one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007831819999921663,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The result contains '\u03bcs' (microseconds) assertion\", 'expected': '500\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 121,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-millisecond durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0008224859999756973,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"result contains 'ms'\", 'expected': '500.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 119,
          "total_tokens": 197
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-second durations as milliseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0008180270000082146,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The result should contain 'm' (minutes) in its string representation.\", 'expected_value': '1m'}",
          "{'description': 'The result should be equal to the expected value.', 'expected_value': '1m 30.5s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_minutes_format",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 124,
          "total_tokens": 246
        },
        "why_needed": "To ensure the `format_duration` function correctly formats durations over a minute, including minutes and seconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_minutes_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.00077824300001339,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected': '3m 5.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 112,
          "total_tokens": 190
        },
        "why_needed": "To ensure the `format_duration` function correctly formats multiple minutes into a human-readable string."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007964069999957246,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The result of calling format_duration(1.0) should be equal to '1.00s'.\", 'expected_value': '1.00s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_one_second",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 101,
          "total_tokens": 203
        },
        "why_needed": "To ensure the `format_duration` function correctly formats a duration of exactly one second as '1.00s'."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_one_second",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007951750000074753,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '5.50s', 'actual': '5.50s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_seconds_format",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 110,
          "total_tokens": 186
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats seconds under a minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_seconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0008319730000039272,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': '1.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 111,
          "total_tokens": 185
        },
        "why_needed": "To ensure the `format_duration` function correctly formats small millisecond durations."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007802060000017264,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'value': '1\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 116,
          "total_tokens": 193
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats very small durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0008561789999816938,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected output', 'value': '2024-01-15T10:30:45+00:00'}"
        ],
        "scenario": "Test ISO Format with UTC",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 143,
          "total_tokens": 218
        },
        "why_needed": "To verify the correct formatting of datetime objects with UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0008456390000048941,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'value': '2024-06-20T14:00:00'}"
        ],
        "scenario": "Tests for ISO Format",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 136,
          "total_tokens": 205
        },
        "why_needed": "To ensure the naive datetime format is correct without timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0008315330000243648,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected value in result', 'value': '123456'}"
        ],
        "scenario": "Test IsoFormat with microseconds",
        "token_usage": {
          "completion_tokens": 62,
          "prompt_tokens": 133,
          "total_tokens": 195
        },
        "why_needed": "To test the functionality of formatting datetime objects with microseconds."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_with_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0008854940000162514,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result.tzinfo is not None', 'description': 'The test asserts that the result of `utc_now()` has a timezone info.', 'expected_result': 'True'}",
          "{'name': 'assert result.tzinfo == UTC', 'description': 'The test asserts that the result of `utc_now()` has a valid UTC timezone.', 'expected_result': 'UTC'}"
        ],
        "scenario": "Test that the current time has a valid UTC timezone.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 109,
          "total_tokens": 256
        },
        "why_needed": "To ensure that the `datetime` object returned by `utc_now()` has a valid timezone."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0008959729999844512,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'before <= result <= after', 'description': 'The before and after times should be within a reasonable tolerance of each other.'}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_is_current_time",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 116,
          "total_tokens": 210
        },
        "why_needed": "To ensure that the `utc_now` function returns a time within a reasonable tolerance of the current UTC time."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_is_current_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0009571870000115723,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Type of result', 'expected': 'datetime', 'actual': 'isinstance(result, datetime)'}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_returns_datetime",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 94,
          "total_tokens": 176
        },
        "why_needed": "This test ensures that the function `utc_now()` returns a datetime object."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_returns_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008923059999972338,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` object should return a `TokenRefreshError` with the message 'Authentication failed'.",
          "The string 'exit 1' should be present in the error message returned by `get_token()`.",
          "The string 'Authentication failed' should be present in the error message returned by `get_token()`."
        ],
        "scenario": "When TokenRefresher raises an error on command failure, then the test verifies that it correctly returns a TokenRefreshError with appropriate error message.",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 310,
          "total_tokens": 471
        },
        "why_needed": "This test prevents potential regression where the TokenRefresher might not raise an error when encountering a command failure, potentially causing unexpected behavior or errors later in the application."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_command_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-109, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008658069999967211,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method of the `TokenRefresher` class should raise a `TokenRefreshError` exception with the message 'empty output'.",
          "The `stdout` and `stderr` attributes of the `get_token()` method should be empty strings.",
          "The `returncode` attribute of the `get_token()` method should be 0 (indicating successful execution)."
        ],
        "scenario": "Verify TokenRefresher raises error on empty output when no token is available.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 297,
          "total_tokens": 441
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not raise an error when there is no token to refresh."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_empty_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009351060000142297,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` class returns the expected token value for both `token1` and `token2` after force refresh.",
          "The number of calls to the `run` method of the subprocess is equal to 2, which matches the expected behavior when forcing a refresh.",
          "Both `token1` and `token2` have different values than before the force refresh.",
          "The output of the subprocess contains the token value with the correct index (e.g. 'token-1' for `token1` and 'token-2' for `token2`).",
          "The error message from the subprocess is empty.",
          "The return code of the subprocess is 0, indicating successful execution.",
          "The output format of the subprocess is set to 'text'."
        ],
        "scenario": "Test that forcing a refresh bypasses the cache and returns a new token.",
        "token_usage": {
          "completion_tokens": 230,
          "prompt_tokens": 346,
          "total_tokens": 576
        },
        "why_needed": "This test prevents a regression where the TokenRefresher does not return a new token when forced to refresh."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000881294999999227,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method returns the expected custom JSON key value.",
          "The `access_token` attribute of the returned object matches the custom key value.",
          "The `json_key` parameter passed to the `TokenRefresher` constructor is used correctly."
        ],
        "scenario": "Test that the `TokenRefresher` uses a custom JSON key for token refresh.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 303,
          "total_tokens": 418
        },
        "why_needed": "This test prevents a potential issue where the default JSON key is used instead of a custom one."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_custom_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008979860000124518,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get-token` command should be a JSON object with the following structure: `{'token': 'json-token-value', 'expires_in': 3600}`.",
          "The extracted token should match the expected value: `"
        ],
        "scenario": "The test verifies that the `TokenRefresher` extracts a JSON object containing the extracted token from the expected JSON output.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 308,
          "total_tokens": 439
        },
        "why_needed": "This test prevents a potential bug where the `TokenRefresher` does not extract the token correctly if the output format is set to 'json'."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008851929999877939,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The extracted token matches the provided input string 'my-secret-token'.",
          "The output of `subprocess.run` contains the correct text after processing.",
          "The error message from `subprocess.run` does not indicate an issue with the token extraction process."
        ],
        "scenario": "The test verifies that the `TokenRefresher` class correctly extracts a token from text output when the `output_format` is set to 'text'.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 298,
          "total_tokens": 433
        },
        "why_needed": "This test prevents a potential bug where the extracted token is not in the expected format, potentially leading to incorrect usage or unexpected behavior."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_text_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-134, 149-150"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009125139999923704,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method of the `TokenRefresher` instance should raise a `TokenRefreshError` exception when it encounters invalid JSON.",
          "The error message returned by the `get_token()` method should indicate that the input is not valid JSON.",
          "The test should fail when running the `get_token()` method on an invalid JSON string.",
          "The test should pass when running the `get_token()` method on a valid JSON string.",
          "The error message returned by the `get_token()` method should be in lowercase (e.g. 'json' instead of 'JSON'),",
          "The test should only fail when running the `get_token()` method on an invalid JSON string, not on other types of exceptions.",
          "The test should pass for all other valid input strings."
        ],
        "scenario": "The test verifies that the TokenRefresher raises an error when it encounters invalid JSON.",
        "token_usage": {
          "completion_tokens": 234,
          "prompt_tokens": 299,
          "total_tokens": 533
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher incorrectly interprets valid JSON as a token refresh request."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009052199999928234,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `invalidate()` of the `TokenRefresher` class should clear the cache by setting the call_count to 1.",
          "The function `invalidate()` of the `TokenRefresher` class should update the token count by setting it to 2 after calling `get_token()`.",
          "The function `invalidate()` of the `TokenRefresher` class should not have any side effects on the token count.",
          "The function `invalidate()` of the `TokenRefresher` class should clear the cache even if an exception is raised during execution.",
          "The function `invalidate()` of the `TokenRefresher` class should update the token count after calling `get_token()` regardless of whether an exception is raised or not."
        ],
        "scenario": "Test TokenRefresher.invalidate() clears cache and updates the token count correctly.",
        "token_usage": {
          "completion_tokens": 211,
          "prompt_tokens": 340,
          "total_tokens": 551
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not update the token count after calling invalidate()."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalidate",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139-141, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011285070000042197,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `TokenRefreshError` exception should be raised with a message indicating that the token was not found.",
          "The error message should include the word 'not found'.",
          "The error message should be case-insensitive (i.e., it should match 'Not Found' regardless of the original casing)."
        ],
        "scenario": "Test that TokenRefresher raises an error when the JSON key is missing.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 325,
          "total_tokens": 450
        },
        "why_needed": "To prevent a potential bug where the TokenRefresher fails to refresh tokens due to a missing required JSON key."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_missing_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.05192240200000242,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should return the same token for each thread that acquires the lock.",
          "Each thread should acquire the lock before executing the `get_token()` method and retrieve the same token.",
          "If multiple threads start retrieving tokens concurrently, they should all return the same token."
        ],
        "scenario": "Test TokenRefresher thread safety by starting multiple threads concurrently and verifying that they all retrieve the same token.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 427,
          "total_tokens": 561
        },
        "why_needed": "This test prevents a potential bug where multiple threads accessing the TokenRefresher instance simultaneously could result in inconsistent or incorrect results due to race conditions."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_thread_safety",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 16,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009613450000074408,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method raises a `TokenRefreshError` with the message 'timed out' after a timeout of 30 seconds.",
          "The `get_token()` method does not raise an exception or log an error when the token refresh times out."
        ],
        "scenario": "The test verifies that the TokenRefresher handles command timeouts correctly.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 279,
          "total_tokens": 390
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher fails to refresh tokens when they timeout."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_timeout_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009009519999949589,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` returns the expected token value.",
          "The output of `refresher.get_token()` is the same as the input, indicating caching.",
          "The number of calls to `refresher.get_token()` is correct (1 in this case).",
          "The cached token has not changed even after multiple requests.",
          "The command 'get-token' is called only once with a valid token.",
          "The output format is set to 'text'."
        ],
        "scenario": "Test that TokenRefresher caches tokens and doesn't call the command again.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 353,
          "total_tokens": 512
        },
        "why_needed": "Prevents a potential bug where multiple requests to get a token would result in the same cached token being returned."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_token_caching",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008670489999929032,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method of the `TokenRefresher` instance raises a `TokenRefreshError` with an exit code of 1.",
          "The `stderr` attribute of the `get_token()` method is set to an empty string, indicating no error output.",
          "The `stdout` attribute of the `get_token()` method is also set to an empty string, indicating no error output.",
          "When the command fails without producing any error output, the `TokenRefresher` instance raises a `TokenRefreshError` with an exit code of 1.",
          "The `exit_code` attribute of the exception raised by the `get_token()` method is set to 1.",
          "The `stderr` and `stdout` attributes are not changed in the case where the command fails without producing any error output.",
          "In this scenario, the `TokenRefresher` instance correctly raises a `TokenRefreshError` with an exit code of 1."
        ],
        "scenario": "Test the TokenRefresher edge case when command fails with no stderr output.",
        "token_usage": {
          "completion_tokens": 261,
          "prompt_tokens": 322,
          "total_tokens": 583
        },
        "why_needed": "To prevent a regression where the TokenRefresher does not raise an exception when the command fails without producing any error output."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_command_failure_no_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008861849999846072,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected exception type', 'value': 'TokenRefreshError'}",
          "{'name': 'Expected error message', 'value': 'empty'}"
        ],
        "scenario": "Token Refresh Coverage",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 151,
          "total_tokens": 228
        },
        "why_needed": "Test case for handling empty command string."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_empty_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-88, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009176940000088507,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `TokenRefresher` instance is created with an invalid command string that contains unescaped quotes, which causes a shlex parse error.",
          "When calling `get_token()` on the `TokenRefresher` instance with an invalid command string, it raises a `TokenRefreshError` exception containing the message 'Invalid command string'.",
          "The `TokenRefresher` class correctly handles the case where the input command string contains unescaped quotes, preventing the shlex parse error and ensuring the correct behavior.",
          "The test verifies that the `TokenRefresher` instance is created with an invalid command string that causes a `TokenRefreshError` exception to be raised when calling `get_token()`."
        ],
        "scenario": "Test the test_invalid_command_string function to verify it handles an invalid command string (shlex parse error).",
        "token_usage": {
          "completion_tokens": 213,
          "prompt_tokens": 251,
          "total_tokens": 464
        },
        "why_needed": "Prevents a potential bug where the TokenRefresher class incorrectly raises a TokenRefreshError when given an invalid command string."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_invalid_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 27,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-137, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000915749999990112,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method should raise a `TokenRefreshError` when receiving a non-dict JSON output.",
          "The error message should contain 'Expected JSON object'.",
          "The error message should contain 'list'."
        ],
        "scenario": "Test that the test_json_not_dict scenario verifies a bug preventing handling of non-dict JSON output.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 328,
          "total_tokens": 438
        },
        "why_needed": "This test prevents regression by ensuring the TokenRefresher handles non-dict JSON output correctly."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_not_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009133650000023863,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'token' key in the JSON output should be a non-empty string.",
          "The 'token' key in the JSON output should not contain any whitespace characters.",
          "The error message should indicate that the token value is empty or not a string.",
          "The test should raise a TokenRefreshError with an appropriate error message when encountering an empty or non-string token value."
        ],
        "scenario": "Test handling when token value is an empty string.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 324,
          "total_tokens": 464
        },
        "why_needed": "Prevents TestTokenRefresherEdgeCases::test_json_token_empty_string from failing due to unexpected behavior of the TokenRefresher class."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010949949999883302,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `json.dumps` function is called with an integer value instead of a string.",
          "The `returncode` of the subprocess call is set to 0 (success), but the error message does not indicate that the token was invalid.",
          "The `stderr` parameter is empty, which may indicate that no error occurred or that the token is valid.",
          "The `json_key` parameter is set to `"
        ],
        "scenario": "Test handling when token value is not a string.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 326,
          "total_tokens": 473
        },
        "why_needed": "Prevents the TokenRefresher from attempting to refresh a non-string token value, which could lead to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_not_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 19,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113, 115-118"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009102000000211774,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` raises a `TokenRefreshError` with the message 'Failed to execute' when executed on a nonexistent command.",
          "The error message includes the string 'Command not found'.",
          "The function does not raise an exception or return an error code when executing a nonexistent command."
        ],
        "scenario": "Test verifies that a TokenRefresher handles an OSError when executing a command.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 280,
          "total_tokens": 402
        },
        "why_needed": "This test prevents the regression of TokenRefreshError not being raised when executing commands that are not found."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_oserror_on_execution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 4,
          "line_ranges": "132, 153-155"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007848959999989802,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'No non-empty lines' in str(exc_info.value)",
          "assert 'Only whitespace lines' in str(exc_info.value)",
          "assert 'Blank line' in str(exc_info.value)",
          "assert 'Only whitespace content lines' in str(exc_info.value)",
          "assert 'Non-empty wrapper but only whitespace content lines' in str(exc_info.value)",
          "assert 'No non-whitespace output' in str(exc_info.value)",
          "assert 'Output is empty' in str(exc_info.value)"
        ],
        "scenario": "Test handling when text output has only whitespace lines after initial strip, specifically when parsing with only blank lines.",
        "token_usage": {
          "completion_tokens": 174,
          "prompt_tokens": 376,
          "total_tokens": 550
        },
        "why_needed": "Prevents a potential bug where the TokenRefresher fails to handle text output with only whitespace lines after an initial strip."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_text_only_whitespace_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008698339999853033,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` object should raise a `TokenRefreshError` exception.",
          "The error message returned by `get_token()` should contain the word 'empty'.",
          "The assertion should fail when the input command is an empty string (i.e., no whitespace characters)."
        ],
        "scenario": "Test the test_whitespace_only_command to ensure it correctly raises a TokenRefreshError for an empty whitespace-only command string.",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 236,
          "total_tokens": 377
        },
        "why_needed": "Prevents a potential bug where the TokenRefresher is not raised with a meaningful error message when given an empty whitespace-only command string."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_whitespace_only_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 73,
          "line_ranges": "399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-487, 491-494, 497, 499, 502-506, 509, 512-514, 516-521, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.005177005999996709,
      "file_path": "tests/test_token_usage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total input tokens should be 30 (10 + 20).",
          "The total output tokens should be 15 (5 + 10).",
          "The number of annotations should be 2.",
          "The total tokens should be 45 (15 + 30)."
        ],
        "scenario": "Test token usage aggregation with mock stash and terminal reporter.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 775,
          "total_tokens": 882
        },
        "why_needed": "Prevents regression in token usage reporting for different test cases."
      },
      "nodeid": "tests/test_token_usage.py::test_token_usage_aggregation",
      "outcome": "passed",
      "phase": "call"
    }
  ]
}