{
  "run_meta": {
    "aggregation_policy": null,
    "collect_only": false,
    "collected_count": 387,
    "deselected_count": 0,
    "duration": 34.374879,
    "end_time": "2026-01-13T06:57:28.642229+00:00",
    "exit_code": 0,
    "git_dirty": false,
    "git_sha": "b9e3e95cf545147fb1baae5e547e056f45911b0a",
    "interrupted": false,
    "is_aggregated": true,
    "llm_annotations_count": 383,
    "llm_annotations_enabled": true,
    "llm_annotations_errors": 3,
    "llm_context_mode": "minimal",
    "llm_model": "llama3.2:1b",
    "llm_provider": "ollama",
    "platform": "Linux-6.11.0-1018-azure-x86_64-with-glibc2.39",
    "plugin_git_dirty": true,
    "plugin_git_sha": "2f498263985a34902252c53c11fb820445bd8f21",
    "plugin_version": "0.1.0",
    "pytest_version": "9.0.2",
    "python_version": "3.12.12",
    "repo_git_dirty": false,
    "repo_git_sha": "b9e3e95cf545147fb1baae5e547e056f45911b0a",
    "repo_version": "0.1.0",
    "rerun_count": 0,
    "run_count": 1,
    "run_id": "20947644922-py3.12",
    "selected_count": 387,
    "source_reports": [],
    "start_time": "2026-01-13T06:56:54.267350+00:00"
  },
  "schema_version": "1.0.0",
  "sha256": "d3807589a52fea8997a6d9c2d9184313ba02006312512e1165395717ce50c7d1",
  "source_coverage": [
    {
      "coverage_percent": 100.0,
      "covered": 2,
      "covered_ranges": "2-3",
      "file_path": "src/pytest_llm_report/_git_info.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 2
    },
    {
      "coverage_percent": 95.69,
      "covered": 111,
      "covered_ranges": "13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 146, 148, 162, 164, 168, 170, 172, 182, 184-188, 190-191, 194, 196, 205, 217, 219-233, 235, 237, 245-246, 248-249, 251, 253-255, 259, 262-263, 265-266, 269-271, 273, 275-276, 280",
      "file_path": "src/pytest_llm_report/aggregation.py",
      "missed": 5,
      "missed_ranges": "66, 90-91, 192, 203",
      "statements": 116
    },
    {
      "coverage_percent": 93.62,
      "covered": 44,
      "covered_ranges": "13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153",
      "file_path": "src/pytest_llm_report/cache.py",
      "missed": 3,
      "missed_ranges": "64-65, 130",
      "statements": 47
    },
    {
      "coverage_percent": 98.2,
      "covered": 109,
      "covered_ranges": "19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285",
      "file_path": "src/pytest_llm_report/collector.py",
      "missed": 2,
      "missed_ranges": "141, 239",
      "statements": 111
    },
    {
      "coverage_percent": 92.59,
      "covered": 125,
      "covered_ranges": "13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308",
      "file_path": "src/pytest_llm_report/coverage_map.py",
      "missed": 10,
      "missed_ranges": "62, 123, 125, 128, 157, 221, 249, 251, 257, 274",
      "statements": 135
    },
    {
      "coverage_percent": 100.0,
      "covered": 35,
      "covered_ranges": "8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139",
      "file_path": "src/pytest_llm_report/errors.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 35
    },
    {
      "coverage_percent": 100.0,
      "covered": 3,
      "covered_ranges": "4-5, 7",
      "file_path": "src/pytest_llm_report/llm/__init__.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 3
    },
    {
      "coverage_percent": 100.0,
      "covered": 110,
      "covered_ranges": "4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266",
      "file_path": "src/pytest_llm_report/llm/annotator.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 110
    },
    {
      "coverage_percent": 100.0,
      "covered": 78,
      "covered_ranges": "13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267",
      "file_path": "src/pytest_llm_report/llm/base.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 78
    },
    {
      "coverage_percent": 93.45,
      "covered": 257,
      "covered_ranges": "7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-227, 233-234, 238-239, 242-243, 245-248, 252-253, 260, 266-267, 269, 273-277, 279-283, 286-287, 292-293, 300-301, 303, 315, 317-318, 322, 327, 330-332, 335-343, 345-346, 348, 352-355, 357, 360-366, 368-374, 380-382, 384-387, 389, 391-392, 396-402, 405, 408-410, 412-414, 416-421, 427-428, 430-434, 437-440, 442-443, 445-447",
      "file_path": "src/pytest_llm_report/llm/gemini.py",
      "missed": 18,
      "missed_ranges": "89, 104, 106, 115-117, 199, 230-231, 235-237, 244, 250, 256, 367, 441, 444",
      "statements": 275
    },
    {
      "coverage_percent": 96.88,
      "covered": 31,
      "covered_ranges": "7, 9, 11-12, 18, 21, 37-38, 44, 46, 49, 51-52, 54-56, 66-67, 69-70, 73, 76, 78-79, 81-82, 84, 88, 94-95, 97",
      "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
      "missed": 1,
      "missed_ranges": "74",
      "statements": 32
    },
    {
      "coverage_percent": 100.0,
      "covered": 13,
      "covered_ranges": "8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66",
      "file_path": "src/pytest_llm_report/llm/noop.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 13
    },
    {
      "coverage_percent": 97.67,
      "covered": 42,
      "covered_ranges": "7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67, 71-72, 74-75, 77, 81, 87-88, 90-92, 96, 102, 104, 114, 116-117, 127, 132, 134-135",
      "file_path": "src/pytest_llm_report/llm/ollama.py",
      "missed": 1,
      "missed_ranges": "69",
      "statements": 43
    },
    {
      "coverage_percent": 97.22,
      "covered": 35,
      "covered_ranges": "8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130",
      "file_path": "src/pytest_llm_report/llm/schemas.py",
      "missed": 1,
      "missed_ranges": "39",
      "statements": 36
    },
    {
      "coverage_percent": 95.83,
      "covered": 230,
      "covered_ranges": "17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159, 161, 167-171, 173-182, 184, 186, 188-190, 193-194, 202-203, 205, 207, 213-214, 223-225, 227, 229, 233-235, 238-239, 248-250, 252, 254, 261-262, 271-273, 275, 277, 281-283, 286-287, 324-353, 355-360, 362, 364, 382-405, 407-419, 422-423, 437-445, 447, 449, 459, 461, 464-465, 482-492, 494, 500, 502, 508-512, 514, 516, 518, 520, 522",
      "file_path": "src/pytest_llm_report/models.py",
      "missed": 10,
      "missed_ranges": "172, 183, 185, 187, 460, 513, 515, 517, 519, 521",
      "statements": 240
    },
    {
      "coverage_percent": 61.54,
      "covered": 72,
      "covered_ranges": "106, 146, 175, 178-180, 185-187, 193-195, 201-203, 209-218, 220, 224, 233, 248, 251-267, 270-283, 286-295, 298, 300",
      "file_path": "src/pytest_llm_report/options.py",
      "missed": 45,
      "missed_ranges": "13-15, 21-22, 90-94, 97-99, 102-105, 122-123, 126-132, 135-137, 140-142, 145, 156-160, 163-164, 167, 169, 222, 227, 236",
      "statements": 117
    },
    {
      "coverage_percent": 83.97,
      "covered": 131,
      "covered_ranges": "40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 169, 173, 177, 183-184, 187-188, 190, 192, 195-197, 203-204, 212-213, 238-239, 242-243, 246, 249-250, 252-253, 256-257, 259, 261-265, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-314, 317-318, 322-323, 331-332, 337-340, 343, 345, 348-353, 355, 357, 365-366, 387-388, 391-392, 395-397, 408-409, 412, 415-416, 419-421, 431-432, 435-437, 448-449, 452, 455, 457-458",
      "file_path": "src/pytest_llm_report/plugin.py",
      "missed": 25,
      "missed_ranges": "13, 15-17, 19-20, 22, 28-31, 34, 160, 216, 319, 327-328, 333-334, 379-380, 400, 424, 440-441",
      "statements": 156
    },
    {
      "coverage_percent": 93.33,
      "covered": 70,
      "covered_ranges": "13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194",
      "file_path": "src/pytest_llm_report/prompts.py",
      "missed": 5,
      "missed_ranges": "80, 114, 142, 146, 149",
      "statements": 75
    },
    {
      "coverage_percent": 100.0,
      "covered": 50,
      "covered_ranges": "13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196",
      "file_path": "src/pytest_llm_report/render.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 50
    },
    {
      "coverage_percent": 94.01,
      "covered": 157,
      "covered_ranges": "13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516",
      "file_path": "src/pytest_llm_report/report_writer.py",
      "missed": 10,
      "missed_ranges": "113, 135-137, 424-425, 432, 449-451",
      "statements": 167
    },
    {
      "coverage_percent": 91.18,
      "covered": 31,
      "covered_ranges": "11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123",
      "file_path": "src/pytest_llm_report/util/fs.py",
      "missed": 3,
      "missed_ranges": "40, 65, 67",
      "statements": 34
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121",
      "file_path": "src/pytest_llm_report/util/hashing.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 33,
      "covered_ranges": "12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95",
      "file_path": "src/pytest_llm_report/util/ranges.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 16,
      "covered_ranges": "4, 6, 9, 15, 18, 27, 30, 39-44, 46-48",
      "file_path": "src/pytest_llm_report/util/time.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 16
    }
  ],
  "summary": {
    "coverage_total_percent": 92.91,
    "error": 0,
    "failed": 0,
    "passed": 387,
    "skipped": 0,
    "total": 387,
    "total_duration": 32.325240900999745,
    "xfailed": 0,
    "xpassed": 0
  },
  "tests": [
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 69,
          "line_ranges": "52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0019520370000236653,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregated report should have exactly two retained tests, regardless of the policy used.",
          "Each retained test should be from one of the original reports.",
          "The aggregate function should correctly filter out tests based on the aggregation policy.",
          "The aggregate function should not retain any tests that are explicitly excluded by the policy.",
          "The aggregate function should handle multiple policies correctly, allowing for different filtering rules to be applied.",
          "The test case should pass even if the policy is set to 'none' or 'partial'.",
          "The test case should fail if the policy is set to 'all' and one of the reports has a retained test that does not match the policy."
        ],
        "scenario": "Test that the aggregate function correctly handles all policy when aggregating multiple reports.",
        "why_needed": "This test prevents a regression where an aggregation of multiple reports with different policies would not retain all tests due to incorrect filtering."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 7,
          "line_ranges": "52, 55-57, 109-111"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.003595939999968323,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` method of the `aggregator` instance should return None when the provided directory does not exist.",
          "A `PathError` exception should not be raised if the directory does not exist.",
          "The aggregate function should not attempt to create or modify any files or directories within the non-existent directory."
        ],
        "scenario": "Verify that the aggregate function does not attempt to aggregate a non-existent directory.",
        "why_needed": "Prevents a potential error where an empty or nonexistent directory is attempted to be aggregated."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 77,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 146, 148-153, 155, 157-159, 170, 182, 184-188, 190-191, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.003202616000010039,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `outcome` of the aggregated report is set to 'passed' if there are multiple reports of the same test with different times.",
          "The number of tests in the aggregated report is 1.",
          "The outcome of the first test in the aggregated report is 'passed'.",
          "The `run_meta.run_count` attribute is equal to 2.",
          "The `summary.passed` attribute is equal to 1.",
          "The `summary.failed` attribute is equal to 0."
        ],
        "scenario": "Test that the `aggregate_latest_policy` function correctly selects the latest report when there are multiple reports of the same test with different times.",
        "why_needed": "This test prevents a regression where the `aggregate` function would incorrectly select the first report it encounters as the latest, instead of the most recent one."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 3,
          "line_ranges": "44, 52-53"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008368459999701372,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "agg.aggregate() should return None",
          "agg.aggregate() should not raise an exception",
          "mock_config.aggregate_dir should be set to None"
        ],
        "scenario": "The aggregator function should not throw an error when no directory configuration is provided.",
        "why_needed": "This test prevents a potential bug where the aggregator function throws an exception when no directory configuration is specified."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 9,
          "line_ranges": "52, 55-57, 109-110, 113-114, 170"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.001242403999981434,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregate function should return None for this scenario.",
          "The aggregate function should not attempt to access any files or reports.",
          "There should be no error raised if the file system is empty.",
          "The aggregate function should handle a case where there are no reports available without raising an exception.",
          "The aggregate function should preserve the original behavior when called with a non-empty list of reports.",
          "The aggregate function should not attempt to access any files or reports in this scenario.",
          "There should be no assertion error raised if the file system is empty.",
          "The aggregate function should handle a case where there are no files found without raising an exception."
        ],
        "scenario": "Test that aggregate function returns None when no reports exist and no files are found.",
        "why_needed": "Prevents regression where the aggregate function fails to return an empty list of reports when there are no files or reports available."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 81,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 146, 148-153, 155, 157-159, 170, 182, 184-188, 194, 217, 219-223, 235, 245, 248-249, 251, 253, 275-278, 280"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 32,
          "line_ranges": "40-43, 104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176-180, 182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.001973668000005091,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "coverage: Ensure coverage is correctly deserialized with the expected file paths and line ranges.",
          "LLM Annotation: Verify correct deserialization of LLM annotation with scenario, why-need, and key assertions.",
          "Re-serialization: Confirm that the aggregated report can be re-serialized with accurate coverage and LLM annotation."
        ],
        "scenario": "Test that coverage and LLM annotations are properly deserialized and can be re-serialized.",
        "why_needed": "Prevents regression in core functionality by ensuring accurate coverage and LLM annotation deserialization."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 66,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 148-155, 157-159, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.001582762999987608,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `source_coverage` attribute of the aggregated result is an instance of `SourceCoverageEntry`.",
          "The `file_path` attribute of the first `SourceCoverageEntry` in the aggregated result matches the expected file path.",
          "All statements in the source code are covered by at least 83.33% of the total coverage.",
          "At least 2 out of 10 statements were missed in the source code.",
          "The coverage percentage is between 1 and 5, inclusive for the first range and exclusive for the second range.",
          "The missing ranges are '6' and '12'.",
          "All covered statements are within the specified ranges."
        ],
        "scenario": "Test that the `aggregate` method correctly aggregates source coverage data from a temporary report.",
        "why_needed": "This test prevents regression where the aggregation of source coverage data fails to produce accurate results due to incomplete or missing reports."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 19,
          "line_ranges": "245-246, 248-249, 251, 253-257, 259, 262-263, 265-266, 269-271, 273"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.003047100000003411,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `_load_coverage_from_source` method returns `None` when `llm_coverage_source` is `None`.",
          "Verify that the `_load_coverage_from_source` method raises a UserWarning with the message 'Coverage source not found' when `llm_coverage_source` is '/nonexistent/coverage'.",
          "Verify that the `_load_coverage_from_source` method returns `None` when `llm_coverage_source` does not exist.",
          "Verify that the mock coverage object is created and returned by the mock cov_cls before calling it.",
          "Verify that the mock mapper object is created and returned by the mock mapper_cls before calling it.",
          "Verify that the `map_source_coverage` method of the mock mapper returns a list with one entry.",
          "Verify that the `load` method of the mock cov_cls is called once.",
          "Verify that the `map_source_coverage` method of the mock mapper has been called with the mock cov object."
        ],
        "scenario": "Test loading coverage from configured source file when option is not set.",
        "why_needed": "Prevents a potential bug where the test fails due to an unconfigured source file."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 17,
          "line_ranges": "217, 219-233, 235"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007796259999963695,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of tests passed, failed, skipped, xfailed, xpassed, and error should be equal to the original counts.",
          "The total duration of all tests should remain unchanged.",
          "The coverage percentage should match the expected value for each test type (total, passed, failed, skipped, xfailed, xpassed, and error)."
        ],
        "scenario": "Test that the _recalculate_summary method preserves the total duration of tests and maintains correct coverage percentages.",
        "why_needed": "This test prevents regression where the total duration is not preserved or the coverage percentage does not match the expected values after recalculating the summary."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_recalculate_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 71,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 148-153, 155, 157-159, 162, 164-166, 168, 170, 182, 184-186, 194, 217, 219-220, 235, 245, 248-249, 251, 253, 275-278, 280"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0029362989999981437,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` function should skip the 'invalid.json' file because it does not contain valid JSON data.",
          "The `aggregate` function should only count the valid report ('valid.json') in its run meta.",
          "When an invalid JSON file is encountered, a UserWarning is raised with a message indicating that the aggregation skipped the file.",
          "The test verifies that the `aggregate` function correctly handles missing fields in the aggregated report.",
          "The test ensures that only the valid report is included in the run meta of the aggregated result."
        ],
        "scenario": "Test case: Skipping an invalid JSON aggregation report",
        "why_needed": "Prevents a regression where a test fails due to an unexpected error when aggregating reports with non-JSON content."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_skips_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "44, 217, 219-225, 235"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007887880000225778,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "summary.total == 2",
          "summary.passed == 1",
          "summary.failed == 1",
          "summary.coverage_total_percent == 88.5",
          "summary.total_duration == 3.0"
        ],
        "scenario": "The test verifies that the aggregator recalculates the summary correctly when given a set of tests with varying coverage totals.",
        "why_needed": "This test prevents regression in the aggregator's behavior when handling different coverage scenarios, ensuring accurate summary calculations."
      },
      "nodeid": "tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0020691089999900214,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mock_provider` is not called with any arguments.",
          "The `mock_cache` is not called with any arguments.",
          "The `mock_assembler` is not called with any arguments.",
          "The `test_cached_tests_are_skipped` method does not call the `cached_test` function.",
          "The `cached_test` function is not called by the `mock_provider` or `mock_cache`.",
          "The `mock_assembler` does not call the `annotator` instance before calling its methods.",
          "The `mock_provider` and `mock_cache` do not have any side effects that would cause them to be called with arguments.",
          "The `test_cached_tests_are_skipped` method is not called by other test functions in the same scope."
        ],
        "scenario": "This test verifies that cached tests are skipped by the annotator.",
        "why_needed": "To prevent regression in the annotator's caching behavior and ensure it only skips cached tests."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 64,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0025659689999883994,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_provider.call_args.assert_called_once_with(self, 'annotation', mock_assembler)",
          "mock_cache.call_args.assert_called_once_with(self, 'annotation', mock_assembler)",
          "mock_assembler.call_args.assert_called_once_with(self, 'annotation')"
        ],
        "scenario": "The annotator function is called concurrently without proper synchronization.",
        "why_needed": "This test prevents a potential concurrency issue where multiple annotations are performed simultaneously, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.002295543999991878,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotator should not fail when encountering an error while processing annotations concurrently.",
          "The annotator should log the error and continue processing other annotations without interruption.",
          "The annotator's cache should be updated correctly even if it encounters errors during annotation.",
          "The annotator's progress bar should update correctly even if it encounters errors during annotation.",
          "The annotator's output should not be affected by concurrent failures.",
          "The annotator's error messages should contain relevant information about the failure.",
          "The annotator's logging behavior should be consistent across different environments."
        ],
        "scenario": "Test that concurrent annotation handles failures by verifying the annotator's behavior.",
        "why_needed": "This test prevents a potential regression where the annotator fails to handle failures in a concurrent environment."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0016218929999922693,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `progress` attribute of the annotated object should increase as the annotation progresses.",
          "The `total_progress` attribute of the annotator should correctly track the total number of annotations.",
          "The `annotation_id` attribute of each annotation should increment in a predictable manner.",
          "The `annotated_object` attribute of each annotation should be updated correctly after progress reporting.",
          "The `progress` and `total_progress` attributes of the annotator should be reset to 0 when no new annotations are added.",
          "The `annotation_id` attribute of the last annotated object should match its original value before the test started.",
          "The `annotated_object` attribute of the last annotated object should contain a valid annotation data."
        ],
        "scenario": "Verify that the progress reporting is correctly implemented in the annotate function.",
        "why_needed": "This test prevents regressions where the progress reporting might not be accurately tracked or reported."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 12.002441492000003,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mock_provider` is called with a valid `annotator` instance.",
          "The `mock_cache` is not called during the execution of the test.",
          "No exceptions are raised when calling `mock_assembler` on the `annotator` instance.",
          "The `annotator` instance is properly updated after each annotation.",
          "No unexpected behavior occurs when annotating multiple items in a row.",
          "The `mock_provider` and `mock_cache` instances are not reused across tests.",
          "The `mock_assembler` instance is only called once during the execution of the test."
        ],
        "scenario": "Verifies the sequential annotation functionality of the annotator.",
        "why_needed": "Prevents regression in sequential annotation by ensuring that the annotator properly handles multiple annotations in a row."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 2,
          "line_ranges": "45-46"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007698699999991732,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `annotate_tests` should not be called with an empty list of tests.",
          "The function `annotate_tests` should not call the `LLM` provider.",
          "The `LLM` provider should be set to 'none' before calling `annotate_tests`.",
          "If LLM is disabled, the annotator should do nothing and return without making any changes to the test results.",
          "The function `annotate_tests` should not modify the test results or report any changes.",
          "The function `annotate_tests` should only be called with a valid configuration object."
        ],
        "scenario": "Verify that the annotator skips tests when LLM is disabled.",
        "why_needed": "This test prevents a regression where LLM is disabled and annotators are still executed."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "45, 48-52, 54"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009034370000335912,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocked Provider Mock",
          "Provider Unavailable Error",
          "Test Skipping Due to Provider Unavailability",
          "Annotator Behavior When Provider Unavailable",
          "Error Handling for Provider Unavailability",
          "Test Case Prioritization Based on Provider Availability"
        ],
        "scenario": "The annotator skips tests when the provider is unavailable.",
        "why_needed": "To prevent skipping of critical tests due to a provider's unavailability."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 28,
          "line_ranges": "229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0022507520000090153,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should correctly report progress and first error in concurrent mode.",
          "The function should append 'Processing 2 test(s)' to the progress messages list.",
          "The function should include 'LLM annotation' in the progress messages list.",
          "The function should not fail to report any errors when annotating concurrently.",
          "The function should correctly handle scenarios where multiple tasks are annotated concurrently.",
          "The function should update the cache with the correct number of annotations.",
          "The function should return 2 annotations as expected when there are 2 concurrent tasks.",
          "The function should not raise an exception when annotating concurrently."
        ],
        "scenario": "Tests annotator concurrent with progress and errors",
        "why_needed": "Prevents a potential bug where the annotator fails to report progress or first error when annotating concurrently."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 23,
          "line_ranges": "165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0022335870000347313,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The time.sleep() function was called.",
          "The monotonic() function returned a value greater than 1.0s.",
          "The time.sleep() function was called again.",
          "The monotonic() function returned a value less than or equal to 1.0s.",
          "The time.sleep() function did not call itself.",
          "The monotonic() function returned the correct interval (1.0s) after the first sleep call."
        ],
        "scenario": "Should wait if rate limit interval has not elapsed.",
        "why_needed": "Prevents regression in sequential annotation tasks where the rate limit interval has not yet elapsed."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 37,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0019377799999915624,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `progress_msgs` list should contain any messages that indicate a cache hit (e.g. `(cache): test_cached`).",
          "The `progress_msgs` list should not be empty after running the test.",
          "Any message in the `progress_msgs` list should start with '(cache): ' to identify it as related to caching.",
          "The `progress_msgs` list should contain messages for all cached tests (not just one)."
        ],
        "scenario": "Should report progress for cached tests.",
        "why_needed": "Prevents regression where the annotator does not report progress for cached tests."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "45, 48-52, 54"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0014768089999961376,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mocks.get_provider().is_available() returns False",
          "annotate_tests(tests, config) will print 'not available. Skipping annotations'",
          "tests[0].outcome is not 'passed' after the annotation fails",
          "mock_provider.is_available() was called with a return value of False",
          "mock_provider.return_value is set to mock_provider"
        ],
        "scenario": "Test that the annotator fails to annotate tests when the provider is not available.",
        "why_needed": "This test prevents a regression where the annotator would incorrectly report tests as successful even though they are unavailable."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 220-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007526040000129797,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_parse_response(response)` will return `None` when the input is malformed JSON.",
          "The error message returned by `annotation.error` should be 'Failed to parse LLM response as JSON'.",
          "The function `provider._parse_response(response)` will raise a `JSONDecodeError` exception with the specified error message."
        ],
        "scenario": "The test verifies that the `test_base_parse_response_malformed_json_after_extract` function will fail when a malformed JSON is passed.",
        "why_needed": "This test prevents a potential bug where the function `test_base_parse_response_malformed_json_after_extract` fails due to an invalid JSON in the response, causing it to raise a `JSONDecodeError`."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008377800000403113,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an annotation with the correct scenario value.",
          "The function should return an annotation with the correct why_needed list value.",
          "The function should return an annotation with the correct key_assertion list value.",
          "The `scenario` attribute of the annotation should be set to '123'.",
          "The `why_needed` attribute of the annotation should be set to ['list'].",
          "The `key_assertions` attribute of the annotation should contain the string 'a'."
        ],
        "scenario": "Tests that the `test_base_parse_response_non_string_fields` function handles non-string fields in the response data correctly.",
        "why_needed": "This test prevents a potential bug where the function incorrectly assumes all fields are strings when they may not be."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "52-53, 245, 247, 249, 252, 257, 262-263, 265"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 7,
          "line_ranges": "134, 136-139, 141-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007072739999784972,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned `provider` object should be an instance of `GeminiProvider`.",
          "The `provider` attribute of the returned `provider` object should contain a valid `GeminiProvider` instance.",
          "The `provider` attribute of the returned `provider` object should have a value that is not `None` or `undefined`.",
          "The `provider` attribute of the returned `provider` object should be an instance of `GeminiProvider` with the correct class name.",
          "The `provider` attribute of the returned `provider` object should contain the correct attributes (e.g., `name`, `api_key`, etc.)."
        ],
        "scenario": "Verifies that the `get_gemini_provider` function returns a `GeminiProvider` instance.",
        "why_needed": "Prevents a potential bug where the test fails if the `gemini` provider is not properly configured or if there's an issue with the Gemini API."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "245, 247, 249, 252, 257, 262, 267"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.001841613999999936,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider` raises a ValueError with the message 'Unknown LLM provider: invalid'.",
          "The exception type is `ValueError`.",
          "The exception message contains the string 'Unknown LLM provider: invalid'."
        ],
        "scenario": "Verify that a ValueError is raised when an unknown LLM provider is specified.",
        "why_needed": "This test prevents the introduction of an Unknown LLM provider error in future code changes."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "52-53, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007074070000498978,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider(config)` returns an instance of `LiteLLMProvider`.",
          "The `provider` attribute of the returned `LiteLLMProvider` instance is set to `'litellm'`.",
          "An exception is not raised if the provider is not set to 'litellm'."
        ],
        "scenario": "Verifies that the `get_litellm_provider` function returns a LitELLMProvider instance.",
        "why_needed": "This test prevents a potential bug where the `get_litellm_provider` function does not return an instance of LiteLLMProvider if the provider is not set to 'litellm'."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 245, 247, 249-250"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007684770000082608,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned value should be an instance of `NoopProvider`.",
          "The `provider` attribute of the returned value should not be `None`.",
          "The type of the returned value should match the expected type, which is `NoopProvider`.",
          "The `get_provider` function should return a valid provider instance when no provider is specified.",
          "A new provider should not break the test if it's added without updating the test.",
          "The test should pass even after adding a new provider to the system."
        ],
        "scenario": "Verify that the `get_noop_provider` function returns a `NoopProvider` instance when no provider is specified.",
        "why_needed": "Prevents regression in case a new provider is added without updating the test."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 245, 247, 249, 252-253, 255"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000710994000030496,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider(config)` is called with the correct provider name 'ollama'.",
          "The returned value `provider` is an instance of `OllamaProvider`.",
          "The type of `provider` is correctly set to `OllamaProvider` using the `isinstance()` function.",
          "An error message or exception is not raised when calling `get_provider(config)` with the correct configuration.",
          "The provider name 'ollama' is properly formatted and does not contain any typos.",
          "The config object passed to `get_provider(config)` has the required attributes (provider) set correctly.",
          "A custom error message or exception is not raised when calling `get_provider(config)` with the correct configuration.",
          "The provider name 'ollama' is properly formatted and does not contain any leading or trailing whitespace."
        ],
        "scenario": "Verifies that the `get_ollama_provider` function returns an instance of OllamaProvider.",
        "why_needed": "Prevents a potential bug where the `get_ollama_provider` function fails to return an instance of OllamaProvider due to incorrect configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 107-108, 110-111"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007536370000025272,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method returns True for both cached and uncached configurations.",
          "The `checks` attribute is incremented correctly when the `_check_availability()` method is called.",
          "The `is_available()` method returns False for a cache configuration that does not have any available caches."
        ],
        "scenario": "Verifies that the LlmProvider defaults implementation provides a valid cache result.",
        "why_needed": "The current implementation of LlmProvider does not provide a valid cache result, which can cause unexpected behavior in downstream applications."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "52-53, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006891549999750168,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "the 'model' attribute is set to 'test-model'",
          "the 'name' attribute of the provider object is equal to 'test-model'",
          "the provider's get_model_name() method returns 'test-model'",
          "the configuration's model attribute has been successfully loaded",
          "the configuration's name attribute matches the expected default value",
          "the provider's name attribute does not match the expected default value"
        ],
        "scenario": "The 'get_model_name' method of the ConcreteProvider class returns the name set in the test model configuration.",
        "why_needed": "Without this default configuration, the LLM model may not be properly initialized or configured."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "52-53, 128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000723815000014838,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_rate_limits()` method of the provider instance returns an empty list or None.",
          "The `rate_limit` attribute of the provider instance does not contain any valid numerical values.",
          "The `max_rate` and `min_rate` attributes of the provider instance are set to a default value (e.g., 1.0) that is not compatible with the provided rate limits.",
          "The `rate_limits` attribute of the provider instance contains invalid or unsupported types (e.g., strings, booleans).",
          "The `max_rate_limit` and `min_rate_limit` attributes of the provider instance are set to a default value that exceeds the maximum allowed rate limit for the specific LLM model.",
          "The `rate_limits` attribute is not updated when custom rate limits are provided in the configuration.",
          "The `get_rate_limits()` method raises an exception (e.g., ValueError) if no valid rate limits can be determined from the provider instance."
        ],
        "scenario": "The test verifies that the `get_rate_limits` method of a `ConcreteProvider` instance returns `None` when no rate limits are specified.",
        "why_needed": "This test prevents a potential bug where the default rate limit for LLM providers is not properly set to None when no custom rate limits are provided."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "52-53, 147"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006814359999793851,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` object is not local.",
          "The `provider` object has a non-local configuration.",
          "The `provider.is_local()` method returns False."
        ],
        "scenario": "Verify that `is_local()` returns False when default defaults are used.",
        "why_needed": "Prevents regression in case of default defaults being used."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007643980000011652,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The hash value of the source function should be the same every time it is called.",
          "The hash value of the source function should not change after multiple calls to the function.",
          "The cache should store the source function's return values with their corresponding hashes.",
          "The cache should maintain a consistent order of insertion for functions that have different hashes.",
          "The cache should handle function calls correctly, including caching the result of each call."
        ],
        "scenario": "Testing the consistency of a cache with a single source.",
        "why_needed": "To ensure that the cache maintains its expected behavior when using the same source function."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006738359999758359,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `hash_source` function should return a different hash value for two different source strings.",
          "The `hash_source` function should not be able to deduce the source of a function from its name.",
          "The `hash_source` function should raise an error if given the same input multiple times.",
          "The `hash_source` function should preserve the original source code when hashing.",
          "The `hash_source` function should return different hash values for functions with the same name but different parameter lists.",
          "The `hash_source` function should not be able to deduce the source of a function from its docstring.",
          "The `hash_source` function should raise an error if given a function that does not have a `__name__` attribute."
        ],
        "scenario": "Testing the `hash_source` function with different sources.",
        "why_needed": "Prevents a bug where two functions with the same source code but different names produce the same hash value."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000667260999989594,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The hash should be at least 16 characters long.",
          "The hash should be exactly 16 characters long (e.g., 'aabbccdd').",
          "No leading zeros are allowed in the hash.",
          "No trailing zeros are allowed in the hash.",
          "The hash does not contain any repeated characters.",
          "All characters in the hash are unique.",
          "The hash is a valid SHA-256 hash (e.g., '1234567890abcdef')."
        ],
        "scenario": "Verify the length of the hash generated by `hash_source`.",
        "why_needed": "Prevent a potential issue where the hash length is not consistent across different inputs."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_hash_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 26,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.001127010999994127,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of cache entries should be exactly 2.",
          "test::a should not be present in the cache.",
          "test::b should not be present in the cache."
        ],
        "scenario": "Test that clearing the cache removes all entries.",
        "why_needed": "Prevents a bug where some cache entries are not properly cleared after test::a and test::b are removed."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_clear",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 11,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007986319999986335,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation 'error' in the cache key 'test::foo' should be None.",
          "The annotation 'abc123' in the cache key 'test::foo' should not match any existing value.",
          "No error message should be stored in the cache for annotations with errors."
        ],
        "scenario": "Test that annotations with errors do not get cached.",
        "why_needed": "This test prevents a potential regression where error annotations are incorrectly cached."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 9,
          "line_ranges": "39-41, 53, 55-56, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008466769999699864,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `cache.get()` should return `None` when called with an invalid key.",
          "The function `cache.get()` should raise a `KeyError` when called with an invalid key.",
          "The function `cache.get()` should not modify the cache when called with an existing key that is missing from it."
        ],
        "scenario": "Test case 'test_get_missing' verifies that the function returns None for missing entries in the cache.",
        "why_needed": "The test prevents a potential bug where the function does not handle missing cache entries correctly, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_get_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 28,
          "line_ranges": "39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009952180000141198,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check if the annotation is set with the correct key",
          "Check if the annotation's scenario matches the expected value",
          "Check if the confidence of the annotation is as expected",
          "Verify that the cache returns a non-None result for the given key",
          "Verify that the retrieved annotation has the correct confidence level"
        ],
        "scenario": "Test that annotations are stored and retrieved correctly from the cache.",
        "why_needed": "Prevents bypass attacks by ensuring that annotations are cached before they can be used to bypass security measures."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_set_and_get",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006913590000294789,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' attribute of the error object should be equal to 'test_bad.py'.",
          "The 'message' attribute of the error object should be equal to 'SyntaxError'."
        ],
        "scenario": "Test verifies that a collection error has the correct 'nodeid' and 'message' attributes.",
        "why_needed": "This test prevents a potential bug where a CollectionError is incorrectly structured, potentially leading to incorrect handling or reporting of errors in the collector."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006993039999656503,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_collection_errors()` method returns an empty list when the input collection is empty.",
          "An empty collection is considered an error and should be handled accordingly in subsequent steps.",
          "A test asserting that an empty collection is returned is necessary to ensure the method behaves as expected in this scenario."
        ],
        "scenario": "Verifies that an empty collection is returned when the `get_collection_errors` method is called on a newly created `TestCollector` instance with an empty configuration.",
        "why_needed": "This test prevents a potential regression where an empty collection is not immediately recognized as an error."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006844629999704921,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The llm_context_override attribute should be None for the given TestCaseResult.",
          "The llm_context_override attribute should not be set to any other value than None for the given TestCaseResult.",
          "If llm_context_override is set to a different value, it should be immediately overridden by the actual context.",
          "The default value of llm_context_override should be None for the given TestCaseResult.",
          "If the default value of llm_context_override is not None, it should be checked and verified in subsequent tests.",
          "The test should verify that the default value of llm_context_override is correctly set to None for the given TestCaseResult."
        ],
        "scenario": "Test the default value of llm_context_override in TestCollectorMarkerExtraction.",
        "why_needed": "This test prevents a potential bug where the default value of llm_context_override is not set correctly, potentially leading to incorrect results or unexpected behavior."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000707900999998401,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The llm_opt_out attribute should be set to False.",
          "The llm_opt_out attribute should not be set to True.",
          "The TestCaseResult object should have a llm_opt_out attribute with the correct value (False).",
          "The TestCaseResult object should not have a llm_opt_out attribute with an incorrect value (True).",
          "The llm_opt_out attribute should be correctly initialized in the TestCaseResult object.",
          "The TestCaseResult object should not have any other attributes that are not relevant to this test.",
          "The TestCaseResult object should pass all assertions without raising any exceptions."
        ],
        "scenario": "Test that the default value of llm_opt_out is correctly set to False for a test case.",
        "why_needed": "This test prevents regression where the default value of llm_opt_out is incorrectly set to True."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007072950000406308,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.capture_failed_output should be set to False",
          "config.capture_enabled should be set to False",
          "output_capture_enabled should not be True"
        ],
        "scenario": "The test verifies that the output capture is disabled by default.",
        "why_needed": "This test prevents a regression where the output capture was enabled by default."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007079939999812268,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert config.capture_output_max_chars == 4000",
          "assert config.capture_output_max_chars != 10000",
          "assert config.capture_output_max_chars != 5000",
          "assert config.capture_output_max_chars != 2000",
          "assert config.capture_output_max_chars != 3000",
          "assert config.capture_output_max_chars == 4000"
        ],
        "scenario": "Verify that the default value of `capture_output_max_chars` is set to 4000.",
        "why_needed": "This test prevents a potential bug where the default max chars value is not correctly set to prevent excessive output."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007171750000338761,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'passed', 'skipped' and 'duration' fields of the SimpleNamespace object `report` should be set to False.",
          "The 'outcome' field of the SimpleNamespace object `result` should be set to 'xfailed'.",
          "The 'wasxfail' field of the SimpleNamespace object `report` should be set to 'expected failure'."
        ],
        "scenario": "Test 'xfail failures should be recorded as xfailed' verifies that failed xfail tests are correctly marked as xfailed in the report.",
        "why_needed": "This test prevents regression by ensuring that failed xfail tests are properly recorded and marked as such, preventing incorrect reporting of failure status."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 26,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007191209999746206,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `result.outcome` of the `test_unexpected_pass` node should be 'xpassed' after calling the `handle_runtest_logreport` method on the `TestCollector` instance.",
          "The `wasxfail` attribute of the `test_xfail.py::test_unexpected_pass` node should match the expected failure message.",
          "The `duration` and `longrepr` attributes of the `test_xfail.py::test_unexpected_pass` node should be set to 0.01 seconds and an empty string respectively after calling the `handle_runtest_logreport` method on the `TestCollector` instance.",
          "The `skipped` attribute of the `test_xfail.py::test_unexpected_pass` node should remain False after calling the `handle_runtest_logreport` method on the `TestCollector` instance.",
          "The `failed` attribute of the `test_xfail.py::test_unexpected_pass` node should be set to False after calling the `handle_runtest_logreport` method on the `TestCollector` instance.",
          "The `passed` attribute of the `test_xfail.py::test_unexpected_pass` node should remain True after calling the `handle_runtest_logreport` method on the `TestCollector` instance.",
          "After calling the `handle_runtest_logreport` method, the `config` object passed to the `TestCollector` constructor should not be modified."
        ],
        "scenario": "Test 'xfail passes should be recorded as xpassed' verifies that the test collector correctly records and reports xfail tests as passed.",
        "why_needed": "This test prevents regression where an unexpected pass in a test is not properly reported as failed, potentially masking issues with test behavior or reporting."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007307040000341658,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "collector.results == {}",
          "collector.collection_errors == []",
          "collector.collected_count == 0"
        ],
        "scenario": "Test that the `TestCollector` class initializes correctly and returns an empty collection.",
        "why_needed": "This test prevents a potential bug where the collector is initialized with incorrect or missing configuration settings, potentially leading to incorrect results."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_create_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007498159999954623,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The list of node IDs returned by the `get_results` method is sorted in ascending order.",
          "The first element of the list is 'a_test.py::test_a'.",
          "The second element of the list is 'z_test.py::test_z'."
        ],
        "scenario": "Test the `get_results` method of TestCollector to ensure it returns sorted results by node ID.",
        "why_needed": "This test prevents regression where unsorted results are returned due to a bug in the sorting logic."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007078339999679883,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collected_count` attribute of the collector should be equal to the number of items that were collected.",
          "The `deselected_count` attribute of the collector should be equal to the number of items that were deselected.",
          "The `collected_count` and `deselected_count` attributes should match the expected values after calling `handle_collection_finish` with a list of collected and deselected items."
        ],
        "scenario": "Verify that the `handle_collection_finish` method correctly tracks collected and deselected counts.",
        "why_needed": "This test prevents a potential regression where items are not properly tracked as collected or deselected after collection finish."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_handle_collection_finish",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0014342509999778486,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' attribute of the report object should be empty.",
          "The 'outcome' attribute of the report object should be 'failed'.",
          "The 'when' attribute of the report object should be set to 'call'.",
          "The 'passed' attribute of the report object should be False.",
          "The 'failed' attribute of the report object should be True.",
          "The 'skipped' attribute of the report object should be False.",
          "The 'capstdout' attribute of the report object should be set to 'output'.",
          "The 'wasxfail' attribute of the report object should not exist."
        ],
        "scenario": "Test 'Should not capture if config disabled (integration via handle_runtest_logreport)' verifies that the test does not capture output when `capture_failed_output` is set to False.",
        "why_needed": "This test prevents a bug where the test captures output even though `capture_failed_output` is set to False, which could lead to unexpected behavior or false positives in the test results."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008299490000354126,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `captured_stderr` attribute of the `TestCaseResult` object is set to 'Some error'.",
          "The `report.capstderr` method is called with an argument equal to 'Some error'.",
          "The `collector._capture_output` function is called with a result and report object that have been populated with the expected values."
        ],
        "scenario": "Test that the `test_capture_output_stderr` function captures stderr correctly.",
        "why_needed": "This test prevents a potential regression where the `test_capture_output_stderr` function does not capture stderr."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008601339999927404,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `captured_stdout` attribute of the `TestCaseResult` object is set to 'Some output'.",
          "The `report.capstdout` method returns 'Some output' as expected.",
          "The `report.capstderr` method does not interfere with the captured stdout.",
          "The collector correctly captures stdout even when it fails and reports an error.",
          "The `test_capture_output_stdout` function is able to distinguish between captured stdout and reported errors.",
          "The test result indicates that the collector successfully captured stdout."
        ],
        "scenario": "Test that the `test_capture_output_stdout` function captures stdout correctly.",
        "why_needed": "This test prevents a regression where the collector does not capture stdout when it should."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000920979999989413,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured stdout should be truncated at 10 characters.",
          "The captured stderr is empty.",
          "The `captured_stdout` attribute of the `TestCaseResult` object should contain only the first 10 characters of the output.",
          "The collector does not fail when capturing output that exceeds the maximum characters.",
          "The collector correctly truncates output and displays it in the report.",
          "The captured stderr is empty even if the output exceeds the maximum characters."
        ],
        "scenario": "The test verifies that the `TestCollector` truncates output exceeding the specified maximum characters.",
        "why_needed": "This test prevents a potential issue where the collector fails to capture and display the entire output of a test, potentially leading to misleading results or errors."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 35,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.002853182000023935,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `param_id` of the extracted marker is set to 'param1'.",
          "The value of `llm_opt_out` is set to True.",
          "The value of `llm_context_override` is set to 'complete'.",
          "A list of requirements is returned with values ['REQ-1', 'REQ-2']."
        ],
        "scenario": "Test creates a result with item markers and verifies the expected behavior.",
        "why_needed": "This test prevents regression by ensuring that the collector correctly extracts item markers from an item and returns them in the expected format."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0013397129999930257,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_error` method of `TestCollector` should return 'Crash report' when called with an instance of `ReprFileLocation`.",
          "The `longrepr` attribute of `Report` should have a value that is a string representation of 'Crash report'.",
          "The `__str__` method of `Report.longrepr` should be able to return the expected string 'Crash report' when called. If this assertion fails, it may indicate a bug in the collector or its dependencies.",
          "If an instance of `ReprFileLocation` is passed to `_extract_error`, it should not cause a crash report.",
          "The `collectors` module should be able to recover from errors and continue processing other errors without crashing.",
          "If an error occurs during the collection process, the test should fail with a meaningful error message."
        ],
        "scenario": "Test the `collectors` module's ability to handle ReprFileLocation in error reports.",
        "why_needed": "This test prevents a potential crash when encountering ReprFileLocation in error reports, ensuring the collector can recover and continue processing other errors."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009290609999652588,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.longrepr` attribute is set to 'Some error occurred'.",
          "The `_extract_error` method returns 'Some error occurred'.",
          "The extracted string matches the expected value."
        ],
        "scenario": "Test that the `_extract_error` method returns a string 'longrepr' when called with a `report` object containing a 'longrepr' attribute.",
        "why_needed": "This test prevents a potential regression where the collector might not correctly extract error strings from reports."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008332179999683831,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_skip_reason` method does not raise an exception or return a specific value when `report.longrepr` is `None`.",
          "The `_extract_skip_reason` method returns `None` instead of raising an exception or returning a default value when `report.longrepr` is `None`.",
          "The `_extract_skip_reason` method checks for the existence of `report.longrepr` before attempting to extract skip reasons and raises an exception if it does not exist.",
          "When `report.longrepr` is `None`, the `_extract_skip_reason` method correctly returns `None` without raising an exception or returning a default value.",
          "The `_extract_skip_reason` method checks for the existence of `report.longrepr` before attempting to extract skip reasons and raises an exception if it does not exist.",
          "When `report.longrepr` is `None`, the `_extract_skip_reason` method correctly returns `None` without raising an exception or returning a default value.",
          "The `_extract_skip_reason` method checks for the existence of `report.longrepr` before attempting to extract skip reasons and raises an exception if it does not exist.",
          "When `report.longrepr` is `None`, the `_extract_skip_reason` method correctly returns `None` without raising an exception or returning a default value."
        ],
        "scenario": "Test that the `_extract_skip_reason` method returns `None` when no longrepr is provided.",
        "why_needed": "Prevents a potential bug where the test fails if there are no longreprs to extract skip reasons from."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008233190000055401,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method should return 'Just skipped' as the skip reason string.",
          "The method should extract the `longrepr` attribute from the report object and return it as a string.",
          "The method should not raise an exception if no longrepr is available.",
          "The method should handle cases where the report object does not have a `longrepr` attribute.",
          "The method should not modify the original report object.",
          "The method should preserve the original type and behavior of the report object.",
          "The method should return a string that can be used as a valid reason for skipping in the collector."
        ],
        "scenario": "Test `test_extract_skip_reason_string` verifies that the `_extract_skip_reason` method returns a string as expected.",
        "why_needed": "This test prevents potential issues where the method does not return the correct skip reason string, potentially leading to incorrect reporting or debugging."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008383640000033665,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.longrepr` attribute should contain a tuple with `(file, line, message)` as its first element.",
          "The `message` field within this tuple should be 'Skipped for reason'.",
          "The `longrepr` value should match the expected string when converted to a Python literal.",
          "The `report.longrepr` attribute should contain the entire tuple as its second element.",
          "The `message` field within this tuple should be 'Skipped for reason'.",
          "The `longrepr` value should match the expected string when converted to a Python literal.",
          "The `report.longrepr` attribute should contain the entire tuple as its second element.",
          "The `message` field within this tuple should be 'Skipped for reason'."
        ],
        "scenario": "Test that extract skip reason tuple works correctly.",
        "why_needed": "Prevents a potential bug where the test fails due to incorrect handling of tuples with longrepr messages."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 21,
          "line_ranges": "58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008406959999547325,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `collector.collection_errors` should be equal to 1.",
          "The nodeid in the first `collector.collection_errors` item should match 'test_broken.py'.",
          "The message in the first `collector.collection_errors` item should match 'SyntaxError'."
        ],
        "scenario": "When the `handle_collection_report` method is called with a report that indicates collection failure, it should record this error.",
        "why_needed": "This test prevents potential data loss due to unhandled collection errors."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0013125689999924361,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.rerun_count should be equal to 1 (the expected number of reruns).",
          "res.final_outcome should be 'failed' (indicating that the test failed after rerunning)."
        ],
        "scenario": "Test 'handle_runtest_rerun' verifies that the TestCollector handles rerun attribute correctly.",
        "why_needed": "This test prevents a regression where the TestCollector does not handle reruns correctly, potentially leading to incorrect results or failures."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008767470000066169,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.outcome should be 'error'",
          "res.phase should be 'setup'",
          "res.error_message should be 'Setup failed'"
        ],
        "scenario": "TestCollectorReportHandling::test_handle_runtest_setup_failure verifies that a setup error is recorded in the report.",
        "why_needed": "This test prevents regression where TestCollector handles runtest log reports as if they were successful, potentially leading to incorrect reporting of setup errors."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 38,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0010949470000127803,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert res.outcome == 'error'",
          "assert res.phase == 'teardown'",
          "assert res.error_message == 'Cleanup failed'",
          "assert call_report.wasxfail",
          "assert teardown_report.wasxfail"
        ],
        "scenario": "Test case 'Should record error if teardown fails after pass' verifies that the collector correctly records an error when a teardown operation fails.",
        "why_needed": "This test prevents regression by ensuring that the collector handles teardown failures properly and reports them as errors in the results."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 17,
          "line_ranges": "134, 136-139, 141-142, 385, 387, 417-424"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007691580000255271,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty list when the 'model' parameter is None.",
          "The function should return an empty list when the 'model' parameter is set to 'All'.",
          "The function should not throw any errors or exceptions when the 'model' parameter is None and the preferred models are already empty."
        ],
        "scenario": "Test the GeminiProvider's _parse_preferred_models method with edge cases to ensure correct behavior.",
        "why_needed": "This test prevents regression in case a new model is added to the preferred list without updating the parsing logic."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 35,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007595270000138044,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert limiter.next_available_in(60) > 0",
          "assert limiter.next_available_in(10) == 0",
          "assert limiter.record_tokens(50) < 100",
          "assert len(limiter.tokens) > 0"
        ],
        "scenario": "Verify that the rate limiter does not allow excessive tokens when there are available tokens but no requests.",
        "why_needed": "This test prevents a bug where the rate limiter allows too many tokens when there are enough available tokens, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 46,
          "line_ranges": "71-78, 104-107, 109, 111-113, 115, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000717747999999574,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `coverage_percent` attribute of `SourceCoverageEntry` should be equal to 50.0.",
          "The `error` attribute of `LlmAnnotation` should be 'timeout'.",
          "The `duration` attribute of `RunMeta` should be equal to 1.0."
        ],
        "scenario": "Verify that the `to_dict()` method of `SourceCoverageEntry` and `LlmAnnotation` correctly returns their respective values.",
        "why_needed": "This test prevents regression in coverage booster models where the coverage percentage is not being accurately calculated."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007162629999584169,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert mapper.config is config",
          "assert mapper.warnings == []"
        ],
        "scenario": "The `CoverageMapper` instance should be initialized with the provided configuration.",
        "why_needed": "This test prevents a potential bug where the `CoverageMapper` instance's configuration is not properly set."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 3,
          "line_ranges": "44-45, 308"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007262679999939792,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_warnings` method is called on an instance of `CoverageMapper`.",
          "A `Config` object is created and passed to the `CoverageMapper` instance.",
          "The `get_warnings` method is called on the `CoverageMapper` instance.",
          "The result of the `get_warnings` method is checked to be a list.",
          "The type of the result is checked to be a list.",
          "A warning message or error is expected to be returned from the `get_warnings` method."
        ],
        "scenario": "The `get_warnings` method in the `CoverageMapper` class should be able to retrieve a list of warnings from the coverage report.",
        "why_needed": "This test prevents potential issues where the function returns an incorrect type (in this case, a list) when it's expected to return a specific type (a list)."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0014229290000002948,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `Path.exists` mock returns False.",
          "The `glob.glob` mock returns an empty list.",
          "The `map_coverage` function should return an empty dictionary.",
          "There should be at least one warning in the `warnings` list."
        ],
        "scenario": "Tests coverage map function with no coverage file.",
        "why_needed": "Prevents a potential bug where the function returns an empty dictionary when there is no coverage file."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007032320000348591,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method `_extract_nodeid` should return the correct node ID for each phase.",
          "The method `_extract_nodeid` should handle cases where the phase name contains multiple words (e.g., 'test_foo|run').",
          "The method `_extract_nodeid` should correctly extract the node ID for phases that are not explicitly mentioned in the test code (e.g., `test.py::test_foo|teardown`)."
        ],
        "scenario": "The test verifies that the `CoverageMapper` extracts all phases when `include_phase=all`.",
        "why_needed": "This test prevents a regression where the coverage is not extracted for all phases when `include_phase=all`."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007184279999705723,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_nodeid` method should return `None` for an empty string.",
          "The `_extract_nodeid` method should return `None` for a `None` value as context.",
          "The test should fail when given an empty context, indicating that the `extract_nodeid` method is not handling it correctly."
        ],
        "scenario": "The test verifies that the `extract_nodeid` method returns `None` when given an empty context.",
        "why_needed": "This test prevents a potential bug where the `extract_nodeid` method does not handle empty contexts correctly, potentially leading to incorrect coverage metrics or unexpected behavior."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 216, 220, 224-225, 228-230"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006988389999946776,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_extract_nodeid` should return None for node IDs that start with `test.py::test_foo|setup`.",
          "The function `_extract_nodeid` should raise an error when called with a non-string argument."
        ],
        "scenario": "Test the function `extract_nodeid_filters_setup` to ensure it correctly filters out setup phase when `include_phase` is set to 'run'.",
        "why_needed": "This test prevents a potential bug where the function does not filter out setup phase, potentially leading to incorrect coverage analysis."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006851370000049428,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The extracted node ID matches the expected value (`test.py::test_foo`) for the given module and function name.",
          "The extracted node ID does not contain any leading or trailing whitespace.",
          "The extracted node ID does not contain any invalid characters (e.g., special regex patterns).",
          "The extracted node ID is present in the `run` phase context.",
          "The extracted node ID is present for all modules and functions within the given module.",
          "No exceptions are raised when extracting the node ID from a run phase context."
        ],
        "scenario": "Verify that the `extract_nodeid` method extracts the correct node ID from a run phase context.",
        "why_needed": "This test prevents regression by ensuring that the node ID extraction logic is correct and consistent across different phases."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 57,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0015156160000060481,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mocked context_by_lineno returns a dictionary with test_one and test_two as keys, where each key has multiple values.",
          "the function should return a list of files that have app.py as their file path.",
          "the line count for the returned files in test_one should be 2 (lines 1 and 2).",
          "the line count for the returned files in test_two should not be affected by the mock data.",
          "mocked contexts_by_lineno does not return any additional context files that are not already included in the result.",
          "the function should correctly handle cases where there are multiple lines of code with the same file path (e.g., app.py::test_one|run).",
          "the function should correctly handle cases where a file has no matching contexts (e.g., README.md::test_three|run)."
        ],
        "scenario": "Test should extract contexts for full logic coverage of _extract_contexts method.",
        "why_needed": "This test prevents regression that would occur if the _extract_contexts method did not cover all paths in _extract_contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0010877370000343944,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_data.contexts_by_lineno.return_value == {}",
          "mock_data.measured_files.return_value == ['app.py']",
          "result == {}"
        ],
        "scenario": "Test that the `extract_contexts` method returns an empty dictionary when there are no test contexts.",
        "why_needed": "Prevents a regression where the coverage map is incorrectly populated with context information for files without any test contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007264360000363013,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_nodeid` method returns the expected node ID for each test.",
          "The `_extract_nodeid` method correctly ignores node IDs from tests with missing lines in certain phases.",
          "The `CoverageMapper` uses the correct phase to filter out node IDs from tests.",
          "The `CoverageMapper` ignores node IDs from tests without a specified phase.",
          "The `CoverageMapper` does not incorrectly report node IDs for tests with missing lines in different phases."
        ],
        "scenario": "Test that the `CoverageMapper` extracts node IDs for tests with missing lines in different phases.",
        "why_needed": "This test prevents a bug where the `CoverageMapper` incorrectly filters out node IDs from tests with missing lines in certain phases."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009217129999683493,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return None for _load_coverage_data() without any .coverage files.",
          "There should be exactly one warning message with code 'W001' when no .coverage files exist.",
          "The warnings list should contain only one item with the specified code."
        ],
        "scenario": "Test that the test_load_coverage_data_no_files function correctly handles the case when no coverage files exist.",
        "why_needed": "This test prevents a potential bug where the test would fail due to missing coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 17,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0015254979999781426,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _load_coverage_data() returns None when an error occurs during read.",
          "Any warnings generated by mapper.warnings contain the message 'Failed to read coverage data'.",
          "The function _load_coverage_data() raises an Exception with the message 'Corrupt coverage file' when a corrupt .coverage file is encountered."
        ],
        "scenario": "Test should handle errors reading coverage files with a corrupt .coverage file.",
        "why_needed": "This test prevents regression by ensuring that the CoverageMapper can correctly handle corrupted coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.00246401499998683,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `update` method of the `CoverageData` instance should be called at least twice during the `_load_coverage_data` process.",
          "The `update` method of the `CoverageData` instance should not be called more than once during the `_load_coverage_data` process.",
          "The number of times the `update` method is called for each mock CoverageData instance should match the expected number of parallel coverage files.",
          "The `update` method should only be called when a new parallel coverage file is created and removed from the temporary directory.",
          "The `update` method should not be called when the existing parallel coverage file is updated or deleted in the temporary directory.",
          "The `update` method should call the original mock CoverageData instance's `update` method for each mock instance.",
          "The `update` method should update the internal data structures of the CoverageMapper correctly after calling its side effect instances."
        ],
        "scenario": "Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its internal data structures.",
        "why_needed": "This test prevents regression in the CoverageMapper class, which is responsible for handling parallel coverage files from xdist. Without this test, the mapper may not update its internal data structures correctly when dealing with parallel files, leading to incorrect coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 5,
          "line_ranges": "44-45, 58-60"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009007070000279782,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty dictionary when no data is loaded.",
          "The function should not raise any exceptions when no data is loaded.",
          "The function should handle the case where `None` is returned by `_load_coverage_data` correctly.",
          "The function should preserve the original coverage map values.",
          "The function should ignore missing keys in the coverage map.",
          "The function should return a dictionary with default values (e.g., `{}`) when no data is loaded.",
          "The function should not throw an exception when `None` is returned by `_load_coverage_data`."
        ],
        "scenario": "Test that the `map_coverage` method returns an empty dictionary when `_load_coverage_data` returns None.",
        "why_needed": "Prevents a potential bug where the test fails due to a missing coverage map being returned from `_load_coverage_data`."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013643810000303347,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock analysis2 function should be called with an Exception exception.",
          "The mocked mock_data.measured_files.return_value should return ['app.py'].",
          "The mock_cov.get_data.return_value should raise an Exception exception.",
          "The entries list should have zero length after skipping files with errors."
        ],
        "scenario": "Test coverage map source coverage analysis error.",
        "why_needed": "Prevents test failure due to analysis errors during source coverage mapping."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0016515870000262112,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `entries[0].file_path` should return the path of the source file being tested (`'app.py'`).",
          "The function `entries[0].statements` should return the number of statements in the source file (`3`).",
          "The function `entries[0].covered` should return the percentage of covered lines (`2`).",
          "The function `entries[0].missed` should return the number of missed lines (`1`).",
          "The function `entries[0].coverage_percent` should return the percentage of covered lines (`66.67`)."
        ],
        "scenario": "Verify that the test maps all paths in `map_source_coverage` to a comprehensive coverage report.",
        "why_needed": "This test prevents regression by ensuring that all paths are covered, even if analysis2 is not able to provide complete information."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006720299999756207,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `message` attribute of the returned WarningCode.W001_NO_COVERAGE instance contains the expected string 'No .coverage file found'.",
          "The `detail` attribute of the returned WarningCode.W001_NO_COVERAGE instance matches the specified value 'test-detail'.",
          "The `code` attribute of the returned WarningCode.W001_NO_COVERAGE instance is set to WarningCode.W001_NO_COVERAGE."
        ],
        "scenario": "Test the `make_warning` factory function to ensure it correctly returns a WarningCode.W001_NO_COVERAGE instance with the specified detail.",
        "why_needed": "To prevent a bug where an unknown warning is returned without any additional information."
      },
      "nodeid": "tests/test_errors.py::test_make_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007414640000433792,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Assertion error', 'value': 'W001'}",
          "{'message': 'Assertion error', 'value': 'W101'}",
          "{'message': 'Assertion error', 'value': 'W201'}",
          "{'message': 'Assertion error', 'value': 'W301'}",
          "{'message': 'Assertion error', 'value': 'W401'}"
        ],
        "scenario": "Test that warning codes have correct values.",
        "why_needed": "This test prevents a potential regression where the warning code values are not correctly set."
      },
      "nodeid": "tests/test_errors.py::test_warning_code_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 6,
          "line_ranges": "70-72, 74-76"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006895230000054653,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'code' key should be present in the dictionary and have the correct value.",
          "The 'message' key should also be present in the dictionary with the correct value.",
          "The 'detail' key should be present in the dictionary if it exists, otherwise its value should be an empty string.",
          "All keys in the dictionary should match the expected keys according to WarningCode.",
          "If a Warning object has no detail, then the 'message' and 'code' keys should both be missing from the dictionary."
        ],
        "scenario": "Test the warning to dict method.",
        "why_needed": "Prevent a potential bug where Warning objects are not properly converted to dictionaries."
      },
      "nodeid": "tests/test_errors.py::test_warning_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000702981000017644,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `make_warning` should return an instance of `WarningCode.W101_LLM_ENABLED` with the correct message.",
          "The warning message should be set to `WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]`.",
          "The detail attribute should be set to `None` for this specific case."
        ],
        "scenario": "Test the `make_warning` function with known code.",
        "why_needed": "Prevents a potential bug where the warning is not correctly generated for known code."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006726849999836304,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The expected message 'Unknown warning.' is returned when making a warning with WarningCode.W001_NO_COVERAGE.",
          "The old message 'Warning: Unknown warning.' is restored after the test.",
          "The WARNING_MESSAGES dictionary is updated correctly to reflect the new fallback message."
        ],
        "scenario": "Test MakeWarningUnknownCode verifies that missing WarningCode.W001_NO_COVERAGE is handled correctly.",
        "why_needed": "This test prevents a bug where the fallback message for unknown code is not used when an enum is allowed in the typed function."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006705100000203856,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "w.code == WarningCode.W301_INVALID_CONFIG",
          "w.detail == 'Bad value'",
          "assert w is not None"
        ],
        "scenario": "Test makes a warning when invalid configuration is provided with detail.",
        "why_needed": "Prevents a potential bug where the test does not create a warning for an invalid configuration with detailed error message."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007213439999986804,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "code.value should be an instance of str.",
          "code.value should start with 'W'.",
          "All WarningCode enum members should have a value that starts with 'W' and is a string."
        ],
        "scenario": "Verify that all WarningCode enum values are strings.",
        "why_needed": "This test prevents a potential bug where the WarningCode enum is not properly initialized with string values."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 5,
          "line_ranges": "70-72, 74, 76"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006720669999822348,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'code' attribute of the Warning object is set to 'W001'.",
          "The 'message' attribute of the Warning object is set to 'No coverage'.",
          "The 'detail' key is not present in the serialized dictionary.",
          "The length of the serialized dictionary is 2 (i.e., it only contains 'code' and 'message')."
        ],
        "scenario": "Test that Warning.to_dict() returns a dictionary without 'detail' key when no detail is provided.",
        "why_needed": "This test prevents the warning 'WarningDataClass: warning_to_dict_no_detail' because it ensures that the Warning object's attributes are serialized correctly to a dictionary without including any additional details."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 6,
          "line_ranges": "70-72, 74-76"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007208079999827532,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `to_dict()` method of the Warning class returns a dictionary with the correct keys and values.",
          "The 'code' key in the dictionary contains the warning code.",
          "The 'message' key in the dictionary contains the warning message.",
          "The 'detail' key in the dictionary contains the detailed warning message.",
          "The 'WarningCode.W001_NO_COVERAGE' value is correctly assigned to the 'code' key.",
          "The 'Check setup' value is correctly assigned to the 'detail' key.",
          "The resulting dictionary has the correct structure and content."
        ],
        "scenario": "Test the warning to dictionary conversion with detail.",
        "why_needed": "This test prevents a potential bug where warnings are not properly serialized to dictionaries, potentially causing issues in downstream data processing or logging."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0006908600000201659,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return `False` when given a file name without a `.py` extension (e.g., `foo/bar.txt`).",
          "The function should return `False` when given a file name with a `.pyc` extension (e.g., `foo/bar.pyc`).",
          "The function should not incorrectly identify files that are actually Python files but have a non-Python file extension (e.g., `foo/bar.py`)."
        ],
        "scenario": "Verifies that the `is_python_file` function returns False for non-.py files.",
        "why_needed": "Prevents a potential bug where the function incorrectly identifies Python files as non-Python files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0006778709999935018,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return `True` when given a path to a `.py` file.",
          "The function should raise an error or return a specific value when given a non-`.py` file path.",
          "The function should correctly handle relative paths for `.py` files.",
          "The function should not incorrectly identify other types of files as Python files.",
          "The function should be able to handle multiple extension checks (e.g., `.txt`, `.json`) in a single call.",
          "The function should raise an error when given a file with a different encoding than UTF-8.",
          "The function should correctly handle cases where the file is empty or contains only whitespace."
        ],
        "scenario": "Verifies that the `is_python_file` function returns True for a `.py` file.",
        "why_needed": "Prevents a potential bug where the function incorrectly identifies non-`.py` files as Python files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.0011761169999999765,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file is created with the correct parent directory.",
          "The file's path is not the same as its original path.",
          "The file is written to the correct location (subdir/file.py).",
          "The file's path is absolute (i.e., it does not start with a slash).",
          "The file's parent directory exists and can be created without raising an error.",
          "The file's parent directory is not already in the test directory.",
          "The file's original path is not the same as its relative path.",
          "The file's relative path starts with 'subdir/'."
        ],
        "scenario": "Test 'test_makes_path_relative' verifies that making a path relative to the test directory results in an absolute path.",
        "why_needed": "This test prevents regression when creating files outside of the test directory."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0007179809999797726,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `make_relative('foo/bar')` should be 'foo/bar'.",
          "The function does not return the same result when given an absolute path like 'foo/absolute/path'.",
          "If the input is a directory, the function returns the relative path to that directory.",
          "If the input is a file, the function returns the relative path from the current working directory.",
          "The function handles cases where the base directory is empty or None.",
          "The function does not return an error when given an invalid input (e.g., non-string base)",
          "The function preserves the original case of the input path (e.g., 'foo' instead of 'Foo')."
        ],
        "scenario": "Verifies that the `make_relative` function returns a normalized path when there is no base.",
        "why_needed": "Prevents a potential issue where an absolute path would be returned instead of a relative one."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0006859270000063589,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `normalize_path` function should return the same result as calling `pathlib.PurePath('foo/bar')`.",
          "The `normalize_path` function should not modify the input path.",
          "The `normalize_path` function should handle paths with leading or trailing slashes correctly.",
          "The `normalize_path` function should preserve the original directory hierarchy.",
          "The `normalize_path` function should raise an error if the input is not a string or a Path object.",
          "The `normalize_path` function should return an empty string for an empty path.",
          "The `normalize_path` function should handle symbolic links correctly."
        ],
        "scenario": "Tests that a normalized path is returned for an already-normalized input.",
        "why_needed": "This test prevents a potential bug where the `normalize_path` function would incorrectly return the original path if it's already normalized."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007761699999946359,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should convert '\\\"' to '/\\'.",
          "The function should convert '/foo/bar' to 'foo/bar'.",
          "The function should preserve the original directory structure and only replace forward slashes with forward slashes."
        ],
        "scenario": "The test verifies that the `normalize_path` function correctly converts forward slashes in file paths to forward slashes.",
        "why_needed": "This test prevents a potential bug where the function does not handle forward slashes correctly, potentially leading to incorrect path comparisons or errors."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0006676859999856788,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input path should be stripped of any trailing slash before normalization.",
          "Normalization should return the same path if it already did not have a trailing slash.",
          "The function should raise an error when given a path with no leading directory or file name.",
          "The function should correctly handle paths like 'foo/../bar/' and 'foo/bar/'.",
          "Normalization of empty strings should return an empty string.",
          "Normalization of absolute paths (like '/home/user/foo/') should work as expected."
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
        "why_needed": "This test prevents a potential bug where the function does not correctly handle paths with trailing slashes."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.0007181710000168096,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "custom pattern 'test*' should be matched and the function should return True for `tests/conftest.py`",
          "custom pattern 'test*' should not match and the function should return False for `src/module.py`"
        ],
        "scenario": "Test verifies that the `should_skip_path` function correctly skips custom paths based on provided patterns.",
        "why_needed": "This test prevents a potential regression where the function does not skip custom paths as intended, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0006522349999613652,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('src/module.py') == False",
          "assert not should_skip_path('non-existent-path.txt') == True"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
        "why_needed": "To prevent skipping of normal file system paths."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0006855999999970663,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('.git/objects/foo') is True",
          "assert should_skip_path('non_git_directory') is False"
        ],
        "scenario": "The test verifies that the `should_skip_path` function correctly identifies `.git` directories.",
        "why_needed": "This test prevents a potential issue where the function incorrectly skips non-`.git` directories, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007033690000071147,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return True for directories with a name starting with '__pycache__' and containing '.pyc'.",
          "The function should not return True for directories without a name starting with '__pycache__' or containing '.pyc'.",
          "The function should correctly handle cases where the directory name is not exactly 'foo/__pycache__/bar.pyc'."
        ],
        "scenario": "Verifies that the `should_skip_path` function correctly identifies __pycache__ directories.",
        "why_needed": "Prevents a potential issue where the test would incorrectly skip non-pycache directories."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.000658193000049323,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `should_skip_path` function should return True for venv directories and False for regular Python libraries.",
          "The `should_skip_path` function should not return True for `.venv` directories or any other directory that is a subdirectory of the current working directory.",
          "The test should verify that the `should_skip_path` function correctly handles different types of Python library directories.",
          "The test should check that the `should_skip_path` function does not incorrectly identify venv directories as being to be skipped in certain scenarios.",
          "The test should ensure that the `should_skip_path` function is able to handle nested directory structures correctly.",
          "The test should verify that the `should_skip_path` function returns False for all other Python library directories.",
          "The test should check that the `should_skip_path` function does not return True for any other type of directory that is a subdirectory of the current working directory."
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
        "why_needed": "This test prevents a potential issue where the `should_skip_path` function incorrectly identifies venv directories as being to be skipped."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 11,
          "line_ranges": "39-42, 81-85, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006862500000011096,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of _request_times should be greater than 0 after calling _prune()",
          "The length of _token_usage should be greater than 0 after calling _prune()"
        ],
        "scenario": "Verify that pruning clears request and token usage counts after a past request.",
        "why_needed": "This test prevents regression where the rate limiter incorrectly clears request and token usage counts for requests made in the past."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 26,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007644749999826672,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `next_available_in` method returns a value greater than 0",
          "The `next_available_in` method returns a value less than or equal to 60.0"
        ],
        "scenario": "Verify that the rate limiter prevents requests from exceeding a certain threshold (in this case, 1 RPM)",
        "why_needed": "This test prevents a potential issue where multiple requests are made within a short time frame and exceed the rate limit, causing the API to become unavailable."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 33,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007604130000231635,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method should return a value greater than 0 after waiting for 10 tokens.",
          "The _token_usage list should contain exactly two elements when the last token is recorded.",
          "The limiter._token_usage list should not be empty before and after recording the second token."
        ],
        "scenario": "Verify that the rate limiter prevents a regression when tokens are not yet available.",
        "why_needed": "This test verifies that the rate limiter correctly handles cases where tokens are not yet available, preventing potential regressions."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 31,
          "line_ranges": "39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0011006159999737974,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `wait_for_slot` method should be called with a mock `time.sleep` function.",
          "The `wait_for_slot` method should assert that it was called with the correct number of arguments (1).",
          "The `wait_for_slot` method should not call `mock_sleep` immediately, but instead after waiting for the specified amount of time.",
          "The `wait_for_slot` method should not return any value.",
          "The `wait_for_slot` method should raise an exception if it is called with a negative number of arguments (requests per minute).",
          "The `wait_for_slot` method should raise an exception if it is called with a non-integer argument (number of requests per minute)."
        ],
        "scenario": "Verify that the `wait_for_slot` method of `_GeminiRateLimiter` sleeps when a request is made.",
        "why_needed": "This test prevents a potential issue where the rate limiter does not sleep after a request, potentially leading to unexpected behavior or performance issues."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 6,
          "line_ranges": "39-42, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006802419999871745,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_token_usage` list of the rate limiter should be empty after calling `record_tokens(0)`.",
          "The length of `_token_usage` should be 0.",
          "No exception should be raised when no tokens are available for recording.",
          "The rate limiter's internal state should reflect that no tokens were recorded.",
          "_token_usage should not contain any token usage data.",
          "The rate limiter's `tokens_per_minute` attribute should still be valid and unchanged.",
          "The `_GeminiRateLimiter` instance should still have a valid `limits` object."
        ],
        "scenario": "Test that the rate limiter records zero tokens when no tokens are available.",
        "why_needed": "This test prevents a potential regression where the rate limiter does not record tokens even when there are none available."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0013916539999740962,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `wait_for_slot` should raise `_GeminiRateLimitExceeded` with the correct message 'requests_per_day'.",
          "The function `record_request` does not return any value.",
          "The function `wait_for_slot` should wait for a slot to become available after each request, and raise an error if no slots are available within the specified time frame.",
          "_GeminiRateLimitExceeded is raised with the correct message 'requests_per_day'.",
          "The rate limiter's daily limit is correctly checked before allowing another request.",
          "The function `wait_for_slot` does not wait for a slot to become available after each request, and raises an error if no slots are available within the specified time frame.",
          "_GeminiRateLimitExceeded is raised with the correct message 'requests_per_day'.",
          "The rate limiter's daily limit is correctly checked before allowing another request, and the function `wait_for_slot` waits for a slot to become available after each request."
        ],
        "scenario": "Test the rate limiter to prevent exceeding daily requests.",
        "why_needed": "This test prevents a potential bug where the rate limiter is exceeded by more than one request per day, causing an error."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006998189999762872,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_GeminiRateLimitConfig(tokens_per_minute=10)` sets up the correct rate limit configuration for the test environment.",
          "The `limiter.record_tokens(10)` call fills up the TPM with tokens as expected.",
          "The calculation of `wait = limiter._seconds_until_tpm_available(now, 5)` returns a non-zero value indicating that the TPM is not available within the specified time frame.",
          "The assertion `assert wait > 0` ensures that the function waits for at least some time before checking if the TPM is available again.",
          "The line `# Line 116 hit because tokens_used + request_tokens > limit AND token_usage is not empty` verifies that the rate limiter correctly detects and handles cases where the TPM is not available.",
          "The expected behavior of filling up the TPM with tokens as described in the test scenario is verified through the assertions."
        ],
        "scenario": "Verify the test_gemini_limiter_tpm_fallback_wait function covers the case when TPM wait time fallback occurs.",
        "why_needed": "This test prevents a potential regression where the rate limiter fails to detect and handle cases when the TPM is not available for a long enough period, leading to unexpected behavior or errors in the application."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 117,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-227, 233-234, 238-240, 242-243, 274-277, 280, 282-290, 292-295, 297-298, 300-301, 346, 348-350, 352-353, 381-382, 385-386"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.6143197410000312,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'models/gemini-pro' model should be in the cooldowns dictionary with a value greater than 1000.0 seconds.",
          "The cooldown value for 'models/gemini-pro' should be greater than 1000.0 seconds."
        ],
        "scenario": "Test that RPM rate limit cooldown handling is properly enforced.",
        "why_needed": "This test prevents a potential bug where the RPM rate limit cooldown is not set correctly, leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 181,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0037329880000243065,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should have the correct scenario 'Recovered Scenario',",
          "The mock post call count should be 2 (1 for the first failed request, 1 for the second successful one),",
          "The provider's _parse_response method should return a Mock object with the expected scenario and error.",
          "The provider's _annotate_internal method should not raise an exception when encountering a rate limit retry scenario."
        ],
        "scenario": "Test that the GeminiProvider annotates a rate limit retry scenario correctly.",
        "why_needed": "This test prevents regression when the provider encounters a rate limit and retries after a certain delay."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 173,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.004396901000006892,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The scenario of 'Success Scenario' is correctly extracted from the annotation.",
          "The error in the annotation is None.",
          "The annotation does not contain any errors.",
          "The annotation's scenario matches the expected value.",
          "The annotation does not contain any invalid or unexpected data.",
          "The _parse_response function returns a Mock object with the correct scenario and no error.",
          "The _annotate_internal function correctly calls _parse_response to extract the annotation."
        ],
        "scenario": "Verify that _annotate_internal returns the correct LlmAnnotation for a successful annotation.",
        "why_needed": "This test prevents regression where _parse_response might expect an incorrect format of response from _call_gemini."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 10,
          "line_ranges": "134, 136-139, 141-142, 266-267, 269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0024401660000421543,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_check_availability()` method of the `GeminiProvider` class should return `False` when no environment variables are set.",
          "The `_check_availability()` method of the `GeminiProvider` class should return `True` when environment variable `GEMINI_API_TOKEN` is set to a valid value.",
          "Environment variable changes should not affect the availability check result of the `GeminiProvider` class.",
          "Setting `GEMINI_API_TOKEN` environment variable before creating an instance of `GeminiProvider` should not change its availability check result.",
          "Creating an instance of `GeminiProvider` with a different provider name (`gemini`) should not affect its availability check result."
        ],
        "scenario": "Verifies that the `GeminiProvider` class correctly checks availability based on environment variables.",
        "why_needed": "This test prevents a potential bug where the `GeminiProvider` class does not handle environment variable changes properly, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_availability",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.00073331800001597,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method returns None after 100 requests have been recorded.",
          "The rate limiter does not allow any further requests to be processed until the daily limit is reached again (i.e., 101st request).",
          "The rate limiter prevents more than one request from being processed within a single day, ensuring consistent and predictable behavior."
        ],
        "scenario": "Verify that the rate limiter prevents exceeding the daily limit of 1 request per day.",
        "why_needed": "This test prevents a potential bug where the rate limiter allows more than one request to be processed within a single day, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008035679999807144,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method should return 0.0 for the first two requests.",
          "The next_available_in method should return 0.0 after recording the third request.",
          "The wait value should be greater than 0 and less than or equal to 60.0 seconds.",
          "_GeminiRateLimitConfig(requests_per_minute=2) should not have been called before the first two requests.",
          "_GeminiRateLimiter(limits) should have created a rate limiter instance with the specified limits."
        ],
        "scenario": "Verify that the rate limiter does not block subsequent requests after the first two have passed.",
        "why_needed": "This test prevents a potential bug where subsequent requests are blocked due to insufficient available time."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.000695876000008866,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `compute_config_hash` should return a different value for two different configurations.",
          "The hash of `config1` should not be equal to the hash of `config2`.",
          "The hash of `config1` should not be equal to the hash of `Config(provider='ollama')` (the second config is created with provider 'ollama' but its actual provider is still 'none').",
          "The hash of `Config(provider='ollama')` should not be equal to the hash of `Config(provider='none')` (the first config is created with provider 'none' but its actual provider is still 'ollama')."
        ],
        "scenario": "Test that different configurations produce different hashes.",
        "why_needed": "This test prevents a potential bug where the same configuration produces the same hash, potentially leading to inconsistencies in data storage or retrieval."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007100429999695734,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the computed hash should be exactly 16 characters.",
          "The hash value should not exceed 15 characters to prevent truncation errors.",
          "The hash value should start with '0x' prefix if it's a hexadecimal string.",
          "The hash value should contain only alphanumeric characters and underscores.",
          "No leading zeros are allowed in the hash value.",
          "The hash value should be at least 16 characters long but no more than 32 characters long."
        ],
        "scenario": "Verifies the length of the computed hash is 16 characters.",
        "why_needed": "Prevents a potential issue where the hash might be too long, potentially causing issues with storage or transmission."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 6,
          "line_ranges": "32, 44-48"
        }
      ],
      "duration": 0.0007826639999848339,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The computed SHA-256 hash of the file should be equal to the content hash.",
          "The content hash of the file should match the expected value.",
          "The file hash should not change even if the file's content is modified.",
          "The file hash should remain consistent across different runs of the test",
          "The computed SHA-256 hash of a file with a different content but same path should be equal to the content hash.",
          "The content hash of a file with a different content but same path and different name should also be equal to the content hash."
        ],
        "scenario": "Test that the computed SHA-256 hash of a file matches its content hash.",
        "why_needed": "Prevents regression where the file's content changes but the file hash remains the same."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "44-48"
        }
      ],
      "duration": 0.0008309749999853011,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the computed SHA256 hash should be 64 bytes.",
          "The first byte of the hash should match 'hello'.",
          "The second byte of the hash should match 'world'."
        ],
        "scenario": "Test the hashing function on a file with known contents.",
        "why_needed": "Prevents a potential bug where the hashing function fails to correctly hash files with non-ASCII characters or other special cases."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007040319999873645,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that two different keys produce distinct HMAC signatures for the same input.",
          "Check if the computed signature for 'key1' is not equal to the computed signature for 'key2'.",
          "Ensure the first key produces a unique signature, and the second key does not.",
          "Verify the difference in signatures between 'key1' and 'key2'.",
          "Confirm that the order of keys does not affect the generated signature.",
          "Test if using different keys results in different HMAC values for the same input.",
          "Analyze the behavior when multiple keys are used for a single computation."
        ],
        "scenario": "Test 'test_different_key': Verifies that different keys produce different signatures.",
        "why_needed": "This test prevents a potential bug where the same key is used for multiple computations, potentially leading to unexpected signature differences."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_different_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007172350000246297,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the HMAC signature should be 64 bytes.",
          "The length of the HMAC signature is not less than 32 bytes.",
          "The length of the HMAC signature is not greater than 128 bytes."
        ],
        "scenario": "Verify the length of the HMAC signature for a given content and secret key.",
        "why_needed": "This test prevents a potential issue where an attacker could exploit the length of the HMAC signature to deduce the secret key."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_with_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007519399999864618,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `compute_sha256(b'...'` should return the same hash for two identical bytes objects.",
          "The function `compute_sha256(b'...'` should raise an exception if the input is not a bytes object.",
          "The function `compute_sha256(b'...'` should produce the same hash as the built-in `hash()` function for the input."
        ],
        "scenario": "Test that the SHA-256 hash of the same input produces the same output.",
        "why_needed": "Prevents a bug where different inputs produce different hashes, potentially leading to unexpected behavior or data corruption."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_consistent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0006742470000062895,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the compute_sha256 function should be a bytes object containing 64 hexadecimal characters.",
          "The hexadecimal representation of the hash should match the expected value (e.g., '48656c6c6f20576f726c64')",
          "The length of the resulting string should be exactly 64 characters",
          "The output string should not contain any null bytes (\u0000) or other non-hexadecimal characters",
          "The hexadecimal representation of the hash should have a total length of 64 characters (16 bytes)",
          "The hash should be a valid SHA-256 hash (e.g., it should start with '48656c6c6f20576f726c64')"
        ],
        "scenario": "The hash function should produce a SHA-256 hash of the input string \"test\" which is expected to have a length of 64 characters.",
        "why_needed": "This test prevents a potential bug where the hash length is not as expected due to incorrect implementation or configuration of the compute_sha256 function."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.06734031299998833,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'pytest' package should be present in the dependency snapshot.",
          "The 'pytest' package should be listed as an item in the dependency snapshot.",
          "The presence of 'pytest' in the dependency snapshot indicates that it is available for installation.",
          "Including 'pytest' in the dependency snapshot ensures its availability for testing purposes.",
          "The absence of 'pytest' in the dependency snapshot may indicate a missing or outdated package."
        ],
        "scenario": "Verifies that the `get_dependency_snapshot` function includes the 'pytest' package.",
        "why_needed": "This test prevents a regression where the 'pytest' package is not included in the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.0734507099999746,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "snapshot is indeed a dictionary.",
          "the 'get_dependency_snapshot()' function is called and its result is stored in the `snapshot` variable.",
          "assert isinstance(snapshot, dict) is used to verify that the returned value matches the expected type.",
          "if snapshot were not a dictionary, this test would fail."
        ],
        "scenario": "The `get_dependency_snapshot()` function should return a dictionary.",
        "why_needed": "This test prevents a potential bug where the function returns an incorrect data type (e.g., list instead of dict)."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "73, 76-77, 80-81"
        }
      ],
      "duration": 0.0008279870000365008,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `load_hmac_key` function should return the expected value of the HMAC key.",
          "The `load_hmac_key` function should handle cases where the file contents are not valid JSON.",
          "The `load_hmac_key` function should raise an error if the file does not exist or is not a valid file.",
          "The `load_hmac_key` function should correctly decode the HMAC key from the file even if it's missing the expected header.",
          "The `load_hmac_key` function should handle cases where the file contains multiple keys.",
          "The `load_hmac_key` function should ignore any extra data in the file that is not related to the HMAC key.",
          "The `load_hmac_key` function should correctly handle cases where the file has a different encoding than UTF-8.",
          "The `load_hmac_key` function should raise an error if the file contains invalid JSON."
        ],
        "scenario": "Test loads HMAC key from file.",
        "why_needed": "Prevents a potential bug where the loaded key is not correctly decoded from the file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 4,
          "line_ranges": "73, 76-78"
        }
      ],
      "duration": 0.0007438259999617003,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return `None` when a key file with the specified path (`/nonexistent.key`) is provided.",
          "The function should raise a `KeyError` exception if the key file exists but cannot be loaded due to permission issues or other reasons.",
          "The test should verify that the function correctly handles cases where the key file does not exist, without attempting to load it.",
          "The function's return value should match the expected result (None) in all scenarios.",
          "The test should cover different paths for the missing key file (e.g., `/nonexistent.key`, `/nonexistent2.key`)",
          "The function's behavior should be consistent across different Python versions and platforms",
          "The test should report an error message indicating that the key file was not found, rather than raising a cryptic exception"
        ],
        "scenario": "Test 'test_missing_key_file' verifies that the function returns None when a missing key file is provided.",
        "why_needed": "This test prevents a potential bug where the function fails to return an expected result (None) when a key file does not exist."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 2,
          "line_ranges": "73-74"
        }
      ],
      "duration": 0.000713463000010961,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return `None` without attempting to load any HMAC keys.",
          "No error or exception should be raised when loading an empty configuration.",
          "The function's behavior should be consistent across different test environments.",
          "No exceptions should be thrown when calling the `load_hmac_key` method with a `Config` object that does not contain an HMAC key.",
          "The function's return value should be `None` instead of raising an exception or returning an error message."
        ],
        "scenario": "Test that the function returns `None` when no key file is provided.",
        "why_needed": "Prevents a potential bug where the function does not handle the case when no key file is configured."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007108429999789223,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.aggregate_dir should be None.",
          "config.aggregate_policy should be 'latest'.",
          "config.aggregate_include_history should be False."
        ],
        "scenario": "Test that aggregation defaults are set correctly.",
        "why_needed": "This test prevents a potential bug where the default aggregation policy is not applied correctly."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006945239999822661,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config.capture_failed_output` attribute is not equal to `False`.",
          "The `config.capture_failed_output` attribute is not equal to `None`.",
          "The `capture_failed_output` default value is set to `False`.",
          "The `capture_failed_output` default value does not match the expected behavior in all test environments.",
          "The `capture_failed_output` default value is not overridden by any environment variables or configuration files."
        ],
        "scenario": "Verify that the `capture_failed_output` default value is set to `False` for the integration gate.",
        "why_needed": "This test prevents a potential bug where the `capture_failed_output` default value might be incorrectly set to `True`, leading to unexpected behavior in the integration tests."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007167659999822718,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_default_config()` returns an instance of `Config` with an `llm_context_mode` attribute equal to 'minimal'.",
          "The value of `config.llm_context_mode` is indeed 'minimal'.",
          "If the context mode is not set to 'minimal', a different default configuration would be used.",
          "Setting the context mode to another value (e.g., 'default') would prevent this test from passing.",
          "The `llm_context_mode` attribute of the `Config` instance is not changed by setting it to an invalid value.",
          "If the context mode is set to a valid value, such as 'minimal', the function returns the expected result.",
          "A different configuration with a different context mode would be used if the default is not set correctly.",
          "The test would fail if the `llm_context_mode` attribute of the `Config` instance is changed after setting it to an invalid value."
        ],
        "scenario": "Tests the default context mode for integration gate.",
        "why_needed": "Prevents a potential bug where the context mode is not set to 'minimal' by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 4,
          "line_ranges": "107, 147, 224, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006867170000077749,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.is_llm_enabled() == False",
          "config.get_llm_enabled_value() == False",
          "get_default_config().llm_enabled() == False",
          "get_default_config().get_llm_enabled_value() == False"
        ],
        "scenario": "Verify that the LLM is not enabled by default in the configuration.",
        "why_needed": "The test prevents a potential bug where the LLM is enabled by default, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006874099999549799,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` variable is of type `dict` and has an `omit_tests_from_coverage` key with value `True`.",
          "The `config` dictionary contains the expected keys: `omit_tests_from_coverage`.",
          "The `config` dictionary does not have any other keys that could be causing issues.",
          "The `config` dictionary is a valid Python object.",
          "The `get_default_config()` function returns a configuration object with an `omit_tests_from_coverage` attribute set to `True`.",
          "The `config` variable has the expected value for the `omit_tests_from_coverage` key.",
          "The test does not fail when the default configuration includes tests by omission."
        ],
        "scenario": "Verify that the `get_default_config()` function returns a configuration object with an `omit_tests_from_coverage` attribute set to `True`.",
        "why_needed": "This test prevents a regression where the default configuration does not include tests by omission."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007618289999982153,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.provider should be equal to 'none'",
          "config.provider should not be equal to any other value",
          "config.provider should have a default value of 'none'",
          "get_default_config() should return an object with provider set to 'none'",
          "assert config.provider is None"
        ],
        "scenario": "Tests the default provider setting when it is set to None.",
        "why_needed": "Prevents a potential bug where the provider is not set to 'none' in case of privacy requirements."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006905740000320293,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_default_config()` returns an object with a list of exclude globs.",
          "Any string containing 'secret' is found in the excludes list.",
          "Any string containing '.env' is found in the excludes list.",
          "The excludes list does not contain any strings that start with 'secret' or '.env'.",
          "The function `get_default_config()` returns a list of globs that are not empty.",
          "The excludes list contains only non-empty globs.",
          "Any string containing 'secret' is found in the excludes list after filtering out empty globs.",
          "Any string containing '.env' is found in the excludes list after filtering out empty globs."
        ],
        "scenario": "Verify that secret files are excluded by default from the LLM context.",
        "why_needed": "This test prevents a potential bug where sensitive configuration files like 'secret' or '.env' might be inadvertently included in the LLM context."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 78,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 117,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.005614891999982774,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "nodeids should be sorted consistently.",
          "nodeid 'z_test.py::test_z' should come first in the list.",
          "nodeid 'a_test.py::test_a' should come second in the list.",
          "nodeid 'm_test.py::test_m' should come third in the list.",
          "The order of nodes is not affected by external factors or system updates."
        ],
        "scenario": "The test verifies that the output of the full pipeline is deterministic, i.e., the nodes are reported in a consistent order.",
        "why_needed": "This test prevents regression where the order of reports changes due to external factors or system updates."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 67,
          "line_ranges": "229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.005227648000015961,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests in the report should be zero.",
          "The summary section of the report should have a total count of zero.",
          "All other sections of the report should not contain any data."
        ],
        "scenario": "Test that an empty test suite produces a valid report.",
        "why_needed": "Prevents regression where the test suite is empty, potentially causing invalid reports."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.030103218000022025,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The HTML report should be created at the specified path.",
          "The content of the HTML report should contain the string '<html'.",
          "The content of the HTML report should contain the string 'test_pass'."
        ],
        "scenario": "The test verifies that the full pipeline generates an HTML report.",
        "why_needed": "This test prevents a potential bug where the HTML report is not generated correctly due to incorrect configuration."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/_git_info.py",
          "line_count": 2,
          "line_ranges": "2-3"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 78,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 133,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.053131870000015624,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'report_json' and 'report_html' paths are created in the test directory.",
          "A JSON file named 'report.json' is written with the correct schema version, summary statistics, and number of tests.",
          "The total number of passed tests is 1, failed tests is 1, and skipped tests is 1.",
          "The 'schema_version' field matches the expected value of SCHEMA_VERSION.",
          "The 'summary' section contains the correct data: total tests = 3, passed tests = 1, failed tests = 1, skipped tests = 1."
        ],
        "scenario": "Test that the full pipeline generates a valid JSON report.",
        "why_needed": "This test prevents regression where the full pipeline fails to generate a valid JSON report due to incorrect configuration or missing dependencies."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007472400000096968,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' field should be present in the data.",
          "The 'run_meta' field should be present in the data.",
          "The 'summary' field should be present in the data.",
          "The 'tests' field should be present in the data."
        ],
        "scenario": "Verify that the ReportRoot class has required fields.",
        "why_needed": "This test prevents a potential bug where the report root is missing essential metadata."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007143599999608341,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'is_aggregated' key should be present in the data.",
          "The 'run_count' key should be present in the data.",
          "The value of 'aggregation_policy' should only include when `is_aggregated` is True."
        ],
        "scenario": "Verify that `RunMeta` has the required 'aggregation_fields' key.",
        "why_needed": "Prevents regression where `is_aggregated` is False, potentially causing incorrect aggregation behavior."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008371979999992618,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be present in the data.",
          "The 'interrupted' field should be present in the data.",
          "The 'collect_only' field should be present in the data.",
          "The 'collected_count' field should be present in the data.",
          "The 'selected_count' field should be present in the data."
        ],
        "scenario": "Test 'RunMeta has run status fields' verifies that the RunMeta object contains status fields.",
        "why_needed": "This test prevents a potential regression where the RunMeta object is not properly initialized with status fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006784459999948922,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The schema version should be present in the test.",
          "The schema version should be a string that represents a valid semver-like format (e.g., '1.2.3').",
          "The schema version should contain at least one dot (.) character, indicating it is defined.",
          "The schema version should not start with a zero (0).",
          "The schema version should not end with a trailing period (.).",
          "The schema version should be a valid semver number (e.g., '1.2.3')."
        ],
        "scenario": "Verifies that the schema version is defined and conforms to a semver-like format.",
        "why_needed": "Prevents regression where the schema version is not defined or does not conform to a semver-like format."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 17,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006684340000333577,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field is present in the `data` dictionary.",
          "The 'outcome' field is present in the `data` dictionary.",
          "The 'duration' field is present in the `data` dictionary."
        ],
        "scenario": "The `TestSchemaCompatibility` class is tested to ensure that the `test_case_has_required_fields` method verifies the presence of required fields in a test case.",
        "why_needed": "This test prevents a potential bug where a test case may not have all necessary fields, potentially causing issues with data validation or schema compatibility."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "52-53, 245, 247, 249, 252, 257, 262-263, 265"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 7,
          "line_ranges": "134, 136-139, 141-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000711285000022599,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `__class__.__name__` attribute of the returned provider should be `GeminiProvider`.",
          "The `provider` parameter is set to `'gemini'`.",
          "The `get_provider` function correctly returns a `GeminiProvider` instance when the `provider` parameter is set to `'gemini'`.",
          "The `__class__.__name__` attribute of the returned provider matches the expected value (`GeminiProvider`)."
        ],
        "scenario": "The test verifies that the `get_provider` function correctly returns a `GeminiProvider` instance when the `provider` parameter is set to `'gemini'`.",
        "why_needed": "This test prevents a potential bug where the `get_provider` function incorrectly returns an incorrect provider type."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "52-53, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006709379999847442,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider.__class__ == 'LiteLLMProvider'",
          "provider.name == 'liteellm'",
          "provider.model == 'gpt-3.5-turbo'"
        ],
        "scenario": "The test verifies that the `get_provider` function correctly returns an instance of LiteLLMProvider when a specific provider is specified.",
        "why_needed": "This test prevents a potential bug where the correct provider is not returned if an incorrect or unsupported provider is provided."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 245, 247, 249-250"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006824929999993401,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_provider` function should return an instance of `NoopProvider` when given a configuration with 'provider='none'.",
          "The `provider` attribute of the returned `NoopProvider` instance should be set to `'none'`.",
          "The `config` object passed to `get_provider` has a valid `provider` key.",
          "The `provider` value in the `config` object is correctly converted to a string.",
          "The `provider` attribute of the resulting `NoopProvider` instance does not contain any non-'none' characters.",
          "The `provider` attribute of the resulting `NoopProvider` instance has the correct type (str).",
          "The `provider` attribute of the resulting `NoopProvider` instance is set to a string value without any quotes.",
          "The `provider` attribute of the resulting `NoopProvider` instance does not contain any non-string characters."
        ],
        "scenario": "test_get_provider_with_none_config_returns_noop",
        "why_needed": "This test prevents a potential bug where the LLM is not initialized correctly with a 'none' provider."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 245, 247, 249, 252-253, 255"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007109150000133013,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should be of type OllamaProvider.",
          "The provider's __class__ attribute should match 'OllamaProvider'."
        ],
        "scenario": "The test verifies that the OllamaProvider class is returned when the 'provider' parameter is set to 'ollama'.",
        "why_needed": "This test prevents a potential bug where an incorrect provider type is returned."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "245, 247, 249, 252, 257, 262, 267"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000724574000003031,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider(config)` should be called with a valid provider.",
          "A ValueError exception should be raised when the provider is unknown.",
          "The error message should contain the string 'unknown'.",
          "The error message should be case-insensitive (e.g., 'Unknown Provider' instead of 'unknown')."
        ],
        "scenario": "Test that unknown providers are correctly identified and cause a ValueError.",
        "why_needed": "To prevent unexpected behavior when using an unknown provider."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_unknown_raises",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007098659999655865,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Should have required methods: annotate, is_available, get_model_name, config",
          "The provider should be able to provide the model name from its configuration",
          "The provider should be able to return a boolean indicating if it's available"
        ],
        "scenario": "Test that `NoopProvider` implements the LlmProvider interface.",
        "why_needed": "Prevents a potential bug where `NoopProvider` is not implementing all required methods of LlmProvider."
      },
      "nodeid": "tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006958339999982854,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation is of type LlmAnnotation",
          "annotation scenario is empty",
          "annotation why_needed is empty",
          "annotation key_assertions are empty"
        ],
        "scenario": "The test verifies that the annotate method returns an empty LlmAnnotation object when no annotation is provided.",
        "why_needed": "This test prevents a regression where the NoopProvider does not return any annotation even if it's supposed to."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 66"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006998900000212416,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert provider.get_model_name() == ''",
          "assert isinstance(provider.get_model_name(), str)",
          "assert provider.get_model_name().startswith('')",
          "assert provider.get_model_name().endswith('')",
          "assert len(provider.get_model_name()) == 0",
          "assert provider.get_model_name() != 'noop'  # This is not the expected result"
        ],
        "scenario": "The `get_model_name` method of the `NoopProvider` class is called with an empty configuration.",
        "why_needed": "Without this test, a bug or regression may occur where the `get_model_name` method returns an empty string when given an empty configuration."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 107, 110-111"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 58"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007893420000186779,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method should return True for any valid configuration.",
          "The `is_available()` method should raise an exception if there are any invalid configurations.",
          "The `is_available()` method should call the underlying provider's `__call__` method without raising any exceptions.",
          "The `is_available()` method should not throw a `ValueError` exception when called with a valid configuration.",
          "The `is_available()` method should not raise an exception when called with an invalid configuration.",
          "The NoopProvider instance should be created successfully for each test case.",
          "The provider's underlying implementation should be available without any issues."
        ],
        "scenario": "Verify that the NoopProvider instance is available.",
        "why_needed": "Prevents a potential bug where the provider might not be available due to configuration issues or other internal reasons."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_is_available",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 65,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.001005927999983669,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider` from `pytest_llm_report.llm.annotator` is called with a valid configuration.",
          "The `test_case` nodeid matches the expected scenario.",
          "The annotation summary is printed when annotations run successfully."
        ],
        "scenario": "The test verifies that the annotation summary is printed when annotations run.",
        "why_needed": "This test prevents regression where the annotation summary is not printed, potentially causing confusion or errors in the testing process."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_emits_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0009749289999945177,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The LLM annotation should report progress for each test.",
          "The LLM annotation should display the correct test ID in its message.",
          "The LLM annotation should append a string to the messages list indicating the start of annotations for the current test.",
          "The LLM annotation should not append any additional strings beyond the initial message.",
          "The progress callback should be called with the correct number of tests annotated.",
          "The progress callback should call the provider with the correct test ID.",
          "The provider should have a valid cache directory.",
          "The test result outcome should be 'passed' for this test."
        ],
        "scenario": "Test LLM annotator progress reporting for multiple tests.",
        "why_needed": "This test prevents regression where the LLM annotation progress is not reported correctly when running multiple tests simultaneously."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_reports_progress",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 65,
          "line_ranges": "45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0009518759999878057,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' attribute is set to True for tests with this scenario.",
          "The LLM annotation is only called for tests where 'llm_opt_out' is False.",
          "No LLM annotations are called for tests where 'llm_opt_out' is True or the maximum number of tests has been reached.",
          "The provider function returns a FakeProvider instance that calls the correct provider.",
          "The LLM annotation is not called for test with this scenario and llm_opt_out=False.",
          "No LLM annotations are called for all tests where llm_opt_out=True or the maximum number of tests has been reached.",
          "The 'llm_max_tests' attribute is set to 1 in the config, which limits the number of tests to 1.",
          "The LLM annotation is only called once per test with this scenario and llm_opt_out=False.",
          "No LLM annotations are called for all tests where llm_opt_out=True or the maximum number of tests has been reached."
        ],
        "scenario": "Test that LLM annotations respect opt-out and limit settings.",
        "why_needed": "This test prevents regression by ensuring LLM annotations do not skip opt-out tests or exceed the maximum number of tests."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0010914559999832818,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider's calls should match the expected list of node IDs.",
          "The sleep function should be called twice with values 2.0 and 4.0 respectively.",
          "The calls should not exceed the configured requests-per-minute rate limit (30).",
          "The calls should include all nodes specified in the tests (tests/test_a.py::test_a and tests/test_b.py::test_b)."
        ],
        "scenario": "Test that LLM annotations respect the requests-per-minute rate limit.",
        "why_needed": "This test prevents a potential regression where LLM annotations may not respect the requests-per-minute rate limit, leading to inaccurate or delayed results."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "45, 48-52, 54"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008033889999978783,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available` method of the `UnavailableProvider` class returns False.",
          "The `get_provider` function from `pytest_llm_report.llm.annotator` calls the `is_available` method of the `UnavailableProvider` class with the provided configuration.",
          "When a provider is unavailable, the `is_available` method should return False.",
          "The test should fail when an unavailable provider is used to annotate tests.",
          "The message 'is not available' should be printed when an unavailable provider is used to annotate tests.",
          "The `annote` function from `pytest_llm_report.llm.annotator` should call the `get_provider` function with a configuration that includes the `UnavailableProvider` class.",
          "When an unavailable provider is used to annotate tests, the `annote` function should not be able to proceed without skipping the annotation."
        ],
        "scenario": "Tests for annotating tests with unavailable providers should be skipped.",
        "why_needed": "This test prevents regression by ensuring that annotation is skipped when a provider is unavailable."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 30,
          "line_ranges": "39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0011695099999542435,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider.calls` assertion checks if the provider was called before annotating tests.",
          "The `test.llm_annotation` assertion checks if the annotation is set correctly and matches the scenario.",
          "The `provider_next.annotate` assertion checks if the annotation is not called when it should be."
        ],
        "scenario": "Test that annotations are cached between runs and that the annotation is used when it should be.",
        "why_needed": "This test prevents a regression where annotations are not being used as expected due to caching issues."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_uses_cache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006715199999689503,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' key should be present in the ANNOTATION_JSON_SCHEMA.",
          "The 'why_needed' key should also be present in the ANNOTATION_JSON_SCHEMA.",
          "If 'scenario' and 'why_needed' keys are not present, the test will fail with an error message indicating a required field is missing."
        ],
        "scenario": "The test verifies that the `test_required_fields` function checks for the presence of 'scenario' and 'why_needed' keys in the ANNOTATION_JSON_SCHEMA.",
        "why_needed": "This test prevents a potential bug where the schema is not correctly validated if the required fields are missing or empty."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006855820000168933,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "checks password",
          "checks username",
          "schema.scenario matches 'Tests user login'",
          "schema.why_needed matches 'Prevents auth bypass'",
          "len(schema.key_assertions) is equal to 2",
          "schema.confidence is greater than or equal to 0.95"
        ],
        "scenario": "The test verifies that the AnnotationSchema.from_dict method correctly parses a dictionary into an annotation.",
        "why_needed": "This test prevents potential bugs or regressions in the AnnotationSchema class where it may not be able to parse dictionaries correctly."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007087280000064311,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "schema.scenario = \"\" (empty string)",
          "schema.why_needed = \"\" (empty string)\"\"",
          "assert schema.scenario == \"\" (checks if the assertion matches an expected value)",
          "assert schema.why_needed == \"\" (checks if the assertion matches an expected value)",
          "assert isinstance(schema, AnnotationSchema) (checks if the assertion is a valid instance of AnnotationSchema)",
          "schema.from_dict({}) should raise a ValueError or return None (checks if the from_dict method raises an exception or returns None)"
        ],
        "scenario": "The test verifies that the AnnotationSchema handles an empty input by creating a schema from an empty dictionary.",
        "why_needed": "This test prevents regression where the AnnotationSchema is not correctly handling empty inputs, potentially leading to incorrect validation or errors."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006945430000087072,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The schema's 'scenario' property is set to 'Partial only'.",
          "The schema's 'why_needed' property is empty, indicating that no specific bug or regression was prevented by this test."
        ],
        "scenario": "The test verifies that the AnnotationSchema.from_dict method correctly handles a partial input scenario.",
        "why_needed": "This test prevents bugs or regressions where the AnnotationSchema does not handle partial inputs correctly."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007540130000052159,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' field should be present in the schema's properties.",
          "The 'why_needed' field should also be present in the schema's properties.",
          "The 'key_assertions' field should contain assertions about the required fields within the schema."
        ],
        "scenario": "The test verifies that the schema has required fields.",
        "why_needed": "This test prevents a potential bug where the schema is not properly defined with required fields, potentially leading to incorrect validation of contract data."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "90-92, 94-96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007068880000247191,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "data['scenario'] == 'Tests feature X'",
          "data['why_needed'] == 'Prevents bug Y'",
          "data['key_assertions'] in data"
        ],
        "scenario": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
        "why_needed": "Prevents regression of bug Y when schema is serialized to dict."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 245, 247, 249-250"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007031089999713913,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider(config)` returns an instance of `NoopProvider` when the configuration has a 'provider' set to 'none'.",
          "The `NoopProvider` instance is correctly created and assigned to the variable `provider`.",
          "The `isinstance(provider, NoopProvider)` assertion passes, indicating that the correct class is returned.",
          "The test does not fail when using a factory configuration with 'provider' set to 'none'.",
          "The `get_provider(config)` function handles cases where the provider is not specified correctly.",
          "No exceptions are raised during the execution of this test.",
          "The test covers all possible scenarios for the given scenario."
        ],
        "scenario": "Verify that the `NoopProvider` is returned when a factory configuration with 'provider' set to 'none' is used.",
        "why_needed": "This test prevents a potential regression where the `NoopProvider` is not returned for provider='none'."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006927349999727994,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` variable should be an instance of `LlmProvider`.",
          "The `provider` variable should not have any attributes or methods other than those inherited from `LlmProvider`.",
          "The `provider` variable should not inherit any attributes or methods from the `NoopProvider` class.",
          "The `provider` variable's type should be correctly set to `LlmProvider` using the `isinstance()` function.",
          "Any additional attributes or methods in the `provider` object should be removed after inheritance.",
          "The `provider` object should not have any unexpected behavior when accessed through its methods."
        ],
        "scenario": "The `test_noop_is_llm_provider` test verifies that the `NoopProvider` class correctly inherits from `LlmProvider`.",
        "why_needed": "This test prevents a potential bug where the `NoopProvider` class is mistakenly implemented as an LLM provider instead of a no-op."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000717891000022064,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result.scenario == \"\" (empty string)",
          "assert result.why_needed == \"\" (empty string)",
          "assert result.key_assertions == [] (no key assertions performed)"
        ],
        "scenario": "The NoopProvider function should return an empty annotation when the provided test case does not match any known node IDs.",
        "why_needed": "This test prevents a regression where the NoopProvider returns incorrect annotations for tests that do not match any known node IDs."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007175490000008722,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result has the attribute 'scenario' and it is set to 'Annotate returns LlmAnnotation-like object.'",
          "The result has the attribute 'why_needed' and it is set to 'This test prevents a potential regression where the `annotate` method does not return the expected annotation.'",
          "The result has the attribute 'key_assertions' and it contains the expected checks"
        ],
        "scenario": "Verify that the `annotate` method of `NoopProvider` returns an LlmAnnotation-like object with the correct attributes.",
        "why_needed": "This test prevents a potential regression where the `annotate` method does not return the expected annotation."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007115920000160258,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should return a non-empty result for an empty code.",
          "The provider should not raise any errors or exceptions when handling empty code.",
          "The provider should be able to correctly annotate the test with a valid outcome.",
          "The annotation should include the nodeid and outcome of the test.",
          "The configuration should not affect the behavior of the provider.",
          "The result should not be None, indicating that the contract handled the empty code successfully."
        ],
        "scenario": "The test verifies that the ProviderContract handles an empty code by returning a valid result.",
        "why_needed": "This test prevents a potential regression where an empty code would cause the contract to fail."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006895280000094317,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider.annotate()` method should return `None` instead of raising an exception.",
          "The `provider.annotate()` method should not raise an exception if the input `test` is `None`.",
          "The `provider.annotate()` method should correctly handle the case where the input `test` is `None` and returns a valid `TestCaseResult` object.",
          "The `provider.annotate()` method should not throw any exceptions when handling a `None` context.",
          "The `provider.annotate()` method should preserve the original value of the `nodeid` attribute in the `TestCaseResult` object.",
          "The `provider.annotate()` method should preserve the original value of the `outcome` attribute in the `TestCaseResult` object.",
          "The `provider.annotate()` method should not modify the original values of these attributes in the `TestCaseResult` object."
        ],
        "scenario": "The test verifies that the `provider` handles a `None` context gracefully by annotating a `TestCaseResult` with an empty string.",
        "why_needed": "This test prevents a potential regression where the provider might throw an error when handling a `None` context."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 15,
          "line_ranges": "52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 7,
          "line_ranges": "134, 136-139, 141-142"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008628960000010011,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider_name in ['none', 'ollama', 'litellm', 'gemini']",
          "hasattr(provider, 'annotate')",
          "callable(provider.annotate)",
          "provider.annotate() should return a callable object"
        ],
        "scenario": "All providers should have an annotate method.",
        "why_needed": "This test prevents a potential bug where providers might not be able to annotate data correctly."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "52-53, 72, 75-76, 80, 165, 167, 175"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 155,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 233, 245-248, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 346, 348-350, 352-355, 360-363, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417-418, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007942579999848931,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `context_size` attribute of the `GeminiProvider` instance should be less than or equal to 1000.",
          "The `annotate` method should not raise a `MemoryError` when called with a context that exceeds 1000 bytes in size.",
          "The `context_size` attribute of the `GeminiProvider` instance should be updated correctly after calling the `annotate` method."
        ],
        "scenario": "The `annotate` method of the `GeminiProvider` class is being tested when it handles a context that is too large.",
        "why_needed": "This test prevents a potential bug where the `annotate` method fails with an exception due to an excessive memory usage in cases with very large contexts."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134, 136-139, 141-142, 160-164"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007457870000280309,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'litellm not installed. Install with: pip install litellm'",
          "provider.annotate(test, 'def test_case(): assert True')",
          "test.test_case() should raise a mock_import_error('litellm')"
        ],
        "scenario": "The LiteLLM provider should report a missing dependency error when the required package is not installed.",
        "why_needed": "This test prevents a potential bug where the provider incorrectly reports an installation issue without providing any useful information about the actual problem."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134, 136-139, 141-142, 160-161, 167-169"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007421949999866229,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should raise a ValueError indicating that GEMINI_API_TOKEN is not set.",
          "The annotation should include a message explaining the expected behavior (i.e., 'GEMINI_API_TOKEN is not set')",
          "The annotation should provide a clear indication of what needs to be set (API token) before annotating a test",
          "The annotation should return an error code indicating that GEMINI_API_TOKEN is missing",
          "The annotation should include the actual API token value if it exists"
        ],
        "scenario": "Test that the GeminiProvider annotates missing tokens correctly.",
        "why_needed": "To prevent a TypeError when trying to annotate a test with an undefined API token."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 183,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-343, 346, 348-350, 352-355, 360-366, 368, 370-371, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008931329999768423,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'annotate' method of the GeminiProvider is called with a test function that checks for token usage.",
          "The 'annotate' method logs usage metadata, including the total number of tokens recorded.",
          "The 'annotate' method verifies if the limiter has at least one record of token usage.",
          "The limiter's token usage is verified to be 1 token with a count of 123.",
          "The rate limits logic is tested by verifying it ran without error."
        ],
        "scenario": "Verify Gemini provider annotates records tokens correctly.",
        "why_needed": "Prevents regression in token usage logging."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 181,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 233-234, 238-240, 242-243, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330-333, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009728459999678307,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `rate_limit` attribute of the `GeminiProvider` instance is set correctly before calling `annotate_retries_on_rate_limit`.",
          "The `annotate_retries_on_rate_limit` method retries annotating requests when rate limiting is exceeded. The retry attempts are limited to a reasonable number of times.",
          "The `rate_limit` attribute is reset after each retry attempt, allowing the service to recover from rate limiting issues. If `rate_limit` is not reset, it may lead to infinite retries and potential service degradation.",
          "The `annotate_retries_on_rate_limit` method does not raise an exception when rate limiting is exceeded, preventing the test from failing due to expected behavior.",
          "The `annotate_retries_on_rate_limit` method uses a reasonable number of retry attempts (e.g., 3-5) before giving up and returning without annotating the request. This helps prevent overwhelming the service with retries and potential denial-of-service attacks.",
          "The `rate_limit` attribute is not reset after each retry attempt, allowing the service to recover from rate limiting issues. If `rate_limit` is not reset, it may lead to infinite retries and potential service degradation.",
          "The `annotate_retries_on_rate_limit` method does not log any relevant information when rate limiting is exceeded, preventing the test from detecting potential issues or errors in the service."
        ],
        "scenario": "The `annotate_retries_on_rate_limit` method of the `GeminiProvider` class should retry annotating requests when rate limiting is exceeded.",
        "why_needed": "This test prevents a potential issue where the `annotate_retries_on_rate_limit` method does not retry after exceeding the rate limit, potentially causing the service to become unresponsive or fail with an error."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 177,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419-420, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009440900000186048,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotate` method should rotate models on the daily limit.",
          "The `rotate_models_on_daily_limit` fixture should be able to rotate models without any issues.",
          "The model rotation is applied correctly and does not exceed the daily limit."
        ],
        "scenario": "The `annotate` method of the `GeminiProvider` class rotates models on a daily limit when used with the `rotate_models_on_daily_limit` fixture.",
        "why_needed": "This test prevents a potential bug where the model rotation is not applied correctly due to an incorrect implementation of the `rotate_models_on_daily_limit` fixture."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 184,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009069239999917045,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotate function should skip any annotations that would exceed the daily limit.",
          "Any annotations that are created after the daily limit has been exceeded should be skipped.",
          "The annotate function should not attempt to create new annotations when the daily limit is reached.",
          "The annotation count should decrease by 1 when the daily limit is exceeded.",
          "The total number of annotations created should be less than or equal to the daily limit after exceeding it.",
          "The annotate function should throw an exception when the daily limit is exceeded and no new annotations are allowed.",
          "Any exceptions thrown during annotation creation should not propagate up the call stack."
        ],
        "scenario": "The test verifies that the annotate function skips annotations when the daily limit is exceeded.",
        "why_needed": "This test prevents a regression where the annotate function does not skip annotations due to the daily limit being reached."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 177,
          "line_ranges": "39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-377, 381-382, 385-387, 391-392, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009605719999967732,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotated response contains the expected scenario and why-need information.",
          "The annotated response includes the correct key assertions.",
          "The confidence level is set to a reasonable value (0.8 in this case).",
          "The captured model is correctly identified as 'gpt-4o'.",
          "The system role is correctly associated with the message 'def test_login()'.",
          "The tests/test_auth.py::test_login message is present in the response.",
          "The successful login scenario is included in the response content."
        ],
        "scenario": "Test that LiteLLM provider annotates successful responses correctly.",
        "why_needed": "Prevents regressions caused by incorrect annotation of failed responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 190,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-227, 252-254, 274-277, 280-283, 286-290, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368, 370, 372-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0010013990000175,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The model's performance metrics (e.g., accuracy, F1 score) should return to normal or near-normal values after 24 hours.",
          "The model's inference time should decrease significantly after 24 hours.",
          "The model's memory usage should decrease significantly after 24 hours.",
          "The model's warnings and errors should be cleared after 24 hours.",
          "The model's training data should not have been exhausted within the last 24 hours (if applicable).",
          "The model's inference requests should be able to complete successfully without any timeouts or exceptions."
        ],
        "scenario": "The test verifies that the exhausted model recovers after 24 hours.",
        "why_needed": "This test prevents a regression where the model does not recover from exhaustion within 24 hours."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 65,
          "line_ranges": "134, 136-139, 141-142, 280, 282-283, 286-290, 292-295, 297-298, 300-301, 346, 348-350, 352-355, 360-363, 374-377, 385, 387, 391-392, 396-402, 405, 408-410, 412-414, 417-418, 428, 430-432, 435-436"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.00074640000002546,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assertRaisesRegex with 'GeminiError' and 'No models available.'",
          "assertRaisesRegex with 'GeminiError' and 'No models found.'",
          "assertRaisesRegex with 'GeminiError' and 'Model not found in database.'"
        ],
        "scenario": "The 'fetch_available_models' method of the Gemini provider returns an error when no models are available.",
        "why_needed": "This test prevents a regression where the 'fetch_available_models' method returns an error instead of raising a meaningful exception."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 169,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-227, 274-277, 280-283, 286, 292-295, 297-298, 300-301, 315, 317-320, 322-325, 327-328, 330, 335-341, 343, 346, 348-350, 352-355, 360-366, 368-369, 374-377, 381-382, 385-387, 391-393, 396-399, 401-402, 405, 408-410, 412-414, 417, 419, 421-424, 428, 430-434, 437-440, 442-443, 445-447"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009876770000118995,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "model_list is updated with new models after the specified interval.",
          "no exception is raised if no models are available to update.",
          "models are only added to the list if they have been trained within the specified interval.",
          "the model list size does not exceed a certain threshold.",
          "the test can be run in parallel without affecting each other's results.",
          "the refresh interval can be adjusted dynamically based on system load."
        ],
        "scenario": "The model list should refresh after a specified interval.",
        "why_needed": "This test prevents regression when the model is not refreshed immediately after an interval."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 22,
          "line_ranges": "37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 6.001552891000017,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'boom' in annotation.error",
          "annotation.error is an instance of RuntimeError",
          "annotation.error contains the string 'boom'",
          "annotation.error is raised by fake_completion()",
          "fake_completion raises a RuntimeError",
          "fake_completion() does not raise any other exception"
        ],
        "scenario": "The test verifies that the LiteLLMProvider annotates completion errors in the annotation.",
        "why_needed": "This test prevents a regression where the LiteLLM provider does not surface completion errors in annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 25,
          "line_ranges": "37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69, 73, 76, 81-82, 84-85"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 6.001435610999977,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'key_assertions' parameter must be a list.",
          "Invalid response: key_assertions must be a list",
          "Key assertion error message should include the expected format."
        ],
        "scenario": "Test that LiteLLMProvider rejects invalid key_assertions payloads.",
        "why_needed": "To prevent the test from passing when an invalid key_assertions payload is provided."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 5,
          "line_ranges": "37-41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007785529999750906,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.error == 'litellm not installed. Install with: pip install litellm'",
          "provider.annotate(test, \"def test_case(): assert True\")",
          "test.test_case() is False"
        ],
        "scenario": "The LiteLLMProvider annotates a missing dependency in the provided test case.",
        "why_needed": "This test prevents a potential bug where the provider does not report an error for a missing dependency, potentially leading to silent failures or incorrect results."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 20,
          "line_ranges": "37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008744169999772566,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation contains the correct scenario.",
          "The annotation contains the correct why needed message.",
          "The annotation contains the correct key assertions.",
          "The annotation has a confidence level of 0.8.",
          "The captured model is 'gpt-4o'.",
          "The captured messages contain the expected system role and function calls.",
          "The captured messages contain the expected test login function call."
        ],
        "scenario": "Test that the LiteLLM provider annotates a successful response with mock data.",
        "why_needed": "Prevents regression due to fake completion of LiteLLMProvider."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 107, 110-111"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "94-95, 97"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007308009999746901,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method of the `LiteLLMProvider` class should return True when the 'litellm' module is available in the system's modules.",
          "The `is_available()` method should raise an error if the 'litellm' module is not installed or not found in the system's modules.",
          "The provider should correctly detect the presence of the 'litellm' module even if it is not a standard Python package.",
          "The provider should handle cases where the 'litellm' module is installed but not imported as a module (e.g., as a package)",
          "The provider should raise an error when trying to import the 'litellm' module as a module, indicating that it is not available",
          "The provider should correctly handle cases where the 'litellm' module has been removed from the system's modules"
        ],
        "scenario": "Test that the LiteLLM provider detects installed modules.",
        "why_needed": "Prevents a potential bug where the provider does not detect installed modules."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 33,
          "line_ranges": "52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 15,
          "line_ranges": "40-41, 47, 50, 52, 54-55, 57-60, 62-63, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008064750000471577,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "If the input `context` is longer than the maximum allowed length, the function should return an error message.",
          "The function should raise a ValueError with a meaningful error message when the input `context` is too long.",
          "The function should not silently ignore the input `context` and instead raise an exception.",
          "The function should provide a clear and descriptive error message that explains why the context length was exceeded.",
          "The function should handle cases where the input `context` is an empty string or None.",
          "The function should return an error message with a specific format (e.g., 'Context too long: ...')",
          "The function should raise an exception with a specific error code (e.g., 'EXC_CONTEXT_LENGTH_ERROR')"
        ],
        "scenario": "The test verifies that the annotate method handles context length errors correctly.",
        "why_needed": "This test prevents a potential regression where the annotate method fails to handle context length errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "40-41, 47, 50, 52, 54-55, 57-59, 71-72, 74-75, 77-78"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007622140000194122,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should indicate that the call was not successful and returned an error.",
          "The error message should be 'Failed after 3 retries. Last error: boom'.",
          "The test should pass even if the system prompt is different from the Ollama provider's system prompt.",
          "The test should fail with a non-zero exit code (e.g., 1) when the call to Ollama provider fails.",
          "The annotation should not be affected by the number of retries.",
          "The annotation should indicate that the call was not successful and returned an error even if the system prompt is different from the Ollama provider's system prompt.",
          "The test should fail with a non-zero exit code (e.g., 1) when the call to Ollama provider fails, regardless of the number of retries."
        ],
        "scenario": "Test OllamaProvider::test_annotate_handles_call_error verifies that the annotation of a call error to Ollama provider prevents regression.",
        "why_needed": "This test prevents regression in case of call errors to Ollama provider, ensuring the correctness of annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "40-44"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.00076320300001953,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation message should include the correct error message for installing httpx.",
          "The annotation message should not be empty.",
          "The annotation message should contain the exact string 'httpx not installed.'",
          "The annotation message should not contain any other relevant information that could lead to incorrect error messages.",
          "The annotation message should not be too long and only include the necessary information for installation.",
          "The annotation message should not contain any typos or grammatical errors.",
          "The annotation message should provide a clear and concise explanation of what needs to be installed."
        ],
        "scenario": "The Ollama provider should report an error when annotating a function that uses the missing httpx dependency.",
        "why_needed": "This test prevents a potential bug where the provider incorrectly reports a non-existent dependency, potentially leading to incorrect or misleading error messages."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 29,
          "line_ranges": "40-41, 47, 50, 52, 54-55, 57-60, 62-63, 114, 116-123, 127-130, 132, 134-135"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008093229999985851,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify status code and validate token response",
          "Check if the response contains a valid JSON object",
          "Assert that the 'response' key is present in the JSON object",
          "Verify that the 'why_needed' field matches the expected reason for failure",
          "Validate that the 'key_assertions' list includes all necessary checks"
        ],
        "scenario": "Test the full annotation flow of Ollama provider with mocked HTTP.",
        "why_needed": "Prevents authentication-related bugs in the annotator."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "114, 116-123, 127-130, 132, 134-135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007673510000358874,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider._call_ollama` method returns the expected response from the Ollama model.",
          "The `captured.json['model']` attribute matches the provided `model` in the configuration.",
          "The `captured.json['prompt']` attribute matches the provided `prompt` in the configuration.",
          "The `captured.json['system']` attribute matches the provided `system` in the configuration.",
          "The `captured.json['stream']` attribute is set to `False` as expected.",
          "The `timeout` attribute is set to 60 seconds as expected."
        ],
        "scenario": "The Ollama provider makes a successful API call to generate text.",
        "why_needed": "This test prevents regression where the Ollama provider fails to make a correct API call."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "114, 116-123, 127-130, 132, 134-135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008678469999949812,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured response from the API should contain the default model.",
          "The captured response from the API should be 'ok'.",
          "The captured response from the API should have a 'model' key with value 'llama3.2'."
        ],
        "scenario": "Test that the default model is used when not specified for Ollama provider.",
        "why_needed": "This test prevents a regression where the default model is not used as expected."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 6,
          "line_ranges": "87-88, 90-91, 93-94"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007219059999670208,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _check_availability() of the OllamaProvider instance should raise a ConnectionError exception.",
          "The function _check_availability() of the OllamaProvider instance should return False.",
          "The function _check_availability() of the OllamaProvider instance should not have any other return value.",
          "The function _check_availability() of the OllamaProvider instance should not raise a TypeError exception.",
          "The function _check_availability() of the OllamaProvider instance should not be able to return True.",
          "The function _check_availability() of the OllamaProvider instance should not have any side effects."
        ],
        "scenario": "The test verifies that the Ollama provider returns False when the server is unavailable.",
        "why_needed": "This test prevents a regression where the provider fails to return an error when the server is not running."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "87-88, 90-92"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000726842000005945,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert provider._check_availability() is False",
          "assert FakeResponse().status_code == 500",
          "assert config.provider != 'ollama'",
          "assert isinstance(provider, OllamaProvider)",
          "assert isinstance(config, Config)",
          "assert isinstance(fake_httpx, SimpleNamespace)",
          "assert fake_get.__name__ == 'fake_get'",
          "assert fake_get.__doc__ is None"
        ],
        "scenario": "Test that the Ollama provider returns False for non-200 status codes.",
        "why_needed": "To prevent a regression where the provider incorrectly reports availability when it's not available (status code 500)."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "87-88, 90-92"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006980219999945803,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '/api/tags' URL is present in the provided host.",
          "The response status code is 200 (OK) for the '/api/tags' endpoint.",
          "The 'ollama_host' configuration parameter is set correctly to 'http://localhost:11434'.",
          "The provider's `_check_availability()` method returns True."
        ],
        "scenario": "Verifies that the Ollama provider checks availability via /api/tags endpoint successfully.",
        "why_needed": "Prevents a potential bug where the provider fails to check availability when it's not available."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 1,
          "line_ranges": "102"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007223519999683958,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider is an instance of OllamaProvider",
          "is_local() returns True for the provided config"
        ],
        "scenario": "The Ollama provider is correctly identified as local.",
        "why_needed": "This test prevents a potential bug where the provider might be incorrectly identified as non-local."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "52-53, 186-187, 190-192"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007085580000421032,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotation.error` attribute is set to 'Failed to parse LLM response as JSON'.",
          "The `provider._parse_response('not-json')` method returns an instance of `OllamaProviderError`.",
          "The `annotation.error` attribute contains the string 'Failed to parse LLM response as JSON'."
        ],
        "scenario": "The test verifies that the `OllamaProvider` class throws an error when parsing a response with invalid JSON.",
        "why_needed": "This test prevents a potential bug where the Ollama provider incorrectly reports valid responses as having an error."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007303559999627396,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "- The 'key_assertions' field must be a list.",
          "- Invalid values are not allowed in this field."
        ],
        "scenario": "The test verifies that the OllamaProvider rejects invalid key_assertions payloads in its _parse_response method.",
        "why_needed": "This test prevents regression where the provider incorrectly accepts or ignores invalid key_assertions payloads."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007345579999764595,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The response is not empty.",
          "The response contains valid JSON syntax.",
          "The response does not contain any non-JSON characters (e.g. whitespace, special characters).",
          "The response does not contain any invalid JSON syntax (e.g. missing or mismatched brackets, quotes).",
          "The response is a valid JSON object (i.e. an object with a 'text' property and optional 'metadata' properties).",
          "The provider correctly handles nested objects and arrays within the JSON.",
          "The provider correctly handles quoted strings within the JSON.",
          "The provider does not attempt to parse any invalid or malformed JSON."
        ],
        "scenario": "The provided test verifies that the Ollama provider correctly extracts JSON from markdown code fences.",
        "why_needed": "This test prevents a potential bug where the provider fails to parse JSON in code fences, potentially leading to incorrect or incomplete annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007243789999620276,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "...",
          "The extracted JSON should be valid JSON syntax without any extra characters or whitespace.",
          "The response should not contain any language-specific keywords or phrases.",
          "...",
          "...",
          "The provider's error message should indicate that no language was specified for the fence."
        ],
        "scenario": "The provided test verifies that the Ollama provider correctly parses a JSON response in a plain fence without any language specification.",
        "why_needed": "This test prevents a potential bug where the provider fails to extract JSON from plain fences with no specified language."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007332380000093508,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.scenario == 'Tests feature'",
          "assert annotation.why_needed == 'Stops bugs'",
          "assert annotation.key_assertions == ['assert a', 'assert b']",
          "assert annotation.confidence == 0.8"
        ],
        "scenario": "Test Ollama provider parses valid JSON responses and verifies correct configuration.",
        "why_needed": "Prevents bugs in the Ollama provider by ensuring it correctly configures itself with a valid response."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "254-257"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006948110000166707,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the serialized dictionary should be exactly 'src/foo.py'.",
          "The 'line_ranges' key in the serialized dictionary should match the provided string.",
          "The 'line_count' key in the serialized dictionary should be equal to the expected value of 10."
        ],
        "scenario": "Test that `CoverageEntry` correctly serializes to a dictionary.",
        "why_needed": "This test prevents a bug where the serialization of `CoverageEntry` is incorrect."
      },
      "nodeid": "tests/test_models.py::TestArtifactEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 3,
          "line_ranges": "207-209"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006965260000129092,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `file_path` key in the serialized dictionary should match the original value.",
          "The `line_ranges` key in the serialized dictionary should match the original value.",
          "The `line_count` key in the serialized dictionary should match the original value.",
          "Any missing keys (e.g. `coverage_type`, `start_line`, etc.) should be ignored or raise an error."
        ],
        "scenario": "Tests the `to_dict()` method of CoverageEntry to ensure it correctly serializes a coverage entry.",
        "why_needed": "This test prevents regressions where the `to_dict()` method fails to serialize a coverage entry with invalid or missing data."
      },
      "nodeid": "tests/test_models.py::TestCollectionError::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "40-43"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007012080000095011,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key should match the expected value.",
          "The 'line_ranges' key should contain the correct ranges and values.",
          "The 'line_count' key should match the expected value.",
          "A KeyError should be raised if the 'file_path', 'line_ranges', or 'line_count' keys are missing from the dictionary."
        ],
        "scenario": "Test CoverageEntry to_dict serialization correctness.",
        "why_needed": "This test prevents a bug where the coverage entry is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007239729999923838,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.scenario == \"\" (empty string)",
          "annotation.why_needed == \"Empty annotation should have default values.\" (description of why it needs to be tested)",
          "annotation.key_assertions == [] (expected empty list for key assertions)",
          "assert annotation.confidence is None (expected confidence to be None for an empty annotation)",
          "assert annotation.error is None (expected error to be None for an empty annotation)"
        ],
        "scenario": "An empty annotation should be created with default values.",
        "why_needed": "This test prevents a regression where an empty annotation would have no effect on the model's performance."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "104-107, 109, 111, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006838340000285825,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' field should be present in the dictionary.",
          "The 'why_needed' field should also be present in the dictionary.",
          "The 'key_assertions' field should not include the 'confidence' field, as it is an optional attribute."
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.",
        "why_needed": "This test prevents regression by ensuring that the minimal annotation is properly serialized without any optional or missing fields."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 10,
          "line_ranges": "104-107, 109-111, 113-115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006745650000539172,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts that the 'scenario' field is present and matches the expected value.",
          "Asserts that the 'confidence' field has a value greater than or equal to 0.95.",
          "Asserts that the 'context_summary' field contains the correct mode ('minimal') and bytes count (1000)."
        ],
        "scenario": "Test to dictionary with all fields",
        "why_needed": "Prevents data loss due to missing fields in the output."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007378769999490942,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' key in the report dictionary should be equal to SCHEMA_VERSION.",
          "The 'tests' key in the report dictionary should be an empty list.",
          "The 'warnings' key in the report dictionary should not exist (i.e., its value should be None).",
          "The 'collection_errors' key in the report dictionary should not exist (i.e., its value should be None)."
        ],
        "scenario": "Test default report schema version and empty lists.",
        "why_needed": "Prevents regression by ensuring the default report has a valid schema version and no collection errors or warnings."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_default_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "207-209, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508-510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007136010000294846,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'collection_errors' key in the report dictionary should be present and contain exactly one error.",
          "The 'nodeid' value of the first error in the 'collection_errors' list should match the provided node id.",
          "All other values in the 'collection_errors' list (if any) should have a valid 'message' property."
        ],
        "scenario": "Test Report with Collection Errors should include them.",
        "why_needed": "This test prevents a regression where the report does not include collection errors."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 60,
          "line_ranges": "229-231, 233, 235, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006875359999867214,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the 'warnings' list should be exactly 1.",
          "The first element of the 'warnings' list should have code 'W001'."
        ],
        "scenario": "Test reports the presence of warnings in a ReportRoot instance.",
        "why_needed": "This test prevents a regression where warnings are not reported when there is no coverage."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 71,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000761041999965073,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The list of nodeids in the output matches the expected order.",
          "Each nodeid appears exactly once in the list.",
          "All nodeids are present in the input data.",
          "Nodeids are in ascending order.",
          "No duplicate nodeids are present in the output.",
          "Nodeids are not empty.",
          "The test passes if all assertions pass."
        ],
        "scenario": "The test verifies that the output of `ReportRoot` is sorted by nodeid.",
        "why_needed": "This test prevents a regression where tests are not sorted correctly by nodeid, potentially causing incorrect report generation."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "229-231, 233-235"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007030229999713811,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'detail' key should be present in the returned dictionary.",
          "The value of the 'detail' key should match '/path/to/file'.",
          "The 'message' key is not included in the returned dictionary."
        ],
        "scenario": "Test `test_to_dict_with_detail` verifies that the `to_dict()` method of a `ReportWarning` object returns a dictionary with the correct detail.",
        "why_needed": "This test prevents a warning about missing coverage information in the report."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "229-231, 233, 235"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006939090000059878,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The warning object should be created with only the required code and message keys.",
          "The 'detail' key should not be present in the warning object.",
          "The 'message' key should contain the expected warning message.",
          "The 'code' key should match the expected warning code.",
          "The 'detail' value should be an empty string or None to indicate no coverage information is available."
        ],
        "scenario": "Test 'test_to_dict_without_detail' verifies that a ReportWarning object is created without detailed warnings.",
        "why_needed": "This test prevents the creation of unnecessary detailed warnings when no coverage information is available."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_without_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 39,
          "line_ranges": "277-279, 281-283, 364-380, 382, 385, 387, 390, 393, 395, 397, 399-405, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007462609999606684,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert d['run_id'] == 'run-123'",
          "assert d['run_group_id'] == 'group-456'",
          "assert d['is_aggregated'] is True",
          "assert d['aggregation_policy'] == 'merge'",
          "assert d['run_count'] == 3",
          "assert len(d['source_reports']) == 2"
        ],
        "scenario": "Verify that RunMeta has aggregation fields.",
        "why_needed": "Prevent regression where RunMeta is missing aggregation fields, potentially leading to incorrect aggregation results."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_aggregation_fields_present",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007034559999965495,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_annotations_enabled' key is not present in the data.",
          "The 'llm_provider' key is not present in the data.",
          "The 'llm_model' key is not present in the data."
        ],
        "scenario": "Test that LLM fields are excluded when annotations are disabled.",
        "why_needed": "This test prevents a regression where the LLM fields are included even when annotations are disabled."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 40,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407-419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006935160000125506,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "data['llm_annotations_enabled'] is True",
          "data['llm_provider'] == 'ollama'",
          "data['llm_model'] == 'llama3.2:1b'",
          "data['llm_context_mode'] == 'complete'",
          "data['llm_annotations_count'] == 10",
          "data['llm_annotations_errors'] == 2"
        ],
        "scenario": "Test LLM traceability fields are included when enabled.",
        "why_needed": "This test prevents regression by ensuring that the LLM traceability fields are properly set to true when llm_annotations_enabled is True."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_traceability_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007052240000007259,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "source_reports is not included in the report dictionary",
          "is_aggregated is set to False for this meta"
        ],
        "scenario": "The test verifies that the `non_aggregated_excludes_source_reports` method of `RunMeta` does not include source reports.",
        "why_needed": "This test prevents a regression where non-aggregated report results do contain source reports."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 49,
          "line_ranges": "277-279, 281-283, 364-380, 382-405, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007309909999548836,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'git_sha' field should be present and have the expected value.",
          "The 'git_dirty' field should be True.",
          "The 'repo_version', 'repo_git_sha', and 'plugin_git_sha' fields should be present and have the expected values.",
          "The 'config_hash' field should be present and have the expected value.",
          "The length of the 'source_reports' list should be 1.",
          "All source reports should be dictionaries with the required keys (path, sha256, run_id).",
          "The 'pytest_invocation', 'pytest_config_summary', and 'run_id' fields should be present and have the expected values.",
          "The 'is_aggregated' field should be True."
        ],
        "scenario": "Test RunMeta to dict with all optional fields.",
        "why_needed": "Prevents regression in case of missing or outdated metadata."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006951199999889468,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be set to 1.",
          "The 'interrupted' field should be True.",
          "The 'collect_only' field should be True.",
          "The 'collected_count' field should equal 10.",
          "The 'selected_count' field should equal 8.",
          "The 'deselected_count' field should equal 2."
        ],
        "scenario": "TestRunMeta::test_run_status_fields verifies that RunMeta includes the necessary run status fields.",
        "why_needed": "This test prevents a potential bug where RunMeta is missing required run status fields, potentially leading to incorrect or incomplete results."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007275649999769485,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The schema version should be split into three parts (e.g., '1.2.3').",
          "Each part of the schema version should consist only of digits (0-9).",
          "All parts of the schema version should be non-empty and not empty strings.",
          "If a part is an empty string, it should be ignored or handled appropriately.",
          "The first part of the schema version should be greater than 0.",
          "The second part of the schema version should be less than or equal to 99.",
          "The third part of the schema version should be less than or equal to 999."
        ],
        "scenario": "Verify that the schema version is correctly formatted (semver),",
        "why_needed": "Prevents a potential bug where an incorrect or malformed semver format is used."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "364-380, 382, 385, 387, 390, 393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007160279999993691,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `schema_version` attribute of the `ReportRoot` object should be equal to `SCHEMA_VERSION`.",
          "The `to_dict()` method of the `ReportRoot` object should return a dictionary with a key named `schema_version` and a value equal to `SCHEMA_VERSION`.",
          "The `schema_version` property of the `ReportRoot` class should have a value equal to `SCHEMA_VERSION`.",
          "The `schema_version` attribute of the `ReportRoot` object should be present in its `to_dict()` method.",
          "The `schema_version` property of the `ReportRoot` class should not be `None` or an empty string."
        ],
        "scenario": "Tests the `ReportRoot` class to ensure it includes the schema version in its report root.",
        "why_needed": "This test prevents a potential bug where the schema version is not included in the report root, potentially causing issues with downstream processing or reporting."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "71-78"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006927539999992405,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the serialized dictionary matches the expected value.",
          "The 'line_ranges' key in the serialized dictionary matches the expected value.",
          "The 'line_count' key in the serialized dictionary matches the expected value.",
          "All line ranges are correctly formatted (e.g. '1-3', '5, 10-15')",
          "No missing or extra line ranges are present in the output",
          "The coverage entry data is not empty"
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object.",
        "why_needed": "This test prevents a potential bug where the coverage entry data is not properly serialized, potentially leading to incorrect or incomplete coverage reports."
      },
      "nodeid": "tests/test_models.py::TestSourceCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "277-279, 281, 283"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006873149999933048,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The dictionary should contain the keys 'scenario', 'why_needed', and 'key_assertions'.",
          "The value of 'scenario' should be present in the dictionary.",
          "The value of 'why_needed' should be present in the dictionary.",
          "The value of 'key_assertions' should be present in the dictionary.",
          "The value of 'confidence' should not be present in the dictionary when it is None."
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.",
        "why_needed": "This test prevents a regression where the minimal annotation is not properly serialized without the 'confidence' field."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "277-279, 281-283"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006978420000223196,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `run_id` attribute should be present and equal to 'run-1' when the `to_dict()` method is called on a `SourceReport` object with a `run_id` parameter.",
          "The `run_id` attribute should not be missing or incorrect when the `to_dict()` method is called on a `SourceReport` object without a `run_id` parameter.",
          "The `run_id` attribute should be present and equal to 'run-1' when the `to_dict()` method is called on a `SourceReport` object with an invalid or missing `run_id` value."
        ],
        "scenario": "The test verifies that the `SourceReport` object's `to_dict()` method returns the `run_id` attribute correctly.",
        "why_needed": "This test prevents a potential bug where the `run_id` is not included in the dictionary representation of the `SourceReport` object."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "449-457, 459, 461"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007010649999870111,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the coverage report should match the provided file path.",
          "The 'line_ranges' key in the coverage report should contain the expected line ranges.",
          "The 'line_count' key in the coverage report should match the specified line count."
        ],
        "scenario": "Test that the `CoverageEntry` class correctly serializes a coverage report.",
        "why_needed": "This test prevents regression where the coverage report is not properly serialized."
      },
      "nodeid": "tests/test_models.py::TestSummary::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 17,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006898380000279758,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field should be set to the expected value.",
          "The 'outcome' field should be set to the expected value.",
          "The 'duration' field should be set to 0.0 (indicating no execution time).",
          "The 'phase' field should be set to 'call'."
        ],
        "scenario": "Test the minimal result of a TestCaseResult object.",
        "why_needed": "This test prevents regression that might occur when creating a minimal result with an empty nodeid or outcome."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 22,
          "line_ranges": "40-43, 161-165, 167, 169, 171, 173, 176-178, 180, 182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007188949999772376,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'coverage' key in the result dictionary should contain exactly one entry.",
          "The 'file_path' value of the first 'coverage' entry should be 'src/foo.py'.",
          "All lines in the 'coverage' list should have a line count greater than zero."
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage verifies that the test result includes a coverage list.",
        "why_needed": "This test prevents regression by ensuring that the test result accurately reflects the code's coverage."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 18,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007043939999675786,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `llm_opt_out` in the `result` dictionary should be `True`.",
          "The key `'llm_opt_out'` exists in the `d` dictionary.",
          "The value of `llm_opt_out` is a boolean value (`True` or `False`).",
          "The test passes without LLM opt-out enabled."
        ],
        "scenario": "Test that the `result` object includes a flag indicating LLM opt-out.",
        "why_needed": "Prevents regression where the LLM is enabled by default and the test passes without it."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "161-165, 167, 169, 171, 173-176, 178, 180, 182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006918280000149934,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `rerun_count` is 2.",
          "The value of `final_outcome` is 'passed'."
        ],
        "scenario": "Test 'test_result_with_rerun' verifies that the TestCaseResult object includes rerun fields.",
        "why_needed": "This test prevents regression by ensuring that reruns are included in the result."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 17,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000690042000030644,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'rerun_count' key should be absent from the `result` dictionary.",
          "The 'final_outcome' key should be absent from the `result` dictionary.",
          "The 'rerun_count' and 'final_outcome' keys should not be present in the `result` dictionary."
        ],
        "scenario": "Test `test_result_without_rerun_excludes_fields` verifies that the `result` dictionary does not contain 'rerun_count' and 'final_outcome' keys.",
        "why_needed": "This test prevents a regression where the result of a test is rerun, potentially hiding important information about its outcome."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007390239999836012,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.llm_context_mode == 'minimal'",
          "cfg.llm_max_tests == 0",
          "cfg.llm_max_retries == 3",
          "cfg.llm_context_bytes == 32000",
          "cfg.llm_context_file_limit == 10",
          "cfg.llm_requests_per_minute == 5",
          "cfg.llm_timeout_seconds == 30",
          "cfg.llm_cache_ttl_seconds == 86400",
          "cfg.include_phase == 'run'",
          "cfg.aggregate_policy == 'latest'"
        ],
        "scenario": "Test that default values are set correctly for the Config class.",
        "why_needed": "This test prevents a potential bug where the default configuration values are not set correctly, potentially leading to unexpected behavior or errors in the application."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000690404000010858,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_default_config()` should return an instance of `Config`.",
          "The attribute `provider` on the returned object should be set to `'none'`.",
          "The value of `provider` should match the expected default provider ('none')."
        ],
        "scenario": "Verify that `get_default_config()` returns a default configuration with no provider.",
        "why_needed": "This test prevents a potential bug where the default configuration is not correctly set to 'none'."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_get_default_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000722270999972352,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert Config(provider='none').is_llm_enabled() is False",
          "assert Config(provider='ollama').is_llm_enabled() is True"
        ],
        "scenario": "Test that the `is_llm_enabled` check returns False for a provider without an LLM.",
        "why_needed": "Prevents regression in case the LLM is not enabled by default."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007183949999785,
      "llm_annotation": {
        "error": "Failed after 3 retries. Last error: Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008401949999665703,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The configuration object has been created with an invalid llm_context_mode.",
          "An error message indicating 'Invalid llm_context_mode' was found in the validation result.",
          "The error message includes the specific value 'mega_max'.",
          "A single error is expected as a result of this validation.",
          "The error message does not contain any additional context or details.",
          "The test verifies that an error is raised when an invalid context mode is provided.",
          "The test verifies that the error message contains the correct information about the invalid context mode."
        ],
        "scenario": "Testing the validation of an invalid context mode.",
        "why_needed": "Prevents a potential bug where an invalid context mode is not properly validated and causes unexpected behavior."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000684077000016714,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `validate()` returns exactly one error message.",
          "The error message contains the string 'Invalid include_phase 'lunch_break'.",
          "The error message includes the specified include phase value."
        ],
        "scenario": "Verify the test validates for an invalid include phase.",
        "why_needed": "Prevents a potential bug where the validation of invalid include phases is not properly handled."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 19,
          "line_ranges": "107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007252309999898898,
      "llm_annotation": {
        "error": "Failed after 3 retries. Last error: Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 22,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-218, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007188900000301146,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_context_bytes' value should be at least 1000.",
          "The 'llm_max_tests' value should be 0 (no limit) or positive.",
          "The 'llm_requests_per_minute' value should be at least 1.",
          "The 'llm_timeout_seconds' value should be at least 1.",
          "The 'llm_max_retries' value should be 0 or positive.",
          "All numeric constraints must be validated successfully."
        ],
        "scenario": "Test validation of numeric constraints for TestConfig.",
        "why_needed": "Prevents regression when setting invalid numeric ranges for LLM."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_numeric_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 17,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000692173000004459,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `validate()` method of the `Config` object returns an empty list of errors when a valid configuration is provided.",
          "No error messages are printed to the console indicating that no errors were found in the configuration.",
          "The test does not fail if the configuration contains invalid or missing values, as it only checks for the absence of errors.",
          "The `validate()` method correctly identifies and returns an empty list when a valid configuration is provided.",
          "The function signature and docstring indicate that it should return an empty list in this case.",
          "The test does not fail if the configuration contains invalid or missing values, as it only checks for the absence of errors.",
          "The `validate()` method correctly identifies and returns an empty list when a valid configuration is provided.",
          "The function signature and docstring indicate that it should return an empty list in this case.",
          "The test does not fail if the configuration contains invalid or missing values, as it only checks for the absence of errors."
        ],
        "scenario": "Valid configuration is validated successfully without any errors.",
        "why_needed": "This test prevents potential bugs where an invalid or malformed configuration could cause the application to crash or behave unexpectedly."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 28,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286-294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008408290000261331,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `aggregate_dir` should be 'aggr_dir'.",
          "The value of `aggregate_policy` should be 'merge'.",
          "The value of `aggregate_run_id` should be 'run-123'.",
          "The value of `aggregate_group_id` should be 'group-abc'."
        ],
        "scenario": "Test loading aggregation options for the `load_config` function.",
        "why_needed": "This test prevents a potential bug where the aggregate policy is set to 'merge' instead of 'merge_run_id'."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_aggregation_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 28,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259, 261, 263-267, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.00087990100001889,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `load_config` should not crash when it encounters an invalid integer value in the INI file.",
          "The default value of `llm_max_retries` is correctly set to 3.",
          "The test does not verify that the fallback value is incorrect (i.e., `llm_report_max_retries` equals 'garbage')."
        ],
        "scenario": "Test the handling of invalid integer values in INI files.",
        "why_needed": "Prevents a potential bug where the test crashes due to an invalid integer value in the INI file."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 25,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008244070000387183,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_pytest_config.option.llm_coverage_source",
          "cfg.llm_coverage_source == 'cov_dir'",
          "cfg.llm_coverage_source is not None",
          "cfg.llm_coverage_source is not empty",
          "cfg.llm_coverage_source does not contain 'default'",
          "cfg.llm_coverage_source contains 'cov_dir'"
        ],
        "scenario": "The test verifies that the `llm_coverage_source` option is set to 'cov_dir' after loading the configuration.",
        "why_needed": "This test prevents a potential bug where the coverage source is not correctly set even when the correct option is provided."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 24,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008459099999527098,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.report_html is None"
        ],
        "scenario": "Test that the default provider and report HTML are correctly loaded when no options are provided.",
        "why_needed": "This test prevents a potential bug where the configuration defaults to 'none' without any explicit option being set, potentially leading to unexpected behavior or errors in subsequent tests."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259-261, 263, 270-272, 274, 276, 278, 280-282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008998260000225855,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "ini_value is set to 'cli_report.html' for llm_report_html option",
          "llm_requests_per_minute value is set to 100"
        ],
        "scenario": "Test that CLI options override ini options.",
        "why_needed": "This test prevents a bug where the CLI overrides ini settings, potentially causing unexpected behavior or incorrect results."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 25,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282-283, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000851677000014206,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_max_retries` option should be set to 9 when loading configuration from CLI.",
          "The `llm_max_retries` option should have a default value of 5.",
          "The test should fail if the `llm_max_retries` option is not set to 9 or has a different value than its default."
        ],
        "scenario": "The test verifies that the `llm_max_retries` option is set to 9 when loading configuration from CLI.",
        "why_needed": "This test prevents a potential bug where the `llm_max_retries` option is not correctly set to its default value of 5, leading to incorrect configuration."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 32,
          "line_ranges": "107, 147, 248, 251-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0009671630000411824,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'ollama'",
          "cfg.model == 'llama3'",
          "cfg.llm_context_mode == 'balanced'",
          "cfg.llm_requests_per_minute == 10",
          "cfg.llm_max_retries == 5",
          "cfg.report_html == 'report.html'",
          "cfg.report_json == 'report.json'"
        ],
        "scenario": "Test loads values from ini options with mock Pytest configuration.",
        "why_needed": "This test prevents a potential regression where the `load_config` function relies on `getini` to retrieve ini values, which may not be available or properly configured."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006848940000168113,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` attribute of the `Config` object should be set to `/reports`.",
          "The `aggregate_policy` attribute of the `Config` object should be set to 'merge'.",
          "The `aggregate_include_history` attribute of the `Config` object should be set to True."
        ],
        "scenario": "Test Config with aggregation settings to ensure correct directory and policy.",
        "why_needed": "This test prevents a potential bug where the configuration is not set correctly for aggregation."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007382079999729285,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` attribute is set to 'report.html'.",
          "The `report_json` attribute is set to 'report.json'.",
          "The `report_pdf` attribute is set to 'report.pdf'."
        ],
        "scenario": "Tests Config with all output paths.",
        "why_needed": "Prevents a potential bug where the test fails if any of the report files are missing or corrupted."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006840300000021671,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.capture_failed_output is True",
          "assert config.capture_failed_output == True",
          "Verify that setting `capture_failed_output` to `False` does not prevent test execution.",
          "Test that the capture output is properly handled when `capture_failed_output` is `True`.",
          "Ensure that the assertion passes even if `capture_output_max_chars` is set to a lower value than 8000.",
          "Verify that the test can still pass without capturing any output.",
          "Check that the test does not raise an exception when `capture_failed_output` is `False`."
        ],
        "scenario": "Verify that the `capture_failed_output` parameter of `Config` is set to `True`.",
        "why_needed": "This test prevents a potential bug where the capture output is not properly handled when `capture_failed_output` is `False`."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006994980000172291,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `metadata_file` attribute of the `Config` object is set to 'metadata.json'.",
          "The `hmac_key_file` attribute of the `Config` object is set to 'key.txt'."
        ],
        "scenario": "Test the configuration of compliance settings.",
        "why_needed": "This test prevents a potential bug where the configuration is not set correctly for compliance settings."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006844049999585877,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.omit_tests_from_coverage is False",
          "config.include_phase == \"all\""
        ],
        "scenario": "Tests the configuration of coverage settings.",
        "why_needed": "Prevents a bug where coverage settings are not properly configured, leading to incorrect test results."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006846040000141329,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string `*.pyc` should be present in the `llm_context_exclude_globs` list.",
          "The string `*.log` should be present in the `llm_context_exclude_globs` list.",
          "Custom Python files (e.g., `custom_file.py`) and log files (e.g., `custom_log.log`) should not be included in the `llm_context_exclude_globs` list."
        ],
        "scenario": "Verify that the `llm_context_exclude_globs` option is correctly excluded custom Python files and log files.",
        "why_needed": "This test prevents a regression where the `llm_context_exclude_globs` option might be incorrectly or unexpectedly included in the list of exclude globs for custom Python files and log files."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007217390000278101,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `*.py` glob matches files with the `.py` extension.",
          "The `*.pyi` glob matches files with the `.pyi` extension.",
          "The `*.py` glob includes all Python source files (`.py`, `.pyi`) in the specified directory.",
          "The `*.pyi` glob includes all Python source files with the `.pyi` extension (e.g., `.pyc`) in the specified directory.",
          "Files not matching either `*.py` or `*.pyi` are excluded from the LLM context.",
          "The include globs are correctly configured to match the expected file types and directories.",
          "The test passes without any errors or warnings, indicating that the configuration is correct."
        ],
        "scenario": "Verify that the `llm_context_include_globs` attribute includes the correct globs.",
        "why_needed": "This test prevents a potential bug where the include globs are not correctly configured, potentially leading to incorrect or missing files being included in the LLM context."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_include_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "107"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006965310000168756,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `include_pytest_invocation` attribute of the test configuration object is set to `False`.",
          "The `include_pytest_invocation` attribute does not match the expected value of `False` for this specific configuration.",
          "The `include_pytest_invocation` attribute is correctly initialized with a boolean value.",
          "A pytest invocation setting is present in the test configuration, but it is not set to `True` or `False` as intended.",
          "The `invocation_redact_patterns` list contains a regular expression that matches API keys, which may be causing issues during testing.",
          "The `include_pytest_invocation` attribute is not being used in this specific test case.",
          "A different configuration object has the same `include_pytest_invocation` value as the one tested here."
        ],
        "scenario": "Verify that `include_pytest_invocation` is set to `False` for the specified configuration.",
        "why_needed": "Prevents a potential bug where the `include_pytest_invocation` setting is incorrectly configured, potentially leading to unexpected behavior or errors during testing."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007164410000086718,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of llm_max_tests is set to 50.",
          "The value of llm_max_concurrency is set to 8.",
          "The value of llm_requests_per_minute is set to 12.",
          "The value of llm_timeout_seconds is set to 60 seconds.",
          "The value of llm_cache_ttl_seconds is set to 3600 seconds (1 hour).",
          "The cache directory is set to .cache."
        ],
        "scenario": "Tests the configuration of LLM execution settings.",
        "why_needed": "Prevents regression in LLMS when using a large number of tests or high concurrency."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006956769999533208,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.llm_include_param_values should be `True`",
          "config.llm_param_value_max_chars should be `200`"
        ],
        "scenario": "Verify that the `llm_include_param_values` parameter is set to `True` and that its maximum value is 200.",
        "why_needed": "This test prevents a potential bug where the LLM param settings are not properly configured, potentially leading to incorrect output or errors."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007086840000170014,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` attribute is set to 'ollama'.",
          "The `model` attribute is set to 'llama3.2'.",
          "The `llm_context_bytes` attribute is set to 64000 bytes.",
          "The `llm_context_file_limit` attribute is set to 20."
        ],
        "scenario": "Tests the configuration of LLM settings provided by OLLAMA.",
        "why_needed": "This test prevents a potential bug where the model and context bytes are not correctly configured for optimal performance."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007548950000000332,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.repo_root",
          "is equal to Path('/project')",
          "config.repo_root is set to /project"
        ],
        "scenario": "Verify that the `repo_root` attribute is set correctly to `/project`.",
        "why_needed": "This test prevents a potential bug where the `repo_root` attribute is not set correctly, potentially leading to issues with repository configuration."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 17,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007418330000064088,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `include_phase` attribute of each configuration object is not present in any error messages.",
          "No error messages are raised when an include phase value is valid (e.g., 'run', 'setup', or 'all').",
          "All included phases are correctly validated and do not cause any validation errors."
        ],
        "scenario": "Test the `Config` class with valid include phase values.",
        "why_needed": "Prevents a potential bug where invalid or missing include phases are passed to the `validate()` method, potentially causing validation errors."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000686035000001084,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `Config().llm_context_exclude_globs` returns a list of strings that includes `*.pyc`, `__pycache__/*`, and `*secret*`, `*password*`.",
          "The test asserts the presence of these globs in the default exclude list.",
          "If any of the assert statements fail, it would indicate an issue with the default exclude globs being set correctly."
        ],
        "scenario": "Verify that the default exclude globs are correctly set for the LLM context.",
        "why_needed": "This test prevents a potential bug where the default exclude globs do not include certain files or directories, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007057920000193008,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '--password' and '--token' pattern names should be found in the invocation_redact_patterns list.",
          "The 'api[_-]?key' pattern name should be found in the invocation_redact_patterns list.",
          "Any other patterns that start with '--api_' or end with '_key' should also be present in the invocation_redact_patterns list."
        ],
        "scenario": "Test default redact patterns for ConfigDefaultsMaximal test.",
        "why_needed": "Prevents a potential bug where the default redact patterns are not correctly detected, potentially leading to incorrect configuration of the application."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 233"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000714092000009714,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should be 'none'.",
          "The llm_context_mode should be 'minimal'.",
          "The llm_context_bytes should be 32000 bytes.",
          "The omit_tests_from_coverage flag should be True.",
          "The include_phase should be 'run'."
        ],
        "scenario": "Test default values of the test_config_defaults_maximal module.",
        "why_needed": "This test prevents regression in case the default configuration settings are not correctly set."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000741199999993114,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `Config` object with provider 'none' should return False when `is_llm_enabled()` is called.",
          "The `Config` object with provider 'ollama' should return True when `is_llm_enabled()` is called.",
          "The `Config` object with provider 'litellm' should return True when `is_llm_enabled()` is called.",
          "The `Config` object with provider 'gemini' should return True when `is_llm_enabled()` is called."
        ],
        "scenario": "Verify the correct enabled status of LLM for different providers.",
        "why_needed": "This test prevents regression in LLM configuration settings."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-197, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006995930000357475,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `validate` method should return exactly one error for an invalid aggregate policy.",
          "The error message should contain the string 'Invalid aggregate_policy 'invalid''.",
          "The error message should be present in the first error found during validation."
        ],
        "scenario": "Test the `validate` method of `Config` class for an invalid aggregate policy.",
        "why_needed": "Prevents a potential bug where an invalid aggregate policy is not properly validated and returns an error message."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "107, 147, 175, 178-179, 185-189, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007461049999619718,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `validate` method should return exactly one error message for an invalid context mode.",
          "The error message should contain 'Invalid llm_context_mode 'invalid''.",
          "The test should fail when an invalid context mode is provided."
        ],
        "scenario": "Test the `validate` method of the `Config` class when an invalid context mode is provided.",
        "why_needed": "This test prevents a potential bug where the `validate` method returns multiple errors for an invalid context mode, making it harder to identify and fix the issue."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-205, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007182989999705569,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `validate()` returns exactly one error message.",
          "The error message contains the string 'Invalid include_phase 'invalid'.",
          "The error message is not empty."
        ],
        "scenario": "Test validates the `include_phase` parameter with an invalid value.",
        "why_needed": "Prevents a potential bug where the test fails due to an incorrect or missing error message for an invalid `include_phase` value."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 19,
          "line_ranges": "107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0008530279999945378,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `validate()` should return exactly one error message for an invalid provider.",
          "The error message for an invalid provider should contain the exact phrase 'Invalid provider 'invalid''.",
          "The test should assert that there is only one error message in total.",
          "The first error message should be the entire string 'Invalid provider 'invalid'.",
          "The error message should not be empty."
        ],
        "scenario": "Test validates an invalid provider.",
        "why_needed": "Prevents a potential bug where the test does not catch and report invalid providers."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 21,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209-217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007354200000122546,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `validate()` should return an error for invalid numeric values.",
          "The function `validate()` should throw an error if 'llm_context_bytes' is not a non-negative integer.",
          "The function `validate()` should throw an error if 'llm_max_tests' is negative.",
          "The function `validate()` should throw an error if 'llm_requests_per_minute' is zero.",
          "The function `validate()` should throw an error if 'llm_timeout_seconds' is zero.",
          "The function `validate()` should return at least 4 errors for invalid numeric values."
        ],
        "scenario": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds",
        "why_needed": "This test prevents regression of invalid numeric values in the config."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 17,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006986199999801102,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `validate` method of the `Config` class should return an empty list for any valid configuration.",
          "Any invalid configuration should raise an exception or return a meaningful error message.",
          "The function should be able to handle different types of input configurations (e.g., dictionaries, lists).",
          "If the input is not a dictionary or list, the function should raise an `TypeError` or return an error message.",
          "The function should check for any invalid keys in the configuration and raise an exception if found.",
          "Any missing required keys should be raised with an appropriate error message.",
          "The function should handle nested configurations correctly (if applicable).",
          "If the input is not a valid dictionary or list, the function should return a meaningful error message."
        ],
        "scenario": "Verifies that an invalid configuration returns an empty list.",
        "why_needed": "Prevents a potential bug where the function does not handle all possible input configurations correctly."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 33,
          "line_ranges": "107, 147, 248, 251-259, 261, 263-265, 270, 272-276, 278, 280, 282, 286, 288, 290-292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007615290000444475,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `cfg` variable should be an instance of `Config`.",
          "The value of `cfg` should not be None.",
          "The type of `cfg` should be `Config`."
        ],
        "scenario": "Verify that the `Config` object has default settings.",
        "why_needed": "This test prevents a potential bug where the configuration is missing or incorrectly configured due to missing plugin options."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006906930000241118,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytestconfig` object should be an instance of `pytest.config.Config`.",
          "The `pytestconfig` object should not be `None`.",
          "The `pytestconfig` object's attributes (e.g. `markers`, `options`) should be accessible."
        ],
        "scenario": "Verify that the `pytestconfig` object is accessible within the test.",
        "why_needed": "Prevent a potential bug where the plugin configuration is inaccessible due to incorrect import or setup."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007000699999935023,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert True is executed without any errors.",
          "assert False is executed with an error message indicating context marker issue.",
          "assert 'Context marker should not cause errors.' is printed to the console.",
          "assert 'LLM marker' or 'context override' is present in the test output.",
          "assert 'Opt-out, context override' is present in the test output.",
          "assert 'Captured stdout/stderr for failed tests (opt-in)' is present in the test output."
        ],
        "scenario": "The test verifies that the context marker does not cause errors.",
        "why_needed": "This test prevents a bug where the LLM marker causes an error in the test execution."
      },
      "llm_context_override": "balanced",
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0006934929999715678,
      "llm_opt_out": true,
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.000691388999996434,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'requirement_marker' function should not raise an exception when called with no arguments.",
          "The 'requirement_marker' function should not throw any errors when executed without any inputs.",
          "The 'requirement_marker' function should not cause any runtime errors when used correctly and in the correct context."
        ],
        "scenario": "The 'requirement_marker' function is being tested to ensure it does not throw any errors.",
        "why_needed": "This test prevents a potential bug where the 'requirement_marker' function could be used in error conditions, causing unexpected behavior or crashes."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
      "outcome": "passed",
      "phase": "call",
      "requirements": [
        "REQ-001",
        "REQ-002"
      ]
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 79,
          "line_ranges": "161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 131,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.03163674600000377,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify existence of 'report.json' file in tmp_path.",
          "Verify correct number of passed tests (1) and failed tests (1) in the JSON data.",
          "Verify presence of 'test_a.py' and 'test_b.py' in the HTML report."
        ],
        "scenario": "Test the integration of report writer with pytest_llm_report.",
        "why_needed": "This test prevents regression when integrating report writer into pytest_llm_report, ensuring that full report generation flow works correctly."
      },
      "nodeid": "tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "387-388, 391, 395-397, 408-409, 415-416"
        }
      ],
      "duration": 0.0012565830000426104,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mocked session.config.stash.get._enabled_key returns False",
          "pytest_collectreport() does not call _enabled_key with True",
          "mock_report.session.config.stash.get.asserts_called_with(_enabled_key, False) is False"
        ],
        "scenario": "Test that collectreport skips when disabled and pytest_collectreport is mocked.",
        "why_needed": "The test prevents a regression where pytest_collectreport fails to report the plugin's collection even though it was disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 12,
          "line_ranges": "387-388, 391, 395-397, 408-409, 415, 419-421"
        }
      ],
      "duration": 0.0015893529999857492,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_collectreport` function should be able to find and stash the `_collector_key` key in the session configuration.",
          "The `stash_get` function should return `True` for the `_enabled_key` key.",
          "The `handle_collection_report` method of the collector should be called with the stash value containing the `_collector_key` key.",
          "The `handle_collection_report` method of the collector should not throw an exception if it cannot find the stash value.",
          "The `stash_get` function should return `True` for all other keys in the session configuration.",
          "The `pytest_collectreport` function should be able to handle a mock report object correctly.",
          "The `mock_collector.handle_collection_report` call should not throw an exception if it cannot find the stash value."
        ],
        "scenario": "Test that `pytest_collectreport` calls the collector when collectreport is enabled.",
        "why_needed": "This test prevents a potential regression where `pytest_collectreport` does not call the collector when collectreport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "387-388, 391, 395-397, 408, 412"
        }
      ],
      "duration": 0.0008111129999974764,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `pytest_collectreport(mock_report)` should not be called with a `session` attribute that is missing.",
          "No exception should be raised if the `session` attribute is deleted from the mock report object.",
          "The `session` attribute of the mock report object should be `None` after deletion.",
          "The function `pytest_collectreport(mock_report)` should not raise an AttributeError when the session attribute is accessed.",
          "The function `pytest_collectreport(mock_report)` should not raise a TypeError if the session attribute is accessed with no arguments.",
          "The function `pytest_collectreport(mock_report)` should not raise a KeyError if the session attribute is accessed without checking for its existence first."
        ],
        "scenario": "Verify that `pytest_collectreport` does not raise an exception when no session is available.",
        "why_needed": "Prevent regression in plugin behavior when a test has no active session."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "387-388, 391, 395-397, 408, 412"
        }
      ],
      "duration": 0.0008991260000357215,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "...",
          "...",
          "...",
          "...",
          "...",
          "...",
          "..."
        ],
        "scenario": "Tests the behavior of `pytest_collectreport` when a session is `None`.",
        "why_needed": "Prevents potential `pytest_collectreport` exceptions that may occur when trying to collect reports for an empty session."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 44,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 29,
          "line_ranges": "169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.0029852320000145482,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_llm_report_provider` option should be set to `'ollama'`.",
          "The `llm_report_html`, `llm_report_json`, `llm_report_pdf`, `llm_evidence_bundle`, `llm_dependency_snapshot`, `llm_requests_per_minute`, `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, and `llm_aggregate_group_id` options should be set to `None`.",
          "The `llm_max_retries` option should also be set to `None`.",
          "The `rootpath` and `stash` options should not be set."
        ],
        "scenario": "Test that LLM enabled warning is raised when using the Ollama LLM report provider.",
        "why_needed": "To prevent a potential bug where the LLM report provider 'ollama' is enabled and raises an error when configured with pytest."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 43,
          "line_ranges": "107, 147, 175, 178-181, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 248, 251-253, 255, 257, 259, 261, 263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 25,
          "line_ranges": "169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-199, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.0025691210000218234,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking `getini` method with an invalid key returns a dictionary containing 'llm_report_provider' as the only valid value.",
          "Setting `option.llm_report_html`, `option.llm_report_json`, `option.llm_report_pdf`, `option.llm_evidence_bundle`, `option.llm_dependency_snapshot`, `option.llm_requests_per_minute`, `option.llm_aggregate_dir`, `option.llm_aggregate_policy`, `option.llm_aggregate_run_id`, and `option.llm_aggregate_group_id` to None.",
          "Setting `llm_report_provider` to an invalid value raises a ValueError with the message 'configuration errors'.",
          "The `pytest_configure` function is called with a valid config object, but it does not raise a UsageError.",
          "Calling `pytest_configure` with an invalid config object should raise a UsageError."
        ],
        "scenario": "Test that validation errors raise UsageError when setting invalid config.",
        "why_needed": "Prevents a potential bug where the plugin does not handle configuration errors properly and raises a UsageError instead of providing meaningful error messages."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 17,
          "line_ranges": "169-171, 173-175, 177-179, 183-184, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.0010983390000092186,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_config.addinivalue_line was not called before calling pytest_configure",
          "addinivalue_line is still called for markers before worker check"
        ],
        "scenario": "Test that configure skips on xdist workers.",
        "why_needed": "This test prevents a potential regression where the plugin might skip configuration due to an incorrect or incomplete worker input."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 29,
          "line_ranges": "169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.0028887259999805792,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The Config.load() method should be called with no arguments.",
          "The Config.validate() method should return an empty list.",
          "The load_config() function should not be called directly.",
          "The option.llm_report_html and option.llm_max_retries attributes should remain unchanged.",
          "The option.llm_max_retries attribute should still be None after the test.",
          "The mock_load object returned by patch should have been called once with no arguments.",
          "The Config.load() method call should not raise an exception."
        ],
        "scenario": "Test that fallback to load_config occurs when Config.load is missing in pytest_configure function.",
        "why_needed": "Prevents a potential bug where the test fails due to missing Config.load method call."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 31,
          "line_ranges": "107, 147, 248, 251-263, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294-295, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0017373700000007375,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_report_provider` option is set correctly.",
          "The `llm_report_model` option is set correctly.",
          "The `llm_report_context_mode` option is set correctly.",
          "The `llm_report_requests_per_minute` option is set to 10.",
          "The `report_html` option is set to 'ini.html'.",
          "The `report_json` option is set to 'ini.json'."
        ],
        "scenario": "Test loading all INI options for plugin configuration.",
        "why_needed": "This test prevents a potential bug where the plugin fails to load configuration due to missing or invalid INI values."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 38,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259-263, 270-283, 286-295, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0017195110000329805,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.report_html == 'cli.html'",
          "cfg.report_json == 'cli.json'",
          "cfg.report_pdf == 'cli.pdf'",
          "cfg.report_evidence_bundle == 'bundle.zip'",
          "cfg.report_dependency_snapshot == 'deps.json'",
          "cfg.llm_requests_per_minute == 20",
          "cfg.aggregate_dir == '/agg'",
          "cfg.aggregate_policy == 'merge'",
          "cfg.aggregate_run_id == 'run-123'",
          "cfg.aggregate_group_id == 'group-abc'"
        ],
        "scenario": "Test CLI options override INI options.",
        "why_needed": "This test prevents regression where the CLI options override INI options, ensuring that the correct report files are used even when the INI file is not present or does not contain the required configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 9,
          "line_ranges": "238, 242-243, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.0011988659999815354,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocked stash.get() with _enabled_key and False returns None, as expected.",
          "Mocked stash.get() with _enabled_key and True does not return None, as expected.",
          "Mocked stash.get() with _enabled_key and an incorrect value (e.g., 1) should have been called once.",
          "The stash.get method is called only once, even if the plugin is enabled multiple times.",
          "The stash.get method is not called when the plugin is disabled or does not exist.",
          "The stash.get method returns None instead of raising an exception when it fails to get a value.",
          "The stash.get method should raise an exception when it fails to get a value, as expected."
        ],
        "scenario": "Test that terminal summary skips when plugin is disabled.",
        "why_needed": "Prevents a regression where terminal summary is not displayed when the plugin is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "238-239, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.001045933000000332,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_terminal_summary` function should return None for the given configuration.",
          "The `workerinput` attribute of the mock config object is set to 'gw0'.",
          "No output is produced from the test (i.e., no output is printed or displayed)."
        ],
        "scenario": "Test that terminal summary skips on xdist worker when configured to skip.",
        "why_needed": "This test prevents a regression where the plugin does not properly handle skipping in xdist workers."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 36,
          "line_ranges": "107, 147, 248, 251, 253, 255, 257, 259, 261, 263, 270-283, 286-295, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.003144530999975359,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.report_html == 'out.html'",
          "mock_config.option.llm_report_html == 'out.html'",
          "cfg.getini('llm_report_html') == None",
          "cfg.rootpath == '/root'"
        ],
        "scenario": "Test config loading from pytest objects (CLI + INI) to ensure it correctly sets report HTML.",
        "why_needed": "This test prevents a potential bug where the report HTML is not set correctly if the `pytest_llm_report` CLI option or INI file does not provide an `llm_report_html` value."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::testload_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 7,
          "line_ranges": "387-388, 391-392, 395-397"
        }
      ],
      "duration": 0.0015082310000025245,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_item.config.stash.get returns False",
          "mock_call is called with no arguments",
          "mock_outcome.get_result returns a MagicMock object",
          "gen.send raises StopIteration and passes mock_outcome",
          "gen.next yields the mock call to the next hookwrapper"
        ],
        "scenario": "Test makereport skips when disabled.",
        "why_needed": "This test prevents a regression where the plugin does not report any issues even though it should."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.001924303000009786,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mock_collector` should be called with the `mock_report` argument when `makereport` is enabled.",
          "The `mock_collector` should have been called once with the `mock_report` argument.",
          "The `mock_collector` should not have been called without any arguments (i.e., when makereport is disabled).",
          "The `mock_collector` should be called before calling `next(gen)` to ensure it has a chance to handle the log report.",
          "The `mock_collector` should not be called after `next(gen)` because there's no need to handle the log report again.",
          "The `mock_collector` should have been called with the correct arguments (i.e., `mock_report`) when `makereport` is enabled."
        ],
        "scenario": "Test makereport calls collector when enabled.",
        "why_needed": "This test prevents a potential regression where the collector is not called when makereport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "387-388, 391, 395-397, 431-432"
        }
      ],
      "duration": 0.0011394719999771041,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_session.config.stash.get.assert_called_with(_enabled_key, False)",
          "pytest_collection_finish(mock_session) should be called with _enabled_key as 'pytest_collection_finish' and False as its argument",
          "mock_session.config.stash.get.return_value should not be True",
          "_enabled_key should be 'pytest_collection_finish'",
          "pytest_collection_finish should not be called with any arguments"
        ],
        "scenario": "Test that collection_finish is skipped when disabled for Pytest.",
        "why_needed": "To prevent a regression where the plugin's hooks are not executed correctly when collection_finish is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "387-388, 391, 395-397, 431, 435-437"
        }
      ],
      "duration": 0.0016075350000051003,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_collector.handle_collection_finish.assert_called_once_with(mock_session.items)",
          "mock_session.items[0].is_pytest_collection_finished() == False",
          "mock_session.items[1].is_pytest_collection_finished() == True"
        ],
        "scenario": "Verify that collection_finish is called when collection finish is enabled.",
        "why_needed": "Prevents a potential bug where the collector does not get notified about the finished items in Pytest."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "387-388, 391, 395-397, 448-449"
        }
      ],
      "duration": 0.001175541999998586,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_sessionstart` function should be called with the `_enabled_key` parameter set to `False`.",
          "The `get` method of the stash configuration should have been called with the `_enabled_key` parameter and a boolean value of `False`.",
          "The `assert_called_with` method of the mock session's `config.stash.get` method should have been called.",
          "The `pytest_sessionstart` function should not be called with any other parameters.",
          "The `get` method of the stash configuration should return a boolean value of `False`.",
          "The `assert_called_with` method of the mock session's `config.stash.get` method should have been called with `_enabled_key` and `False` as arguments."
        ],
        "scenario": "Test that sessionstart skips when disabled and verifies the expected behavior.",
        "why_needed": "To prevent a regression where pytest_sessionstart fails to check the enabled status of the plugin."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 11,
          "line_ranges": "387-388, 391, 395-397, 448, 452, 455, 457-458"
        }
      ],
      "duration": 0.0009895359999632092,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The key _collector_key should be present in the mock stash.",
          "The key _start_time_key should be present in the mock stash.",
          "A MagicMock object with type _collector_key and _start_time_key should be created.",
          "A MagicMock object with type _enabled_key should be set to True in the stash_dict.",
          "_enabled_key should have a value of True in the stash_dict.",
          "The pytest_sessionstart function should create a collector when called with an enabled configuration.",
          "The collector should be initialized correctly, i.e., it should contain both _collector_key and _start_time_key."
        ],
        "scenario": "Test that sessionstart initializes collector when enabled.",
        "why_needed": "Prevents a potential bug where the collector is not initialized when pytest_sessionstart is called with an enabled configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 99,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.001902993999976843,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that `--llm-report` is added as an option.",
          "Verify that `--llm-coverage-source` is also added as an option.",
          "Check if both options are present in the parsed arguments.",
          "Verify that the group name matches 'llm-report' and the report type matches 'LLM-enhanced test reports'.",
          "Ensure that the `addoption` method of the group returns a tuple with two elements, where the first element is the option string."
        ],
        "scenario": "Test pytest_addoption adds expected arguments to the parser.",
        "why_needed": "pytest_addoption prevents a potential bug where it does not add all required options to the parser."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 99,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.0018376400000192916,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `pytest_addoption(parser)` is called with a `MagicMock` object as the parser.",
          "The `addini.call_args_list` attribute of the parsed parser contains lines starting with 'llm_report_'.",
          "The string 'llm_report_html' is found in the ini calls.",
          "The string 'llm_report_json' is found in the ini calls.",
          "The string 'llm_report_max_retries' is found in the ini calls."
        ],
        "scenario": "Test pytest_addoption adds INI options for llm_report plugin.",
        "why_needed": "This test prevents a regression where the plugin does not add INI options for pytest."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 58,
          "line_ranges": "238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 307, 309-315, 317-318, 331-332, 337-338, 365-375, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.004088342999978067,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option is set to 'out.html' and the `CoverageMapper` class is mocked with a mock `Coverage` object.",
          "The `MockStash` dictionary is created with the provided stash data.",
          "The `patch` decorator ensures that the `pathlib.Path.exists` function returns True when it should return False, simulating coverage file existence.",
          "The `patch` decorator ensures that the `Coverage` class is mocked with a mock object and its methods are called correctly.",
          "The `MockStash` dictionary is loaded into the `CoverageMapper` instance using the `load` method.",
          "The `report` method of the `Coverage` object is called to generate the coverage report, which should return 85.5 as expected.",
          "The test verifies that the coverage percentage calculation logic works correctly by checking if the reported coverage matches the expected value."
        ],
        "scenario": "Test coverage percentage calculation logic for terminal summary.",
        "why_needed": "This test prevents regression in coverage percentage calculation logic when using the terminal summary feature."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 59,
          "line_ranges": "238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331-332, 337-340, 343, 345, 348-350, 357-362, 365-375, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.002758949999986271,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `pytest_terminal_summary_llm_enabled` test passes without any errors.",
          "Check if the correct configuration is passed to `pytest_terminal_summary`.",
          "Ensure that the LLM annotator is called with the provided config.",
          "Verify that the annotation is performed correctly and returns the expected result.",
          "Confirm that the report writer is patched correctly and can write the output.",
          "Test that the coverage mapper is not used in this scenario.",
          "Verify that the `pytest_llm_report.llm.annotator.annotate_tests` function is called only once.",
          "Check if the correct model name is retrieved from the provider."
        ],
        "scenario": "Test terminal summary with LLM enabled runs annotations.",
        "why_needed": "Prevents regression in terminal summary functionality when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 45,
          "line_ranges": "238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.001926528000012695,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "...",
          "Mocking stash with only enabled key and no config key should not trigger the creation of a collector.",
          "Mocking stash with only enabled key but no config key should still create a collector.",
          "Mocking stash with only disabled key and no config key should not trigger the creation of a collector.",
          "...",
          "...",
          "..."
        ],
        "scenario": "Test terminal summary creates collector if missing.",
        "why_needed": "This test prevents a regression where the plugin does not create a collector even when it is supposed to be missing."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 21,
          "line_ranges": "238, 242, 246, 249-250, 252-253, 256-257, 259, 261-265, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.002212868000015078,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The stash object returned by pytest_terminal_summary should support both get() and [] indexing.",
          "The aggregation function should be called once with the correct arguments (0, stash).",
          "The ReportWriter should write JSON and HTML files correctly.",
          "The aggregation report should contain the expected data."
        ],
        "scenario": "Test terminal summary with aggregation enabled.",
        "why_needed": "Prevents a regression where the plugin does not aggregate terminal summaries correctly when report_html and report_json are set to out.html and out.json respectively."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "107, 147, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 52,
          "line_ranges": "238, 242, 246, 249, 268-269, 271, 273, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-305, 322-325, 331-332, 337-338, 365-375, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.003960084000027564,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `load` method of `CoverageMapper` should not raise an exception if it encounters a file that cannot be loaded.",
          "The `coverage.Coverage` object returned by `CoverageMapper` should contain the correct coverage statistics for all files.",
          "The `report_writer.ReportWriter` instance should not throw an error when writing to the report HTML file.",
          "The `pytest_terminal_summary` function should return a summary with the expected coverage statistics even if it encounters an error during load.",
          "The `pytest_llm_report.report_writer.ReportWriter` instance should be able to write the report without raising an exception.",
          "The `pytest_llm_report.options.Config` object returned by `Config` should contain the correct configuration settings for coverage calculation.",
          "The `pytest_terminal_summary` function should return a summary with the expected error message when it encounters an OSError during load."
        ],
        "scenario": "Test coverage calculation error when loading a large number of files.",
        "why_needed": "This test prevents regression where the plugin fails to calculate coverage due to an OSError during load."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 51,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194"
        }
      ],
      "duration": 0.0071336530000394305,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'utils.py' file should be present in the assembled context.",
          "The 'def util()' function should be found in the 'utils.py' file within the assembled context.",
          "The line ranges of '1-2' and line count of '2' should match the coverage entry for 'utils.py'."
        ],
        "scenario": "Test the ContextAssembler to assemble a balanced context for a test file.",
        "why_needed": "This test prevents regression when the llm_context_mode is set to 'balanced' and the test file contains unbalanced dependencies."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180"
        }
      ],
      "duration": 0.0009229359999949338,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'test_1' function should be present in the source code of the assembled context.",
          "The 'test_1' function should be included in the assembly output.",
          "The 'test_a.py::test_1' path should match the expected location in the assembled context.",
          "The 'test_a.py::test_1' function name should be present in the source code of the assembled context.",
          "The 'test_a.py::test_1' function signature should match the expected signature in the assembled context.",
          "The assembly output should contain the 'test_1' function definition.",
          "The assembly output should not contain any errors or warnings related to the 'test_1' function."
        ],
        "scenario": "Assembling a complete context for the 'test_a.py' test file and verifying that the 'test_1' function is included in the source code.",
        "why_needed": "This test prevents regression where the 'test_a.py' test file is not properly assembled into a complete context, potentially leading to incorrect test results or errors."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 30,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0008977029999641672,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The source code of the test file is present in the assembled context.",
          "The context contains no references to external modules or functions.",
          "The test function `test_1` is present in the assembled context.",
          "The context has an empty dictionary as its value.",
          "No additional dependencies are included in the context.",
          "No external imports are used in the test function."
        ],
        "scenario": "Verifies that the ContextAssembler can assemble a minimal context for a test file with a single test function.",
        "why_needed": "This test prevents regression when using the 'minimal' llm_context_mode, as it ensures the assembler only creates a minimal context without any additional dependencies."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194"
        }
      ],
      "duration": 0.0009232269999870368,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "ContextAssemblyResult: The assembled context contains only the truncated part of the file.",
          "The length of the assembled context is within the allowed limit (40 bytes).",
          "The file name in the assembled context does not exceed the specified limit (20 bytes + truncation message)."
        ],
        "scenario": "Tests the ContextAssembler with balanced context limits to ensure it correctly truncates long files.",
        "why_needed": "This test prevents a potential bug where the ContextAssembler does not truncate long files according to the specified context limit."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 26,
          "line_ranges": "33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0008746969999720022,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_get_test_source` returns an empty string for a non-existent file.",
          "The function `_get_test_source` includes the full path of the test file in its output.",
          "The function `_get_test_source` correctly identifies the 'test' keyword in the nested test name with parameters."
        ],
        "scenario": "Test the ContextAssembler with edge cases where a non-existent file is provided.",
        "why_needed": "This test prevents potential issues when using the ContextAssembler with files that do not exist, as it ensures the assembler correctly handles such scenarios."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 5,
          "line_ranges": "33, 191-194"
        }
      ],
      "duration": 0.0013603079999597867,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'test.pyc' is excluded because it has been compiled and its contents are not relevant to the LLM's processing.",
          "The file 'secret/key.txt' is excluded because it contains sensitive information that should be protected by the ContextAssembler.",
          "The file 'public/readme.md' is included in the exclusion list because it does not contain any sensitive or confidential information."
        ],
        "scenario": "The test verifies that the ContextAssembler should exclude certain files from being processed by the LLM.",
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly excludes important files, leading to incorrect results or errors."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_should_exclude",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0006899669999711477,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert compress_ranges([1, 2, 3]) == '1-3'",
          "assert compress_ranges([4, 5, 6]) == '4-6'",
          "assert compress_ranges([-1, -2, -3]) == '-1-3'",
          "assert compress_ranges([]) == ''",
          "assert compress_ranges([1]) == '1'",
          "assert compress_ranges([1, 2]) == '1-2'"
        ],
        "scenario": "The 'compress_ranges' function is tested to ensure consecutive lines are compressed correctly.",
        "why_needed": "This test prevents a potential bug where consecutive lines in the input list do not use range notation."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0007112079999842535,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert compress_ranges([1, 2, 2, 3, 3, 3]) == '1-3'",
          "assert compress_ranges([4, 5, 6, 7, 8, 9]) == None",
          "assert compress_ranges([1, 1, 2, 2, 3, 3]) == '1-3'"
        ],
        "scenario": "The test verifies that the function correctly handles duplicate ranges.",
        "why_needed": "This test prevents a potential bug where the function incorrectly identifies non-duplicate ranges as duplicates."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "29-30"
        }
      ],
      "duration": 0.0007030870000335199,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty string for an empty input list.",
          "The function should handle the case of an empty list without raising any exceptions or errors.",
          "The output of `compress_ranges([])` should be a string representing an empty range (e.g., '[]') rather than None or some other value.",
          "Any additional ranges in the input list should not affect the output of `compress_ranges([])`.",
          "The function should handle lists containing multiple ranges correctly (e.g., `[1, 2]` and `[3, 4]`).",
          "If a non-empty range is present in the input list, it should be included in the compressed result.",
          "The function should not return an empty string for a list with one or more ranges (e.g., `[1, 5]` and `[2, 6]`)."
        ],
        "scenario": "Testing the `compress_ranges` function with an empty input list.",
        "why_needed": "This test prevents a potential bug where an empty list is not correctly compressed to an empty string."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0006790930000306616,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should correctly group the ranges and singles into separate output strings.",
          "The function should handle ranges with inclusive or exclusive endpoints correctly.",
          "The function should not include any invalid range combinations (e.g., -1-3, etc.).",
          "The function should preserve the original order of elements within each range.",
          "The function should handle edge cases where there are no values in a range.",
          "The function should correctly identify ranges with overlapping endpoints.",
          "The function should not include any invalid or duplicate output strings."
        ],
        "scenario": "Test the function when given a mixed range of values.",
        "why_needed": "This test prevents regression in cases where ranges are present alongside single values."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007120010000107868,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input list should be sorted in ascending order.",
          "Each element in the list should be an integer.",
          "Non-consecutive elements should be separated by commas.",
          "Consecutive elements should be separated by spaces.",
          "All elements should be comma-separated, with no leading or trailing spaces.",
          "No non-integer values should be present in the input list.",
          "The output string should match the expected result."
        ],
        "scenario": "The 'test_non_consecutive_lines' test verifies that non-consecutive lines are comma-separated.",
        "why_needed": "This test prevents a regression where consecutive lines are not separated by commas."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "29, 33, 35-37, 39, 50, 52, 65-66"
        }
      ],
      "duration": 0.0006887149999670328,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input list should be empty or contain only one element.",
          "The output should be the original string '5'.",
          "No error message or exception should be raised."
        ],
        "scenario": "The 'single_line' test verifies that the `compress_ranges` function does not attempt to compress a single-line string.",
        "why_needed": "This test prevents regression when the `compress_ranges` function is called with a single-element list of integers, as it may incorrectly try to apply range notation."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0006776459999855433,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function compresses the input list into '1-2' if it contains two consecutive numbers.",
          "If the input list does not contain two consecutive numbers, the function should return an error message or raise a meaningful exception.",
          "The function should handle edge cases where the input list is empty or only contains one number correctly.",
          "Two consecutive numbers in the input list should be treated as separate ranges.",
          "Non-consecutive numbers in the input list should not be compressed to range notation.",
          "The function should raise a meaningful exception when given an invalid input, such as non-numeric values or out-of-range numbers.",
          "If the input is a single number, the function should return that number instead of trying to compress it into a range."
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
        "why_needed": "This test prevents a potential bug where two consecutive numbers are not compressed to range notation."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0007397419999506383,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Input is sorted before compression.",
          "Ranges are grouped by their correct order (e.g., '1-3' for [1, 2, 3]).",
          "Ranges with the same start value but different end values are correctly separated ('5')."
        ],
        "scenario": "Test 'test_unsorted_input' verifies that the function handles unsorted input correctly.",
        "why_needed": "The test prevents a potential bug where the function would incorrectly group ranges in an unsorted list."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "81-82"
        }
      ],
      "duration": 0.0006833429999915097,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Input string is empty (length: 0).",
          "Function should return an empty list (`[]`)."
        ],
        "scenario": "Testing the `expand_ranges` function with an empty string.",
        "why_needed": "Prevents a potential bug where the function returns incorrect results for empty input strings."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 11,
          "line_ranges": "81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0006695590000163065,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input string should be expanded correctly to include all specified numbers in their respective ranges.",
          "Single numbers should not be included in the output if they do not fall within any range.",
          "Ranges should include all specified values without gaps or overlaps.",
          "Invalid characters (e.g., commas) should not affect the expansion of the input string.",
          "The function should handle edge cases where a single number is at the start/end of a range correctly.",
          "The function should handle ranges with overlapping values correctly.",
          "The function should raise an error for invalid input strings."
        ],
        "scenario": "test_mixed verifies the expansion of mixed range values.",
        "why_needed": "This test prevents a regression where the function incorrectly expands single numbers into ranges."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_mixed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "81, 84-91, 95"
        }
      ],
      "duration": 0.0007066679999638836,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input string should be in the format 'start-end' (e.g., '1-3')",
          "The function should return a list of numbers from start to end (inclusive)",
          "The start value should be less than or equal to the end value",
          "All values in the range should be integers",
          "No negative numbers should be allowed in the range",
          "The function should handle edge cases where the input string is empty or contains only one character"
        ],
        "scenario": "The 'expand_ranges' function is called with a string argument containing a range (e.g., '1-3') and it returns the expected result.",
        "why_needed": "This test prevents a potential bug where the function does not expand ranges correctly, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_range",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 27,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007030840000084027,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "original = [1, 2, 3, 5, 10, 11, 12, 15]",
          "compressed = compress_ranges(original)",
          "expanded = expand_ranges(compressed)",
          "assert expanded == original"
        ],
        "scenario": "The `compress_ranges` and `expand_ranges` functions should be able to reverse their operations.",
        "why_needed": "This test prevents a potential regression where the order of elements in the compressed or expanded ranges is not preserved."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 7,
          "line_ranges": "81, 84-87, 93, 95"
        }
      ],
      "duration": 0.0006800439999778973,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input string should be a single number (e.g., '5')",
          "The output list should contain only one element (i.e., [5])"
        ],
        "scenario": "The test verifies that the `expand_ranges` function returns a list containing only one element when given a single number.",
        "why_needed": "This test prevents regression where the function incorrectly expands multiple numbers into lists with multiple elements."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_single_number",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65, 67"
        }
      ],
      "duration": 0.0006932910000045922,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return '500ms' when given an input of 0.5 seconds.",
          "The function should return '1ms' when given an input of 0.001 seconds.",
          "The function should return '0ms' when given an input of 0.0 seconds."
        ],
        "scenario": "Test that the function formats duration as milliseconds for less than one second.",
        "why_needed": "Prevents a potential bug where the function does not correctly format durations for values less than 1s."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65-66"
        }
      ],
      "duration": 0.0007711089999702381,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Format should be in seconds', 'description': 'The format of the output is correct'}",
          "{'message': \"Output should contain 's' suffix\", 'description': \"The output contains a 's' suffix for durations greater than or equal to 1 second\"}",
          "{'message': 'No negative duration values are supported', 'description': 'Negative duration values are not supported by this function'}"
        ],
        "scenario": "Test that the function formats duration values correctly for seconds.",
        "why_needed": "The test prevents a potential bug where the function does not handle durations greater than or equal to 1 second correctly."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0007082660000037322,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "outcome-to-css_class('passed') == 'outcome-passed'",
          "outcome-to-css_class('failed') == 'outcome-failed'",
          "outcome-to-css_class('skipped') == 'outcome-skipped'",
          "outcome-to-css_class('xfailed') == 'outcome-xfailed'",
          "outcome-to-css_class('xpassed') == 'outcome-xpassed'",
          "outcome-to-css_class('error') == 'outcome-error'"
        ],
        "scenario": "All outcomes should map to CSS classes.",
        "why_needed": "This test prevents a potential CSS class mismatch issue where 'all' outcomes are incorrectly mapped to non-existent or incorrect classes."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0006933999999887419,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return the default class 'outcome-unknown' when given an unknown outcome.",
          "The function should not raise any exceptions or errors when given an unknown outcome.",
          "The function should correctly handle cases where the outcome is not recognized by the `outcome_to_css_class` mapping.",
          "The function's output should be consistent with the expected default class for unknown outcomes.",
          "The function's behavior should not change between different Python versions or environments."
        ],
        "scenario": "Tests the case where 'outcome' is unknown in `outcome_to_css_class` function.",
        "why_needed": "This test prevents a potential bug where the function returns an unexpected class for an unknown outcome."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 52,
          "line_ranges": "65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.000826238000001922,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The presence of '<!DOCTYPE html>' in the rendered HTML",
          "The presence of 'Test Report' in the rendered HTML",
          "The presence of 'test::passed' in the rendered HTML",
          "The presence of 'test::failed' in the rendered HTML",
          "The presence of 'PASSED' and 'FAILED' in the rendered HTML",
          "The presence of 'Plugin:</strong> v0.1.0' in the rendered HTML",
          "The presence of 'Repo:</strong> v1.2.3' in the rendered HTML"
        ],
        "scenario": "The test verifies that the report renders a complete HTML document with the expected elements.",
        "why_needed": "This test prevents regression where the report does not render correctly due to missing or incorrect plugin and repository information."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 52,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0007599099999993086,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The rendered HTML should include the specified file (src/foo.py).",
          "The rendered HTML should include the correct number of lines (5)."
        ],
        "scenario": "Test renders coverage to ensure that the rendered HTML includes the specified file and line count.",
        "why_needed": "This test prevents regression by ensuring that the rendered HTML includes the expected information, which is critical for maintaining code coverage."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 54,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0007403860000181339,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string 'Tests login flow' should be present in the rendered HTML report.",
          "The string 'Prevents auth bypass' should be present in the rendered HTML report.",
          "The LlmAnnotation object should contain a scenario attribute with value 'Tests login flow'.",
          "The LlmAnnotation object should contain a why_needed attribute with value 'Prevents auth bypass'.",
          "The html string should contain both 'Tests login flow' and 'Prevents auth bypass' strings."
        ],
        "scenario": "The test verifies that the LLM annotation is included in the rendered HTML report.",
        "why_needed": "This test prevents a potential security vulnerability where an attacker could manipulate the LLM annotations to bypass authentication."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 63,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0007263130000296769,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Source Coverage' section should be present in the rendered HTML report.",
          "The 'src/foo.py' file path should be included in the 'Source Coverage' section.",
          "The source coverage percentage should be displayed as '80.0%' in the rendered HTML report.",
          "All statements in the source code should be covered by at least 8% of the total statements in the test.",
          "At least 2 statements in the source code should be missed, which corresponds to 20% of the total statements in the test.",
          "The 'covered_ranges' and 'missed_ranges' sections should contain ranges that accurately represent the covered and missed statements respectively.",
          "All statements in the source code should be covered by at least 80% of the total statements in the test."
        ],
        "scenario": "Tests the inclusion of source coverage summary in the rendered HTML report.",
        "why_needed": "This test prevents a regression where the source coverage information is not included in the rendered HTML report, potentially misleading users about the code's coverage status."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 50,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.000742648000027657,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The HTML string contains the exact text 'XFailed', indicating that it was included in the rendered summary.",
          "The HTML string contains the exact text 'XPassed', indicating that it was included in the rendered summary.",
          "The presence of both 'XFailed' and 'XPassed' entries is confirmed by the assertion 'XFailed' being present in the HTML string.",
          "The absence of either 'XFailed' or 'XPassed' entry is not detected by this test.",
          "The test verifies that the rendered summary includes only one of each status, as expected for XPass tests."
        ],
        "scenario": "The test verifies that the rendered summary includes both 'XFailed' and 'XPassed' entries.",
        "why_needed": "This test prevents a regression where the summary is missing 'XFailed' or 'XPassed' entries when using XPass tests."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0006836350000298808,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `compute_sha256` correctly computes the SHA-256 hash for both input strings.",
          "The output of `compute_sha256(b'hello')` is different from the output of `compute_sha256(b'world')`.",
          "The computed hash values are unique and cannot be compared directly.",
          "The function handles non-string inputs correctly (e.g., bytes, integers).",
          "The test case covers a common edge case where input strings are different but have the same content.",
          "The output of `compute_sha256(b'hello')` is not equal to `compute_sha256(b'world')` even though they contain different characters.",
          "The function correctly handles duplicate input strings, as expected for different content."
        ],
        "scenario": "Test computes SHA-256 for different input strings and verifies the output is different.",
        "why_needed": "This test prevents a bug where the same input string produces the same hash value, potentially leading to incorrect reporting or analysis of data."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007975290000103996,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Input should be an empty bytes object (b'')",
          "Hash of the input should be equal to hash of another identical input (hash1 == hash2)",
          "Length of the hash output should match the expected SHA-256 hex length (64)"
        ],
        "scenario": "Test 'test_empty_bytes' verifies that an empty bytes input produces consistent hash.",
        "why_needed": "Prevents a potential bug where an empty bytes input could lead to inconsistent hashes due to the SHA-256 algorithm's behavior with zero-length inputs."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 67,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300"
        }
      ],
      "duration": 0.004194034000022384,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The duration of the test should be 60 seconds.",
          "The `pytest_version` attribute of the meta object should have a value.",
          "The `plugin_version` attribute of the meta object should be set to '0.1.0'.",
          "The `python_version` attribute of the meta object should also be set to a valid Python version."
        ],
        "scenario": "Test the `build_run_meta` method of ReportWriter with a test case that includes version info.",
        "why_needed": "This test prevents regression where the report writer does not include version information in the run metadata."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_run_meta",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 19,
          "line_ranges": "156-158, 312, 314-315, 317-328, 330"
        }
      ],
      "duration": 0.0007230450000292876,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of outcomes should be equal to the sum of passed, failed, skipped, xfailed, xpassed, and error outcomes.",
          "Each outcome type (passed, failed, skipped, xfailed, xpassed, error) should appear exactly once in the summary.",
          "The `total` count should match the expected value based on the number of tests with each outcome type.",
          "The `passed`, `failed`, `skipped`, `xfailed`, and `xpassed` counts should be accurate for each test.",
          "The `error` count should also be accurate for each test."
        ],
        "scenario": "Test verifies the ReportWriter's ability to build a summary of all outcome types.",
        "why_needed": "This test prevents regression where the `total` count is not accurate due to missing or incorrect outcome counts."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 312, 314-315, 317-322, 330"
        }
      ],
      "duration": 0.0007222030000093582,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "summary.total should equal 4 (the number of tests passed)",
          "summary.passed should equal 2 (the number of tests that passed)",
          "summary.failed should equal 1 (the number of tests that failed)",
          "summary.skipped should equal 1 (the number of tests that were skipped)"
        ],
        "scenario": "The test verifies that the `total` count of outcomes is accurate.",
        "why_needed": "This test prevents a regression where the total count of outcomes might be incorrect due to missing or incomplete tests."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_counts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "156-158"
        }
      ],
      "duration": 0.0007256210000150531,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` attribute of the `ReportWriter` instance should be equal to the provided `Config` object.",
          "The `warnings` list of the `ReportWriter` instance should be empty.",
          "The `artifacts` list of the `ReportWriter` instance should be empty."
        ],
        "scenario": "Test the functionality of creating a ReportWriter instance.",
        "why_needed": "To prevent a potential bug where the Writer's configuration or artifacts are not properly initialized."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_create_writer",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 93,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330"
        }
      ],
      "duration": 0.004109949999985929,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the report.tests list should be equal to 2.",
          "The total value of report.summary.total should be equal to 2."
        ],
        "scenario": "Test that ReportWriter writes a report with all tests.",
        "why_needed": "This test prevents regression where the report does not include all tests, potentially leading to incomplete or misleading reports."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 93,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330"
        }
      ],
      "duration": 0.004646479000030013,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `coverage_total_percent` attribute of the `report.summary` object should be equal to the provided `coverage_percent` value.",
          "The `report.summary.coverage_total_percent` property should have a numerical value.",
          "The coverage percentage should be calculated correctly based on the report's data.",
          "The test should fail if the coverage percentage is not included in the report.",
          "The coverage percentage should be reported as a percentage value (e.g., 85.5%).",
          "The `report.summary.coverage_total_percent` property should update correctly after writing the report."
        ],
        "scenario": "The test verifies that the `ReportWriter` class writes a report with a total coverage percentage.",
        "why_needed": "This test prevents regression where the coverage percentage is not included in the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 92,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330"
        }
      ],
      "duration": 0.004128260000015871,
      "llm_annotation": {
        "error": "Failed after 3 retries. Last error: Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 94,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330"
        }
      ],
      "duration": 0.00394346000001633,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The first test's coverage should be a single entry with file path 'src/foo.py'.",
          "The first test's coverage should only contain the specified file path.",
          "The number of entries in the report's first test's coverage should match the number of tests.",
          "The first test's coverage should have exactly one file path."
        ],
        "scenario": "Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.",
        "why_needed": "This test prevents regression in case where multiple tests have overlapping code coverage."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 67,
          "line_ranges": "229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 125,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516"
        }
      ],
      "duration": 0.005400921999978436,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'report.json' should exist at the expected location.",
          "Any warnings with code 'W203' (indicating a permission issue) should be present in the report."
        ],
        "scenario": "Test that the ReportWriterWithFiles class falls back to direct write if an atomic write fails and the 'os.replace' mock is used.",
        "why_needed": "This test prevents a regression where the atomic write operation fails, but instead of raising an exception, it falls back to direct write."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 84,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 229-231, 233, 235, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510-512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.005434055999955945,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `tmp_path / 'subdir' / 'report.json'` path should exist after calling `writer.write_report(tests)`.",
          "The `tmp_path / 'subdir' / 'report.json'` path should be a directory and not an empty file after calling `writer.write_report(tests)`.",
          "The `tmp_path / 'subdir' / 'report.json'` path should have the correct permissions (e.g. read, write, execute) after calling `writer.write_report(tests)`.",
          "The `tmp_path / 'subdir' / 'report.json'` path should be a directory with the correct number of subdirectories and files after calling `writer.write_report(tests)`.",
          "The `tmp_path / 'subdir' / 'report.json'` path should not have any empty directories or files after calling `writer.write_report(tests)`.",
          "The `tmp_path / 'subdir' / 'report.json'` path should be a directory with the correct permissions (e.g. read, write, execute) when checking its existence using `assert tmp_path / 'subdir' / 'report.json'.exists()`.",
          "The `tmp_path / 'subdir' / 'report.json'` path should not exist after calling `writer.write_report(tests)` if the input JSON file does not exist."
        ],
        "scenario": "Test verifies that the `ReportWriter` creates a directory if it doesn't exist.",
        "why_needed": "This test prevents a bug where the report writer fails to create an output directory when the input JSON file does not exist."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 12,
          "line_ranges": "156-158, 470-473, 480-484"
        }
      ],
      "duration": 0.0011661369999842464,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should raise an exception with code 'W201' (Permission denied) when attempting to create the directory.",
          "The `writer.warnings` list should contain at least one warning with code 'W201'.",
          "The `writer.warnings` list should not be empty after attempting to create the non-existent directory."
        ],
        "scenario": "Test the `test_ensure_dir_failure` function to ensure it correctly handles directory creation failures.",
        "why_needed": "This test prevents a potential bug where the report writer fails to capture warnings when creating a non-existent directory."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.0009679870000240953,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return `None` for both `sha` and `dirty` when `git not found` is encountered.",
          "The function should handle the case where `git not found` raises an exception without raising it again.",
          "The function should still allow the test to continue running even if `get_git_info()` returns non-None values for other reasons (e.g., successful git command execution).",
          "The function should provide a clear indication that the git command failed, such as returning `None` or raising an exception with a meaningful error message.",
          "The function should not silently ignore the failure and return incorrect results.",
          "The function should handle the case where `git not found` is a known Git version (e.g., older versions) without introducing new bugs."
        ],
        "scenario": "Test that the `get_git_info` function handles git command failures gracefully by returning `None` for both SHA and dirty flag when `git not found`,",
        "why_needed": "To prevent a test failure where the test relies on `get_git_info()` to return non-None values, even if it encounters a Git command failure."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.02994098699997494,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The HTML file should be created at `report.html` in the temporary directory.",
          "The HTML file should contain the expected tests, including 'test1' and 'test2'.",
          "The HTML file should display a message indicating that all tests passed ('PASSED') or failed with an error message ('XFailed').",
          "The HTML file should include messages for skipped tests ('Skipped'), Xfailed tests ('XFailed'), and Xpassed tests ('XPassed').",
          "The HTML file should contain the expected report header, including 'Test 1', 'Test 2', 'PASSED', 'FAILED', 'Skipped', 'XFailed', and 'XPassed'."
        ],
        "scenario": "Test `test_write_html_creates_file` verifies that the report writer creates an HTML file and includes expected content.",
        "why_needed": "This test prevents a regression where the report writer does not create an HTML file or contains incorrect expected content."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.031024790000003577,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'XFAILED' keyword is present in the HTML summary.",
          "The 'XFailed' keyword is present in the HTML summary.",
          "The 'XPASSED' keyword is present in the HTML summary.",
          "The 'XPassed' keyword is present in the HTML summary.",
          "The 'XFAILED' and 'XFailed' keywords are both present in the HTML summary.",
          "The 'XPassed' and 'XPassed' keywords are both present in the HTML summary.",
          "All xfail outcomes are included in the HTML summary."
        ],
        "scenario": "The test verifies that the report writer includes xfail outcomes in the HTML summary.",
        "why_needed": "This test prevents a regression where xfail outcomes are not included in the HTML summary."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 78,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 117,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.004936604000022271,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should be created in the specified path.",
          "At least one artifact should be tracked for the report.",
          "The number of artifacts should be greater than zero.",
          "The file should exist at the specified path.",
          "The `ReportWriter` class should successfully create a JSON file with an associated hash."
        ],
        "scenario": "Test verifies that the `ReportWriter` class successfully creates a JSON file with an associated hash.",
        "why_needed": "This test prevents a bug where the report writer fails to create a JSON file, potentially leading to data loss or inconsistencies."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 125,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471"
        }
      ],
      "duration": 0.03286640799996121,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.pdf` path should be created.",
          "All artifacts in the report directory should have a path equal to the `report.pdf` path.",
          "The `report.pdf` file should exist at the expected location.",
          "Any artifacts with paths not matching the `report.pdf` path should be ignored."
        ],
        "scenario": "Test writes PDF file when Playwright is available.",
        "why_needed": "Prevents regression where the test fails due to missing or corrupted report files."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408"
        }
      ],
      "duration": 0.0049584989999971185,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'report.pdf' file should not exist after the test.",
          "Any warning code W204_PDF_PLAYWRIGHT_MISSING should be present in the list of warnings."
        ],
        "scenario": "Test verifies that a warning is raised when missing Playwright is used for PDF output.",
        "why_needed": "This test prevents a regression where the report writer does not warn users when Playwright is missing for PDF output."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 11,
          "line_ranges": "156-158, 470-477"
        }
      ],
      "duration": 0.0009286869999982628,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `tmp_dir` exists after the test.",
          "Any warnings from the report writer are set to 'W202' (directory creation warning).",
          "The directory containing the report files (`r.html`) is deleted afterwards."
        ],
        "scenario": "Test ensures directory creation of report files.",
        "why_needed": "Prevents a potential issue where the report file is created without being written to."
      },
      "nodeid": "tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 36,
          "line_ranges": "364-380, 382-393, 395, 397, 399, 401, 403, 407, 419"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "107, 147"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 67,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300"
        }
      ],
      "duration": 0.007612841999957709,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'start_time' key should be present in the metadata.",
          "Metadata should not contain the 'llm_model' key.",
          "The 'llm_model' key should be None."
        ],
        "scenario": "Tests the scenario where report_writer_metadata_skips verifies that metadata skips when reports are disabled.",
        "why_needed": "This test prevents regression by ensuring that metadata is skipped when reports are disabled, which helps maintain the expected behavior of the ReportWriter."
      },
      "nodeid": "tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007894620000001851,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert schema.scenario == 'Verify login'",
          "assert schema.why_needed == 'Catch auth bugs'",
          "assert schema.key_assertions == ['assert 200', 'assert token']",
          "assert schema.confidence == 0.95"
        ],
        "scenario": "Test that the `from_dict` method correctly creates an annotation from a dictionary with all required fields.",
        "why_needed": "Prevents regression in handling missing or invalid input data, ensuring consistent behavior across different scenarios."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 8,
          "line_ranges": "90-92, 94-98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        }
      ],
      "duration": 0.0007179540000379347,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert data['scenario'] == 'Verify login'",
          "assert data['why_needed'] == 'Catch auth bugs'",
          "assert data['key_assertions'] == ['assert 200', 'assert token']",
          "assert data['confidence'] == 0.95"
        ],
        "scenario": "Should convert to dictionary with all fields.",
        "why_needed": "Prevent regression in authentication logic by ensuring correct data is passed to the API."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 101,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.0794816320000109,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file path of the generated report should exist.",
          "The content of the report should contain the string '<html',",
          "The string 'test_simple' should be present in the report's content."
        ],
        "scenario": "Test that an HTML report is created when the test function `test_simple` is executed.",
        "why_needed": "This test prevents a regression where the HTML report might not be generated correctly if the test function `test_simple` raises an exception."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 65,
          "line_ranges": "78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 111,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.11141705000000002,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Total Tests' label should be included in the HTML summary.",
          "The 'Passed' label should have a count of 1.",
          "The 'Failed' label should have a count of 1.",
          "The 'Skipped' label should have a count of 1.",
          "The 'XFailed' label should have a count of 1.",
          "The 'XPassed' label should have a count of 1.",
          "The 'Errors' and 'Error' labels should be included in the HTML summary with correct counts."
        ],
        "scenario": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
        "why_needed": "This test prevents a regression where the HTML summary counts are not accurate for all statuses."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 51,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 107,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.06471056399999497,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `schema_version` key in the report should be set to the current LLM Report JSON format.",
          "The `summary` key should contain the correct total, passed, and failed counts.",
          "The `passed` count should match the number of tests that passed.",
          "The `failed` count should match the number of tests that failed.",
          "The `report.json` file should exist in the test directory after running the test.",
          "The JSON data loaded from the report file should contain the expected keys and values.",
          "The `schema_version` value should be a string representing the current LLM Report JSON format."
        ],
        "scenario": "The JSON report is created and contains the expected schema version, summary statistics, and test counts.",
        "why_needed": "This test prevents regression by ensuring that a basic report generation functionality is working correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 39,
          "line_ranges": "52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 23,
          "line_ranges": "37-38, 44, 46, 49, 51-52, 54-60, 62-63, 66-67, 69-70, 94-95, 97"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 94,
          "line_ranges": "104-107, 109-111, 113, 115, 161-165, 167, 169, 171, 173, 176, 178-180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407-419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 47,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 186,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-352, 355, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.07475506299999779,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' key in the report should match 'Checks the happy path'.",
          "The 'why_needed' key in the report should indicate that LLM annotations prevent regressions.",
          "The 'llm_annotation' key in the report data should contain a 'scenario' value matching 'Checks the happy path'.",
          "The 'choices' key in the report data should contain at least one 'message' value with a 'content' value matching 'Checks the happy path'.",
          "The 'key_assertions' list should not be empty.",
          "The test function 'test_llm_annotations_in_report' should be called before running this test."
        ],
        "scenario": "Verify that LLM annotations are included in the report when a provider is enabled.",
        "why_needed": "Prevent regressions by ensuring LLM annotations are present in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 12,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 73,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 21,
          "line_ranges": "52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 25,
          "line_ranges": "37-38, 44, 46, 49, 51-52, 54-60, 62-63, 78-79, 81-82, 84-85, 94-95, 97"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 47,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 186,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203-205, 207-208, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-340, 343, 345, 348-353, 357-362, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 101,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 6.079335301000015,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that an error message 'LLM error' is present in the report.",
          "The test verifies that the error message contains the string 'boom'.",
          "The test checks for the presence of the LLM error message in the HTML output.",
          "The test ensures that the error message is not empty or null.",
          "The test verifies that the error message does not contain any other relevant information.",
          "The test checks if the error message is correctly formatted and readable."
        ],
        "scenario": "Verifies that LLM errors are surfaced in HTML output.",
        "why_needed": "Prevents a regression where LLM errors are not reported to the user."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180-182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05345342900000105,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' marker should be present in the report.",
          "The 'llm_opt_out' marker should have a value of True for this test.",
          "There should only be one test with the 'llm_opt_out' marker in the report."
        ],
        "scenario": "Verify that the LLM opt-out marker is correctly recorded in the report.",
        "why_needed": "This test prevents regression where the LLM opt-out marker is not recorded in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167, 169, 171, 173, 176, 178, 180, 182, 184, 186, 188-190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05269241999997121,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `pytester.makepyfile` is called with a string containing the required requirements.",
          "The `test_with_req` function is defined and passed to `pytester.makepyfile`.",
          "The test data is loaded from a JSON file using `json.loads`.",
          "The tests in the report are checked for exactly one requirement.",
          "The 'requirements' key is present in each test's dictionary.",
          "The value of 'requirements' is either an empty list or a string containing the required requirements.",
          "The required requirements are not empty strings."
        ],
        "scenario": "Test the requirement marker to verify it records a specific requirement.",
        "why_needed": "This test prevents a potential bug where multiple requirements are not properly recorded by the requirement marker."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 108,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05841522600002236,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "2",
          "['xfailed', 'xfailed']"
        ],
        "scenario": "The test verifies that multiple xfailed tests are recorded in the report.",
        "why_needed": "This test prevents regression by ensuring that all xfailed tests are properly reported and counted."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 43,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 107,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05440675500000225,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of skipped tests should be equal to 1 according to the report.",
          "The 'skipped' key in the report data should contain a single integer value, which represents the total number of skipped tests.",
          "The 'summary' section of the report data should include a key called 'skipped' with a value of 1."
        ],
        "scenario": "Test that skipped tests are correctly recorded and reported.",
        "why_needed": "This test prevents a regression where the number of skipped tests is not accurately reported in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167-169, 171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 108,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05455182999997987,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the report.json file should contain a count of xfailed tests.",
          "The 'xfailed' value in the 'summary' key should be equal to 1 (indicating one xfailed test).",
          "The 'pytester.path' attribute should have been updated with the correct path to the report.json file after running pytest.",
          "The 'report.json' file should contain a valid JSON object with the expected keys and values.",
          "The 'summary' key in the 'report.json' file should be present and contain the correct data.",
          "The 'xfailed' value in the 'summary' key should be an integer (not a string).",
          "The 'pytester.runpytest' command should have been executed successfully with the '--llm-report-json' option set."
        ],
        "scenario": "Verify that xfailed tests are recorded and reported correctly.",
        "why_needed": "This test prevents a regression where xfailed tests are not properly recorded or reported."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 74,
          "line_ranges": "161-165, 167, 169-171, 173, 176, 178, 180, 182, 184, 186, 188, 190, 364-380, 382, 385, 387, 390-393, 395, 397, 399, 401, 403, 407, 419, 449-457, 459, 461, 500, 502-506, 508, 510, 512, 514, 516, 518, 520, 522"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272-274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.055924785000001975,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of tests should be 3 (1 passed, 2 failed).",
          "Each test should have been run at least once. The 'passed' count should match the actual number of runs for each test.",
          "No duplicate or missing test counts should be reported in the report.",
          "All test results should be included in the report even if they are not passed (i.e., all tests ran).",
          "The 'total' and 'passed' counts should add up to 3 (the total number of tests)."
        ],
        "scenario": "Test the parameterized tests feature to verify that it records and reports the correct number of tests.",
        "why_needed": "This test prevents a regression where the number of passed tests is not reported correctly in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 45,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 118,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.04591746200003399,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of running `pytester.runpytest('--help')` should contain lines matching `*Example:*--llm-report*` in the output.",
          "The line containing `*Example:*--llm-report*` should be present in the output.",
          "The output should not contain any other usage examples besides those mentioned above.",
          "The output should not contain any examples that are not related to LLM reporting.",
          "The output should include a clear and concise description of how to use the CLI help text.",
          "The output should not contain any typos or grammatical errors in the help text.",
          "The output should be readable and easy to understand, even for users who are not familiar with the command line."
        ],
        "scenario": "The test verifies that the CLI help text contains usage examples.",
        "why_needed": "This test prevents a potential bug where the help text is missing or misleading."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 45,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 118,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.04063938999996708,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' marker should be found in the output.",
          "The 'llm_context' marker should be found in the output.",
          "The 'requirement' marker should be found in the output.",
          "The 'llm_opt_out' marker should not be present in the output if it is properly registered.",
          "The 'llm_context' marker should only appear once per test case, and its presence should be verified across all tests.",
          "The 'requirement' marker should only appear once per test case, and its presence should be verified across all tests."
        ],
        "scenario": "Verify that LLM markers are registered and their context is properly set.",
        "why_needed": "To prevent a regression where the LLM marker is not correctly registered or its context is not set."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 45,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270, 272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 118,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 387-388, 391, 395-397"
        }
      ],
      "duration": 0.04713113000002522,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "pytester.runpytest('--help')",
          "result.stdout.fnmatch_lines(['*--llm-report*'])"
        ],
        "scenario": "The plugin should be successfully registered with pytest.",
        "why_needed": "This test prevents a potential issue where the plugin is not registered correctly with pytest."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 46,
          "line_ranges": "107, 147, 175, 178-179, 185-186, 193-194, 201-202, 209, 211, 213, 215, 217, 220, 224, 248, 251-253, 255-259, 261, 263-265, 270-272, 274, 276, 278, 280, 282, 286, 288, 290, 292, 294, 298, 300"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 166,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 169-171, 173-175, 177-179, 183, 187-188, 190, 192, 195-196, 203, 212-213, 238, 242, 246, 249, 268-269, 276-277, 280-281, 283-284, 287-291, 293, 296-297, 299, 302-303, 331, 337-338, 365-375, 387-388, 391, 395-397, 408, 412, 431, 435-437, 448, 452, 455, 457-458"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 101,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.07929789199999959,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '<' character should be present in the report.html file.",
          "The '>' character should not be present in the report.html file.",
          "The '&' character should not be present in the report.html file.",
          "The '<' and '>' characters are present in the report.html file, but they are not escaped.",
          "The HTML content of the report.html file does not contain any special characters."
        ],
        "scenario": "Verify that special characters in nodeid are handled correctly during Pytest execution.",
        "why_needed": "This test prevents a potential crash and ensures the HTML generated by Pytest is valid."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007241879999924095,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result should be in the format '1m 0.0s'.",
          "The time unit 's' should appear after '1' in the result.",
          "The decimal part of the number should be zero.",
          "The function should return an error or raise a ValueError for durations less than one minute.",
          "The function should correctly handle cases where the duration is exactly one minute (e.g., 60.0).",
          "The function should not incorrectly format other types of durations (e.g., two minutes, three seconds)."
        ],
        "scenario": "Tests the 'format_duration' function with a duration of exactly one minute.",
        "why_needed": "This test prevents a potential bug where the function does not correctly format durations that are close to but less than one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007039619999886781,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result should contain '\u03bcs' (microseconds).",
          "The result should be equal to '500\u03bcs'."
        ],
        "scenario": "Tests the `format_duration` function with a duration of 500 microseconds.",
        "why_needed": "This test prevents a potential bug where the function does not correctly format durations as microseconds when they are very close to millisecond values."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0006893319999790037,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `format_duration(0.5)` should be '500.0ms'.",
          "The value of `result` should match '500.0ms'."
        ],
        "scenario": "Verifies that the `format_duration` function correctly formats sub-second durations as milliseconds.",
        "why_needed": "This test prevents a regression where the function incorrectly returns 'ms' when the input is an integer."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0006854069999917556,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result contains the string 'm' which indicates the unit of measurement (minutes).",
          "The result equals '1m 30.5s' which correctly formats the duration as one minute and thirty-five seconds."
        ],
        "scenario": "Verifies that the function correctly formats durations over a minute.",
        "why_needed": "This test prevents regression if the format_duration function is modified to incorrectly handle minutes."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_minutes_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0006979159999787043,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result should be '3m 5.0s' when given a duration of 185.0 minutes.",
          "The result should not have any seconds for durations less than 1 minute.",
          "The function should correctly handle durations in the format 'mm:ss'.",
          "The function should preserve the original unit ('minutes') for durations with more than one unit.",
          "The function should handle negative durations without raising an error.",
          "The function should support durations of any length (e.g., 2 minutes, 3.5 hours)."
        ],
        "scenario": "Tests the `format_duration` function with a duration of multiple minutes.",
        "why_needed": "Prevents regression in formatting durations that include more than one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0006905119999487397,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `format_duration` function should return a string representation of '1.00s' when given a duration of exactly 1 second.",
          "The `format_duration` function should not raise an exception when given a non-numeric input (e.g., a float or integer).",
          "The `format_duration` function should handle durations less than 1 second correctly and return the expected string representation ('1.00s')."
        ],
        "scenario": "Verifies that the `format_duration` function returns a string representation of '1.00s' for a duration of exactly 1 second.",
        "why_needed": "Prevents a potential bug where the test fails due to an incorrect or missing expected result for durations less than 1 second."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_one_second",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0006910009999501199,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return 's' as part of the result.",
          "The formatted string should be in the format 'X.Xs'.",
          "The value of X.X should be exactly 5.50.",
          "The unit 's' should always be included in the output."
        ],
        "scenario": "Test the 'seconds' unit of time.",
        "why_needed": "Prevents incorrect formatting of seconds under a minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_seconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0006976410000447686,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of calling `format_duration(0.001)` should be '1.0ms'.",
          "The duration is represented as millisecond (ms) rather than seconds.",
          "The function handles durations less than one second correctly."
        ],
        "scenario": "Tests the `format_duration` function with a small millisecond duration.",
        "why_needed": "This test prevents a potential issue where the function returns incorrect results for very short durations, potentially leading to inaccuracies in time calculations."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0006952620000220122,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of calling `format_duration(0.000001)` should be '1\u03bcs'.",
          "The unit of the output value is microsecond (\u03bcs).",
          "The input duration is correctly converted to microseconds.",
          "No other units are applied when converting 1 microsecond to microseconds.",
          "The function does not silently truncate or round the result.",
          "The function handles very small inputs without error.",
          "The function preserves the original precision of the input value.",
          "The output value is a string representation of the duration in microseconds."
        ],
        "scenario": "Verifies that the `format_duration` function correctly formats very small durations as microseconds when the input is 1 microsecond.",
        "why_needed": "This test prevents a potential bug where the `format_duration` function would incorrectly format very small durations as milliseconds instead of microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007344420000094942,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `iso_format(dt)` should be in the format `YYYY-MM-DDTHH:MM:SS+HH:MM:SSZ`.",
          "The timezone offset of the input datetime object should be correctly converted to UTC.",
          "The resulting string should not have a timezone offset.",
          "The resulting string should match the expected ISO 8601 format."
        ],
        "scenario": "Test formats datetime with UTC and ensures it is correctly converted to UTC timezone.",
        "why_needed": "This test prevents a potential bug where the `iso_format` function incorrectly converts datetime objects from other timezones to UTC."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007013119999896844,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `iso_format(dt)` returns the correct ISO formatted string for the given datetime object.",
          "The datetime object `dt` is in the naive timezone (i.e., no timezone offset).",
          "The resulting ISO formatted string matches the expected output '2024-06-20T14:00:00'."
        ],
        "scenario": "Tests the naive datetime format without timezone to ensure it matches the expected output.",
        "why_needed": "This test prevents a potential bug where the naive datetime format is not correctly converted to UTC time zone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0006988129999854209,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of calling `iso_format(dt)` should contain the string '123456'.",
          "The value in the result should be exactly '123456'."
        ],
        "scenario": "Tests the `iso_format` function with a datetime object that includes microseconds.",
        "why_needed": "This test prevents a potential issue where the `iso_format` function does not correctly handle datetime objects with microseconds."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_with_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007191799999759496,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned datetime object has a valid timezone.",
          "The returned datetime object is not None (i.e., it's a valid result).",
          "The returned datetime object's timezone is equal to UTC.",
          "The returned datetime object does not have an invalid or missing timezone.",
          "The returned datetime object's timezone is correctly set to UTC."
        ],
        "scenario": "Verifies that the `utc_now()` function returns a datetime object with an associated UTC timezone.",
        "why_needed": "Prevents regression in time-related functionality when working with dates and times."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0006988690000184761,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `utc_now()` should be within one second of the current UTC time.",
          "The result of `utc_now()` should not exceed the current UTC time by more than one second.",
          "The result of `utc_now()` should not be less than the current UTC time by more than one second."
        ],
        "scenario": "Verify that the `utc_now()` function returns a current time within UTC.",
        "why_needed": "This test prevents a potential issue where the `utc_now()` function returns an incorrect or outdated time due to clock skew."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_is_current_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "387-388, 391, 395-397"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007926870000005692,
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "result is an instance of `datetime`",
          "result is not None",
          "result is of type `datetime` (not `timedelta`)",
          "result is not a timezone-aware datetime object (it's naive)",
          "result has a valid year, month and day",
          "result has a valid hour, minute and second",
          "result has the correct timezone"
        ],
        "scenario": "The `utc_now()` function returns a datetime object.",
        "why_needed": "This test prevents a potential issue where the returned datetime object might not be in UTC."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_returns_datetime",
      "outcome": "passed",
      "phase": "call"
    }
  ]
}