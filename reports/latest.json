{
  "run_meta": {
    "aggregation_policy": null,
    "collect_only": false,
    "collected_count": 623,
    "deselected_count": 0,
    "duration": 120.438941,
    "end_time": "2026-01-21T07:48:14.683685+00:00",
    "exit_code": 0,
    "git_dirty": true,
    "git_sha": "af30b58f9617c57b114b26afe3e20619a38888d8",
    "interrupted": false,
    "is_aggregated": true,
    "llm_annotations_count": 620,
    "llm_annotations_enabled": true,
    "llm_annotations_errors": 2,
    "llm_context_mode": "minimal",
    "llm_model": "llama3.2:1b",
    "llm_provider": "ollama",
    "llm_total_input_tokens": 135663,
    "llm_total_output_tokens": 75900,
    "llm_total_tokens": 211563,
    "platform": "Linux-6.11.0-1018-azure-x86_64-with-glibc2.39",
    "plugin_git_dirty": true,
    "plugin_git_sha": "a03dbe622cdc018f89b74731aed91adf1a582867",
    "plugin_version": "0.2.1",
    "pytest_version": "9.0.2",
    "python_version": "3.12.12",
    "repo_git_dirty": true,
    "repo_git_sha": "af30b58f9617c57b114b26afe3e20619a38888d8",
    "repo_version": "0.2.1",
    "rerun_count": 0,
    "run_count": 1,
    "run_id": "21201358217-py3.12",
    "selected_count": 623,
    "source_reports": [],
    "start_time": "2026-01-21T07:46:14.244744+00:00"
  },
  "schema_version": "1.1.0",
  "sha256": "11246eb819e69f814ba5edffac605477ec420fc6ac4835ea2f78eeefd473f70c",
  "source_coverage": [
    {
      "coverage_percent": 100.0,
      "covered": 2,
      "covered_ranges": "2-3",
      "file_path": "src/pytest_llm_report/_git_info.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 2
    },
    {
      "coverage_percent": 95.04,
      "covered": 115,
      "covered_ranges": "13, 15-19, 21, 36, 39, 45, 47, 53-54, 56-58, 60, 62-65, 70, 74-75, 78-81, 85, 88-90, 94, 104, 110, 113-115, 117-121, 123-124, 129, 131-132, 134-135, 138-139, 145-147, 149, 152, 155, 158, 160, 162, 176, 178, 182, 184, 186, 196, 198-202, 204-205, 208, 210, 219, 231, 233-247, 249, 251, 259-260, 262-263, 265, 267-269, 273, 276-277, 279-280, 283, 285-286, 288, 290-291, 295",
      "file_path": "src/pytest_llm_report/aggregation.py",
      "missed": 6,
      "missed_ranges": "67, 91-92, 111, 206, 217",
      "statements": 121
    },
    {
      "coverage_percent": 93.62,
      "covered": 44,
      "covered_ranges": "13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153",
      "file_path": "src/pytest_llm_report/cache.py",
      "missed": 3,
      "missed_ranges": "64-65, 130",
      "statements": 47
    },
    {
      "coverage_percent": 99.1,
      "covered": 110,
      "covered_ranges": "19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140-141, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285",
      "file_path": "src/pytest_llm_report/collector.py",
      "missed": 1,
      "missed_ranges": "239",
      "statements": 111
    },
    {
      "coverage_percent": 94.34,
      "covered": 50,
      "covered_ranges": "13-15, 18, 27, 29-31, 33, 35-36, 38-41, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 72, 81-82, 86, 88-90, 93, 96, 108, 111, 124, 126-127, 129-130, 133, 135",
      "file_path": "src/pytest_llm_report/context_util.py",
      "missed": 3,
      "missed_ranges": "53, 83-84",
      "statements": 53
    },
    {
      "coverage_percent": 95.56,
      "covered": 129,
      "covered_ranges": "13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127-128, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-250, 252-254, 257, 259-260, 263-264, 271, 273-274, 276-279, 281-283, 285, 299-300, 302, 308",
      "file_path": "src/pytest_llm_report/coverage_map.py",
      "missed": 6,
      "missed_ranges": "62, 123, 125, 157, 221, 251",
      "statements": 135
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 73, 77-79, 83, 132, 142",
      "file_path": "src/pytest_llm_report/errors.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 3,
      "covered_ranges": "4-5, 7",
      "file_path": "src/pytest_llm_report/llm/__init__.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 3
    },
    {
      "coverage_percent": 86.36,
      "covered": 133,
      "covered_ranges": "4, 6-10, 12-15, 21-22, 25-30, 33, 47-48, 50-52, 56, 58-59, 65, 67-68, 70, 73-74, 76, 84, 86-90, 95-96, 98-99, 106-107, 112-113, 116, 121-126, 130, 132, 134, 137, 144, 156, 181-182, 184, 186, 188-189, 199, 211, 213-216, 221-223, 226, 249-252, 254-255, 260, 262, 264-267, 269-270, 277-279, 281, 283-284, 289-290, 292-293, 298-301, 303, 306, 329-332, 334, 336, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381",
      "file_path": "src/pytest_llm_report/llm/annotator.py",
      "missed": 21,
      "missed_ranges": "77-81, 160-168, 173, 286-287, 345, 364-365, 371",
      "statements": 154
    },
    {
      "coverage_percent": 95.42,
      "covered": 125,
      "covered_ranges": "13, 15-18, 20, 30, 33, 47, 50, 53, 59, 65-66, 68, 87-88, 96, 101, 103, 105, 128, 134-135, 137-138, 149, 155, 157, 163, 165, 174, 176, 185-186, 188, 191-198, 200, 202, 212, 214-217, 219-222, 224, 232, 243, 245, 247, 264, 266-267, 270-272, 274-275, 277, 279, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314, 316, 325-326, 329-331, 333-334, 337-339, 342-347, 351, 353, 359-360, 363-364, 367-369, 372, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404, 406",
      "file_path": "src/pytest_llm_report/llm/base.py",
      "missed": 6,
      "missed_ranges": "91-92, 230, 284, 292, 296",
      "statements": 131
    },
    {
      "coverage_percent": 95.56,
      "covered": 86,
      "covered_ranges": "8, 10-13, 20, 23-24, 27-29, 31-32, 34, 36-37, 39, 44, 53-55, 58, 67-68, 70, 73, 92-93, 95, 97, 103-106, 108-110, 112, 122-123, 126-128, 136, 139, 156-157, 160, 162, 164-167, 170-176, 181-185, 187-188, 190, 192-194, 196-197, 203-206, 209-210, 213-214, 216-218, 222, 224",
      "file_path": "src/pytest_llm_report/llm/batching.py",
      "missed": 4,
      "missed_ranges": "158, 207, 211, 220",
      "statements": 90
    },
    {
      "coverage_percent": 97.85,
      "covered": 318,
      "covered_ranges": "7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-89, 91-97, 99-114, 121-122, 125, 128, 134-135, 137-141, 143-144, 146, 164-166, 173-175, 178, 181-182, 184, 186-189, 191-192, 198-206, 208-210, 212-213, 215, 218, 221-230, 232-233, 235-237, 239-243, 246-247, 249-252, 254-255, 259, 261, 263, 268, 272-276, 279-281, 283, 288-293, 295, 299-305, 308-309, 311-312, 318-319, 322, 326, 332-333, 335, 339-343, 345-349, 352-353, 358-359, 366-367, 369, 383, 385-386, 390, 410, 413-415, 418-422, 424-427, 432, 434-435, 437, 441-444, 446, 449-463, 469, 471-473, 475-478, 480, 486, 488-491, 493, 495, 497-498, 502-508, 511, 514-516, 518-521, 523-528, 534, 537, 539-543, 547-548, 550-559, 562-564, 567-570, 574",
      "file_path": "src/pytest_llm_report/llm/gemini.py",
      "missed": 7,
      "missed_ranges": "115-117, 298, 310, 313-314",
      "statements": 325
    },
    {
      "coverage_percent": 98.7,
      "covered": 76,
      "covered_ranges": "8, 10, 12-13, 21, 31, 37-38, 41-42, 44, 51, 60-62, 64, 82-83, 89, 92, 95-96, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 122, 124-126, 129-130, 132, 135, 137, 139, 141-142, 144, 148, 170, 182-183, 186-188, 190, 192-193, 196-198, 204, 206, 211, 213, 215, 221-222, 224, 227-231, 234, 236, 242-243, 245",
      "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
      "missed": 1,
      "missed_ranges": "207",
      "statements": 77
    },
    {
      "coverage_percent": 100.0,
      "covered": 13,
      "covered_ranges": "8, 10, 12-13, 20, 26, 32, 34, 51, 53, 59, 61, 67",
      "file_path": "src/pytest_llm_report/llm/noop.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 13
    },
    {
      "coverage_percent": 98.61,
      "covered": 71,
      "covered_ranges": "7, 9, 11-12, 18, 24, 42-43, 49, 52-53, 55, 58, 60-61, 63-67, 70, 74-77, 83, 85-86, 92, 94, 96-98, 100-101, 103, 107, 113-114, 116-118, 122, 128, 130, 138, 140, 142-144, 149-150, 156, 158, 160-162, 165-167, 172-173, 178, 180, 190, 192-193, 204, 209, 211-212",
      "file_path": "src/pytest_llm_report/llm/ollama.py",
      "missed": 1,
      "missed_ranges": "90",
      "statements": 72
    },
    {
      "coverage_percent": 97.22,
      "covered": 35,
      "covered_ranges": "8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130",
      "file_path": "src/pytest_llm_report/llm/schemas.py",
      "missed": 1,
      "missed_ranges": "39",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 71,
      "covered_ranges": "7, 9-14, 17, 20, 23-24, 36-39, 41-43, 47, 59-60, 63-66, 69-72, 74, 83, 85-88, 90-91, 93, 101-103, 107-109, 111, 113-116, 120, 132-136, 139-140, 143-145, 148-150, 153-156, 158, 160-162",
      "file_path": "src/pytest_llm_report/llm/token_refresh.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 71
    },
    {
      "coverage_percent": 93.94,
      "covered": 31,
      "covered_ranges": "4, 6, 9, 20, 23, 42-43, 46-47, 51-53, 55-56, 66, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90, 93-94, 96, 98",
      "file_path": "src/pytest_llm_report/llm/utils.py",
      "missed": 2,
      "missed_ranges": "48, 78",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 253,
      "covered_ranges": "17-18, 20, 23, 26-27, 36-38, 40, 42, 49-50, 59-61, 63, 65, 72-73, 86-92, 94, 96, 107-108, 120-126, 128, 130, 135-143, 146-147, 169-185, 187-188, 190, 192, 194, 201-224, 227-228, 236-237, 239, 241, 247-248, 257-259, 261, 263, 270-271, 280-282, 284, 286, 290-292, 295-296, 333-362, 364-372, 374, 376, 394-417, 419-437, 440-441, 455-463, 465, 467, 477-479, 482-483, 500-510, 512, 518, 520, 526-540",
      "file_path": "src/pytest_llm_report/models.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 253
    },
    {
      "coverage_percent": 78.73,
      "covered": 211,
      "covered_ranges": "122, 170, 199, 202-204, 209-211, 217-219, 225-227, 233-235, 241-242, 245-254, 257-259, 265-267, 271-274, 276, 284, 293, 308, 311-312, 320-325, 327, 332-337, 340-345, 348-349, 352-353, 356-357, 360-369, 372-375, 378-393, 396-397, 400-405, 408-409, 412-413, 416-421, 426-427, 430-431, 436-439, 444-447, 449, 451, 453, 460-461, 463-464, 466-467, 470-475, 479, 482-495, 498, 502-503, 507, 510, 514-515, 519-520, 524, 527, 531, 534-536, 540-541, 545-546, 550, 553, 557, 560, 564-565, 569, 572-574, 578, 581-584, 587, 591-592, 596, 599-608, 611, 613",
      "file_path": "src/pytest_llm_report/options.py",
      "missed": 57,
      "missed_ranges": "13-15, 21-22, 98-102, 105-107, 110-115, 118-121, 138-139, 142-149, 152-155, 158-160, 163-166, 169, 180-184, 187-188, 191, 193, 278, 287, 296",
      "statements": 268
    },
    {
      "coverage_percent": 86.81,
      "covered": 158,
      "covered_ranges": "41, 44, 50, 56, 62, 68, 74, 81, 90, 96, 102, 108, 114, 122, 128, 134, 142, 148, 155, 161, 169, 176, 185, 192, 199, 208, 215, 223, 229, 235, 241, 247, 254, 260, 268, 274, 283, 289, 297, 304, 311, 328, 332, 336, 342-343, 346-347, 349, 351, 354-356, 362-363, 371-372, 399-400, 403-404, 407, 410-411, 413-414, 417-418, 420, 422-426, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 476-477, 485-487, 491-494, 497, 499, 502-507, 509, 512-514, 516-521, 523, 534-535, 558-559, 562-563, 566-568, 579-580, 583, 586-587, 590-592, 602-603, 606-608, 619-620, 623, 626, 628-629",
      "file_path": "src/pytest_llm_report/plugin.py",
      "missed": 24,
      "missed_ranges": "13, 15-18, 20-21, 23, 29-32, 35, 319, 377, 481-482, 488, 548-549, 571, 595, 611-612",
      "statements": 182
    },
    {
      "coverage_percent": 97.27,
      "covered": 107,
      "covered_ranges": "13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 114, 116, 118, 139-140, 142-144, 147, 152-153, 155-157, 159-161, 163-164, 166-167, 170-171, 173, 177, 180, 189, 192-194, 196-197, 201, 203, 216-217, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249, 251, 268, 275, 284-287",
      "file_path": "src/pytest_llm_report/prompts.py",
      "missed": 3,
      "missed_ranges": "80, 185, 233",
      "statements": 110
    },
    {
      "coverage_percent": 90.77,
      "covered": 59,
      "covered_ranges": "13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 140-142, 147, 155-157, 159, 172-177, 191, 210-211, 224, 267, 269, 285",
      "file_path": "src/pytest_llm_report/render.py",
      "missed": 6,
      "missed_ranges": "148-149, 212, 217-218, 222",
      "statements": 65
    },
    {
      "coverage_percent": 98.2,
      "covered": 164,
      "covered_ranges": "13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 113, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 310, 319, 321-322, 324-335, 337, 339, 347, 350-352, 355-356, 359-361, 364, 367, 375, 383, 385-386, 389, 392, 395, 398, 406, 408-409, 415, 417, 419, 421-432, 439, 441-442, 444-446, 454-458, 460, 462, 465, 468-469, 471, 477-481, 487-488, 495, 502, 504, 506-508, 510, 513-514, 516, 522-523",
      "file_path": "src/pytest_llm_report/report_writer.py",
      "missed": 3,
      "missed_ranges": "135-137",
      "statements": 167
    },
    {
      "coverage_percent": 97.06,
      "covered": 33,
      "covered_ranges": "11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-65, 67, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123",
      "file_path": "src/pytest_llm_report/util/fs.py",
      "missed": 1,
      "missed_ranges": "40",
      "statements": 34
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121",
      "file_path": "src/pytest_llm_report/util/hashing.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 33,
      "covered_ranges": "12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95",
      "file_path": "src/pytest_llm_report/util/ranges.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 16,
      "covered_ranges": "4, 6, 9, 15, 18, 27, 30, 39-44, 46-48",
      "file_path": "src/pytest_llm_report/util/time.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 16
    }
  ],
  "summary": {
    "coverage_total_percent": 93.04,
    "error": 0,
    "failed": 0,
    "passed": 623,
    "skipped": 0,
    "total": 623,
    "total_duration": 116.91686198899987,
    "xfailed": 0,
    "xpassed": 0
  },
  "tests": [
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008256700000117689,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assertion 1', 'description': 'Assertion 1 should be executed before the assertion 2', 'expected_result': True}",
          "{'name': 'assertion 2', 'description': 'Assertion 2 should be executed after the assertion 3', 'expected_result': False}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 118,
          "total_tokens": 257
        },
        "why_needed": "This test is needed because it checks for complexity estimation in tests that have mocks and multiple assertions."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 185-186, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008743109999898024,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert provider._estimate_test_complexity() == 0', 'expected_value': 0, 'message': 'Expected provider._estimate_test_complexity to return 0'}",
          "{'name': 'assert provider._estimate_test_complexity(None) == 0', 'expected_value': 0, 'message': 'Expected provider._estimate_test_complexity to return 0'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
        "token_usage": {
          "completion_tokens": 158,
          "prompt_tokens": 136,
          "total_tokens": 294
        },
        "why_needed": "The test is needed because it checks the behavior of the `Config` class when given an empty source."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021952799999951367,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'complexity_score', 'description': 'The complexity score of the test should be low.'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 115,
          "total_tokens": 201
        },
        "why_needed": "To ensure that simple tests have low complexity scores and are not misleading."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008174639999936062,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"The provided 'prompt_tier' should be one of: 'basic', 'advanced', or 'none'.\", 'code': 400}"
        ],
        "scenario": "Test invalid prompt tier configuration",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 126,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the `prompt_tier` field is validated correctly and raises an error when it's not a valid value."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007816869999999199,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Should return an empty list for valid values', 'expected_result': [], 'actual_result': 'errors'}",
          "{'message': 'Should throw an AssertionError for invalid values', 'expected_result': 'TestError', 'actual_result': 'None'}"
        ],
        "scenario": "Valid prompt tiers",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 142,
          "total_tokens": 255
        },
        "why_needed": "To ensure that the `prompt_tier` parameter is correctly validated and does not cause any issues."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_valid_prompt_tiers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-220, 222, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007758760000058373,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'use_standard_prompt_for_complex_tests', 'expected_value': True, 'actual_value': False}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 122,
          "total_tokens": 205
        },
        "why_needed": "Auto mode should use standard prompt for complex tests."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008079870000017308,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'selected_prompt_type', 'value': 'MINIMAL_SYSTEM_PROMPT'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 155,
          "total_tokens": 243
        },
        "why_needed": "To ensure that the auto-tiering mechanism is working correctly for simple tests, where minimal prompts are sufficient."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 212, 214-215, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007817770000002611,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config_override', 'description': 'The config override should be applied to the test provider.'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 122,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the minimal prompt is always used for config override tests."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 212, 214, 216-217, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007605680000040138,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'standard_system_prompt', 'actual_result': 'STANDARD_SYSTEM_PROMPT'}"
        ],
        "scenario": "Config override to standard should always use standard prompt.",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 148,
          "total_tokens": 230
        },
        "why_needed": "This test is necessary because it ensures that the config override to standard does not cause any issues with the system prompts."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_standard_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 71,
          "line_ranges": "53, 56-57, 60, 62-64, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002064575000005675,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregated report contains both tests from each individual report.",
          "All retained tests are present in the aggregated report.",
          "No duplicate tests are included in the aggregated report.",
          "The number of tests in the aggregated report matches the expected value.",
          "Each test is included only once in the aggregated report.",
          "Duplicate test names are not included in the aggregated report.",
          "All retained tests have a unique outcome.",
          "The aggregate policy 'all' is applied to all test cases."
        ],
        "scenario": "Test aggregating all policy for multiple test cases in a single run",
        "token_usage": {
          "completion_tokens": 154,
          "prompt_tokens": 364,
          "total_tokens": 518
        },
        "why_needed": "Prevents regression when running multiple test cases with the same aggregate policy"
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 8,
          "line_ranges": "53, 56-58, 110, 113-115"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00385251000000153,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert aggregator.aggregate() is None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 104,
          "total_tokens": 184
        },
        "why_needed": "To test that the aggregate function returns None when the aggregation directory does not exist."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 79,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 204-205, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0036909279999974842,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result from `aggregate()` should contain only one test.",
          "The outcome of the first test in the result should be 'passed'.",
          "The second test in the result should have an outcome of 'passed' (latest).",
          "The aggregated run meta should indicate that both runs were aggregated.",
          "The summary for the aggregated run should show 1 passed and 0 failed tests.",
          "The `run_meta` object should contain a `is_aggregated` flag set to True."
        ],
        "scenario": "Test that the latest policy is picked when aggregating reports with different times.",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 477,
          "total_tokens": 639
        },
        "why_needed": "This test prevents a regression where the latest policy might not be chosen due to inconsistent report times."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 3,
          "line_ranges": "45, 53-54"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007978879999939181,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'agg', 'expected_type': 'NoneType', 'message': \"Expected agg to be None, but got <type 'Aggregator'>\"}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 110,
          "total_tokens": 206
        },
        "why_needed": "The test is necessary because the aggregator requires a directory to aggregate data from."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "53, 56-58, 110, 113-114, 117-118, 184"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012954609999979994,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "aggregator.aggregate() should return None.",
          "pathlib.Path.exists() should return True for a non-empty directory.",
          "pathlib.Path.glob() should return [] for a non-existent file or directory.",
          "The aggregate function should not attempt to aggregate any files or reports.",
          "No error should be raised when calling the aggregate function with no reports or files.",
          "The aggregate function should return an empty list or None as expected."
        ],
        "scenario": "Test that aggregate function returns None when no reports exist and no files are found.",
        "token_usage": {
          "completion_tokens": 154,
          "prompt_tokens": 201,
          "total_tokens": 355
        },
        "why_needed": "Prevents regression where the aggregate function returns an empty list or None when there are no reports or files to aggregate."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 87,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138-141, 145-147, 149-150, 152-153, 155, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 40,
          "line_ranges": "42-45, 65-68, 130-133, 135-137, 139, 141-143, 190, 194-199, 201, 203, 205, 207, 210-214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022794479999959094,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "coverage is properly deserialized with the correct file paths, line ranges, and line counts.",
          "LLM annotation is properly deserialized with the correct scenario, why needed message, key assertions, confidence level, and token usage information.",
          "Coverage and LLM annotation can be re-serialized without any issues."
        ],
        "scenario": "Test that coverage and LLM annotations are properly deserialized and can be re-serialized.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 1002,
          "total_tokens": 1126
        },
        "why_needed": "Prevents regression in core functionality by ensuring proper deserialization of coverage and LLM annotations."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 67,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 162-169, 171-173, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017290149999951154,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `source_coverage` attribute of each report should contain a single `SourceCoverageEntry` object.",
          "The `file_path` attribute of the `SourceCoverageEntry` object should match the expected file path.",
          "All statements in the `coverage_percent` and `covered_ranges` attributes should be integers or floats.",
          "All statements in the `missed` attribute should be less than or equal to 0.",
          "The `source_coverage` attribute of each report should contain a list with exactly one element.",
          "Each `SourceCoverageEntry` object should have all required attributes (file_path, statements, missed, covered, coverage_percent, covered_ranges, missed_ranges).",
          "All values in the `coverage_percent` and `covered_ranges` attributes should be within valid ranges for source coverage percentages and ranges."
        ],
        "scenario": "Test the aggregation function with source coverage summary.",
        "token_usage": {
          "completion_tokens": 221,
          "prompt_tokens": 395,
          "total_tokens": 616
        },
        "why_needed": "This test prevents regression in the aggregation function, where it fails to correctly handle source coverage summaries."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 19,
          "line_ranges": "259-260, 262-263, 265, 267-271, 273, 276-277, 279-280, 283, 285-286, 288"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003318297999996389,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that calling _load_coverage_from_source() returns None when llm_coverage_source is set to None.",
          "Verify that calling _load_coverage_from_source() raises a UserWarning with message 'Coverage source not found' when llm_coverage_source is set to '/nonexistent/coverage'.",
          "Verify that calling _load_coverage_from_source() returns the mock coverage object created by mocking Coverage in pytest_llm_report.coverage_map.CoverageMapper.",
          "Verify that calling _load_coverage_from_source() calls the mock cov.report() method with a return value of 80.0 when llm_coverage_source is set to '.coverage' and coverage_percentage is 80.0.",
          "Verify that calling _load_coverage_from_source() returns None when llm_coverage_source is set to a valid file path.",
          "Verify that the mock cov.report() method was called with the correct arguments (cov, mapper) in all test scenarios."
        ],
        "scenario": "Test loading coverage from configured source file when option is not set.",
        "token_usage": {
          "completion_tokens": 247,
          "prompt_tokens": 584,
          "total_tokens": 831
        },
        "why_needed": "Prevents regression in case the user doesn't configure a source file for LLM coverage."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 17,
          "line_ranges": "231, 233-247, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008322019999980057,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests is updated to match the latest summary.",
          "The passed count remains unchanged.",
          "The failed count remains unchanged.",
          "The skipped count remains unchanged.",
          "The xfailed count remains unchanged.",
          "The xpassed count remains unchanged.",
          "The error count remains unchanged.",
          "The coverage percentage is preserved and updated correctly.",
          "The total duration of the latest summary is updated to match the new test results."
        ],
        "scenario": "Test that the recalculate_summary method updates the latest summary correctly when new test results are added.",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 473,
          "total_tokens": 643
        },
        "why_needed": "This test prevents regression in the aggregation process, where the total count of tests might not be updated correctly if new test results are added after recalculating the summary."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_recalculate_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 72,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123-124, 129, 131-132, 162-167, 169, 171-173, 176, 178-180, 182, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003077516000004721,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that only valid reports are counted in the aggregate result.",
          "The test verifies that missing fields in an invalid JSON report are not included in the aggregate result.",
          "The test verifies that a UserWarning is raised when skipping an invalid JSON report, indicating that it's being handled correctly."
        ],
        "scenario": "Test that skipping an invalid JSON report prevents regression.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 352,
          "total_tokens": 468
        },
        "why_needed": "This test verifies that the aggregation function correctly skips reports with invalid JSON files, preventing potential regressions."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_skips_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "45, 231, 233-239, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008153499999963287,
      "file_path": "tests/test_aggregation_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "summary.total == 2",
          "summary.passed == 1",
          "summary.failed == 1",
          "summary.coverage_total_percent == 88.5",
          "summary.total_duration == 3.0"
        ],
        "scenario": "The test verifies that the aggregator recalculates the summary correctly when there are multiple tests with different outcomes.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 299,
          "total_tokens": 412
        },
        "why_needed": "This test prevents regression where a single test's outcome affects the overall coverage calculation."
      },
      "nodeid": "tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-91, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001645207999999343,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 'Mock provider instance'}",
          "{'name': 'mock_cache', 'expected_value': 'Mock cache instance'}",
          "{'name': 'mock_assembler', 'expected_value': 'Mock assembler instance'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 112,
          "total_tokens": 231
        },
        "why_needed": "To test the functionality of batch optimization message generation."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 50,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-128, 130, 134, 156, 181-182, 184, 211, 213-219, 221, 223"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010606510000030767,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked cache should be initialized with mock provider', 'expected_value': 'mock_provider', 'actual_value': 'mock_cache'}",
          "{'name': 'Mocked cache should have no pending operations when not in use', 'expected_value': [], 'actual_value': 'mock_cache'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 101,
          "total_tokens": 234
        },
        "why_needed": "To ensure that the progress reporting is cached correctly and not lost in case of a failure."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 95,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-124, 130, 132, 134, 137-141, 144-151, 156, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017515279999997801,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'value': 'The mock provider is not called during test execution.'}",
          "{'name': 'mock_cache', 'value': 'The mock cache is not populated with test results.'}",
          "{'name': 'mock_assembler', 'value': 'The mock assembler does not call the annotated function.'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 102,
          "total_tokens": 236
        },
        "why_needed": "To ensure that cached tests are skipped correctly."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 90,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221, 223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003097074000010025,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider should not raise an exception when called concurrently', 'expected_value': 'None'}",
          "{'name': 'Mock cache should be created and populated correctly when called concurrently', 'expected_value': {'key1': 'value1', 'key2': 'value2'}}"
        ],
        "scenario": "tests/test_annotator.py::TestConcurrentAnnotation",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 98,
          "total_tokens": 220
        },
        "why_needed": "To ensure that annotators can annotate data concurrently without causing performance issues or data corruption."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221-223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0024946630000073355,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Mocked annotator failed to annotate the document.', 'expected_exception': 'annotator.exceptions.AnnotatorException'}",
          "{'message': 'Mocked annotator encountered an error while annotating the document.', 'expected_exception': 'annotator.exceptions.AnnotatorException'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 116,
          "total_tokens": 242
        },
        "why_needed": "This test is necessary to ensure that concurrent annotation handles failures correctly."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002978970999990338,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked progress bar updates are being reported correctly', 'expected_value': 'True'}",
          "{'name': 'The progress bar is not being updated when it should be', 'expected_value': 'False'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 96,
          "total_tokens": 205
        },
        "why_needed": "To ensure that the annotator is reporting progress correctly and accurately."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015402009999974098,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 'Mock provider instance'}",
          "{'name': 'mock_cache', 'expected_value': 'Mock cache instance'}",
          "{'name': 'mock_assembler', 'expected_value': 'Mock assembler instance'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 101,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the annotator correctly displays progress messages during report generation."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 91,
          "line_ranges": "47, 50-51, 58-59, 65, 67-68, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016239380000087067,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_type': 'MockProvider'}",
          "{'name': 'mock_cache', 'expected_type': 'MockCache'}",
          "{'name': 'mock_assembler', 'expected_type': 'MockAssembler'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 104,
          "total_tokens": 225
        },
        "why_needed": "The test respects the opt-out and limit settings for annotators."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-257, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001809166000001028,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider.get_rate_limit() returns a valid limit', 'expected_value': 10, 'actual_value': 5}",
          "{'name': 'mock_provider.get_rate_limit() returns an invalid limit', 'expected_value': 20, 'actual_value': 15}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 112,
          "total_tokens": 241
        },
        "why_needed": "To ensure that the annotator respects rate limits and does not exceed them."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 12.001996031000004,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Annotations are applied sequentially', 'description': 'The annotator should apply each annotation in sequence, without skipping any steps.'}",
          "{'name': 'No annotations are skipped due to cache or provider issues', 'description': 'Even if the cache or provider is experiencing issues, the annotator should still apply all annotations in sequence.'}"
        ],
        "scenario": "Sequential annotation",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 98,
          "total_tokens": 228
        },
        "why_needed": "To ensure that annotations are applied in the correct order and to avoid any potential issues with concurrent access to the annotator."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 24.00209343600001,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'Error message should be logged and returned in the response', 'actual': 'Mocked logging and caching are not being used correctly.'}",
          "{'expected': 'Error message should be logged and returned in the response with a specific key', 'actual': 'Mocked logging is not being used to log error messages.'}"
        ],
        "scenario": "Sequential annotation error tracking",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 105,
          "total_tokens": 227
        },
        "why_needed": "This test is necessary to ensure that the annotator correctly handles errors during sequential annotation."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 2,
          "line_ranges": "47-48"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008165919999925109,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'does_not_call_config', 'description': \"The config function should be called with a Config object that has 'llm' set to False\"}",
          "{'name': 'does_not_skip_test', 'description': 'The test should not skip when LLM is disabled'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 108,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the annotator does not skip tests when LLM is disabled."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "47, 50-54, 56"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000894618999993213,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected': 'MockProvider instance was created'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 101,
          "total_tokens": 182
        },
        "why_needed": "Because the annotator should not be skipped when a provider is unavailable."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 359-360"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000845446999989008,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected error message', 'description': \"The error message returned by `extract_json_from_response` should be 'Failed to parse LLM response as JSON'.\"}",
          "{'name': 'Expected invalid JSON string', 'description': 'The input JSON string should contain invalid characters or syntax.'}"
        ],
        "scenario": "Test Base Parse Response Malformed JSON After Extract",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 152,
          "total_tokens": 274
        },
        "why_needed": "To test that the `extract_json_from_response` function correctly handles malformed JSON responses."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342-346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009095069999887073,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of the 'scenario' key should be an integer.",
          "The value of the 'why_needed' key should contain a list.",
          "The value of the 'key_assertions' key should contain the string 'a'."
        ],
        "scenario": "Tests that the `base_parse_response` function handles non-string fields correctly when provided with a JSON object containing integers, lists, and other types.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 269,
          "total_tokens": 399
        },
        "why_needed": "This test prevents regression in case the `base_parse_response` function is modified to handle non-string fields without proper error handling or validation."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007835709999994833,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider type', 'expected_type': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 104,
          "total_tokens": 187
        },
        "why_needed": "To ensure the `get_gemini_provider` function returns a valid instance of `GeminiProvider`."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020633729999985917,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected exception message', 'value': 'Unknown LLM provider: invalid'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 106,
          "total_tokens": 186
        },
        "why_needed": "To test that a ValueError is raised when an unknown LLM provider is specified."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008399560000214024,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of `LiteLLMProvider`', 'expected_type': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 109,
          "total_tokens": 206
        },
        "why_needed": "To ensure that the `get_litellm_provider` function returns a valid instance of `LiteLLMProvider`."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008238560000108919,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider type', 'expected_type': 'NoopProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 104,
          "total_tokens": 178
        },
        "why_needed": "To test the functionality of getting a provider without any configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007853339999996933,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of OllamaProvider', 'expected_result': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 108,
          "total_tokens": 204
        },
        "why_needed": "To ensure that the `get_ollama_provider` function returns an instance of `OllamaProvider` as expected."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 134-135, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008624790000055782,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider.is_available() is True",
          "provider.is_available() is True",
          "checks == 1"
        ],
        "scenario": "Verify that the test function checks for available caches correctly when a provider implements _check_availability.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 280,
          "total_tokens": 374
        },
        "why_needed": "This test prevents regression in cases where a provider does not implement _check_availability, causing tests to fail due to missing cache availability."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007556989999955022,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert provider.get_model_name() == 'test-model'\", 'expected_value': 'test-model'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestLlmProviderDefaults",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 114,
          "total_tokens": 204
        },
        "why_needed": "To ensure that the `get_model_name` method returns the default model name from the configuration when no custom model is provided."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 155"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007722100000080445,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.get_rate_limits() is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_base_maximal.py",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 108,
          "total_tokens": 183
        },
        "why_needed": "This test ensures that the rate limits are set to None by default when creating a ConcreteProvider instance."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 174"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008093790000032186,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_local() should return False', 'expected_value': False, 'actual_value': 'is_local'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 105,
          "total_tokens": 198
        },
        "why_needed": "To test that the default value for `is_local` is indeed `False`."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 35,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 192-194, 196-200, 203-206, 209-210, 213-214, 216-218, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008021959999950923,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Context files should be added to the prompt.",
          "The prompt should include the source file `src/module.py`.",
          "The prompt should include the function `def helper()` from the source file `src/module.py`."
        ],
        "scenario": "Test that context files are included in the batch prompt.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 261,
          "total_tokens": 366
        },
        "why_needed": "This test prevents a potential issue where context files are not added to the prompt, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_context_files_included",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "34, 39-40, 156-157, 160, 162, 164-168, 170-177, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008755730000018502,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The prompt should include 'Test Group: test.py::test_add[*]' and 'Parameterizations (2 variants)'",
          "The prompt should contain '[1+1=2]' and '[0+0=0]'",
          "The prompt should mention 'ONE annotation'"
        ],
        "scenario": "Test the parametrized batch prompt functionality.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 330,
          "total_tokens": 440
        },
        "why_needed": "This test prevents regression when using parameterized batches, ensuring that all variants are included in the prompt."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_parametrized_batch_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 15,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008121740000035516,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The generated prompt should contain 'Test: test.py::test_foo'.",
          "'```python' is present in the prompt.",
          "The source code of the test should be included in the prompt.",
          "The presence of 'Parameterizations' in the prompt should be avoided."
        ],
        "scenario": "Verifies that a single test generates the expected normal prompt.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 269,
          "total_tokens": 376
        },
        "why_needed": "Prevents regression when testing batched requests with multiple tests."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_single_test_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007723900000087269,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The source code should be the same across all executions.",
          "The length of the resulting hash should remain constant at 32 bytes.",
          "If the source code changes, the hash should not change.",
          "_compute_source_hash(source) should return the same hash for different source codes.",
          "The hash should be a valid SHA-256 hash."
        ],
        "scenario": "Test that the same source code produces the same hash for consistent hashing.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 220,
          "total_tokens": 351
        },
        "why_needed": "Prevents a bug where different versions of the test function produce different hashes, potentially leading to inconsistent results."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007370339999965836,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'different', 'condition': {'source': 'def test_a(): pass'}, 'expected_result': {'hash': '1234567890abcdef'}}",
          "{'assertion_type': 'different', 'condition': {'source': 'def test_b(): pass'}, 'expected_result': {'hash': 'fedcba987654321'}}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 127,
          "total_tokens": 270
        },
        "why_needed": "To ensure that different sources produce different hashes, which is a requirement for batch processing."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007369829999959165,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _compute_source_hash() returns an empty string for an empty input', 'expected_result': '', 'actual_result': '_compute_source_hash()'}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 94,
          "total_tokens": 188
        },
        "why_needed": "The current implementation of compute_source_hash() does not handle an empty source correctly."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008349780000003193,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.validate() returns errors', 'description': 'The `validate()` method should return an error if `batch_max_tests` is not set or is less than 1.', 'expected_type': 'list[str]', 'expected_value': ['batch_max_tests']}",
          "{'name': \"any('batch_max_tests' in e for e in errors)\", 'description': \"The method should return True if 'batch_max_tests' is present in any of the error messages.\", 'expected_type': 'bool'}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 126,
          "total_tokens": 304
        },
        "why_needed": "The `batch_max_tests` configuration value is required to ensure the correct behavior of batched tests."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007709559999966586,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding must be a non-negative integer', 'value': -1}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 126,
          "total_tokens": 200
        },
        "why_needed": "Context line padding must be non-negative."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008020559999977195,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.validate() returns an error message', 'description': 'The validate method should return a list of errors.', 'expected': [\"context_compression must be one of 'none', 'gzip', or 'lz4'\"], 'actual': [\"context_compression must be one of 'invalid', 'none', 'gzip', 'lz4'\"]}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 122,
          "total_tokens": 243
        },
        "why_needed": "To test that an invalid compression mode fails the validation process."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007916149999971367,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Context compression mode is required for valid context compression.', 'expected_value': 'none'}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 60,
          "prompt_tokens": 133,
          "total_tokens": 193
        },
        "why_needed": "Valid compression modes should pass."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_valid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007577019999871482,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected result', 'value': 'test.py::test[a-b-c]'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 109,
          "total_tokens": 187
        },
        "why_needed": "To ensure that complex parameters are fully stripped from the base node ID."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007723100000021077,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'tests/test_foo.py::test_add[1+1=2]', 'actual': '_get_base_nodeid('}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 133,
          "total_tokens": 229
        },
        "why_needed": "To ensure that the `parametrized_nodeid` function correctly strips parameters from nodeids."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007467010000254959,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'tests/test_foo.py::test_bar', 'actual_value': 'tests/test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 123,
          "total_tokens": 210
        },
        "why_needed": "This test checks that a simple nodeid without params returns unchanged."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "53-54, 67-68, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008495740000000751,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "len(batches) == 3",
          "len(batches[0].tests) == 2",
          "len(batches[1].tests) == 2",
          "len(batches[2].tests) == 1"
        ],
        "scenario": "Large groups should be split by batch_max_tests to avoid memory issues.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 364,
          "total_tokens": 477
        },
        "why_needed": "This test prevents a potential memory leak in large group sizes where the tests are not properly split into batches."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batch_max_size_respected",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 6,
          "line_ranges": "92-93, 95, 97-99"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007779510000034406,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Number of batches should be equal to number of tests', 'expected_value': 2, 'actual_value': 1}"
        ],
        "scenario": "Test case for batching disabled",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 170,
          "total_tokens": 251
        },
        "why_needed": "To ensure that each test is separate and not affected by the batch parameterization."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batching_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 27,
          "line_ranges": "34, 39-40, 53-54, 67, 70, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007979780000084702,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of batches should be equal to 1.",
          "Each batch should contain exactly 3 tests.",
          "Each batch should have an is_parametrized attribute set to True.",
          "Each batch's base nodeid should match the path 'test.py::test_add'."
        ],
        "scenario": "Test that parametrized tests are grouped together by batch.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 346,
          "total_tokens": 459
        },
        "why_needed": "This test prevents a regression where parametrized tests are not properly grouped together in batches."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_parametrized_tests_grouped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008164130000238856,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Each test should be a separate batch.",
          "Batching should not affect the number or structure of individual tests.",
          "The expected number of batches (2) and individual tests (1+1=2) should match the actual output."
        ],
        "scenario": "Test 'test_single_tests_no_grouping' verifies that single tests are handled correctly without grouping.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 278,
          "total_tokens": 392
        },
        "why_needed": "This test prevents regression in case of batch testing with no groupings, ensuring each test is executed individually."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_single_tests_no_grouping",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007899530000088362,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'same_source', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_consistent_hash",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 107,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the cache is storing and retrieving data from the same source, which produces the same hash."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007502279999869188,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Different source should produce different hash.', 'expected_result': 'different', 'actual_result': 'same'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 108,
          "total_tokens": 190
        },
        "why_needed": "To ensure that the hash function is working correctly and producing different hashes for different source code."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007879679999973632,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'hash', 'expected': \"a string of 16 hexadecimal digits (e.g., '1234567890abcdef')\", 'actual': '<hash object>'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_hash_length",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 100,
          "total_tokens": 192
        },
        "why_needed": "The hash length is not sufficient to uniquely identify the cache key."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_hash_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 26,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001297965999981443,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Clearing the cache should remove all existing entries.",
          "Adding multiple entries should result in only two remaining entries.",
          "Cache should return None for both 'test::a' and 'test::b' after clearing.",
          "The cache should be empty after clearing, with no matching annotations."
        ],
        "scenario": "Verify that the cache is cleared and all entries are removed.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 283,
          "total_tokens": 407
        },
        "why_needed": "The test prevents a bug where the cache might not be properly cleared when adding multiple entries, potentially leading to incorrect results or data loss."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_clear",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 11,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008584619999965071,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotation is set and retrieved correctly', 'expected_result': 'abc123'}",
          "{'name': 'result is None when annotation is retrieved from cache', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_cache.py",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 157,
          "total_tokens": 247
        },
        "why_needed": "To ensure that annotations with errors are not cached."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 9,
          "line_ranges": "39-41, 53, 55-56, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009777450000001409,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cache entry should be None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_cache.py::TestLlmCache::test_get_missing",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 128,
          "total_tokens": 199
        },
        "why_needed": "To test that the cache returns None for missing entries."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_get_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 28,
          "line_ranges": "39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011265649999927518,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check if the annotation is set correctly for the given key.",
          "Verify that the annotation's scenario matches the expected value.",
          "Ensure that the annotation's confidence level matches the expected value.",
          "Confirm that the retrieved result has the same scenario and confidence as the original annotation.",
          "Verify that the retrieved result does not contain any null values.",
          "Check if the cache stores annotations in a consistent manner across different runs."
        ],
        "scenario": "Test that annotations are stored and retrieved correctly from the cache.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 286,
          "total_tokens": 428
        },
        "why_needed": "Prevents bypass attacks by ensuring that the cache stores and retrieves annotations in a consistent manner."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_set_and_get",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008171540000034838,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'assert', 'expected_value': 'nodeid', 'actual_value': 'test_bad.py'}",
          "{'assertion_type': 'assert', 'expected_value': 'message', 'actual_value': 'SyntaxError'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 124,
          "total_tokens": 238
        },
        "why_needed": "The current test does not verify the correct structure of collection errors."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007742030000201794,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert get_collection_errors is a list', 'expected_value': [], 'actual_value': 'collector.get_collection_errors() == []'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 114,
          "total_tokens": 216
        },
        "why_needed": "The test is checking if the `get_collection_errors` method returns an empty list when the collection is initially empty."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007722889999968174,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'result.llm_context_override is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 136,
          "total_tokens": 206
        },
        "why_needed": "Default llm_context_override should be None."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007889299999987998,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result.llm_opt_out is False', 'expected_value': False, 'message': 'Expected llm_opt_out to be False'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 136,
          "total_tokens": 227
        },
        "why_needed": "Default llm_opt_out should be False."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007909240000003592,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.capture_failed_output', 'expected_value': True}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 104,
          "total_tokens": 182
        },
        "why_needed": "The test captures output by default, which can lead to unexpected behavior if not expected."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007930779999867354,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert capture_output_max_chars is equal to 4000', 'expected_value': 4000, 'actual_value': 10000}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 108,
          "total_tokens": 217
        },
        "why_needed": "The default value of `capture_output_max_chars` is not sufficient to handle large output files. This test ensures that the default value is set correctly."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007855040000208646,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result.outcome', 'expected_value': 'xfailed'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 206,
          "total_tokens": 287
        },
        "why_needed": "To ensure that xfail failures are recorded as xfailed in the test results."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 26,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007609580000007554,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_name': 'result.outcome', 'expected_value': 'xpassed'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 205,
          "total_tokens": 282
        },
        "why_needed": "xfail passes should be recorded as xpassed."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008271229999934349,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "collector.results == {}",
          "collector.collection_errors == []",
          "collector.collected_count == 0"
        ],
        "scenario": "The `TestCollector` class initializes correctly with an empty collection.",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 205,
          "total_tokens": 291
        },
        "why_needed": "This test prevents a potential bug where the `TestCollector` instance is not initialized properly, leading to incorrect results or behavior."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_create_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007663279999974293,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Results should be sorted by nodeid.', 'expected_result': ['a_test.py::test_a', 'z_test.py::test_z']}"
        ],
        "scenario": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 227,
          "total_tokens": 316
        },
        "why_needed": "To ensure that the collector correctly sorts the results by nodeid."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008072949999871071,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collected_count` attribute should be set to 3 (the number of collected items).",
          "The `deselected_count` attribute should be set to 1 (the number of deselected items)."
        ],
        "scenario": "Verify that the `handle_collection_finish` method correctly tracks collected and deselected counts.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 256,
          "total_tokens": 365
        },
        "why_needed": "This test prevents a potential bug where the count of collected items is not updated correctly after the collection finish."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_handle_collection_finish",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015125889999865194,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected_result', 'type': 'assertion', 'message': \"Expected `collector.results['t'].captured_stdout` to be None\"}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 211,
          "total_tokens": 309
        },
        "why_needed": "Capture output via handle_runtest_logreport is disabled for integration test."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009210890000019845,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected captured stderr to be empty', 'description': 'The collector should not capture any output from stderr.', 'expected_value': '', 'actual_value': 'Some error'}"
        ],
        "scenario": "TestCollectorInternals::test_capture_output_stderr",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 157,
          "total_tokens": 245
        },
        "why_needed": "To test that the collector correctly captures stderr."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009030440000117324,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'captured_stdout', 'expected_value': 'Some output'}"
        ],
        "scenario": "TestCollectorInternals::test_capture_output_stdout",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 157,
          "total_tokens": 228
        },
        "why_needed": "To verify that the collector correctly captures stdout and stores it in the TestCaseResult."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010462129999950776,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 174,
          "total_tokens": 262
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 35,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021915739999940342,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "item.get_closest_marker('llm_opt_out') returns MagicMock()",
          "item.get_closest_marker('llm_context') returns MagicMock(args=['complete'])",
          "item.get_closest_marker('requirement') returns MagicMock(args=['REQ-1', 'REQ-2'])",
          "result.param_id is set to 'param1'",
          "result.llm_opt_out is True",
          "result.llm_context_override is set to 'complete'",
          "result.requirements contains ['REQ-1', 'REQ-2']"
        ],
        "scenario": "Test `create_result_with_item_markers` verifies that the collector extracts item markers correctly.",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 382,
          "total_tokens": 560
        },
        "why_needed": "This test prevents a potential bug where the collector does not extract item markers from the item, potentially leading to incorrect report generation."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014184620000037285,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'expected', 'expected_value': 'Crash report'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 165,
          "total_tokens": 249
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009972619999985,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected output', 'value': 'Some error occurred'}"
        ],
        "scenario": "test_collector_maximal",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 130,
          "total_tokens": 199
        },
        "why_needed": "To test the `_extract_error` method's ability to return a string representing an error message."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009089959999926123,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert collector._extract_skip_reason(report) is None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 130,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the `_extract_skip_reason` method returns `None` when no longrepr is provided."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009222310000041034,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert _extract_skip_reason returns 'Just skipped'\", 'expected_value': 'Just skipped'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 133,
          "total_tokens": 218
        },
        "why_needed": "To ensure the `_extract_skip_reason` method returns a string as expected."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009605030000159331,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 164,
          "total_tokens": 255
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 21,
          "line_ranges": "58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008916939999892293,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `handle_collection_report` method should update the `collection_errors` list with the nodeid and message of the error.",
          "The `collection_errors` list should contain exactly one item with the specified nodeid and message.",
          "The nodeid in the first collection_error should match the value passed to the `report.nodeid` attribute.",
          "The message in the first collection_error should match the value passed to the `report.message` attribute.",
          "The error type (in this case, a 'SyntaxError') should be present in the first collection_error."
        ],
        "scenario": "Test verifies that the `handle_collection_report` method records a collection error and updates the `collection_errors` list with the relevant information.",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 273,
          "total_tokens": 469
        },
        "why_needed": "This test prevents a potential bug where the collector does not record errors in a collection report, potentially leading to missing important information about the failure."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 42,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140-141, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238, 261, 264-265, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002339831999989883,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.rerun_count should be equal to 1",
          "res.final_outcome should be 'failed'",
          "collector.results['t::r'] should contain a 'rerun' key with value 1",
          "collector.results['t::r'].final_outcome should be 'failed'"
        ],
        "scenario": "Test 'handle_runtest_rerun' verifies that the TestCollector handles rerun attribute correctly.",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 281,
          "total_tokens": 413
        },
        "why_needed": "This test prevents a potential regression where the TestCollector does not handle reruns correctly, potentially leading to incorrect results or failures."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009474389999866162,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.outcome should be set to 'error'",
          "res.phase should be set to 'setup'",
          "res.error_message should match 'Setup failed'"
        ],
        "scenario": "Test 'handle_runtest_setup_failure' verifies that a setup error is recorded in the report.",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 300,
          "total_tokens": 391
        },
        "why_needed": "This test prevents regression by ensuring that setup errors are properly reported and handled."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 38,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013170209999771032,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `teardown` report should contain an error message indicating 'Cleanup failed'.",
          "The `outcome` of the result for the current test should be set to 'error'.",
          "The `phase` of the result for the current test should be set to 'teardown'.",
          "The `error_message` of the result for the current test should contain 'Cleanup failed'.",
          "If the teardown operation fails, the collector should not proceed with collecting results for the next test.",
          "If the teardown operation succeeds, the collector should still report an error and stop collecting results for the current test."
        ],
        "scenario": "Test that handle_runtest_teardown_failure verifies that a teardown failure records an error and prevents the collector from proceeding with the next test.",
        "token_usage": {
          "completion_tokens": 207,
          "prompt_tokens": 391,
          "total_tokens": 598
        },
        "why_needed": "This test prevents regression by ensuring that when a teardown operation fails, it correctly reports an error and stops the collection of results for the current test."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007850440000254366,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"Context compression should be 'none' or 'gzip'\", 'expected_value': 'none|gzip', 'actual_value': 'invalid'}"
        ],
        "scenario": "Test invalid compression mode",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 124,
          "total_tokens": 200
        },
        "why_needed": "To test that an invalid compression mode fails validation."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_invalid_compression_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008254700000236426,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding should be a non-negative integer.', 'expected_type': 'int', 'actual_type': -1}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 121,
          "total_tokens": 204
        },
        "why_needed": "Negative padding is not allowed in context lines."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007878790000006575,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.validate() should return an empty list of errors', 'description': 'The validate method should not return any error messages for valid compression modes.'}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 135,
          "total_tokens": 211
        },
        "why_needed": "To ensure that valid compression modes are validated correctly."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_valid_compression_modes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000778280999981007,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'context_line_padding should be 0 or less.', 'actual': 0}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 122,
          "total_tokens": 199
        },
        "why_needed": "Zero padding is a valid configuration for the context line padding."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008841000000074928,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_compression', 'expected_value': 'lines'}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 119,
          "total_tokens": 193
        },
        "why_needed": "Context compression should be enabled by default ('lines')."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007890810000219517,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context_compression', 'expected_value': 'lines'}"
        ],
        "scenario": "TestContextCompression::test_compression_mode_lines",
        "token_usage": {
          "completion_tokens": 63,
          "prompt_tokens": 113,
          "total_tokens": 176
        },
        "why_needed": "Lines compression mode is required for this test."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_mode_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007686730000102671,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_line_padding', 'expected_value': 2, 'actual_value': 0}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 106,
          "total_tokens": 188
        },
        "why_needed": "To ensure that line padding is set correctly in the default context."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0008530509999786773,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The count of '# ...' should be zero for contiguous lines without gaps.",
          "# L3:",
          "# L4:",
          "# L5:"
        ],
        "scenario": "Test that contiguous covered lines do not have gap indicators when there is no padding.",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 293,
          "total_tokens": 379
        },
        "why_needed": "Prevents regression where contiguous lines without gaps are incorrectly marked as uncovered."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_contiguous_lines_no_gap",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 216-217"
        }
      ],
      "duration": 0.0007789919999936501,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'result == \"\"', 'expected_result': ''}"
        ],
        "scenario": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 130,
          "total_tokens": 213
        },
        "why_needed": "This test is needed because it checks for an empty coverage scenario, which can occur when no lines are covered."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 24,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249"
        }
      ],
      "duration": 0.0008363000000031207,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '# ...' line is present in the result.",
          "The '# L3:' line is present in the result.",
          "The '# L15:' line is present in the result.",
          "Gap indicator between ranges is included in the output.",
          "Multiple covered lines are correctly identified with gap indicators."
        ],
        "scenario": "Test Extract Covered Lines: Multiple covered ranges should be extracted with gap indicators.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 274,
          "total_tokens": 394
        },
        "why_needed": "This test prevents a regression where multiple covered lines are not correctly identified with gap indicators."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_multiple_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0008215320000033444,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert '# L2:' in result",
          "assert '# L3:' in result",
          "assert '# L4:' in result"
        ],
        "scenario": "Single covered line should be extracted with padding.",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 302,
          "total_tokens": 383
        },
        "why_needed": "This test prevents a bug where the single covered line is not extracted correctly due to incorrect padding."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0008031270000117274,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The extracted covered lines should not have negative line numbers (L0 and L4).",
          "The extracted covered lines should only contain lines from the first to third lines (L1, L2, and L3)."
        ],
        "scenario": "Test Extract Covered Lines: Padding should not go beyond file boundaries.",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 288,
          "total_tokens": 392
        },
        "why_needed": "This test prevents a potential issue where padding exceeds the file boundary, potentially causing incorrect results or errors."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_padding_boundary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009312680000164164,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The prompt should contain 'short content'\", 'expected_result': 'short content'}",
          "{'assertion': \"The prompt should not contain 'truncated'\", 'expected_result': 'no-truncation-needed'}"
        ],
        "scenario": "tests/test_context_limits.py::test_no_truncation_needed",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 158,
          "total_tokens": 268
        },
        "why_needed": "This test is needed because the current implementation may truncate context due to performance reasons."
      },
      "nodeid": "tests/test_context_limits.py::test_no_truncation_needed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 25,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00176798900000108,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "F1 should get full allocation of budget (40 tokens).",
          "F2 should get extra budget beyond required allocation (180 tokens).",
          "F2 content is not fully allocated, with excess characters (>480 tokens) that are truncated."
        ],
        "scenario": "tests/test_context_limits.py::test_smart_distribution verifies the context limits for F1 and F2 to ensure they are allocated fair share of budget without unnecessary truncation.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 773,
          "total_tokens": 904
        },
        "why_needed": "This test prevents regression that could cause F1's content to be truncated unnecessarily when its required tokens exceed available budget."
      },
      "nodeid": "tests/test_context_limits.py::test_smart_distribution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009621750000121665,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file \"f1\" contains more than 100 tokens (400 characters).",
          "The file \"f2\" contains more than 100 tokens (400 characters).",
          "The string 'Present' is truncated and appears in the prompt.",
          "The budget per file is approximately 80 tokens, which is within the allowed limit of 200 tokens for both files.",
          "The overhead is small, as expected."
        ],
        "scenario": "Verify that the splitting logic correctly identifies files with large contents and truncates strings while maintaining budget limits.",
        "token_usage": {
          "completion_tokens": 160,
          "prompt_tokens": 317,
          "total_tokens": 477
        },
        "why_needed": "This test prevents a potential regression where the splitting logic fails to truncate strings for files with large contents, leading to incorrect results or unexpected behavior."
      },
      "nodeid": "tests/test_context_limits.py::test_splitting_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "243, 245, 264, 266, 270-272, 274-275"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010003079999876263,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The prompt should be truncated to fit within 100 tokens minus system prompt overhead.",
          "Context should be very small or empty if limit is exceeded.",
          "Prompt should contain '[... truncated]' or 'Relevant context' if no budget is left."
        ],
        "scenario": "Test truncation logic for large context files when limit is exceeded.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 397,
          "total_tokens": 506
        },
        "why_needed": "This test prevents regression where the context is too long and needs to be truncated due to memory constraints."
      },
      "nodeid": "tests/test_context_limits.py::test_truncation_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007587350000051174,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'line1\\n\\nline2', 'actual_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 128,
          "total_tokens": 211
        },
        "why_needed": "To test the functionality of collapsing empty lines in a context."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007663680000007389,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The result is equal to 'line1\\n\\nline2'.\", 'expected_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 127,
          "total_tokens": 217
        },
        "why_needed": "To test the functionality of collapsing empty lines in a multi-line source."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008228050000127496,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The source string has two or fewer consecutive newlines.', 'expected_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 125,
          "total_tokens": 219
        },
        "why_needed": "To test if the `collapse_empty_lines` function correctly preserves up to 2 consecutive newlines."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007725700000094093,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'line1\\nline2\\nline3', 'actual_result': 'line1\\nline2\\nline3'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 121,
          "total_tokens": 203
        },
        "why_needed": "Preserve single newlines in collapsed lines"
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007921569999780331,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '', 'actual': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 137,
          "total_tokens": 207
        },
        "why_needed": "Because empty lines are not being collapsed by the current implementation."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 45,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010322870000152307,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The optimized source code contains all necessary key assertions.', 'expected_result': 'The optimized source code contains all necessary key assertions.'}",
          "{'assertion': 'The combined optimization process does not introduce any new errors or warnings.', 'expected_result': 'No new errors or warnings were introduced by the combined optimization process.'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 96,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the combined optimization process is applied correctly and optimizes the code efficiently."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 36,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008820559999946909,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring removal', 'expected_result': 'True'}",
          "{'name': 'comment presence', 'expected_result': 'False'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 100,
          "total_tokens": 197
        },
        "why_needed": "To ensure that default context stripping only removes docstrings, without affecting comments."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007983989999900132,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 95,
          "total_tokens": 164
        },
        "why_needed": "The function should be able to handle an empty source without raising an error."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000789190999995526,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected result', 'value': '   \\n\\n   \\n'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 115,
          "total_tokens": 202
        },
        "why_needed": "To handle cases where the source code contains only whitespace characters, such as blank lines or multiple consecutive whitespace characters."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 44,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009492719999855126,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'strip_comments', 'expected_output': 'docstring', 'actual_output': 'comment'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 95,
          "total_tokens": 172
        },
        "why_needed": "To remove unnecessary documentation from the code."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 14,
          "line_ranges": "81-82, 86, 88-90, 93, 108, 124, 126, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008918240000070909,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'expected_output', 'expected_output': 'def foo():\\n  # This is a comment\\n  pass'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 95,
          "total_tokens": 178
        },
        "why_needed": "To optimize the context by removing unnecessary comments."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007681109999850833,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function `foo()` is kept in the optimized context.', 'expected_result': 'def foo():', 'actual_result': 'def foo():'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 94,
          "total_tokens": 202
        },
        "why_needed": "The current implementation of `optimize_context` does not strip unnecessary code, which can lead to unexpected behavior if the user explicitly requests it."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007920970000157013,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The output of the `strip_comments` function is equal to the expected string.', 'expected_value': '\\'url = \"http://example.com#anchor\"\\'', 'actual_value': 'result'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 134,
          "total_tokens": 243
        },
        "why_needed": "This test ensures that the `strip_comments` function correctly removes comments from strings containing hashes."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000769113999979254,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'basic behavior', 'expected_result': '# comment', 'actual_result': 'The escaped quote handling is simplified, check basic behavior'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 133,
          "total_tokens": 221
        },
        "why_needed": "To ensure that the context utility correctly handles escaped quotes in strings."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008490939999887814,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '\"don\\'t # worry\"', 'actual': 'x = \"don\\'t # worry\"  # comment'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 101,
          "total_tokens": 184
        },
        "why_needed": "To strip quotes from a string containing both single and double quotes."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008236059999831014,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'source_code', 'expected_result': 'def foo():\\n  # This line will be stripped\\nreturn 1'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_no_comments",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 91,
          "total_tokens": 181
        },
        "why_needed": "To strip comments from the source code, ensuring that only relevant information is exposed."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_no_comments",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008115130000021509,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'result = \\'url = \"http://example.com#anchor\"\\'.split(\\''}",
          "expected_result': '"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 135,
          "total_tokens": 239
        },
        "why_needed": "Preserves # inside double-quoted strings."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008156810000059522,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'result == \"url = \\'http://example.com#anchor\\'\",', 'expected_result': '\"url = \\'http://example.com#anchor\\'\",'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 135,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the `strip_comments` function preserves # inside single-quoted strings, which is essential for maintaining the integrity of the original source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007905429999937041,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'x = 1', 'actual_result': 'x = 1'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 119,
          "total_tokens": 195
        },
        "why_needed": "To remove simple end-of-line comments from the source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008228949999988799,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'strip_comments', 'expected_result': 'The line is no longer a comment'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 99,
          "total_tokens": 176
        },
        "why_needed": "To strip standalone comments from the test source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 4,
          "line_ranges": "27, 29-31"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008519889999831776,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'def foo( unclosed paren', 'actual_value': 'def foo( parentheses ', 'message': 'The expected value does not match the actual value.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 119,
          "total_tokens": 234
        },
        "why_needed": "The test is checking if the function `strip_docstrings` handles syntax errors correctly by returning the original source code when an unclosed parenthesis is found."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 30,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008996580000086851,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function should remove all docstrings, not just the first one.', 'expected_result': 'All docstrings in the module should be removed.'}",
          "{'assertion': 'The function should only strip the first occurrence of a docstring.', 'expected_result': 'Only the first docstring should be stripped.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 95,
          "total_tokens": 230
        },
        "why_needed": "The function `strip_docstrings` is used to strip unnecessary docstrings from Python code."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008509769999989203,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'strip_context preserves triple-quoted strings', 'expected_value': 'foo()', 'actual_value': 'def foo():\\n    \"\"\"\\n    def foo():\\n```python\\n'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 103,
          "total_tokens": 209
        },
        "why_needed": "Preserve multiline data strings in docstrings when stripping context."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 25,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008556860000226152,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Regular string is preserved', 'expected_value': '\\'x = \"hello world\"\\'', 'actual_value': '\\'x = \"hello world\"\\''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 102,
          "total_tokens": 190
        },
        "why_needed": "Preserve regular strings in test context."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 27,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008384639999974297,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Ensure the function correctly preserves strings in lists/dicts.', 'expected_value': 'Should preserve strings in lists/dicts.'}",
          "{'description': 'Verify the function handles nested quotes correctly.', 'expected_value': 'Should handle nested quotes correctly'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 146,
          "total_tokens": 265
        },
        "why_needed": "Preserve strings in structures is a critical test case for context utilities."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009884749999855558,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'remove', 'expected_result': 'striped'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 97,
          "total_tokens": 180
        },
        "why_needed": "Because of the presence of a multiline docstring, it is causing issues with test coverage and readability."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009344430000055581,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"source.strip('\"}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 106,
          "total_tokens": 205
        },
        "why_needed": "To remove triple double-quoted docstrings from the test source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008507569999949283,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'source should be modified to not include triple single-quoted docstrings', 'expected_result': 'source = \"def foo():\\n    # triple single-quoted docstring\\n    pass\"'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 106,
          "total_tokens": 214
        },
        "why_needed": "To remove triple single-quoted docstrings from the test source."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 19,
          "line_ranges": "134-135, 137-141, 143-144, 476, 478, 524-531"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008514879999950153,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function provider._parse_preferred_models() should return ['m1', 'm2'] when the config.model is set to 'm1' or 'm2'.",
          "The function provider._parse_preferred_models() should return an empty list when the config.model is None.",
          "The function provider._parse_preferred_models() should return ['All'] when the config.model is set to 'All'."
        ],
        "scenario": "Test the Gemini model parsing edge cases for coverage boosters.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 273,
          "total_tokens": 417
        },
        "why_needed": "This test prevents regression in case a new model is added to the preferred list without updating the existing tests."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 35,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008615069999962088,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method should return 0 when both limits are exceeded (50+60 > 100).",
          "The next_available_in method should not be able to return a positive value when only one limit is exceeded (10 > 0)."
        ],
        "scenario": "Verify that the rate limiter prevents over and under token limits for edge cases.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 273,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a potential regression in the rate limiter's behavior when dealing with edge cases where there are more tokens available than requested."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 47,
          "line_ranges": "96-103, 130-133, 135, 137-139, 141, 143, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007768579999947178,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `d['coverage_percent']` should be equal to 50.0.",
          "The value of `ann.to_dict()['error']` should be equal to 'timeout'.",
          "The value of `meta.to_dict()['duration']` should be equal to 1.0."
        ],
        "scenario": "Verify that the `to_dict()` method of `SourceCoverageEntry` returns the correct coverage percentage.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 318,
          "total_tokens": 446
        },
        "why_needed": "This test prevents a potential bug where the coverage percentage is not correctly calculated for certain types of source code."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007990199999881042,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mapper.config is equal to config', 'expected_value': 'config', 'actual_value': 'mapper.config'}",
          "{'name': 'mapper.warnings are empty', 'expected_value': [], 'actual_value': []}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 109,
          "total_tokens": 212
        },
        "why_needed": "Mapper initialization should be tested."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 3,
          "line_ranges": "44-45, 308"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000781676999991987,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should return a list of warnings.', 'expected_type': 'list'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 110,
          "total_tokens": 186
        },
        "why_needed": "To ensure the get_warnings method returns a valid list of warnings."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001504073000006656,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `map_coverage()` method should return an empty dictionary when no coverage file exists.",
          "The `map_coverage()` method should have at least one warning in this scenario.",
          "The `map_coverage()` method should not throw any exceptions when no coverage file is present."
        ],
        "scenario": "Test verifies that the `map_coverage` method returns an empty dictionary when no coverage file is present.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 277,
          "total_tokens": 401
        },
        "why_needed": "Prevents a potential bug where the test fails with an incorrect result (empty dict) when there's no coverage file."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008127750000141987,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The node ID extracted for each phase should be the same as the original node ID.",
          "The node IDs for different phases should be in the same order.",
          "Any additional phase information (e.g., method name) should be ignored when extracting node IDs."
        ],
        "scenario": "The test verifies that the `CoverageMapper` correctly extracts node IDs for all phases when the `include_phase` parameter is set to 'all'.",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 279,
          "total_tokens": 417
        },
        "why_needed": "This test prevents a potential regression where the coverage map might not include all phases if the `include_phase` parameter is not set to 'all'."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000779402999995682,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'assert mapper._extract_nodeid([]) == None', 'expected_result': 'None'}",
          "{'assertion': 'assert mapper._extract_nodeid(None) == None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 128,
          "total_tokens": 246
        },
        "why_needed": "To handle cases where the context is empty or None, allowing for proper coverage extraction."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 216, 220, 224-225, 228-230"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007526829999733309,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Expected value to be None', 'expected_value': 'None'}"
        ],
        "scenario": "Test coverage mapping",
        "token_usage": {
          "completion_tokens": 63,
          "prompt_tokens": 139,
          "total_tokens": 202
        },
        "why_needed": "To filter out setup phase when include_phase=run."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008140169999819591,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'test.py::test_foo', 'actual_value': 'test.py::test_foo'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 145,
          "total_tokens": 231
        },
        "why_needed": "To extract the correct node ID from the run phase context."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 29,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152, 156, 160-162, 167-170, 199, 202"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012219230000027892,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_data.contexts_by_lineno.side_effect is set to contexts_side_effect",
          "call_count[0] should be incremented by 1 before raising the exception",
          "the exception raised is an instance of `Exception`",
          "the error message is 'Error accessing contexts'",
          "the context with ID 1 has a key-value pair with value ['test::test_foo|run']",
          "the number of calls to `contexts_by_lineno` is 2"
        ],
        "scenario": "Test that the `contexts_by_lineno` method raises an exception when called with a mocked context that fails.",
        "token_usage": {
          "completion_tokens": 155,
          "prompt_tokens": 332,
          "total_tokens": 487
        },
        "why_needed": "Prevents regression in handling unexpected exceptions during context extraction."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_contexts_by_lineno_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 7,
          "line_ranges": "44-45, 118, 121-122, 127-128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010890439999968748,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is a dictionary', 'expected_value': {}, 'message': 'The result should be an empty dictionary'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 136,
          "total_tokens": 226
        },
        "why_needed": "To test that the function returns an empty dictionary when there are no measured files."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022355669999853944,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Value comparison', 'expected_value': {}, 'actual_value': {}}"
        ],
        "scenario": "Test Extract Contexts",
        "token_usage": {
          "completion_tokens": 60,
          "prompt_tokens": 154,
          "total_tokens": 214
        },
        "why_needed": "Skip non-Python files"
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007955830000128117,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mapper is created successfully', 'expected_result': 'True'}"
        ],
        "scenario": "TestLoadCoverageData",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 166,
          "total_tokens": 234
        },
        "why_needed": "The test is necessary because it checks for coverage when `coverage.py` is not installed."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_coverage_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010991529999841987,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'type': 'assertion', 'name': 'result is None', 'description': 'The function _load_coverage_data() should return None when no .coverage file exists.'}",
          "{'type': 'assertion', 'name': \"any('W001' in w.code for w in mapper.warnings)\", 'description': \"The function _load_coverage_data() should have a warning 'W001' in the warnings list.\"}"
        ],
        "scenario": "TestLoadCoverageData",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 153,
          "total_tokens": 292
        },
        "why_needed": "To test the scenario when no .coverage file exists."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0014582370000084666,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that no coverage is added to the report when analysis2 raises an exception.",
          "The test verifies that a warning is generated with the message 'COVERAGE_ANALYSIS_FAILED' for each measured file.",
          "The test verifies that all warnings are properly handled and do not prevent further analysis or reporting."
        ],
        "scenario": "Test the exception handling of analysis2 when it raises an exception.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 286,
          "total_tokens": 404
        },
        "why_needed": "To prevent regression and ensure that warnings are properly generated when analysis2 raises an exception."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_analysis_exception_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 18,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259-261, 273-274, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013729460000035942,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should return an empty list of lines covered by the code.', 'expected_result': []}"
        ],
        "scenario": "tests/test_coverage_map_coverage",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 178,
          "total_tokens": 250
        },
        "why_needed": "To test the coverage map when there are no statements in a file."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_empty_statements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.00171240399998851,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mapped_data` attribute of the `result` object should contain a single entry with 'covered' set to 2 and 'missed' set to 1.",
          "The `mapped_data` attribute of the `result` object should not be empty.",
          "The `covered` value in the first test data entry is correct (i.e., it has been measured by the coverage tool).",
          "The `missed` value in the first test data entry is correct (i.e., it does not have any unmeasured files)."
        ],
        "scenario": "Test that test files are included when omit_tests_from_coverage is False.",
        "token_usage": {
          "completion_tokens": 168,
          "prompt_tokens": 322,
          "total_tokens": 490
        },
        "why_needed": "Prevents regression in coverage reporting when omitting tests from the coverage report."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_include_test_files_when_not_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 10,
          "line_ranges": "44-45, 243-244, 246-249, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013952090000088901,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected to skip non-Python files', 'description': 'The test should skip non-Python files'}"
        ],
        "scenario": "Test Map Source Coverage",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 154,
          "total_tokens": 219
        },
        "why_needed": "Skip non-Python files"
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-255, 257, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.001284360999989076,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "{'id': 'test_skip_test_files_when_configured', 'description': 'Test that test files are skipped when omit_tests_from_coverage is True.', 'key_assertions': [{'name': 'expected_result', 'type': 'list', 'value': []}]}",
        "token_usage": {
          "completion_tokens": 295,
          "prompt_tokens": 182,
          "total_tokens": 477
        },
        "why_needed": "{'id': 'test_skip_test_files_when_configured', 'description': 'Test that test files are skipped when omit_tests_from_coverage is True.', 'key_assertions': [{'name': 'expected_result', 'type': 'list', 'value': []}]}"
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_test_files_when_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007951519999949141,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mapper should return the nodeid for any phase in the config.",
          "The mapper should return the same nodeid for different phases (e.g., 'setup', 'run', 'teardown')",
          "All three assertions should pass with the same expected result"
        ],
        "scenario": "Test that all phases are accepted when configured.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 305,
          "total_tokens": 408
        },
        "why_needed": "Prevents regression where some phases might be missed due to incorrect configuration."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_all_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007475430000170036,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert mapper._extract_nodeid returns None for empty string', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 115,
          "total_tokens": 205
        },
        "why_needed": "The test is necessary to ensure that the `extract_nodeid` function handles empty strings correctly."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007978170000058071,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
        "token_usage": {
          "completion_tokens": 58,
          "prompt_tokens": 114,
          "total_tokens": 172
        },
        "why_needed": "None input returns None."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008007929999962471,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "When the 'phase' parameter is missing, it should return the nodeid of the current file.",
          "When the 'phase' parameter does not match 'run', it should return None for that phase.",
          "When the 'phase' parameter does not match 'setup' or 'teardown', it should return None for those phases."
        ],
        "scenario": "Test that run phase is the default filter.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 297,
          "total_tokens": 411
        },
        "why_needed": "Prevents unexpected behavior when run phases are not specified."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_run_phase_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008018759999970371,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') is None"
        ],
        "scenario": "Test that setup phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 293,
          "total_tokens": 418
        },
        "why_needed": "This test prevents a bug where the setup phase is incorrectly filtered, leading to false positives in coverage reports."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_setup_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233-234, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008065340000200649,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') is None"
        ],
        "scenario": "Test that teardown phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 296,
          "total_tokens": 423
        },
        "why_needed": "This test prevents a regression where the teardown phase is not properly filtered, potentially leading to false positives in coverage reports."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_teardown_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 6,
          "line_ranges": "44-45, 216, 220, 224, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008041399999854093,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'test_foo.py::test_bar', 'actual_value': 'test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 136,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the node id is extracted correctly when there are no phase delimiters in the code."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 57,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0017385830000193891,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'file_path': 'test_app.py::test_one', 'line_count': 2}",
          "{'file_path': 'test_app.py::test_two', 'line_count': 3}"
        ],
        "scenario": "Test extracts all contexts for full logic coverage.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 413,
          "total_tokens": 507
        },
        "why_needed": "Prevents regression in coverage analysis when _extract_contexts is called with a file that has multiple logical contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013158499999974538,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result is empty', 'expected_value': {}, 'message': 'Expected result to be an empty dictionary, but got {}'}"
        ],
        "scenario": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 174,
          "total_tokens": 274
        },
        "why_needed": "To test the behavior of the _extract_contexts method when there are no test contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008235960000035902,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_extract_nodeid` is called with a valid path and returns the expected node ID.",
          "The function `_extract_nodeid` is called with an invalid path (e.g., without a phase) and returns `None` as expected.",
          "The function `_extract_nodeid` is called with a context that does not contain any code paths (e.g., `test.py::test_no_phase`) and returns the expected node ID."
        ],
        "scenario": "Test extracting node IDs for maximal coverage in different phases.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 323,
          "total_tokens": 473
        },
        "why_needed": "This test prevents regression by ensuring that the `CoverageMapper` correctly extracts node IDs from code paths regardless of phase."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010156869999775608,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return None for _load_coverage_data() without any .coverage files.",
          "The number of warnings should be 1.",
          "The first warning code should be 'W001'.",
          "The current working directory should remain unchanged after the test.",
          "No coverage file should have been loaded into the mapper.",
          "The mapper's warnings list should contain only one item."
        ],
        "scenario": "Test that the test_load_coverage_data_no_files function correctly handles the case when no coverage files exist.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 276,
          "total_tokens": 418
        },
        "why_needed": "This test prevents a potential regression where the test might fail due to missing coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 17,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015843340000003536,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _load_coverage_data() should return None and set warnings to include 'Failed to read coverage data' when an exception is raised by the mock CoverageData.",
          "The function _load_coverage_data() should not raise any exceptions itself, but instead propagate them up the call stack.",
          "The function _load_coverage_data() should log the error message in the warnings list.",
          "The function _load_coverage_data() should set the 'error' level warning to include 'Failed to read coverage data'.",
          "The function _load_coverage_data() should not return any value (i.e., it should be None)."
        ],
        "scenario": "Test ensures that the CoverageMapper can handle errors reading coverage files correctly.",
        "token_usage": {
          "completion_tokens": 189,
          "prompt_tokens": 343,
          "total_tokens": 532
        },
        "why_needed": "This test prevents a regression where the CoverageMapper fails to report coverage data when an error occurs while reading it."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0025777889999858417,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock instances of `CoverageData` are returned by the `_load_coverage_data()` method and updated accordingly.",
          "The `update()` method is called on the `CoverageData` instance for each mock instance.",
          "At least two calls to `update()` are made on the `CoverageData` instance.",
          "The `update()` method is called on the first mock instance (`mock_main_data`), and then on the second mock instance (`mock_parallel_data1`).",
          "The `update()` method is called on the third mock instance (`mock_parallel_data2`) but no call is made to it.",
          "No calls are made to the `update()` method on any other instances of `CoverageData`.",
          "The `update()` method is only called for the first two mock instances, and not for the third one."
        ],
        "scenario": "Test should handle parallel coverage files from xdist and verify that the CoverageMapper correctly updates its internal data structures.",
        "token_usage": {
          "completion_tokens": 248,
          "prompt_tokens": 378,
          "total_tokens": 626
        },
        "why_needed": "This test prevents regression in handling parallel coverage files, ensuring that the CoverageMapper correctly updates its internal data structures when loading coverage data with multiple parallel files."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 5,
          "line_ranges": "44-45, 58-60"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011046230000033574,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_load_coverage_data` method of the `CoverageMapper` instance should return `None` when called with an empty dictionary.",
          "The `map_coverage` method of the `CoverageMapper` instance should return an empty dictionary when passed a non-empty dictionary.",
          "The test should not fail if there is no coverage data loaded, i.e., `_load_coverage_data` returns None."
        ],
        "scenario": "Test that the `map_coverage` method returns an empty dictionary when `_load_coverage_data` returns None.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 228,
          "total_tokens": 370
        },
        "why_needed": "Prevents a potential bug where the test fails if there is no coverage data loaded."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013755720000006022,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocked `analysis2` raises an exception when called with error message.",
          "Mocked `get_data` returns mock data but does not raise an exception.",
          "Mocked `map_source_coverage` skips files with errors.",
          "Expected no entries in the list of coverage results.",
          "Asserts that the length of the entries is 0, indicating no skipped files due to error."
        ],
        "scenario": "Test should handle errors during coverage analysis.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 274,
          "total_tokens": 410
        },
        "why_needed": "Prevents regression in case of unexpected errors during analysis, ensuring test coverage is not affected by unhandled exceptions."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0016747040000097968,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method should return exactly one entry for the given mock data.",
          "The file path of the returned entry should be 'app.py'.",
          "The number of statements in the returned entry should be 3.",
          "The coverage percentage of the returned entry should be 66.67%.",
          "All covered lines should have a count greater than or equal to 1.",
          "No uncovered lines should have a count less than 2.",
          "All missing lines should have a count greater than 0.",
          "The coverage percentage of each line should add up to 100%."
        ],
        "scenario": "Tests coverage of map_source_coverage method with comprehensive test data.",
        "token_usage": {
          "completion_tokens": 174,
          "prompt_tokens": 345,
          "total_tokens": 519
        },
        "why_needed": "Prevents regression in coverage analysis when testing all paths in map_source_coverage."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007450290000008408,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned object has the correct `code` attribute set to `WarningCode.W001_NO_COVERAGE`.",
          "The message of the returned object contains the expected string 'No .coverage file found'.",
          "The detail of the returned object matches the provided value 'test-detail'."
        ],
        "scenario": "Test the `make_warning` factory function with a valid warning code and message.",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 236,
          "total_tokens": 368
        },
        "why_needed": "Prevents a potential bug where an unknown or invalid warning code is passed to the `make_warning` function, causing it to raise a `ValueError`."
      },
      "nodeid": "tests/test_errors.py::test_make_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008029570000189779,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Assertion failed: WarningCode.W001_NO_COVERAGE.value == \"W001\"', 'expected': 'WarningCode.W001_NO_COVERAGE', 'actual': 'WarningCode.W001'}",
          "{'message': 'Assertion failed: WarningCode.W101_LLM_ENABLED.value == \"W101\"', 'expected': 'WarningCode.W101_LLM_ENABLED', 'actual': 'WarningCode.W101'}",
          "{'message': 'Assertion failed: WarningCode.W201_OUTPUT_PATH_INVALID.value == \"W201\"', 'expected': 'WarningCode.W201_OUTPUT_PATH_INVALID', 'actual': 'WarningCode.W201'}",
          "{'message': 'Assertion failed: WarningCode.W301_INVALID_CONFIG.value == \"W301\"', 'expected': 'WarningCode.W301_INVALID_CONFIG', 'actual': 'WarningCode.W301'}",
          "{'message': 'Assertion failed: WarningCode.W401_AGGREGATE_DIR_MISSING.value == \"W401\"', 'expected': 'WarningCode.W401_AGGREGATE_DIR_MISSING', 'actual': 'WarningCode.W401'}"
        ],
        "scenario": "Test that warning codes have correct values.",
        "token_usage": {
          "completion_tokens": 290,
          "prompt_tokens": 240,
          "total_tokens": 530
        },
        "why_needed": "Prevents a potential bug where the incorrect value is assigned to a warning code, potentially causing unexpected behavior or errors in downstream processing."
      },
      "nodeid": "tests/test_errors.py::test_warning_code_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000780926000004456,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'code' key should be present in the dictionary with value 'W001'.",
          "The 'message' key should be present in the dictionary with value 'No coverage'.",
          "The 'detail' key should be present in the dictionary with value 'some/path'."
        ],
        "scenario": "Test ReportWarning.to_dict() method.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 276,
          "total_tokens": 388
        },
        "why_needed": "Prevents a warning about the ReportWarning.to_dict() method being used with warnings that have no detail."
      },
      "nodeid": "tests/test_errors.py::test_warning_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007627210000009654,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "w.code == WarningCode.W101_LLM_ENABLED",
          "w.message == WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]",
          "w.detail is None"
        ],
        "scenario": "Test creates warning with standard message for known code.",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 222,
          "total_tokens": 304
        },
        "why_needed": "Prevents regression where a known code might not create a warning."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007718990000000758,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected', 'type': 'assertion', 'message': \"The message for missing warning code should be 'Unknown warning.'\"}"
        ],
        "scenario": "test_make_warning_unknown_code",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 202,
          "total_tokens": 277
        },
        "why_needed": "To handle unknown code in the typed function."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007525430000043798,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'w.code == WarningCode.W301_INVALID_CONFIG', 'expected_value': 'WarningCode.W301_INVALID_CONFIG'}",
          "{'name': \"w.detail == 'Bad value'\", 'expected_value': 'Bad value'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 127,
          "total_tokens": 234
        },
        "why_needed": "To test the creation of a warning with detail."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007787710000002335,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert isinstance(code.value, str)', 'description': 'Ensure that each code value is a string.'}",
          "{'name': \"assert code.value.startswith('W')\", 'description': \"Ensure that each code starts with 'W', which indicates a warning code.\"}"
        ],
        "scenario": "Error Codes",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 109,
          "total_tokens": 221
        },
        "why_needed": "The `test_codes_are_strings` test is checking that the WarningCode enum values are strings."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007572410000022956,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equals', 'expected_value': {'code': 'W001', 'message': 'No coverage'}, 'actual_value': {'code': 'W001_NO_COVERAGE', 'message': 'No coverage'}}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 147,
          "total_tokens": 267
        },
        "why_needed": "To ensure that the warning data is correctly serialized without any additional details."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007563199999935932,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}, 'actual': {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 161,
          "total_tokens": 288
        },
        "why_needed": "The test is needed to ensure that the `to_dict()` method of the `ReportWarning` class correctly serializes a warning object with detailed information."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007867969999892921,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Should return False', 'value': False}",
          "{'message': 'for non-.py files', 'value': 'The file does not have a .py extension.'}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 115,
          "total_tokens": 210
        },
        "why_needed": "Because the test is checking for non-.py files, which are not Python files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007427940000184208,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function is called with a valid .py file path.', 'condition': \"assert is_python_file('foo/bar.py') == True\"}",
          "{'description': 'The function raises an error when given a non-.py file path.', 'condition': \"is_python_file('non_py_file.txt')\"}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_python_file",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 98,
          "total_tokens": 226
        },
        "why_needed": "The function `is_python_file` should be able to identify .py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.001140130000010231,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The file path is absolute after making it relative.', 'expected_result': '/subdir/file.py'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 144,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the `make_relative` function correctly handles path relative paths."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0007600870000032955,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 107,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the `make_relative` function returns a normalized path when no base is provided."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007516409999936968,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert normalize_path returns original path for already-normalized paths', 'expected_value': 'foo/bar', 'actual_value': \"normalize_path('foo/bar') == 'foo/bar'\"}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 96,
          "total_tokens": 200
        },
        "why_needed": "This test is needed because it checks if the `normalize_path` function correctly handles already-normalized paths."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007586229999958505,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'foo\\\\bar', 'expected': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 100,
          "total_tokens": 174
        },
        "why_needed": "To ensure that the `normalize_path` function correctly handles paths with forward slashes."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007902730000068914,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '/foo/bar/', 'actual': 'foo/bar/'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 102,
          "total_tokens": 179
        },
        "why_needed": "To ensure that the `normalize_path` function correctly removes trailing slashes from file paths."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.000826110000019753,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'should skip a path matching a custom pattern', 'expected_result': True, 'actual_result': 'True'}",
          "{'description': 'should not skip a path not matching any custom pattern', 'expected_result': False, 'actual_result': 'False'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 126,
          "total_tokens": 243
        },
        "why_needed": "to test the custom exclusion of paths based on patterns"
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007577030000049945,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('src/module.py') is False"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 96,
          "total_tokens": 163
        },
        "why_needed": "This test ensures that the `should_skip_path` function does not incorrectly skip normal file system paths."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007566600000075141,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'path': '.git/objects/foo'}, 'actual': {'should_be': True, 'is_true': False}}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 99,
          "total_tokens": 184
        },
        "why_needed": "The test should be skipped when the path contains a .git directory."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007681120000029296,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'foo/__pycache__/bar.pyc', 'expected_result': True}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 109,
          "total_tokens": 192
        },
        "why_needed": "The test should be skipped because it's trying to access a __pycache__ directory."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007994700000040211,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Assertion failed', 'expected': True, 'actual': False}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 121,
          "total_tokens": 201
        },
        "why_needed": "The test checks if the `should_skip_path` function correctly identifies venv directories as being to be skipped."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007853339999996933,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert is_python_file('module.txt') is False",
          "assert is_python_file('module.pyc') is False",
          "assert is_python_file('module') is False"
        ],
        "scenario": "Non-.py files should not be considered Python files.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 210,
          "total_tokens": 318
        },
        "why_needed": "This test prevents a potential bug where the function `is_python_file` incorrectly identifies non-.py files as Python files, leading to incorrect results or unexpected behavior in downstream code."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007767779999880986,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert is_python_file('module.py') is True",
          "assert is_python_file('path/to/module.py') is True",
          "assert is_python_file(Path('module.py')) is True"
        ],
        "scenario": "The test verifies that a module file (.py) returns True.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 212,
          "total_tokens": 309
        },
        "why_needed": "This test prevents regression where the function `is_python_file` incorrectly identifies non-Python files."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 12,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63, 65, 67"
        }
      ],
      "duration": 0.0011859049999998206,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should only include paths that are directly under the base in its result.",
          "The function should exclude any file extensions from being included in the result.",
          "The function should handle cases where the input paths have different levels of nesting (e.g., `project1/subdir/file.py` vs. `/path/to/project2/subdir/file.py`).",
          "The function should not include any redundant or unnecessary information in its output (e.g., file extensions, directory names)."
        ],
        "scenario": "Test makes sure `make_relative` returns the correct absolute path when the input path is not relative to the base.",
        "token_usage": {
          "completion_tokens": 172,
          "prompt_tokens": 301,
          "total_tokens": 473
        },
        "why_needed": "Prevents a potential bug where `make_relative` would incorrectly return an absolute path for paths that are not under the base."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_path_not_under_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.00108184000001188,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_output': 'subdir/file.py', 'actual_output': 'subdir/file.py'}"
        ],
        "scenario": "Test Make Relative",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 147,
          "total_tokens": 213
        },
        "why_needed": "To test the functionality of making relative paths in the file system."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0007500279999987924,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'path/to/file.py', 'actual_value': 'normalized_path'}"
        ],
        "scenario": "test_make_relative_with_none_base",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 116,
          "total_tokens": 184
        },
        "why_needed": "To test the functionality of making a relative path with a None base."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007759759999999005,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The normalized path contains a single forward slash.', 'expected_result': '/path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 114,
          "total_tokens": 198
        },
        "why_needed": "To ensure that backslashes are correctly converted to forward slashes in file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0009031360000051336,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': '/path/to/file.py', 'expected_result': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 110,
          "total_tokens": 191
        },
        "why_needed": "Normalization of a Path object is necessary to ensure consistency and correctness in file system operations."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.000796245000003637,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '/path/to/dir/', 'actual': 'path/to/dir'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 111,
          "total_tokens": 185
        },
        "why_needed": "To ensure the function correctly handles paths with trailing slashes."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007996109999908185,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should skip regular path', 'expected_result': {'scenario': 'False', 'why_needed': ''}, 'actual_result': {'scenario': 'True', 'why_needed': ''}}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 120,
          "total_tokens": 216
        },
        "why_needed": "Regular paths are not skipped by default."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.000774774000007028,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "should be True"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 102,
          "total_tokens": 169
        },
        "why_needed": "Because the test case checks for a specific path (\\.git/hooks/pre-commit) and it's not present in the test directory."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007798439999930906,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'should be True', 'description': 'The function should return True for paths starting with a skip directory name'}",
          "{'message': '.venv', 'description': 'The function should return True for the path .venv'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 124,
          "total_tokens": 234
        },
        "why_needed": "To ensure that the function correctly skips paths starting with a skip directory name."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007730510000101276,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('src/__pycache__/module.cpython-312.pyc') is True"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 116,
          "total_tokens": 193
        },
        "why_needed": "Because the __pycache__ directory contains a cached Python file that should be skipped."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007393180000008215,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'type': 'assertion', 'name': 'should_skip_path', 'value': '/usr/lib/python3.12/site-packages/pkg/mod.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_site_packages",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 111,
          "total_tokens": 205
        },
        "why_needed": "The test is checking if site-packages directories are skipped by the fs coverage tool."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_site_packages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.000774844999995139,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'venv/lib/python3.12/site.py', 'expected_result': True}",
          "{'path': '.venv/lib/python3.12/site.py', 'expected_result': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 130,
          "total_tokens": 242
        },
        "why_needed": "The test case checks if venv directories are skipped by the `should_skip_path` function."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.001671997999977748,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should_skip_path', 'expected_result': True}",
          "{'name': 'assert_path', 'expected_result': False, 'message': \"Expected 'src/module.py' to be included in the path\"}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 132,
          "total_tokens": 244
        },
        "why_needed": "Custom exclude patterns are needed to skip certain files based on their content."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 50,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-227, 232-233, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0032961660000125903,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'Gemini requests-per-day limit reached' in res.error",
          "assert 'Daily limit exceeded' in res.error",
          "assert 'requests per day' in res.error",
          "assert 'limit hit' in res.error",
          "assert 'daily_limit_hit' in provider._rate_limiters['m1']",
          "mock_limiter.next_available_in.return_value == None",
          "provider._models == ['m1']"
        ],
        "scenario": "The test verifies that the GeminiProvider class's _annotate_internal method throws an error when it hits the daily limit for a model.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 367,
          "total_tokens": 531
        },
        "why_needed": "This test prevents regression where the provider does not raise an error when exceeding the daily limit for a model."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotate_loop_daily_limit_hit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 100,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-236, 239-244, 263-265, 268, 293, 295, 299-303, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0027824220000240985,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking _GeminiRateLimiter with `requests_per_minute=100`",
          "Setting `_call_gemini` to raise `_GeminiRateLimitExceeded` on the mock call",
          "_GeminiRateLimitExceeded should be raised when rate limit is exceeded",
          "The error message should contain 'requests-per-day' or 'rate limits reached'",
          "The model should be marked as exhausted after exceeding the rate limit",
          "No candidate models should remain in the pipeline after exhausting the model",
          "_GeminiRateLimitExceeded RPD should break the loop and mark model exhaustion"
        ],
        "scenario": "Test that _GeminiRateLimitExceeded RPD is raised when rate limit is exceeded",
        "token_usage": {
          "completion_tokens": 190,
          "prompt_tokens": 730,
          "total_tokens": 920
        },
        "why_needed": "To prevent the test from passing when rate limit is exceeded, we need to add a check for this scenario."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotation_exceptions_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 173,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181-182, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 254-255, 259, 340, 343, 346, 348-356, 358-361, 363-364, 366-367, 435, 437-439, 441-442, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-498, 502-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-564, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.2094524239999771,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "1. Mock imports to avoid errors",
          "_rate_limiters['m1'] == _GeminiRateLimiter(requests_per_minute=100)",
          "_annotate_internal('TestCaseResult(nodeid=",
          "src",
          "custom)",
          "Context too long: Context too long error",
          "_parse_rate_limits([",
          "requests_per_day",
          "value=100])",
          "_ensure_models_and_limits('token') == ['fallback']",
          "models, limits = provider._fetch_available_models('token')",
          "assert limits['gemini-custom'] == 12345"
        ],
        "scenario": "Prevents regression in coverage gaps by ensuring proper rate limiting and annotation logic.",
        "token_usage": {
          "completion_tokens": 185,
          "prompt_tokens": 821,
          "total_tokens": 1006
        },
        "why_needed": "This test prevents a bug where the GeminiProvider does not properly handle rate limiting and annotation logic, leading to coverage gaps."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_coverage_gaps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 13,
          "line_ranges": "134-135, 137-141, 143-144, 524-527"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008221630000093683,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_parse_preferred_models_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 10,
          "line_ranges": "39-42, 81-82, 84, 87-89"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007863760000077491,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Limiter._daily_requests should be an empty list after prune', 'expected_result': [], 'actual_result': {'scenario': 'TestGeminiProvider tests', 'why_needed': 'To ensure the Gemini provider is correctly pruning daily requests that are older than 24 hours.', 'key_assertions': [{'name': 'Limiter._daily_requests should be an empty list after prune', 'expected_result': [], 'actual_result': {}}, {'name': 'assert len(limiter._daily_requests) == 0', 'expected_result': 0, 'actual_result': 0}]}, 'message': 'AssertionError: assert len(limiter._daily_requests) == 0 failed for test tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_prune_daily_requests'}"
        ],
        "scenario": "TestGeminiProvider tests",
        "token_usage": {
          "completion_tokens": 219,
          "prompt_tokens": 157,
          "total_tokens": 376
        },
        "why_needed": "To ensure the Gemini provider is correctly pruning daily requests that are older than 24 hours."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_prune_daily_requests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 4,
          "line_ranges": "39-42"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007221949999802746,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `remaining` variable decreases by 50 each iteration of the loop.",
          "The loop finishes without returning if `remaining + request_tokens <= limit`.",
          "If `request_tokens` is massive (> 100), it should return 0.0 at line 106/108."
        ],
        "scenario": "Verify that the test_tpm_available_fallback function waits for 30 seconds after using all tokens before returning.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 524,
          "total_tokens": 660
        },
        "why_needed": "This test prevents a regression where the Gemini provider does not wait for 30 seconds after using all tokens, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_tpm_available_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 14,
          "line_ranges": "134-135, 137-141, 143-144, 164-165, 167-169"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008822359999953733,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation contains the string 'google-generativeai not installed'.",
          "The annotation includes the key 'error' with the value containing the string 'google-generativeai not installed'.",
          "The annotation includes the key 'message' with the value containing the string 'google-generativeai not installed'."
        ],
        "scenario": "Test that a 'google-generativeai' import error is reported when the module is not installed.",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 259,
          "total_tokens": 391
        },
        "why_needed": "This test prevents a potential bug where the GeminiProvider does not report an import error for modules without the required library."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_import_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002916623999993817,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The error message should contain 'GEMINI_API_TOKEN is not set'.",
          "The annotation should report that the token is missing.",
          "The annotation should include the key 'GEMINI_API_TOKEN' in its error message.",
          "The annotation should indicate that the token was not found or not set.",
          "The annotation should provide a clear and concise error message.",
          "The annotation should be able to identify the specific environment variable being tested (in this case, `GEMINI_API_TOKEN`).",
          "The annotation should report any other relevant information about the failure (e.g., the test result itself)."
        ],
        "scenario": "Test that annotation fails when token is missing from the environment.",
        "token_usage": {
          "completion_tokens": 186,
          "prompt_tokens": 313,
          "total_tokens": 499
        },
        "why_needed": "To prevent a test failure due to an unannotated `GEMINI_API_TOKEN` setting."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_no_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 214,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-237, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0054027609999991455,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by the `_annotate_internal` method matches the expected scenario.",
          "Mock `mock_post.call_count` is equal to 2 (first call with status code 429 and second call with status code 200).",
          "The `scenario` attribute of the annotation is set to 'Recovered Scenario'.",
          "No other critical checks are performed in this test. Only the scenario, retry status, and post call count assertions are made.",
          "Mock `mock_get.return_value.json.return_value` does not contain any unexpected data or errors.",
          "The `scenario` attribute of the annotation is set to 'Recovered Scenario' even if the provider returns an error.",
          "No other critical checks are performed in this test. Only the scenario, retry status, and post call count assertions are made.",
          "Mock `mock_get.return_value.status_code` is 429 (expected) or 200 (expected).",
          "The `scenario` attribute of the annotation is set to 'Recovered Scenario' even if the provider returns an error."
        ],
        "scenario": "Test that the GeminiProvider correctly annotates a rate limit retry scenario.",
        "token_usage": {
          "completion_tokens": 279,
          "prompt_tokens": 636,
          "total_tokens": 915
        },
        "why_needed": "This test prevents regression in case of rate limit retries, ensuring that the provider can handle such scenarios without crashing or returning unexpected results."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 208,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-568, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.40368964199998914,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' attribute of the annotation returned by _annotate_internal is set to 'Success Scenario'.",
          "The error attribute of the annotation returned by _annotate_internal is None.",
          "The 'error' attribute of the annotation returned by _parse_response is None.",
          "The 'scenario' attribute of the parsed response from _call_gemini matches the expected scenario.",
          "The 'tokens' attribute of the parsed response from _call_gemini is empty.",
          "The 'totalTokenCount' attribute of the parsed response from _call_gemini is 100.",
          "The 'candidates' attribute of the parsed response from _call_gemini contains a single successful candidate with no error message.",
          "The 'types' attribute of the parsed response from _call_gemini matches the expected types for successful scenarios."
        ],
        "scenario": "Test that _annotate_success returns the correct scenario when successful",
        "token_usage": {
          "completion_tokens": 224,
          "prompt_tokens": 649,
          "total_tokens": 873
        },
        "why_needed": "Prevents regression in GeminiProvider where it incorrectly assumes a specific response format from _call_gemini."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134-135, 137-141, 143-144, 332-333, 335"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002624806000000035,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider._check_availability()` method should return False when the GEMINI_API_TOKEN is set but the provider's configuration is 'gemini'.",
          "The `provider._check_availability()` method should return True when the GEMINI_API_TOKEN is not set and the provider's configuration is 'gemini'.",
          "The provider's `_check_availability()` method should raise an exception when the environment variable GEMINI_API_TOKEN is not set.",
          "When setting the GEMINI_API_TOKEN to a valid token, the provider's `_check_availability()` method should return True.",
          "When setting the GEMINI_API_TOKEN to an invalid token or empty string, the provider's `_check_availability()` method should raise an exception."
        ],
        "scenario": "Verifies that the GeminiProvider class correctly checks for availability by setting environment variables and asserting the result.",
        "token_usage": {
          "completion_tokens": 219,
          "prompt_tokens": 235,
          "total_tokens": 454
        },
        "why_needed": "This test prevents a potential bug where the provider does not detect when it has no available data."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_availability",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 111,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-237, 239-244, 263-265, 268, 272-276, 279-281, 283-286, 288-292, 318-320, 322-323, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 60.00324957700002,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the model exhaustion at node 't' for 'm1' is correctly annotated with a ResourceExhausted exception.",
          "Verify that the cooldowns for 'm1' are correctly set after a daily limit has been exceeded.",
          "Verify that the cooldowns for 'm1' can be set to a value greater than the current time after retrying after cleanup.",
          "Verify that the model exhaustion at node 't' for 'm1' is not retried after the cooldown period has expired.",
          "Verify that the provider correctly logs the error and sets the cooldowns for 'm1'.",
          "Verify that the provider correctly updates the _model_exhausted_at dictionary with the correct information.",
          "Verify that the provider correctly returns a valid result even when there are no more retries left.",
          "</key_assertions>"
        ],
        "scenario": "Test that the GeminiProvider class correctly annotates retry exceptions when using a rate limiter with a daily limit and then again after cleanup.",
        "token_usage": {
          "completion_tokens": 249,
          "prompt_tokens": 651,
          "total_tokens": 900
        },
        "why_needed": "This test prevents regression in cases where the provider is not able to handle retry exceptions properly due to rate limiting or other issues."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_exceptions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 97,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-94, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 254, 259, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003455956000010474,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of _model_exhausted_at for the given model is not set before calling _annotate_internal.",
          "_model_exhausted_at is cleared when models check pass and their IDs are in provider._models.",
          "The value of _model_exhausted_at is not set after calling _annotate_internal.",
          "The value of _model_exhausted_at for the given model is set to a time in the past (24h ago) after calling _annotate_internal.",
          "_model_exhausted_at is cleared when models check pass and their IDs are in provider._models.",
          "The value of _model_exhausted_at is not set after calling _annotate_internal.",
          "The value of _model_exhausted_at for the given model is set to a time in the past (24h ago) after calling _annotate_internal.",
          "</key_assertions>"
        ],
        "scenario": "Test that the GeminiProvider's _annotate_internal function clears _model_exhausted_at when models check pass.",
        "token_usage": {
          "completion_tokens": 256,
          "prompt_tokens": 482,
          "total_tokens": 738
        },
        "why_needed": "This test prevents a regression where the GeminiProvider's _annotate_internal function fails to clear _model_exhausted_at after successful model checks."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_loop_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-356, 358-361, 363-364, 366-367"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011261139999874104,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Equal', 'expected_value': 10, 'actual_value': 0}"
        ],
        "scenario": "Test GeminiProvider Detailed",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 156,
          "total_tokens": 225
        },
        "why_needed": "To test the error handling of rate limiting in GeminiProvider"
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_ensure_rate_limits_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010176299999784533,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'models are empty', 'description': 'The expected number of models is 0, but the actual result is an empty list.', 'expected_result': [], 'actual_result': []}",
          "{'name': 'limit_map is empty', 'description': 'The expected limit map is {}, but the actual result is an empty dictionary.', 'expected_result': {}, 'actual_result': {}}"
        ],
        "scenario": "Test fetch available models with network error",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 132,
          "total_tokens": 272
        },
        "why_needed": "To test the error handling of GeminiProvider when it encounters a network error."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 476-477, 537, 539-543, 547-548, 550-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014779849999797534,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'm1' model is not included in the list of available models.",
          "The 'm2' model is not included in the list of available models.",
          "The 'm3' model is included in the list of available models.",
          "The 'inputTokenLimit' value for the 'm3' model does not match its supported generation methods.",
          "The 'limit_map' dictionary does not contain the expected key-value pairs for the 'm3' model.",
          "The GeminiProvider incorrectly assumes valid JSON when fetching available models from an external API."
        ],
        "scenario": "Verify that the GeminiProvider fetches available models with invalid JSON data.",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 340,
          "total_tokens": 519
        },
        "why_needed": "This test prevents a potential bug where the GeminiProvider incorrectly assumes valid JSON when fetching available models from an external API."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 486, 488-491, 493"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020910450000144465,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked get_max_context_tokens call', 'expected_calls': [1], 'message': 'The `get_max_context_tokens` method should be called once.'}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 144,
          "total_tokens": 259
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method of the GeminiProvider class calls the `mock_ensure` function when necessary."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "134-135, 137-141, 143-144, 449-457, 459-460, 463-466"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007883689999914623,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.requests_per_minute is None', 'expected_value': 'None'}",
          "{'name': 'config.tokens_per_minute == 100', 'expected_value': 100}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 156,
          "total_tokens": 266
        },
        "why_needed": "To test the parsing of rate limits types and ensure they are correctly converted to Gemini provider configuration."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 11,
          "line_ranges": "39-42, 81-85, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007758259999945949,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of _request_times should be equal to 1 after calling _prune with the current timestamp.",
          "The length of _token_usage should be equal to 1 after calling _prune with the current timestamp.",
          "The value in _request_times[0] should be equal to the current timestamp minus 10 seconds."
        ],
        "scenario": "Test the _prune method of the _GeminiRateLimiter class to ensure it prunes requests within a certain time frame.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 323,
          "total_tokens": 473
        },
        "why_needed": "This test prevents a potential issue where requests are not pruned after a certain amount of time, potentially leading to excessive token usage or other problems."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_prune_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 6,
          "line_ranges": "39-42, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000785535000005666,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert len(limiter._token_usage) == 0', 'expected_result': 0, 'actual_result': 0}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_record_tokens_invalid",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 127,
          "total_tokens": 223
        },
        "why_needed": "The test is failing because the rate limiter does not handle invalid token counts correctly."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_record_tokens_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008237860000122055,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Limiter should record the request and return None after 100 attempts', 'description': 'The limiter should record the request and return None after 100 attempts, indicating that it has reached its limit.'}"
        ],
        "scenario": "Test the rate limiting feature",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 129,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the Gemini API does not exceed a certain number of requests per day without being throttled."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000831140000002506,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "limiter.next_available_in(100) == 0.0",
          "limiter.record_request()",
          "limiter.next_available_in(100) == 0.0",
          "limiter.record_request()"
        ],
        "scenario": "Verify that the rate limiter does not block requests for a short period after the first two.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 280,
          "total_tokens": 391
        },
        "why_needed": "This test prevents a potential issue where the rate limiter blocks subsequent requests immediately after the initial two requests."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 100-101, 103-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008320720000085657,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The time until TPM availability is greater than 0 seconds when no tokens are requested or more than limit are used.",
          "The time until TPM availability does not exceed 60.0 seconds when more than limit are used.",
          "Tokens do not expire before reaching the limit of 100.",
          "Tokens do not expire after exceeding the limit by more than 10%.",
          "Tokens do not expire immediately when usage is within the limit but exceeds it.",
          "The rate limiter waits for tokens to expire before returning 'TPM available' when no tokens are requested or more than limit are used."
        ],
        "scenario": "Verify that the rate limiter waits for tokens to expire before returning a 'TPM available' status when no tokens are requested or more than limit are used.",
        "token_usage": {
          "completion_tokens": 210,
          "prompt_tokens": 377,
          "total_tokens": 587
        },
        "why_needed": "This test prevents a potential regression where the rate limiter incorrectly returns 'TPM available' when no tokens are requested, but usage exceeds the limit."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_seconds_until_tpm_available_branches",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008499449999987974,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `wait_for_slot` method raises a `GeminiRateLimitExceeded` exception with the correct `limit_type` (in this case, 'requests_per_day')",
          "The error message indicates that the limit was exceeded",
          "The test verifies that the exception is raised only when the daily limit is exceeded"
        ],
        "scenario": "Verify that the `wait_for_slot` method raises a `GeminiRateLimitExceeded` exception when the daily limit is exceeded.",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 263,
          "total_tokens": 414
        },
        "why_needed": "This test prevents a potential regression where the rate limiter does not raise an exception when the daily limit is exceeded, potentially causing unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_daily_limit_exceeded",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001562532000008332,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `next_available_in` method returns a value within the expected range (10.0s) when called with an argument equal to 1.",
          "The `wait_for_slot` method sleeps for at least 10.0 seconds before returning.",
          "The `mock_sleep.assert_called_once_with(10.0)` assertion ensures that the sleep function was called once with a value of 10.0."
        ],
        "scenario": "Test that `wait_for_slot` sleeps for the expected amount of time when waiting for a slot to become available.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 325,
          "total_tokens": 482
        },
        "why_needed": "This test prevents regression in case the rate limiter is not properly implemented or if there are other factors affecting the sleep duration."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_sleeps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0008126050000214491,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'different hashes', 'actual': 'same hashes'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 119,
          "total_tokens": 184
        },
        "why_needed": "Different configs should produce different hashes."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007733319999942978,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert len(h) == 16', 'expected_result': 16, 'message': 'Computed hash length should be 16 characters.'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 109,
          "total_tokens": 208
        },
        "why_needed": "To ensure the computed hash is short and can be stored in a database or used as a unique identifier."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 6,
          "line_ranges": "32, 44-48"
        }
      ],
      "duration": 0.0008792600000049333,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': \"b'test content'\", 'actual_value': \"compute_file_sha256(path).decode('utf-8') == compute_sha256(content)\"}"
        ],
        "scenario": "File hash consistency with bytes",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 144,
          "total_tokens": 225
        },
        "why_needed": "Test that the file hash matches the content hash for consistent results."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "44-48"
        }
      ],
      "duration": 0.0009465669999997317,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The hash should be a 64-byte string.', 'expected_value': 0}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 124,
          "total_tokens": 198
        },
        "why_needed": "To test the correctness of file hashing."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.000760778000000073,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Different keys should produce different signatures.', 'expected_result': 'different signature'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_different_key",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 125,
          "total_tokens": 197
        },
        "why_needed": "To ensure that different keys produce different signatures."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_different_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007919659999799933,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_length': 64, 'actual_length': 0}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_with_key",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 108,
          "total_tokens": 181
        },
        "why_needed": "To verify that the HMAC computation is correct and produces the expected signature."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_with_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007912159999818869,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Same content should produce same hash.', 'expected_result': 'The hash of two identical strings should be equal.'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_consistent",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 115,
          "total_tokens": 203
        },
        "why_needed": "To ensure that the hash function is consistent and produces the same output for the same input."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_consistent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007369330000130958,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 64, 'actual': 8, 'description': 'The actual length of the hash should be 8 bytes (64 hex chars)'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_length",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 103,
          "total_tokens": 192
        },
        "why_needed": "To ensure the hash length is correct and consistent across different inputs."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.08123340299999882,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'value': 'pytest'}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 102,
          "total_tokens": 176
        },
        "why_needed": "To ensure the 'pytest' package is included in the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.08342007699999954,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'snapshot is a dict', 'expected': 'True'}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 98,
          "total_tokens": 171
        },
        "why_needed": "To ensure the function correctly returns a dictionary representation of the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "73, 76-77, 80-81"
        }
      ],
      "duration": 0.000905008000017915,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'hmac_key_file': \"tmp_path / 'hmac.key'\"}, 'actual': {'hmac_key_file': 'str(key_file)'}}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 145,
          "total_tokens": 238
        },
        "why_needed": "To test the functionality of loading a HMAC key from a file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 4,
          "line_ranges": "73, 76-78"
        }
      ],
      "duration": 0.0008511480000095162,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_type': 'NoneType', 'message': 'Expected the HMAC key to be None, but got <value>'}"
        ],
        "scenario": "Test Load Hmac Key: Missing Key File",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 126,
          "total_tokens": 202
        },
        "why_needed": "The test should fail when a key file does not exist."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 2,
          "line_ranges": "73-74"
        }
      ],
      "duration": 0.0007673000000067987,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert key is None', 'description': 'The load_hmac_key function should return None if no key file is specified.'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 110,
          "total_tokens": 200
        },
        "why_needed": "To test that the function correctly returns None when no key file is configured."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000761017999991509,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.aggregate_dir is None (default aggregation directory should be empty)",
          "config.aggregate_policy == 'latest' (default aggregation policy is 'latest')",
          "config.aggregate_include_history is False (default aggregation include history is False)"
        ],
        "scenario": "Test the default aggregation configuration.",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 201,
          "total_tokens": 291
        },
        "why_needed": "Prevents a regression where aggregation defaults are not set correctly."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007777099999941584,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config', 'type': 'assertion', 'value': 'get_default_config() is not None'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 107,
          "total_tokens": 187
        },
        "why_needed": "The test captures failed output by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007787720000180798,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.llm_context_mode', 'value': 'minimal'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 107,
          "total_tokens": 184
        },
        "why_needed": "To ensure that the context mode is set to 'minimal' by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 4,
          "line_ranges": "123, 171, 284, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008093879999933051,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_llm_enabled', 'expected_result': False, 'actual_result': 'get_default_config().is_llm_enabled() == False'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 109,
          "total_tokens": 198
        },
        "why_needed": "The LLM is not enabled by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007868560000190428,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.omit_tests_from_coverage', 'expected_value': True}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 109,
          "total_tokens": 189
        },
        "why_needed": "The test is necessary because it checks the default behavior of omitting tests from coverage."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008074250000049688,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Provider should be None', 'expected_value': 'none'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 101,
          "total_tokens": 172
        },
        "why_needed": "To ensure the provider is set to None by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007611590000067281,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Excluding secret files and environment variables', 'description': \"The function should exclude 'secret' and '.env' files from the list of excluded globs.\", 'expected_result': ['exclude_globs']}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 132,
          "total_tokens": 240
        },
        "why_needed": "This test is needed because the default configuration includes secret files and environment variables by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.011082498999996915,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "nodeids should be sorted alphabetically and numerically",
          "nodeids should not contain duplicates",
          "nodeids should only include unique values from the input data",
          "nodeid 'z_test.py::test_z' should always come first in the list",
          "nodeid 'a_test.py::test_a' should come second in the list",
          "nodeid 'm_test.py::test_m' should come third in the list"
        ],
        "scenario": "The test verifies that the output of a deterministic pipeline is sorted by nodeid.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 313,
          "total_tokens": 462
        },
        "why_needed": "This test prevents regression where the order of nodeids in the report changes unexpectedly."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.010400468000000274,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests in the report should be zero.",
          "The 'summary' section of the report should have a 'total' key with a value of zero.",
          "There should be no failed or skipped tests in the report.",
          "All test names and descriptions should be present in the report.",
          "The report format should be valid JSON as expected by the framework.",
          "The test suite should not contain any invalid or missing data."
        ],
        "scenario": "Test that an empty test suite produces a valid report.",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 240,
          "total_tokens": 386
        },
        "why_needed": "This test prevents a regression where the test suite is empty and still produces a valid report."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04431294299999422,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path to the generated HTML report exists and can be accessed.",
          "The content of the HTML report contains the string '<html' as expected.",
          "The string 'test_pass' is present in the content of the HTML report as expected."
        ],
        "scenario": "The test verifies that the full pipeline generates an HTML report.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 270,
          "total_tokens": 377
        },
        "why_needed": "This test prevents regression where the HTML report is not generated correctly due to a change in configuration settings."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/_git_info.py",
          "line_count": 2,
          "line_ranges": "2-3"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 138,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06546490199997379,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "data['schema_version'] == SCHEMA_VERSION",
          "data['summary']['total'] == 3",
          "data['summary']['passed'] == 1",
          "data['summary']['failed'] == 1",
          "data['summary']['skipped'] == 1"
        ],
        "scenario": "The test verifies that a full pipeline generates a valid JSON report with the correct schema version, summary statistics, and number of tests.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 419,
          "total_tokens": 553
        },
        "why_needed": "This test prevents regression where the JSON report generation is not producing a valid output or contains incorrect information."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008000409999908698,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' field should be present in the data.",
          "The 'run_meta' field should be present in the data.",
          "The 'summary' field should be present in the data.",
          "The 'tests' field should be present in the data."
        ],
        "scenario": "Test that the ReportRoot class has required fields when created.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 250,
          "total_tokens": 358
        },
        "why_needed": "This test prevents a potential bug where the report root is missing required fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007721889999743325,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 306,
          "prompt_tokens": 136,
          "total_tokens": 442
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009404049999943709,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be present in the data.",
          "The 'interrupted' field should be present in the data.",
          "The 'collect_only' field should be present in the data.",
          "The 'collected_count' field should be present in the data.",
          "The 'selected_count' field should be present in the data."
        ],
        "scenario": "Test \"RunMeta has run status fields\" verifies that the `to_dict()` method returns a dictionary with required status fields.",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 237,
          "total_tokens": 388
        },
        "why_needed": "This test prevents regression where `RunMeta` does not have a `to_dict()` method or its output is missing required status fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007960850000188202,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'SCHEMA_VERSION', 'type': 'string'}",
          "{'name': '.', 'type': 'boolean'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 103,
          "total_tokens": 196
        },
        "why_needed": "The schema version is defined, which is necessary for compatibility with older versions of the gate."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007658369999887782,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' key should be present in the data dictionary.",
          "The 'outcome' key should be present in the data dictionary.",
          "The 'duration' key should be present in the data dictionary."
        ],
        "scenario": "The `TestSchemaCompatibility` class verifies that the `TestCaseResult` object has all required fields.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 223,
          "total_tokens": 342
        },
        "why_needed": "This test prevents a potential bug where the `TestCaseResult` object is missing one of its required fields, which could lead to incorrect analysis or reporting."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.0015134779999926,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `result` variable is not None.",
          "The `result.error` attribute is not None.",
          "The `provider.annotate()` method raises an exception with the message 'API error'.",
          "The `mock_completion` function is called with a side effect that raises an exception.",
          "The `test_source` is set to a function that does not raise any exceptions.",
          "The `context_files` dictionary is empty."
        ],
        "scenario": "Test that all retries are exhausted when API call fails.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 346,
          "total_tokens": 493
        },
        "why_needed": "Prevents regression where LiteLLMProvider returns an annotation with no error even when all retries are exhausted."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_all_retries_exhausted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 38,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001219549000012421,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the annotation returns an annotation with an error status code when a non-401 error is encountered.",
          "The test verifies that the annotation does not force token refresh when a non-401 error is encountered.",
          "The test verifies that the annotation correctly handles an internal server error (500) instead of 401.",
          "The test verifies that the annotation raises an exception with the 'Internal server error' message when a non-401 error is encountered.",
          "The test verifies that the annotation does not raise an exception when a non-401 error is encountered, allowing for token refresh to continue.",
          "The test verifies that the annotation correctly returns None in case of no error.",
          "The test verifies that the annotation correctly sets the 'error' attribute on the result object to None."
        ],
        "scenario": "Test that non-401 errors don't force token refresh.",
        "token_usage": {
          "completion_tokens": 215,
          "prompt_tokens": 367,
          "total_tokens": 582
        },
        "why_needed": "Prevents regression in case of non-401 error without forcing token refresh."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_non_401_error_no_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 47,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.002962307000018,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `result.error` attribute is `None` when the test should fail due to a transient error.",
          "The `test.scenario` attribute is set to `'test scenario'` even though an error occurred.",
          "The `test.context_files` dictionary remains empty after the API call fails twice, then succeeds.",
          "The `mock_completion` function raises an exception when the API call fails for the second time.",
          "The `result` object contains a meaningful message indicating that the test failed due to a transient error."
        ],
        "scenario": "Test that retry succeeds after transient error and ensures the test fails with a meaningful error message.",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 433,
          "total_tokens": 603
        },
        "why_needed": "To prevent the test from succeeding immediately after a transient error, allowing for retries to be attempted."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_retry_succeeds_after_transient_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 54,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-188, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.371371831999994,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the LLM provider calls the `token_refresh_command` with a new token when an API call fails with a 401 status code.",
          "The test checks that the `litellm_token_output_format` is set to 'text' for the output of the `token_refresh_command`.",
          "The test verifies that the `mock_completion` function is called twice, once with a 401 error and once without it.",
          "The test ensures that the `result` variable is not `None` after the token refresh attempt.",
          "The test checks that the `call_count[0] >= 2` condition is met to verify that the LLM provider has retried after token refresh.",
          "The test verifies that the `test_source` and `context_files` are empty for the annotated result.",
          "The test ensures that the `error` variable is raised with a 401 status code when the API call fails."
        ],
        "scenario": "Test that 401 error triggers token refresh (test_token_refresh_on_401)",
        "token_usage": {
          "completion_tokens": 252,
          "prompt_tokens": 473,
          "total_tokens": 725
        },
        "why_needed": "To ensure the LLM token refresh mechanism works correctly for 401 errors."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_token_refresh_on_401",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008349579999844536,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected_value': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 131,
          "total_tokens": 214
        },
        "why_needed": "To ensure that the GeminiProvider is correctly instantiated when using the 'gemini' provider."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007738020000260803,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 140,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the LiteLLMProvider class is correctly instantiated when a specific provider ('litellm') is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007806760000050872,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is None', 'expected': 'NoopProvider', 'actual': 'None'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 115,
          "total_tokens": 205
        },
        "why_needed": "To ensure that the GetProvider function returns a NoopProvider when the provider is set to 'none'."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008582199999978002,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected_value': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 154,
          "total_tokens": 239
        },
        "why_needed": "To ensure that the OllamaProvider class is correctly instantiated when a specific provider ('ollama') is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007848030000161543,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 125,
          "total_tokens": 195
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_unknown_raises",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007655070000112119,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'annotate' method should be present in the provider.",
          "The 'is_available' method should be present in the provider.",
          "The 'get_model_name' method should be present in the provider.",
          "The 'config' attribute should be present in the provider."
        ],
        "scenario": "Tests the implementation of NoopProvider.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 232,
          "total_tokens": 337
        },
        "why_needed": "Prevents a regression where NoopProvider does not implement required interface methods."
      },
      "nodeid": "tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008673080000107802,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation is always an instance of LlmAnnotation",
          "annotation scenario is an empty string",
          "annotation why_needed is an empty string",
          "annotation key_assertions are an empty list"
        ],
        "scenario": "The test verifies that the `annotate` method of a NoopProvider returns an empty annotation when no annotation is provided.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 249,
          "total_tokens": 359
        },
        "why_needed": "This test prevents regression in case the `annotate` method does not return an annotation if none is provided."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 67"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007723800000007941,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected value', 'value': '', 'expected_type': ''}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 114,
          "total_tokens": 205
        },
        "why_needed": "The test is failing because the `get_model_name` method of the `NoopProvider` class does not handle an empty input correctly."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 59"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008445250000193028,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_available() should return True', 'expected_value': True, 'actual_value': 'assert provider.is_available() is True'}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_is_available",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 108,
          "total_tokens": 200
        },
        "why_needed": "The LLM is not available because the NoopProvider is not properly initialized."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_is_available",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007303209999918181,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"The field 'scenario' is required.\", 'description': \"The field 'scenario' must be present in the annotation.\"}",
          "{'message': \"The field 'why_needed' is required.\", 'description': \"The field 'why_needed' must be present in the annotation.\"}"
        ],
        "scenario": "Test that the schema requires 'scenario' and 'why_needed'.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 115,
          "total_tokens": 239
        },
        "why_needed": "The schema requires these fields because they are specified in the ANNOTATION_JSON_SCHEMA."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007769179999854714,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' field is set to the expected value.",
          "The 'why_needed' field is set to the expected value.",
          "The 'key_assertions' list contains the correct number of elements.",
          "The confidence level is set to the expected value (0.95).",
          "</key_assertions>"
        ],
        "scenario": "Test that AnnotationSchema.from_dict parses a dictionary correctly.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 274,
          "total_tokens": 387
        },
        "why_needed": "Prevents data corruption or incorrect parsing of user input."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007667799999921954,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'schema.scenario', 'value': '', 'expected_type': 'str'}",
          "{'name': 'schema.why_needed', 'value': '', 'expected_type': ''}"
        ],
        "scenario": "Tested that the AnnotationSchema class correctly returns an empty string when given an empty dictionary.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 109,
          "total_tokens": 220
        },
        "why_needed": "The test is necessary because the AnnotationSchema class should handle empty input and return a valid object."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007721090000245567,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"schema.scenario should be 'Partial only'\", 'expected_value': 'Partial only'}",
          "{'assertion': 'schema.why_needed should be an empty string', 'expected_value': ''}"
        ],
        "scenario": "Test case for testing AnnotationSchema",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 119,
          "total_tokens": 218
        },
        "why_needed": "This test is needed to ensure that the AnnotationSchema handles partial input correctly."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007560990000001766,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `schema_has_required_fields` function should be called with a JSON schema that has 'scenario', 'why_needed', and 'key_assertions' properties.",
          "The `schema_has_required_fields` function should validate that the required fields are present in the JSON schema.",
          "The `schema_has_required_fields` function should check for the presence of 'scenario', 'why_needed', and 'key_assertions' keys in the JSON schema.",
          "The `schema_has_required_fields` function should perform checks to ensure these required fields are included in the JSON schema.",
          "The `schema_has_required_fields` function should validate that the required fields meet their respective validation rules.",
          "The `schema_has_required_fields` function should report an error if any of the required fields are missing or invalid.",
          "The `schema_has_required_fields` function should provide detailed information about why a field is required (if applicable)."
        ],
        "scenario": "The test verifies that the `schema_has_required_fields` function is called with a JSON schema that contains required fields.",
        "token_usage": {
          "completion_tokens": 259,
          "prompt_tokens": 215,
          "total_tokens": 474
        },
        "why_needed": "This test prevents a potential bug where the `schema_has_required_fields` function is not called with a valid JSON schema."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "90-92, 94-96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007628809999857822,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assertion 1: The 'scenario' key in the serialized data matches the expected value.",
          "assertion 2: The 'why_needed' key in the serialized data matches the expected value.",
          "assertion 3: The 'key_assertions' list in the serialized data contains all expected assertions."
        ],
        "scenario": "This test verifies that the `AnnotationSchema` class correctly serializes to a dictionary.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 247,
          "total_tokens": 374
        },
        "why_needed": "This test prevents regression by ensuring that the `AnnotationSchema` class handles scenario and why needed information properly."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008446349999928771,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected_value': 'noop', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 118,
          "total_tokens": 203
        },
        "why_needed": "The test is necessary because the NoopProvider class does not implement the required interface."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007818869999880462,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'isinstance(provider, LlmProvider)', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 117,
          "total_tokens": 201
        },
        "why_needed": "To ensure that the NoopProvider class correctly implements the LlmProvider interface."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007871970000223882,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "- result.scenario is an empty string",
          "- result.why_needed is an empty string",
          "-1"
        ],
        "scenario": "NoopProvider returns empty annotation when no function is annotated with 'noop'",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 253,
          "total_tokens": 329
        },
        "why_needed": "To prevent a test from failing due to an empty annotation in the NoopProvider."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000816131999982872,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result has an attribute named 'scenario' that contains a string describing the scenario.",
          "The result has an attribute named 'why_needed' that contains a string explaining why this test is necessary.",
          "The result has an attribute named 'key_assertions' that stores the critical checks performed during the test."
        ],
        "scenario": "The test verifies that the annotate method returns a TestCaseResult object with the correct attributes.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 263,
          "total_tokens": 397
        },
        "why_needed": "This test prevents regression in the LlmAnnotation-like object annotation process, ensuring it correctly handles scenarios where annotations are not present or have incorrect attributes."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008290960000181258,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is not None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 145,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the provider handles empty code gracefully and returns a valid result."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000837351000001263,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is_not_none', 'expected_value': 'None'}"
        ],
        "scenario": "Provider handles None context gracefully",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 148,
          "total_tokens": 215
        },
        "why_needed": "The contract should be able to handle a None context without raising an error."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 15,
          "line_ranges": "65-66, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008101909999993495,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_type': 'function', 'actual_type': 'callable', 'message': 'Expected annotate to be a callable. Got {0}.'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 145,
          "total_tokens": 235
        },
        "why_needed": "To ensure that all providers have an `annotate` method."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 187,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 263-265, 299, 311-312, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524-525, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009270600000093054,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Test that annotate_handles_context does not raise an exception for small inputs', 'expected_result': 'No exception is raised'}",
          "{'name': 'Test that annotate_handles_context returns the expected result for a small input', 'expected_result': {'scenario': '...', 'why_needed': '...', 'key_assertions': ['...']}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 98,
          "total_tokens": 259
        },
        "why_needed": "This test is necessary because the `annotate_handles_context` method of the `GeminiProvider` class has a time complexity of O(n), where n is the number of handles. This can cause performance issues when dealing with large numbers of handles."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-195, 471-473, 497-498, 502-503, 537"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008794409999950403,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation message includes the correct dependency name and installation instructions.",
          "The annotation message is not empty or null.",
          "The annotation message contains the expected error message.",
          "The annotation message does not contain any other relevant information.",
          "The annotation message is not too long (less than 50 characters).",
          "The annotation message includes the correct dependency name and installation instructions.",
          "The annotation message is a string, not a list or tuple.",
          "The annotation message contains only one line of text."
        ],
        "scenario": "Test that the LiteLLM provider correctly reports a missing dependency when an import error occurs.",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 270,
          "total_tokens": 445
        },
        "why_needed": "This test prevents a potential bug where the provider does not report a missing dependency when an import error occurs, potentially masking a dependency issue."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008245770000030461,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'error' attribute of the annotation object should be set to 'GEMINI_API_TOKEN is not set'.",
          "The 'error' attribute of the annotation object should be set to 'GEMINI_API_TOKEN is not set'.",
          "The 'error' attribute of the annotation object should be set to 'GEMINI_API_TOKEN is not set'."
        ],
        "scenario": "Test that the 'annotate' method of the GeminiProvider raises an error when the API token is missing.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 440,
          "total_tokens": 597
        },
        "why_needed": "This test prevents a potential bug where the 'annotate' method fails to raise an error when the API token is missing, potentially leading to unexpected behavior or errors in downstream code."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 220,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-455, 457, 459-460, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009611140000060914,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotate_records_tokens` test verifies that the provider records token usage correctly.",
          "The provider's `_rate_limiters` dictionary contains an entry for 'gemini-1.5-pro' with a single entry under '_token_usage'.",
          "The value of '_token_usage' is 123, indicating that at least one token was recorded.",
          "The test also verifies that the rate limits logic runs without error."
        ],
        "scenario": "Verify that the `annotate_records_tokens` test prevents regressions by ensuring tokens are recorded correctly.",
        "token_usage": {
          "completion_tokens": 155,
          "prompt_tokens": 783,
          "total_tokens": 938
        },
        "why_needed": "To prevent regressions caused by a bug in the `GeminiProvider` class where token usage is not properly recorded."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001069387000001143,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Llama model is retried', 'description': 'The Llama model should be retried after a certain number of attempts.'}",
          "{'name': 'Error message is displayed', 'description': 'An error message should be displayed when the rate limit is exceeded.'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 98,
          "total_tokens": 214
        },
        "why_needed": "To ensure that the LLM provider can retry annotating tasks when rate limiting occurs."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 210,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526-527, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001119711000001189,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'model_rotation', 'description': 'The model rotation should be applied every day'}",
          "{'name': 'annotation_process', 'description': 'The annotation process should not interfere with the model rotation'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 100,
          "total_tokens": 225
        },
        "why_needed": "To ensure that the LLM model is rotated on a daily basis and that the annotation process does not interfere with this rotation."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009933339999861346,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Llama model is not handling large amount of data in a single session efficiently"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 98,
          "total_tokens": 172
        },
        "why_needed": "skips on daily limit because it's a performance bottleneck and LLMs are not designed to handle large amounts of data in a single session."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 209,
          "line_ranges": "39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00104849799998874,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect"
        ],
        "scenario": "Test that LiteLLM provider annotates a valid response with the correct information.",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 474,
          "total_tokens": 538
        },
        "why_needed": "Prevents regression by ensuring the provider correctly annotates responses from LiteLLM."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 222,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001142934999990075,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The recovered model's performance is within a reasonable range.",
          "No additional warnings or errors are raised when using the recovered model."
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 104,
          "total_tokens": 178
        },
        "why_needed": "To ensure that the LLM provider can recover from an exhausted model after 24 hours."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 68,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-349, 352-356, 358-361, 363-364, 366-367, 435, 437-439, 441-444, 449-452, 463-466, 476, 478, 497-498, 502-508, 511, 514-516, 518-521, 524-525, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008435230000145566,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected error message', 'value': 'No available models found.'}",
          "{'name': 'Expected error type', 'value': 'LlamaProviderError'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 92,
          "total_tokens": 180
        },
        "why_needed": "To handle the case where there are no available models."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 201,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001133366999994223,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Model list should be refreshed after a certain interval', 'expected_value': 'The model list should be updated with new data after a specific time period.'}"
        ],
        "scenario": "The `model_list_refresh` method of the `GeminiProvider` class is called after an interval.",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 96,
          "total_tokens": 195
        },
        "why_needed": "To ensure that the model list is updated correctly and consistently across different runs."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 50,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009770839999987402,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the LLM provider retries after a 401 Unauthorized error and captures the refreshed token.",
          "Verify that the LLM provider fails with a 401 Unauthorized error on the first attempt to refresh its token.",
          "Verify that the LLM provider succeeds with a new token after refreshing its token.",
          "Verify that the captured keys match the expected tokens for both attempts.",
          "Verify that the retry count is incremented correctly for each attempt.",
          "Verify that the error message is correct and includes the expected reason (401 Unauthorized).",
          "Verify that the LLM provider uses the new token in subsequent requests without any issues."
        ],
        "scenario": "Test the retry mechanism for LiteLLM provider on a 401 error after token refresh.",
        "token_usage": {
          "completion_tokens": 197,
          "prompt_tokens": 580,
          "total_tokens": 777
        },
        "why_needed": "The test prevents a regression where the LLM provider does not retry when it encounters a 401 Unauthorized error after refreshing its token."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_401_retry_with_token_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008184959999937291,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'boom' in annotation.error",
          "assert annotation.error is not None"
        ],
        "scenario": "Test that the LiteLLMProvider annotates completion errors correctly.",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 307,
          "total_tokens": 371
        },
        "why_needed": "This test prevents regression when a completion error occurs during annotation."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 43,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 206, 211"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008474509999985003,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'response_data' dictionary must contain a list of key_assertion payloads.",
          "The 'response_data' dictionary must not be empty.",
          "The 'response_data' dictionary should have at least one key_assertion payload.",
          "The 'response_data' dictionary should not be None.",
          "The 'response_data' dictionary should have a valid JSON string."
        ],
        "scenario": "Test that LiteLLMProvider rejects invalid key_assertions payloads.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 346,
          "total_tokens": 474
        },
        "why_needed": "To prevent regression and ensure the correct behavior of the LiteLLMProvider."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 8,
          "line_ranges": "37-38, 41, 82-86"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008050609999941116,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation error message is set to 'litellm not installed. Install with: pip install litellm' as expected.",
          "The provider correctly reports an import error for the missing dependency.",
          "The test passes even when the mock_import_error function returns a different error message."
        ],
        "scenario": "Test that a LiteLLMProvider annotates missing dependencies correctly.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 271,
          "total_tokens": 386
        },
        "why_needed": "This test prevents a bug where the provider does not report missing dependencies and instead silently installs them."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009172009999929287,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect",
          "model gpt-4o",
          "role system",
          "tests/test_auth.py::test_login"
        ],
        "scenario": "Test that LiteLLMProvider annotates a successful response with the correct information.",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 475,
          "total_tokens": 560
        },
        "why_needed": "Prevents regressions by ensuring the provider correctly handles successful responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008706740000263835,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `provider._annotate_internal` is called with the correct argument `prompt_override = 'CUSTOM PROMPT'`.",
          "The captured messages from the fake completion are added to the list of expected messages. The message should contain a key named 'content'.",
          "The value of the 'content' key in the captured message matches the expected value 'CUSTOM PROMPT'."
        ],
        "scenario": "Test that LiteLLMProvider overrides the prompt when provided.",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 373,
          "total_tokens": 505
        },
        "why_needed": "This test prevents a bug where the provider does not override the prompt for custom prompts."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009872329999893736,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `prompt_tokens` attribute of the `token_usage` object is set to the correct value (100).",
          "The `completion_tokens` attribute of the `token_usage` object is set to the correct value (50).",
          "The `total_tokens` attribute of the `token_usage` object is set to the correct value (150)."
        ],
        "scenario": "Test LiteLLM provider extracts token usage from response.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 426,
          "total_tokens": 555
        },
        "why_needed": "The test prevents a potential bug where the provider does not accurately calculate token usage in certain scenarios."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182-183, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008647740000071735,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `api_base` attribute of the LiteLLM provider should be set to `https://proxy.corp.com/v1` before calling the completion function.",
          "The `litellm_api_base` configuration parameter should be set to `https://proxy.corp.com/v1` for the test case to pass.",
          "The `api_base` attribute of the LiteLLM provider should not be modified after setting it in the test configuration.",
          "The completion function should return a response with the correct `api_base` value if it is set correctly.",
          "If the `api_base` attribute is not set, the completion function should raise an exception or return an error message indicating that the API base was not provided.",
          "The `litellm_api_base` configuration parameter should be set to a valid URL for the test case to pass.",
          "If the `litellm_api_base` configuration parameter is not set correctly, the test case should fail with an error message indicating that the API base was not provided."
        ],
        "scenario": "Test: tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough verifies that the LiteLLM provider passes `api_base` to the completion call.",
        "token_usage": {
          "completion_tokens": 301,
          "prompt_tokens": 387,
          "total_tokens": 688
        },
        "why_needed": "This test prevents regression where the API base is not passed correctly to the completion call, potentially causing issues with downstream integrations or data processing."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008827270000040244,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "- The API key is captured and stored in the `captured` dictionary.",
          "- The API key matches the expected value 'static-key-placeholder' as per the `response_data`.",
          "- The `litellm_api_key` attribute of the `liteellm` object is set to 'static-key-placeholder'."
        ],
        "scenario": "The `liteellm` provider passes the static API key to the completion call.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 384,
          "total_tokens": 508
        },
        "why_needed": "This test prevents a regression where the API key is not passed through to the completion function."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 36,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 132-133, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008354469999858338,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_error_without_refresher",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 51,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 129-130, 132-133, 141-142, 170-174, 176-178, 182, 186-188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 31,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.001544155000005,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test checks that the annotation of the provider contains an 'error' key indicating an authentication failure.",
          "The test verifies that the 'error' value includes the string 'Authentication failed',",
          "The test ensures the 'error' value does not contain any other relevant information, such as the token or refresh command used to authenticate.",
          "The test checks that the provider's annotation is set correctly even after a second authentication attempt fails.",
          "The test verifies that the provider returns an error message indicating that authentication failed when it encounters this scenario.",
          "The test ensures that the provider does not retry authentication if it fails on the first attempt, as intended by the bug being tested."
        ],
        "scenario": "The test verifies that the LiteLLMProvider does not retry authentication if it fails on the second attempt.",
        "token_usage": {
          "completion_tokens": 221,
          "prompt_tokens": 419,
          "total_tokens": 640
        },
        "why_needed": "This test prevents a bug where the provider retries authentication even after failing with an auth error, which could lead to unexpected behavior or errors in certain scenarios."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_retry_fails_on_second_attempt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000801424000002271,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The _parse_response method of the provider should return None for an invalid response.",
          "The _parse_response method of the provider should raise a ValueError for an invalid response.",
          "The error attribute of the annotation returned by _parse_response should be set to 'Context too long for this model'.",
          "The key_assertions in the annotation returned by _parse_response should include 'scenario', 'why_needed', and 'error'.",
          "The value of the 'error' key in the annotation returned by _parse_response should match the expected error message.",
          "The value of the 'key_assertions' list in the annotation returned by _parse_response should be a list containing 'scenario', 'why_needed', and 'error'."
        ],
        "scenario": "The test verifies that the LiteLLMProvider class correctly handles a context too long error when parsing a response.",
        "token_usage": {
          "completion_tokens": 218,
          "prompt_tokens": 370,
          "total_tokens": 588
        },
        "why_needed": "This test prevents a regression where the provider fails to handle invalid responses with an error message."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_context_too_long_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227-228, 230-231"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007621699999731391,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 16384, 'actual_value': 16384}"
        ],
        "scenario": "test_get_max_context_tokens_dict_format",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 218,
          "total_tokens": 285
        },
        "why_needed": "To ensure the correct dictionary format is returned when getting max context tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_dict_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227, 232-234"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007696549999991475,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 16, 'actual_value': 0}",
          "{'name': 'fallback_on_error', 'expected_value': 'Fallback token'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 101,
          "total_tokens": 202
        },
        "why_needed": "To ensure the LLMProvider can handle errors and return a fallback value for max context tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 9,
          "line_ranges": "37-38, 41, 221-222, 224, 227-229"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000811322000004111,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected result', 'type': 'int', 'value': 8192}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 213,
          "total_tokens": 306
        },
        "why_needed": "To test the LiteLLM provider's ability to get the maximum context tokens from the litellm module."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 6,
          "line_ranges": "37-38, 41, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007913049999785926,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'setitem', 'expected_value': {'litellm': 'fake_litellm'}}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 160,
          "total_tokens": 243
        },
        "why_needed": "To ensure the LiteLLM provider can detect installed modules."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 41,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009123229999943305,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The API key should be 'dynamic-token-789' after token refresh.",
          "The provider should correctly set the API key in the configuration.",
          "The token refresh command and interval should be correctly configured.",
          "The LitellmProvider should properly handle token refresh requests.",
          "The case result should indicate that the test passed successfully.",
          "The captured data from the subprocess call should match the expected response."
        ],
        "scenario": "Test the LiteLLMProvider's token refresh integration.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 442,
          "total_tokens": 587
        },
        "why_needed": "The test prevents a potential bug where the TokenRefresher is not properly refreshed, causing the LLM to use outdated tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_token_refresh_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 42,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008830470000020796,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `fake_completion` raises a `ConnectionError` within 3 calls.",
          "The function `fake_completion` raises a `ConnectionError` within 2 additional calls.",
          "The function `fake_completion` does not raise a `ConnectionError` after the third call.",
          "The LLM provider's retry count is reset to 0 after each call.",
          "The LLM provider retries on transient errors, ensuring it can recover from such events."
        ],
        "scenario": "Test 'LiteLLM provider retries on transient errors'.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 426,
          "total_tokens": 573
        },
        "why_needed": "Prevents a regression where the LLM provider fails to retry transient errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_transient_error_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 70,
          "line_ranges": "65-66, 87-89, 97-99, 101, 103, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 243, 245, 264, 266-267, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 27,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 85-86, 92, 138, 140, 142-144, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009637990000044283,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Llama model is not properly initialized', 'description': 'The Llama model should be properly initialized before using it for annotation.'}",
          "{'name': 'Context length error is handled correctly', 'description': 'The context length error should be handled correctly by the LLM provider and return a fallback solution.'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 103,
          "total_tokens": 225
        },
        "why_needed": "To ensure that the LLM provider correctly handles context length errors during annotation."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 18,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 97-98, 100-101, 103-104"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008878060000085952,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should have an error message indicating that the call was unsuccessful with the specified system prompt.",
          "The annotation should indicate that the last error occurred after 2 retries.",
          "The annotation should correctly identify the cause of the failure as 'boom'."
        ],
        "scenario": "The test verifies that the annotate function of OllamaProvider correctly handles call errors by checking if the last error message is 'boom'.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 347,
          "total_tokens": 473
        },
        "why_needed": "This test prevents a regression where the LLM provider fails to handle call errors and instead raises an exception."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "42-46"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008712460000026567,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'httpx not installed. Install with: pip install httpx'",
          "provider.annotate(test, 'def test_case(): assert True')",
          "test_case() should raise an ImportError when httpx is missing",
          "mock_import_error('httpx') should be called before annotating the test",
          "config.provider should be set to 'ollama' before creating the provider instance",
          "OllamaProvider(config) should create a provider instance with the correct config",
          "test should pass when httpx is installed correctly"
        ],
        "scenario": "The Ollama provider should report an error when the httpx dependency is missing.",
        "token_usage": {
          "completion_tokens": 182,
          "prompt_tokens": 268,
          "total_tokens": 450
        },
        "why_needed": "This test prevents a potential bug where the provider incorrectly reports a non-existent issue, potentially leading to incorrect usage guidance or downstream errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 13,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 96"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008006420000015169,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'OllamaProvider.annotate_runtime_error_immediate_fail returns correct result', 'description': 'The function should return a specific JSON object with certain key-value pairs.'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 99,
          "total_tokens": 194
        },
        "why_needed": "The test is likely to fail because the annotation of runtime errors by Ollama Provider is not implemented yet."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_runtime_error_immediate_fail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008855020000169134,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "check status of response after successful login",
          "validate token sent in response",
          "assert that no exception is raised during annotation process"
        ],
        "scenario": "Test that the annotate method correctly annotates a full flow with mocked HTTP responses.",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 414,
          "total_tokens": 494
        },
        "why_needed": "Prevents authentication issues caused by unverified HTTP requests."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52-53, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008835579999981746,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by _annotate_internal method of LiteLLMProvider should not contain any error.",
          "The content of the captured messages should match 'CUSTOM PROMPT'.",
          "The key assertion 'a' in the captured message should be present and have a valid type."
        ],
        "scenario": "Test that LiteLLMProvider overrides the prompt when provided with a custom prompt.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 373,
          "total_tokens": 494
        },
        "why_needed": "To ensure that the LiteLLM provider uses the custom prompt when it is provided, rather than using the default one."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 40,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71, 74-80, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008928359999913482,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total tokens should be equal to the sum of prompt and completion tokens.",
          "The prompt tokens should not exceed 100.",
          "The completion tokens should not exceed 50.",
          "The total tokens should be greater than or equal to 150.",
          "The token usage is not None.",
          "The token usage is correctly calculated based on the provided data."
        ],
        "scenario": "Test LiteLLM provider annotates with token usage.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 426,
          "total_tokens": 549
        },
        "why_needed": "Prevents regression in token usage calculation when annotating responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008741709999924296,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The response from the Ollama model is as expected and matches the provided prompt.",
          "The URL of the generated text is correctly set to the specified host and port.",
          "The JSON payload sent by the Ollama provider includes the required 'model', 'prompt', 'system', and 'stream' fields.",
          "The timeout value is correctly set to 60 seconds as expected.",
          "The response from the Ollama model does not exceed the maximum allowed length of 2048 characters."
        ],
        "scenario": "Ollama provider makes correct API call for successful response.",
        "token_usage": {
          "completion_tokens": 154,
          "prompt_tokens": 470,
          "total_tokens": 624
        },
        "why_needed": "Prevents bug where OllamaProvider returns incorrect or incomplete responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008046700000079454,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured response from the Ollama provider contains the expected model.",
          "The value of `model` in the captured response matches the default model ('llama3.2').",
          "The absence of a specified model in the configuration does not prevent the provider from using the default model."
        ],
        "scenario": "Test that the Ollama provider uses the default model when not specified.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 344,
          "total_tokens": 473
        },
        "why_needed": "This test prevents a regression where the model is not set to 'llama3.2' even if it's not provided in the configuration."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 6,
          "line_ranges": "113-114, 116-117, 119-120"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007756550000124207,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Provider is not set up to return a response', 'expected': 'False', 'actual': 'True'}"
        ],
        "scenario": "Test OLLAMA Provider: Check Availability Failure",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 183,
          "total_tokens": 266
        },
        "why_needed": "The test checks if the OLLAMA provider returns False when the server is unavailable."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008025959999997667,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider._check_availability() is False', 'expected_value': False}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 197,
          "total_tokens": 284
        },
        "why_needed": "To test that the Ollama provider returns False for non-200 status codes."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007934289999980138,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '/api/tags' URL is present in the request.",
          "The response from the server has a status code of 200.",
          "The provider's check_availability method does not raise any exceptions when called with a valid configuration.",
          "The provider's check_availability method returns True for a successful check.",
          "The provider's check_availability method does not return False for an unsuccessful check."
        ],
        "scenario": "Verify that the Ollama provider checks availability successfully by returning a 200 status code for the /api/tags endpoint.",
        "token_usage": {
          "completion_tokens": 155,
          "prompt_tokens": 296,
          "total_tokens": 451
        },
        "why_needed": "This test prevents a potential bug where the provider returns an error or exception when checking availability, causing the application to fail."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008246570000096654,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context_length_key', 'expected_value': 'max_context_tokens'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 99,
          "total_tokens": 174
        },
        "why_needed": "To ensure the `get_max_context_tokens` method returns the correct context length key for a given scenario."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_context_length_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 11,
          "line_ranges": "138, 140, 142-147, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007952420000094662,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected `get_max_context_tokens` to return a fallback value', 'description': 'When an error occurs, `get_max_context_tokens` should return a fallback value'}",
          "{'name': 'Fallback value is returned for non-LLM errors', 'description': 'The fallback value should be returned for all types of errors (not just LLM errors)'}",
          "{'name': 'Fallback value is returned for LLM errors', 'description': 'The fallback value should be returned when an error occurs with the LLM provider'}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 101,
          "total_tokens": 263
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns a fallback value when an error occurs."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008077850000063336,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 512, 'actual_value': 0}",
          "{'name': 'context_token_count', 'expected_value': 8, 'actual_value': 4}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 99,
          "total_tokens": 211
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns the correct maximum context tokens for a given model info."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_model_info",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 15,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 158, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014218489999962003,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context token count', 'expected_value': 8, 'actual_value': 5}",
          "{'name': 'Token type distribution', 'expected_value': \"{'B': 55.0, 'I': 30.0, 'O': 15.0}\", 'actual_value': \"{'B': 55.0, 'I': 31.0, 'O': 14.0}\"}"
        ],
        "scenario": "Tests for OLLAMA provider",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 97,
          "total_tokens": 232
        },
        "why_needed": "To ensure the correct number of context tokens is returned from the parameters"
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_parameters",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 10,
          "line_ranges": "138, 140, 142-147, 149, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007818370000052255,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Response status code should be 400', 'expected_value': 400, 'actual_value': 0}",
          "{'name': 'Response body should contain an error message', 'expected_value': 'Error: max_context_tokens must be at least 200', 'actual_value': ''}"
        ],
        "scenario": "Tests for LLM providers",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 101,
          "total_tokens": 224
        },
        "why_needed": "To ensure the LLM provider returns a valid response when the maximum context tokens is not 200."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_non_200_status",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 1,
          "line_ranges": "128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007980180000117798,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of OllamaProvider', 'expected_value': 'True'}",
          "{'name': 'provider is configured with a valid provider name', 'expected_value': 'ollama'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 123,
          "total_tokens": 235
        },
        "why_needed": "To ensure the Ollama provider always returns `is_local=True`."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "65-66, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008439329999987422,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'annotation.error', 'value': 'Failed to parse LLM response as JSON'}"
        ],
        "scenario": "Ollama provider reports invalid JSON responses",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 138,
          "total_tokens": 214
        },
        "why_needed": "To ensure the Ollama provider correctly handles and reports invalid JSON responses in its tests."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008283739999797035,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Invalid response: key_assertions must be a list'}"
        ],
        "scenario": "test_parse_response_invalid_key_assertions",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 174,
          "total_tokens": 269
        },
        "why_needed": "to test Ollama provider's behavior when receiving invalid key_assertions payloads"
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008549350000066624,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected JSON format', 'value': '{\"scenario\": \"tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence\", \"why_needed\": \"To test that the LLM provider correctly extracts JSON from markdown code fences.\"}'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 127,
          "total_tokens": 251
        },
        "why_needed": "To test that the LLM provider correctly extracts JSON from markdown code fences."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008190869999964434,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected response is a dictionary', 'description': 'The extracted JSON should be in a dictionary format.'}",
          "{'name': 'Expected keys are correct', 'description': 'The keys in the extracted JSON should match the expected ones.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 128,
          "total_tokens": 250
        },
        "why_needed": "To test the functionality of extracting JSON from plain markdown fences (no language)."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009409060000109548,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert a is not None",
          "assert b is not None",
          "assert 'scenario' in annotation.scenario",
          "assert 'why_needed' in annotation.why_needed",
          "assert 'key_assertions' in annotation.key_assertions"
        ],
        "scenario": "Test Ollama provider parses valid JSON responses.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 292,
          "total_tokens": 394
        },
        "why_needed": "Prevents bugs in the LLM providers by ensuring correct parsing of responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008212309999748868,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total allocated tokens for small.py should be exactly 10.",
          "The total allocated tokens for large.py should be between 30 and 45 (inclusive).",
          "The allocation of tokens to small.py is sufficient to cover its content, leaving some wiggle room for overhead estimation changes."
        ],
        "scenario": "Verify water-fill algorithm satisfies smaller files first.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 396,
          "total_tokens": 510
        },
        "why_needed": "Prevents regression where larger files are prioritized over smaller ones, potentially leading to incorrect token distribution."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_constrained",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 2,
          "line_ranges": "42-43"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000744598000011365,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': {'message': 'Expected an empty dictionary to be returned when {} is passed with 100 as the token budget.'}, 'expected_result': {}}",
          "{'assertion': {'message': \"Expected an empty dictionary to be returned when {'f1': 'c'} is passed with 0 as the token budget.\"}, 'expected_result': {}}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 115,
          "total_tokens": 246
        },
        "why_needed": "Verify that the function handles empty input or no budget correctly."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007737919999897258,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The expected range for allocations of `l1.py` is between 35 and 50 tokens.",
          "The expected range for allocations of `l2.py` is also between 35 and 50 tokens.",
          "The absolute difference in allocations should be less than or equal to 1 token."
        ],
        "scenario": "Verify fair sharing when neither fits.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 327,
          "total_tokens": 443
        },
        "why_needed": "Prevents bug where one model gets significantly more tokens than the other, leading to unfair distribution of token budget."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_fair_share",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008056009999961589,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert len(allocations) == 3', 'expected_result': 3, 'actual_result': 0}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 133,
          "total_tokens": 227
        },
        "why_needed": "To ensure the LLM Utils library handles token budget allocation correctly when dealing with a large number of files."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000794440999982271,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of allocations for each file matches the required amount (10 tokens).",
          "Each allocation is within the estimated overhead range (6-16 tokens per file).",
          "All files receive full content as expected."
        ],
        "scenario": "Verify that all files receive sufficient token allocation when the budget is sufficient.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 332,
          "total_tokens": 440
        },
        "why_needed": "This test prevents a potential bug where files with larger content are not allocated enough tokens, leading to incomplete or corrupted code."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_sufficient",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007883699999808869,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert estimate_tokens('') == 1",
          "# Expected minimum token estimation of 1"
        ],
        "scenario": "Verify the minimum token estimation for an empty string.",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 217,
          "total_tokens": 302
        },
        "why_needed": "This test prevents a potential bug where the function does not return a valid estimate for an empty string, potentially leading to incorrect results or errors in downstream processing."
      },
      "nodeid": "tests/test_llm_utils.py::test_estimate_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "263-266"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008053120000113267,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' attribute of the `CoverageEntry` object should match the expected value.",
          "The 'line_ranges' attribute of the `CoverageEntry` object should match the expected format.",
          "The 'line_count' attribute of the `CoverageEntry` object should be equal to the expected value."
        ],
        "scenario": "Test the `CoverageEntry` class to ensure it correctly serializes its attributes.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 255,
          "total_tokens": 385
        },
        "why_needed": "This test prevents a bug where the `CoverageEntry` object is not properly serialized, potentially causing issues with data transfer or storage."
      },
      "nodeid": "tests/test_models.py::TestArtifactEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 3,
          "line_ranges": "241-243"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008258200000170746,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the serialized dictionary should be equal to 'src/foo.py'.",
          "The 'line_ranges' key in the serialized dictionary should be equal to '1-3, 5, 10-15'.",
          "The 'line_count' key in the serialized dictionary should be equal to 10.",
          "The 'file_path' value in the expected dictionary should match the actual value in the test data.",
          "The 'line_ranges' value in the expected dictionary should match the actual value in the test data.",
          "The 'line_count' value in the expected dictionary should match the actual value in the test data.",
          "All keys and values in the serialized dictionary should be present in the expected dictionary.",
          "Any missing keys or values in the serialized dictionary should raise an AssertionError."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object.",
        "token_usage": {
          "completion_tokens": 236,
          "prompt_tokens": 255,
          "total_tokens": 491
        },
        "why_needed": "This test prevents a potential bug where the serialized data does not match the expected format, potentially causing issues with downstream processing or debugging."
      },
      "nodeid": "tests/test_models.py::TestCollectionError::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "65-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007577519999983906,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the d dictionary should match the expected value.",
          "The 'line_ranges' key in the d dictionary should contain the correct range values.",
          "The 'line_count' key in the d dictionary should be equal to the expected value."
        ],
        "scenario": "Tests CoverageEntry to_dict method.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 255,
          "total_tokens": 364
        },
        "why_needed": "This test prevents a potential bug where the coverage entry serialization fails or is incorrect when dealing with complex line ranges."
      },
      "nodeid": "tests/test_models.py::TestCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000868590000010272,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.scenario == \"\" (empty string)",
          "annotation.why_needed == \"Empty annotation does not have any default values.\"",
          "annotation.key_assertions == [] (no key assertions are performed in this test)",
          "assert annotation.confidence is None (checks if confidence is set to None)",
          "assert annotation.error is None (checks if error is set to None)"
        ],
        "scenario": "An empty annotation should be created with default values.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 212,
          "total_tokens": 343
        },
        "why_needed": "This test prevents a potential bug where an empty annotation does not have any default values."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 9,
          "line_ranges": "130-133, 135, 137, 139, 141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007659979999914412,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Required keys: 'scenario', 'why_needed', and 'confidence'",
          "Optional key: 'key_assertions' (does not need to be present)",
          "Missing field: 'confidence' (should be excluded when None)"
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 230,
          "total_tokens": 342
        },
        "why_needed": "This test prevents a potential bug where the minimal annotation is missing some required fields."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "130-133, 135-137, 139-141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007590749999906166,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts that the 'scenario' field is present and matches the expected value.",
          "Asserts that the 'confidence' field is present and matches the expected value.",
          "Asserts that the 'context_summary' field has the correct mode and byte count."
        ],
        "scenario": "Test that the full annotation is included in the dictionary.",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 284,
          "total_tokens": 388
        },
        "why_needed": "Prevents a potential bug where only some fields are included in the output."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007938699999954224,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert d['schema_version'] == SCHEMA_VERSION",
          "assert d['tests'] == []",
          "assert 'warnings' not in d",
          "assert 'collection_errors' not in d"
        ],
        "scenario": "Test Default Report",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 231,
          "total_tokens": 324
        },
        "why_needed": "Prevents a potential bug where the default report is missing required schema version and empty test lists."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_default_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007725500000219654,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report should contain at least one collection error.",
          "Each collection error should have a unique nodeid.",
          "The first collection error should be for the specified nodeid 'test_bad.py'."
        ],
        "scenario": "Test Report Root test with collection errors to ensure it includes them.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 237,
          "total_tokens": 331
        },
        "why_needed": "This test prevents a regression where the report does not include all collection errors."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000787408000007872,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"Expected length of 'warnings' key to be 1\", 'description': 'The number of warnings in the report should be exactly one.'}",
          "{'name': \"Expected value of 'code' in first warning to match 'W001'\", 'description': \"The code of the first warning should match 'W001'.\"}"
        ],
        "scenario": "TestReportRoot::test_report_with_warnings",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 144,
          "total_tokens": 270
        },
        "why_needed": "To test that the ReportRoot class correctly identifies warnings in a report."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 73,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007871379999926376,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"nodeids == ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z']\", 'expected_result': ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z']}"
        ],
        "scenario": "Test: tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 215,
          "total_tokens": 338
        },
        "why_needed": "This test ensures that the output of ReportRoot is sorted by nodeid."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007297599999844806,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert detail is correct', 'expected_value': '/path/to/file', 'actual_value': \"assert d['detail'] == '/path/to/file'\"}"
        ],
        "scenario": "The `to_dict` method of the `ReportWarning` class is being tested.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 131,
          "total_tokens": 259
        },
        "why_needed": "This test is needed because it checks if the `detail` key is included in the dictionary returned by the `to_dict` method."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007540349999999307,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'detail' key is present and contains the actual detailed message.",
          "The 'code' key matches the expected code.",
          "The 'message' key matches the expected message.",
          "The 'warning_type' key (currently set to 'ReportWarning') is not included in the dictionary."
        ],
        "scenario": "Test to dictionary without detail should exclude it.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 223,
          "total_tokens": 335
        },
        "why_needed": "This test prevents a warning that could indicate a missing or misleading detail in the report."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_without_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 39,
          "line_ranges": "286-288, 290-292, 376-392, 394, 397, 399, 402, 405, 407, 409, 411-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007909440000162249,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'run_id' key is present in the output dictionary.",
          "The 'run_group_id' key is present in the output dictionary.",
          "The 'is_aggregated' key is True.",
          "The 'aggregation_policy' key is set to 'merge'.",
          "The 'run_count' key is equal to 3.",
          "The length of 'source_reports' list is 2."
        ],
        "scenario": "Verify that RunMeta has aggregation fields.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 343,
          "total_tokens": 483
        },
        "why_needed": "Prevents regression in aggregation feature where run_id, run_group_id and run_count are expected to be present."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_aggregation_fields_present",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007621700000015608,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_annotations_enabled' key is present in the data.",
          "The 'llm_provider' key is present in the data.",
          "The 'llm_model' key is present in the data."
        ],
        "scenario": "Test LLM fields are excluded when annotations not enabled.",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 232,
          "total_tokens": 326
        },
        "why_needed": "Prevents regression where LLM fields are included despite annotation being disabled."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 43,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419-431, 433, 435, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007806050000169762,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert data['llm_annotations_enabled'] is True",
          "assert data['llm_provider'] == 'ollama'",
          "assert data['llm_model'] == 'llama3.2:1b'",
          "assert data['llm_context_mode'] == 'complete'",
          "assert data['llm_annotations_count'] == 10",
          "assert data['llm_annotations_errors'] == 2"
        ],
        "scenario": "Test LLM traceability fields are included when enabled.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 327,
          "total_tokens": 462
        },
        "why_needed": "Prevents regression in case llm_annotations_enabled is disabled."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_traceability_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007631419999825084,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Non-aggregated report should not include source_reports', 'expected_result': 'False'}"
        ],
        "scenario": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 130,
          "total_tokens": 211
        },
        "why_needed": "To ensure that non-aggregated reports do not include source_reports."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 49,
          "line_ranges": "286-288, 290-292, 376-392, 394-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008118240000101196,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'git_sha' field is set to 'abc1234'.",
          "The 'git_dirty' field is set to True.",
          "The 'repo_version' field is set to '1.0.0'.",
          "The 'repo_git_sha' field is set to 'abc1234'.",
          "The 'repo_git_dirty' field is set to True.",
          "The 'plugin_git_sha' field is set to 'def5678'.",
          "The 'plugin_git_dirty' field is set to False.",
          "The 'config_hash' field is set to 'def5678'.",
          "The length of the 'source_reports' list is 1."
        ],
        "scenario": "Test RunMeta to dict with all optional fields.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 483,
          "total_tokens": 681
        },
        "why_needed": "Prevents regression in case of missing or outdated plugin version, which could lead to incorrect aggregation policy."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007776189999901817,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be set to 1.",
          "The 'interrupted' field should be set to True.",
          "The 'collect_only' field should be set to True.",
          "The 'collected_count' field should match the number of runs collected (10).",
          "The 'selected_count' field should match the number of selected runs (8).",
          "The 'deselected_count' field should match the number of deselected runs (2)."
        ],
        "scenario": "TestRunMeta::test_run_status_fields verifies that the RunMeta class includes run status fields.",
        "token_usage": {
          "completion_tokens": 173,
          "prompt_tokens": 285,
          "total_tokens": 458
        },
        "why_needed": "This test prevents a regression where the RunMeta class does not include all necessary run status fields, potentially causing incorrect interpretation of the results."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007596659999933308,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'schema_version should be semver format', 'expected_output': {'format': 'semver'}, 'actual_output': {'format': 'string'}}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 115,
          "total_tokens": 206
        },
        "why_needed": "This test ensures that the schema version is formatted correctly as semver."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009287129999790977,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'ReportRoot.schema_version', 'expected_value': 'SCHEMA_VERSION'}",
          "{'name': 'report.to_dict().schema_version', 'expected_value': 'SCHEMA_VERSION'}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 119,
          "total_tokens": 225
        },
        "why_needed": "This test ensures that the ReportRoot class includes the schema version in its JSON representation."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "96-103"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008009430000015527,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key should contain the expected value.",
          "The 'line_ranges' key should contain the expected string representation.",
          "The 'line_count' key should contain the expected integer value.",
          "Each assertion should be true for a CoverageEntry object.",
          "The dictionary structure should match the expected format.",
          "Any additional keys or values in the dictionary should not affect the test result."
        ],
        "scenario": "The test verifies that a CoverageEntry object can be serialized correctly into a dictionary.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 256,
          "total_tokens": 399
        },
        "why_needed": "This test prevents a potential bug where the coverage data is not properly encoded in the JSON representation."
      },
      "nodeid": "tests/test_models.py::TestSourceCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "286-288, 290, 292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007419819999938682,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The dictionary should contain 'scenario' key",
          "The dictionary should contain 'why_needed' key",
          "The dictionary should contain 'key_assertions' key",
          "The dictionary should not contain 'confidence' key when it's None"
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with required fields.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 229,
          "total_tokens": 343
        },
        "why_needed": "This test prevents a potential bug where the minimal annotation is missing some required fields."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "286-288, 290-292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007451480000213451,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"Expected value for 'run_id'\", 'value': 'run-1', 'type': 'string'}"
        ],
        "scenario": "TestSourceReport::test_to_dict_with_run_id",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 134,
          "total_tokens": 211
        },
        "why_needed": "To ensure SourceReport objects have a 'run_id' attribute in their dictionary representation."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008109719999822573,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the actual file path of the CoverageEntry.",
          "The 'line_ranges' key in the dictionary should match the expected line ranges.",
          "The 'line_count' key in the dictionary should match the actual number of lines in the CoverageEntry.",
          "All values in the dictionary should be strings, as they represent file paths, line ranges, and line counts."
        ],
        "scenario": "Test `test_to_dict` verifies that a CoverageEntry object is correctly serialized to a dictionary.",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 254,
          "total_tokens": 392
        },
        "why_needed": "This test prevents regression in coverage entry serialization."
      },
      "nodeid": "tests/test_models.py::TestSummary::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007734819999996034,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `d['nodeid']` is equal to `'test_foo.py::test_bar'`.",
          "The value of `d['outcome']` is equal to `'passed'`.",
          "The value of `d['duration']` is equal to `0.0`.",
          "The value of `d['phase']` is equal to `'call'`."
        ],
        "scenario": "The `TestTestCaseResult` class should be able to create a minimal result with the required fields.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 244,
          "total_tokens": 394
        },
        "why_needed": "This test prevents regression in cases where only the 'nodeid', 'outcome', and 'duration' are known."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 24,
          "line_ranges": "65-68, 190, 194-199, 201, 203, 205, 207, 210-212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007716789999960838,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The coverage list should contain exactly one entry.",
          "The file path of the first coverage entry should match 'src/foo.py'.",
          "All line ranges and counts in the coverage entry should be present."
        ],
        "scenario": "Test verifies that the `TestCaseResult` includes a coverage list.",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 256,
          "total_tokens": 352
        },
        "why_needed": "This test prevents regression in cases where the coverage is not included in the result."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007611190000034185,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert llm_opt_out is True', 'expected_value': True, 'message': 'Expected llm_opt_out to be True'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 145,
          "total_tokens": 239
        },
        "why_needed": "To ensure that the LLM opt-out flag is correctly set in the result."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203, 205, 207-210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007724390000021231,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'rerun_count', 'expected_value': 2, 'message': 'Expected rerun count to be 2'}",
          "{'name': 'final_outcome', 'expected_value': 'passed', 'message': \"Expected final outcome to be 'passed'\"}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 162,
          "total_tokens": 279
        },
        "why_needed": "The test result should include rerun fields."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007759759999999005,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result should be an empty dictionary', 'description': 'The result of the test case should be an empty dictionary.'}",
          "{'name': 'rerun_count should be None', 'description': 'The rerun count should be None.'}",
          "{'name': 'final_outcome should be passed', 'description': \"The final outcome of the test case should be 'passed'.\"}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 152,
          "total_tokens": 313
        },
        "why_needed": "This test is needed because it checks if the result of a test case does not include fields that indicate reruns."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "96-103, 241-243, 263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008280350000120507,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result['param_id'] == 'a-b-c',",
          "assert result['param_summary'] == 'a=1, b=2, c=3',",
          "assert result['captured_stdout'] == 'stdout content',",
          "assert result['captured_stderr'] == 'stderr content',",
          "assert result['requirements'] == ['REQ-100'],",
          "assert result['llm_opt_out'] is True,",
          "assert result['llm_context_override'] == 'complete',",
          "assert len(result['coverage']) == 1,",
          "assert result['llm_annotation']['scenario'] == 'Tests foo'"
        ],
        "scenario": "test_to_dict_with_all_optional_fields verifies that the `to_dict` method includes all optional fields when set.",
        "token_usage": {
          "completion_tokens": 203,
          "prompt_tokens": 454,
          "total_tokens": 657
        },
        "why_needed": "This test prevents regression by ensuring that the `to_dict` method handles optional parameters correctly."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 59,
          "line_ranges": "263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530-532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008188770000003842,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `result['artifacts']` should be 2.",
          "The path of `result['artifacts'][0]` should be `report.html`.",
          "All artifact entries in `result['artifacts']` should have a 'path' key."
        ],
        "scenario": "Test to_dict includes artifacts when set.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 264,
          "total_tokens": 369
        },
        "why_needed": "This test prevents a regression where the report is not properly formatted with artifacts."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_artifacts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007976169999892591,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `result['collection_errors']` is 1 and it contains only one item with key `nodeid` equal to `broken_test.py`.",
          "The value of `result['collection_errors'][0]` has a valid node id `broken_test.py`.",
          "The value of `result['collection_errors'][0]['message']` is `SyntaxError` which is the expected error message for `CollectionError` type."
        ],
        "scenario": "Test to_dict includes collection_errors when set.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 243,
          "total_tokens": 387
        },
        "why_needed": "Prevents a potential bug where the test reports collection errors without including them in the dictionary."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534-536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000788841000002094,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'custom_metadata' key should be present in the result dictionary.",
          "The 'custom_metadata' key should contain the expected project, environment, and build_number values.",
          "The 'build_number' value should match the provided value of 123.",
          "The custom metadata should override any default values set by the ReportRoot class.",
          "The custom metadata should be included in the report even if it's empty or None.",
          "The custom metadata should not be overwritten by other metadata attributes (e.g., 'project' and 'environment').",
          "The custom metadata should be preserved across different test runs with the same configuration."
        ],
        "scenario": "Test to_dict includes custom_metadata when set.",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 264,
          "total_tokens": 443
        },
        "why_needed": "Prevents a bug where the custom metadata is not included in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_custom_metadata",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000793729000008625,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'signature123', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 128,
          "total_tokens": 204
        },
        "why_needed": "HMAC signature is included in the report for security purposes."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536-538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008158310000112579,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'expected_value': 'abcdef1234567890'}"
        ],
        "scenario": "Test to_dict includes sha256 when set.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 131,
          "total_tokens": 228
        },
        "why_needed": "Because the ReportRoot class has a SHA-256 hash stored in its instance variable."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 63,
          "line_ranges": "96-103, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532-534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000808676999980662,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'source_coverage' key in the result dictionary should contain exactly one SourceCoverageEntry.",
          "The 'file_path' of the first SourceCoverageEntry in the 'source_coverage' list should match the specified file path.",
          "All other keys and values in the 'source_coverage' list should be present and have the expected values."
        ],
        "scenario": "Test to_dict includes source_coverage when set.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 282,
          "total_tokens": 412
        },
        "why_needed": "Prevents a potential bug where the test does not verify that the report includes source coverage, potentially leading to incorrect reporting or missing important information."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007956440000214116,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"len(result['warnings']) == 1\", 'description': \"The length of the 'warnings' key in the result dictionary should be 1.\", 'expected_value': 1, 'message': 'Expected length of warnings to be 1.'}",
          "{'name': \"result['warnings'][0]['code'] == 'W001'\", 'description': \"The code of the first warning in the 'warnings' list should be 'W001'.\", 'expected_value': 'W001', 'message': \"Expected code of the first warning to be 'W001'.\"}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 151,
          "total_tokens": 347
        },
        "why_needed": "The test is necessary because it checks the behavior of `report.to_dict()` when warnings are present."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 12,
          "line_ranges": "467-475, 477-479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000756389000002855,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The coverage_total_percent value should be equal to 85.5', 'expected_value': 85.5}"
        ],
        "scenario": "The test should include the coverage_total_percent key in the result dictionary.",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 153,
          "total_tokens": 314
        },
        "why_needed": "Because of the following reason: The `to_dict()` method is expected to return a dictionary with all the assertions, including the `coverage_total_percent` value. Without it, the assertion would fail because the key is missing."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_with_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009070020000194745,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "..."
        ],
        "scenario": "...",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 131,
          "total_tokens": 214
        },
        "why_needed": "..."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_without_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 42,
          "line_ranges": "65-68, 130-133, 135, 137, 139, 141, 143, 190, 194-199, 201-207, 210-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008106409999868447,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'param_id' field is present and matches 'a-b-c'.",
          "The 'param_summary' field is present and matches 'a=1, b=2, c=3'.",
          "The 'captured_stdout' field is present and matches 'stdout content'.",
          "The 'captured_stderr' field is present and matches 'stderr content'.",
          "All required parameters ('REQ-100') are included in the result.",
          "Optional fields (e.g. `llm_opt_out`, `llm_context_override`) are excluded from the result.",
          "The length of the coverage entry is 1, which is expected for a single test case.",
          "The 'llm_annotation' field contains the correct scenario name."
        ],
        "scenario": "test_to_dict_with_all_optional_fields verifies that the `to_dict` method includes all optional fields when set.",
        "token_usage": {
          "completion_tokens": 228,
          "prompt_tokens": 454,
          "total_tokens": 682
        },
        "why_needed": "This test prevents regression in case where `llm_opt_out=True` and `llm_context_override='complete'`."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220-222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007802039999944554,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured stderr is correctly included in the result."
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 149,
          "total_tokens": 216
        },
        "why_needed": "Because the `to_dict` method includes captured stderr in the test case results."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218-220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007474129999991419,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': 'Debug output here'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 149,
          "total_tokens": 223
        },
        "why_needed": "Captured stdout is necessary for accurate coverage analysis."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203-207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007944909999935135,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 163,
          "total_tokens": 251
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_param_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007817070000157855,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': ['REQ-001', 'REQ-002'], 'actual_value': ['REQ-001', 'REQ-002']}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 151,
          "total_tokens": 253
        },
        "why_needed": "This test is necessary to ensure that the `to_dict` method includes requirements when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007826289999854907,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `llm_context_exclude_globs` returns a list of excluded files.",
          "The file `*.pyc` is included in the default exclude globs.",
          "The directory `__pycache__/*` is also included in the default exclude globs.",
          "The string `*secret*` is not included in the default exclude globs.",
          "The string `*password*` is correctly excluded as a file extension.",
          "The function `llm_context_exclude_globs` returns an empty list when no files or directories are specified."
        ],
        "scenario": "Test the default exclude globs for LLM context.",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 222,
          "total_tokens": 391
        },
        "why_needed": "This test prevents a potential bug where the default exclude globs are not correctly set."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008147990000111349,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `--password` pattern should match any string containing '--password' in its contents.",
          "The `--token` pattern should match any string containing '--token' in its contents.",
          "The `--api[_-]?key` pattern should match any string containing '--api[_-]?key' in its contents, where `_` is a non-alphanumeric character and `key` is the actual key being redacted."
        ],
        "scenario": "Verifies the default redact patterns used by the Config class.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 228,
          "total_tokens": 372
        },
        "why_needed": "Prevents a potential security vulnerability where sensitive information like passwords and tokens are not properly redacted."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008538929999986067,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.llm_context_mode == 'minimal'",
          "cfg.llm_max_tests == 0",
          "cfg.llm_max_retries == 10",
          "cfg.llm_context_bytes == 32000",
          "cfg.llm_context_file_limit == 10",
          "cfg.llm_requests_per_minute == 5",
          "cfg.llm_timeout_seconds == 30",
          "cfg.llm_cache_ttl_seconds == 86400",
          "cfg.include_phase == 'run'",
          "cfg.aggregate_policy == 'latest'",
          "cfg.is_llm_enabled() is False",
          "cfg.omit_tests_from_coverage is True"
        ],
        "scenario": "Test that default values are set correctly for the TestConfig class.",
        "token_usage": {
          "completion_tokens": 205,
          "prompt_tokens": 318,
          "total_tokens": 523
        },
        "why_needed": "This test prevents a regression where the default values of the TestConfig class are not set correctly, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007703449999780787,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg is an instance of Config', 'expected_type': 'Config'}",
          "{'name': \"cfg.provider == 'none'\", 'expected_value': 'none'}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_get_default_config",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 104,
          "total_tokens": 209
        },
        "why_needed": "To ensure that the default configuration is properly initialized and provides a 'provider' key with value 'none'."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_get_default_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000807666000014251,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return False when the `provider` is not explicitly set or defaults to `'none'`.",
          "The function should return True for providers with a specified LLM provider, such as `'ollama'`.",
          "The function should return True for providers with a default LLM provider, such as `'litellm'`.",
          "The function should not return False when the `provider` is set to `'gemini'`."
        ],
        "scenario": "Test that the `is_llm_enabled` check returns False for providers without a specified LLM provider.",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 263,
          "total_tokens": 426
        },
        "why_needed": "Prevents regression in case the `provider` is set to 'none' and the test relies on it returning True."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-221, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007909550000135823,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The error message should contain the string 'Invalid aggregate_policy ''\", 'expected_value': \"'Invalid aggregate_policy '''\"}"
        ],
        "scenario": "test_validate_invalid_aggregate_policy",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 128,
          "total_tokens": 201
        },
        "why_needed": "To test the validation of an invalid aggregation policy."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-213, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008086379999951987,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected number of errors', 'value': 1, 'description': 'The function should return exactly one error message.'}",
          "{'name': 'Expected error message content', 'value': \"Invalid llm_context_mode 'mega_max'\", 'description': 'The error message should contain the invalid context mode string.'}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 131,
          "total_tokens": 257
        },
        "why_needed": "To test the validation of an invalid context mode."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-229, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007797629999970468,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The length of errors is 1', 'expected_result': 1}",
          "{'assertion': \"The string 'Invalid include_phase ' is in errors[0]\", 'expected_result': \"Invalid include_phase 'lunch_break'\"}"
        ],
        "scenario": "test_validate_invalid_include_phase",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 129,
          "total_tokens": 241
        },
        "why_needed": "To ensure that the `include_phase` parameter is valid and raises an error when it's invalid."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 28,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007887600000060502,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Invalid provider 'invalid_provider'\", 'type': 'assertion'}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 122,
          "total_tokens": 190
        },
        "why_needed": "To test the validation of an invalid provider."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 31,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245-254, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007847630000128447,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "llm_context_bytes must be at least 1000",
          "llm_max_tests must be 0 (no limit) or positive",
          "llm_requests_per_minute must be at least 1",
          "llm_timeout_seconds must be at least 1",
          "llm_max_retries must be 0 or positive"
        ],
        "scenario": "Test validation of numeric constraints for TestConfig.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 329,
          "total_tokens": 474
        },
        "why_needed": "This test prevents regression where the default values for llm_context_bytes, llm_max_tests, llm_requests_per_minute, llm_timeout_seconds, and llm_max_retries are not validated correctly."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_numeric_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007398889999876701,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'errors should be empty', 'expected': [], 'actual': {'scenario': 'tests/test_options.py::TestConfig::test_validate_valid_config', 'why_needed': 'To ensure that the `validate` method returns an empty list of errors when a valid configuration is provided.', 'key_assertions': ['...']}}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_valid_config",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 100,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the `validate` method returns an empty list of errors when a valid configuration is provided."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00375832400001741,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` option is set to 'aggr_dir'.",
          "The `aggregate_policy` option is set to 'merge'.",
          "The `aggregate_run_id` option is set to 'run-123'.",
          "The `aggregate_group_id` option is set to 'group-abc'."
        ],
        "scenario": "Test the ability to load aggregation options correctly.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 295,
          "total_tokens": 415
        },
        "why_needed": "This test prevents a bug where aggregation options are not loaded correctly, potentially causing issues with downstream processing."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_aggregation_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0036892740000098456,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 138,
          "total_tokens": 208
        },
        "why_needed": "To test that the disabled batch flag works correctly."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004130271999997603,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `load_config` should be able to handle the case when pyproject.toml is not present.",
          "The default value for `llm_max_retries` should be used in this scenario.",
          "The test should fail with a meaningful error message indicating that the LLM report generation failed due to missing pyproject.toml file."
        ],
        "scenario": "Test handling when pyproject.toml doesn't exist.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 413,
          "total_tokens": 543
        },
        "why_needed": "This test prevents a bug where the LLM report generation fails with an error due to missing pyproject.toml file."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_config_missing_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607-608, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004371023999993895,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'cov_dir', 'actual': 'None'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 126,
          "total_tokens": 190
        },
        "why_needed": "To test the coverage source option."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0037532940000062354,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg.provider == \"none\"', 'expected_value': 'None'}",
          "{'name': 'cfg.report_html is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_defaults",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 116,
          "total_tokens": 210
        },
        "why_needed": "To test the functionality of loading configuration with no options set."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 132,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492-494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004477613000005931,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'CLI options override PyProject.toml', 'expected_value': {'override': True}}",
          "{'name': 'PyProject.toml values not overridden by CLI options', 'expected_value': {}}"
        ],
        "scenario": "Load configuration from CLI overrides in PyProject",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 134,
          "total_tokens": 229
        },
        "why_needed": "To ensure that CLI options override PyProject settings."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 133,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0050999620000027335,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'CLI provider option overrides pyproject.toml', 'expected_value': 'pyproject.toml'}"
        ],
        "scenario": "Load configuration from CLI provider override",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 130,
          "total_tokens": 201
        },
        "why_needed": "To test that CLI provider option overrides pyproject.toml."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_provider_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494-495, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0037514210000040293,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'llm_max_retries': 2}, 'actual': {'llm_max_retries': 2}}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 130,
          "total_tokens": 211
        },
        "why_needed": "To test the functionality of loading retries from CLI."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 134,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386-388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005342906999999286,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected': 'True', 'actual': 'True'}",
          "{'name': 'pyproject.toml contents are correct', 'expected': \"{'...'}\", 'actual': {'scenario': '...', 'why_needed': '...', 'key_assertions': ['...']}}"
        ],
        "scenario": "Test loading values from pyproject.toml.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 119,
          "total_tokens": 246
        },
        "why_needed": "To test the functionality of the `load_config` method in the Options class."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 88,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470-474, 476-477, 479, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003613682999997536,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `prompt_tier` of the configuration should be 'minimal'.",
          "The `batch_parametrized_tests` flag should be False.",
          "The `context_compression` of the configuration should be 'none'."
        ],
        "scenario": "Test loads token optimization options from CLI and verifies the expected configuration.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 264,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a potential bug where the `llm_prompt_tier` option is set to 'minimal' but the `batch_parametrized_tests` option is still enabled, leading to unexpected behavior in the test."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_token_optimization_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488, 490-492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005206291000007468,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_dependency_snapshot` option is correctly set to 'deps.json' after applying a CLI override.",
          "The `report_dependency_snapshot` configuration value matches the expected value of 'deps.json'.",
          "A mock configuration object is created with the correct `llm_dependency_snapshot` option value.",
          "The `load_config` function successfully loads the mock configuration object.",
          "The `cfg.report_dependency_snapshot` attribute is set to the expected value of 'deps.json' after applying a CLI override.",
          "A critical check is performed: the `llm_dependency_snapshot` option is present and has the correct value in the configuration.",
          "A test case is executed with mock configuration data, ensuring that the `llm_dependency_snapshot` option is correctly set."
        ],
        "scenario": "Testing the `test_cli_dependency_snapshot` test function to verify that it correctly sets the `llm_dependency_snapshot` option to 'deps.json' when a CLI override is used.",
        "token_usage": {
          "completion_tokens": 261,
          "prompt_tokens": 213,
          "total_tokens": 474
        },
        "why_needed": "This test prevents a potential regression where the `llm_dependency_snapshot` option is not set to 'deps.json' when a CLI override is applied, which could lead to unexpected behavior or errors in dependency snapshot reporting."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_dependency_snapshot",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488-490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.006136337000015146,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `cfg.report_evidence_bundle` should be set to 'bundle.zip'.",
          "The value of `mock.option.llm_evidence_bundle` should match 'bundle.zip'."
        ],
        "scenario": "Verify that the `report_evidence_bundle` option is correctly set to 'bundle.zip' when CLI override is enabled.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 217,
          "total_tokens": 331
        },
        "why_needed": "This test prevents a potential bug where the `report_evidence_bundle` option is not updated correctly when CLI overrides are applied."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_evidence_bundle",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484-486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005633724000006168,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_report_json` option is set to 'output.json' in the mock configuration.",
          "The value of `report_json` in the loaded configuration matches 'output.json'.",
          "The `llm_report_json` option is present and has a value of 'output.json' in the loaded configuration."
        ],
        "scenario": "Verify that the test CLI overrides the default report format to 'output.json'.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 212,
          "total_tokens": 334
        },
        "why_needed": "This test prevents a potential bug where the default report format is not set correctly."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486-488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005367052999986299,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'report_pdf' option should be set to 'output.pdf'.",
          "Mocking the config object with a mock configuration and verifying it sets the 'report_pdf' option correctly.",
          "Verifying that the assert statement passes when the 'report_pdf' option is set correctly.",
          "Testing the error case where the 'report_pdf' option is not set at all.",
          "Mocking the config object without setting the 'report_pdf' option and verifying it does not affect the test.",
          "Verifying that the mock configuration overrides any default value for the 'report_pdf' option.",
          "Testing the case where the 'llm_report_pdf' option is also overridden by a custom value."
        ],
        "scenario": "Testing the 'test_cli_report_pdf' test function to verify that it sets the 'report_pdf' option correctly.",
        "token_usage": {
          "completion_tokens": 211,
          "prompt_tokens": 212,
          "total_tokens": 423
        },
        "why_needed": "This test prevents a bug where the 'report_pdf' option is not set correctly for CLI overrides."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_pdf",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-237, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007811149999952249,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"litellm_token_output_format should be one of ['json', 'xml']\", 'description': 'The token output format should be either json or xml.'}"
        ],
        "scenario": "Test validation of invalid token output format",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 130,
          "total_tokens": 221
        },
        "why_needed": "To ensure that the token output format is always valid and consistent with the expected format."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_invalid_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241-242, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007777600000054008,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The token refresh interval must be at least 60 seconds', 'expected_value': 60, 'actual_value': 30}"
        ],
        "scenario": "Test validation when token refresh interval is too short",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 146,
          "total_tokens": 239
        },
        "why_needed": "Because the token refresh interval is set to 30 seconds, which is too short and may cause issues with authentication."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_token_refresh_interval_too_short",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008294770000247809,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is_not_empty', 'expected_value': 'litellm_token_output_format', 'actual_value': 'text'}",
          "{'assertion_type': 'not_equal_to', 'expected_value': 3600, 'actual_value': 3600}"
        ],
        "scenario": "test_validate_valid_litellm_config",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 142,
          "total_tokens": 260
        },
        "why_needed": "To ensure that the LiteLLM configuration is valid and does not raise any validation errors."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_valid_litellm_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438-440, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015228579999870817,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'include_history = True\\naggregate_include_history = True', 'actual': 'include_history = True\\naggregate_include_history = True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 118,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the aggregate_include_history feature is properly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436-438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014935430000093675,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has an aggregate_policy section', 'expected': 'pyproject.toml should exist and have an aggregate_policy section'}",
          "{'name': 'aggregate_policy is present in pyproject.toml', 'expected': 'aggregate_policy should be present in pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 121,
          "total_tokens": 256
        },
        "why_needed": "To ensure that the aggregate policy is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 150,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340-346, 348-350, 352-354, 356-357, 360-369, 372-375, 378-392, 396, 400, 402, 404, 408-410, 412-413, 416-422, 426-428, 430-432, 436-440, 444-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022318890000008196,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'expected': {'status': 0, 'message': ''}, 'actual': {'status': 0, 'message': ''}}",
          "{'name': 'pyproject.toml file has all config keys', 'expected': {'status': 1, 'message': 'Missing required config key'}, 'actual': {}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 120,
          "total_tokens": 270
        },
        "why_needed": "To ensure that all config keys are loaded when loading the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390-392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014971089999846754,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"pyproject.toml exists and has a 'cache_dir' key\", 'expected_value': 'True'}",
          "{'name': \"pyproject.toml contains a valid 'cache_dir' value\", 'expected_value': \"The 'cache_dir' value in the pyproject.toml file is correct.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 113,
          "total_tokens": 245
        },
        "why_needed": "To ensure that the cache directory is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388-390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015187399999945228,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and has ttl_seconds setting', 'value': 'True'}",
          "{'name': 'pyproject.toml file content contains ttl_seconds setting', 'value': {'ttl_seconds': 3600}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 116,
          "total_tokens": 236
        },
        "why_needed": "To ensure that the cache TTL seconds is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418-420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001523098999996364,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'empty build section', 'actual': 'not empty build section'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 116,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the `capture_failed_output` feature flag is correctly set when loading a PyProject file with an empty 'build' section."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420-422, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014742970000156674,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected value for capture_output_max_chars', 'value': 100, 'expected_type': 'int'}",
          "{'name': 'Expected error message for invalid capture_output_max_chars value', 'value': 'Invalid capture_output_max_chars value. It should be an integer between 1 and 999.', 'expected_type': 'str'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
        "token_usage": {
          "completion_tokens": 160,
          "prompt_tokens": 119,
          "total_tokens": 279
        },
        "why_needed": "To ensure that the `capture_output_max_chars` option in `pyproject.toml` is correctly loaded and used to set the maximum number of characters for capturing output."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362-364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015268049999974664,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context bytes are loaded from pyproject.toml', 'expected_value': 'context_bytes', 'actual_value': 'pyproject.toml'}",
          "{'name': 'Context bytes are not loaded from other files', 'expected_value': 'other_files', 'actual_value': 'None'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 113,
          "total_tokens": 241
        },
        "why_needed": "To ensure that the context_bytes functionality in Pytest is working correctly."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368-369, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015334779999989223,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'exclude globs from pyproject.toml', 'actual': 'include globs from pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 119,
          "total_tokens": 229
        },
        "why_needed": "To ensure that the 'context_exclude_globs' setting in the PyProject is properly loaded and excluded from the test coverage."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364-366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016587229999913689,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context file limit is correctly set to 1000', 'expected_value': 1000, 'actual_value': '1234'}",
          "{'name': 'Context file limit is not exceeded by default settings', 'expected_value': 1000}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 116,
          "total_tokens": 241
        },
        "why_needed": "To ensure that the context file limit is properly set in the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366-368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015208949999987453,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context include globs are correctly loaded', 'expected_value': 'True'}",
          "{'name': 'Context include globs are not empty', 'expected_value': 'False'}"
        ],
        "scenario": "Tests for `tests/test_options_coverage.py`",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 119,
          "total_tokens": 224
        },
        "why_needed": "To ensure that the `context_include_globs` option is correctly loaded from `pyproject.toml`."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015326259999994818,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"hmac_key_file = 'path/to/hmac/key.txt'\", 'actual': 'pyproject.toml contents'}",
          "{'name': 'hmac key file path', 'expected': '/path/to/hmac/key.txt', 'actual': 'pyproject.toml contents'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 118,
          "total_tokens": 258
        },
        "why_needed": "To ensure that the hmac key file is loaded correctly and used for signing requests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372-374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001591006000012385,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The include_param_values option should be present in the pyproject.toml file', 'expected_value': {'scenario': 'tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values', 'why_needed': 'To ensure that the `include_param_values` option is correctly loaded from the `pyproject.toml` file.'}}",
          "{'name': 'The include_param_values option should be a boolean value', 'expected_value': {'scenario': 'tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values', 'why_needed': 'To ensure that the `include_param_values` option is correctly loaded from the `pyproject.toml` file.'}}",
          "{'name': 'The include_param_values option should be set to True', 'expected_value': {'scenario': 'tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values', 'why_needed': 'To ensure that the `include_param_values` option is correctly loaded from the `pyproject.toml` file.'}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
        "token_usage": {
          "completion_tokens": 297,
          "prompt_tokens": 116,
          "total_tokens": 413
        },
        "why_needed": "To ensure that the `include_param_values` option is correctly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001488333999986935,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"include_phase = ['main', 'util']\", 'actual': \"include_phase = ['main', 'test']\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 113,
          "total_tokens": 215
        },
        "why_needed": "To ensure that the include phase is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426-428, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014975409999919975,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'include_pytest_invocation = [\"path/to/pytestInvocation.py\"]', 'actual': 'include_pytest_invocation = []'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 122,
          "total_tokens": 236
        },
        "why_needed": "To ensure that the include_pytest_invocation option is properly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430-432, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015078600000038023,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'Redacted invocation_redact_patterns key from pyproject.toml contents', 'actual': 'Redacted invocation_redact_patterns key in pyproject.toml contents'}"
        ],
        "scenario": "test_load_invocation_redact_patterns",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 121,
          "total_tokens": 232
        },
        "why_needed": "To ensure that the 'invocation_redact_patterns' key in pyproject.toml is correctly loaded and redacted for sensitive information."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_invocation_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340-342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016708859999994274,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'The contents of pyproject.toml should be a valid Python project file with the following structure:\\n\\n[tool.pyproject.toml]\\n\\t[tool.litellm_api_base]\\n\\t\"litellm_api_base\": \"path/to/litellm_api_base\"\\n', 'actual': 'The contents of pyproject.toml should be a valid Python project file with the following structure:\\n\\n[tool.pyproject.toml]\\n\\t[tool.litellm_api_base]\\n\\t\"litellm_api_base\": \"path/to/litellm_api_base\"\\n', 'error_code': 0}"
        ],
        "scenario": "test_load_litellm_api_base",
        "token_usage": {
          "completion_tokens": 204,
          "prompt_tokens": 122,
          "total_tokens": 326
        },
        "why_needed": "To ensure that the `litellm_api_base` module is loaded correctly from the PyPI."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342-344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015722810000227128,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'api-key = \"...\"', 'actual': 'api-key = \"...\"'}",
          "{'name': 'litellm_api_key loaded', 'expected': 'True', 'actual': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 122,
          "total_tokens": 253
        },
        "why_needed": "To ensure that the litellm API key is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356-357, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015295409999964704,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"pyproject.toml exists and has a 'litellm_token_json_key' section\", 'expected_value': 'True'}",
          "{'name': \"pyproject.toml has a 'litellm_token_json_key' section with the correct key\", 'expected_value': {'key': 'litellm_token_json_key', 'value': 'some_value'}}"
        ],
        "scenario": "Test loading litellm_token_json_key from pyproject.toml",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 125,
          "total_tokens": 267
        },
        "why_needed": "To ensure the coverage of litellm token JSON key is properly loaded in the project."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352-354, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015470230000005358,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"The contents of the pyproject.toml file should contain a section named 'tool' with a sub-section named 'litellm' and a key named 'token_output_format'.\"}",
          "{'name': 'litellm token output format', 'expected': \"The value of the 'token_output_format' key in the pyproject.toml file should be a string representing the expected output format.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 125,
          "total_tokens": 295
        },
        "why_needed": "To ensure that the litellm token output format is correctly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344-346, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015856159999998454,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has the correct content', 'expected_content': '```\\ntoml version 0.13.2\\n[tool.pyproject.toml]\\nversion = \"0.13.2\"\\n\\tpy_requires = [\"python >= 3.8\"]\\n\\tpython_requires = {exact: \"py>=3.7,upper-exact\"}\\n\\ntoctreatesource = False\\n\\ttoctree = []\\n\\thelprootsync = True\\n[tool.pyproject.toml]\\nversion = \"0.13.2\"\\n\\tpy_requires = [\"python >= 3.8\"]\\n\\tpython_requires = {exact: \"py>=3.7,upper-exact\"}\\n\\ttoctreatesource = False\\n\\ttoctree = []\\n\\thelprootsync = True\\n```\\n', 'actual_content': '```\\ntoml version 0.13.2\\n[tool.pyproject.toml]\\nversion = \"0.13.2\"\\n\\tpy_requires = [\"python >= 3.8\"]\\n\\tpython_requires = {exact: \"py>=3.7,upper-exact\"}\\n\\ntoctreatesource = False\\n\\ttoctree = []\\n\\thelprootsync = True\\n[tool.pyproject.toml]\\nversion = \"0.13.2\"\\n\\tpy_requires = [\"python >= 3.8\"]\\n\\tpython_requires = {exact: \"py>=3.7,upper-exact\"}\\n\\ttoctreatesource = False\\n\\ttoctree = []\\n\\thelprootsync = True\\n```\\n', 'error_message': ''}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
        "token_usage": {
          "completion_tokens": 457,
          "prompt_tokens": 125,
          "total_tokens": 582
        },
        "why_needed": "To ensure that the litellm_token_refresh_command is loaded correctly from pyproject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348-350, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0026494619999937186,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"refresh_interval = '1h'\", 'actual': 'None'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 125,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the litellm_token_refresh_interval option is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 73,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 449, 451, 453-456, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001528199000006225,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 158,
          "total_tokens": 259
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_malformed_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380-382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014942740000094545,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected max_concurrency value in pyproject.toml', 'value': 4, 'expected_type': 'int'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 116,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the `max_concurrency` option is correctly loaded from the `pyproject.toml` file and used to configure the concurrency settings."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378-380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015203339999914078,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected': 'pyproject.toml should exist'}",
          "{'name': 'pyproject.toml file content', 'expected': 'pyproject.toml should contain the correct setting for max_tests'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 113,
          "total_tokens": 245
        },
        "why_needed": "To ensure that the `max_tests` setting in `pyproject.toml` is correctly loaded and used to determine the number of tests to run."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444-446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001510975999991615,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected_result': 'True'}",
          "{'name': 'pyproject.toml is a file', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 113,
          "total_tokens": 218
        },
        "why_needed": "To ensure that the metadata file is loaded correctly and provides accurate information about the project."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336-337, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015329469999869616,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"{'ollama_host': 'host_name'}\", 'actual': \"{'ollama_host': 'host_name'}\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 119,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the ollama_host configuration is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001497319999998581,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'omitted tests are not included in the coverage report', 'actual': 'included tests are included in the coverage report'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 121,
          "total_tokens": 229
        },
        "why_needed": "To ensure that omit_tests_from_coverage is correctly loaded from pyproject.toml when coverage is enabled."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374-375, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015402600000129496,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected value for max_chars', 'value': 100, 'expected_type': 'int'}",
          "{'name': 'Error message for invalid max_chars value', 'value': 'max_chars must be an integer between 1 and 9999', 'expected_type': 'str'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 119,
          "total_tokens": 257
        },
        "why_needed": "To ensure that the `max_chars` parameter in `pyproject.toml` is correctly loaded and validated."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416-418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015297019999991335,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'Collect only files', 'actual': 'Collect all files'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 116,
          "total_tokens": 209
        },
        "why_needed": "To ensure that the `report_collect_only` option is correctly loaded from the PyProject file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015413320000163822,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has a correct timeout_seconds setting', 'value': 'True'}",
          "{'name': 'timeout_seconds value in pyproject.toml is within the expected range', 'value': 60}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 113,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the timeout_seconds feature is properly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400-402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004363589999996975,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'file existence', 'expected_result': 'pyproject.toml exists in the test directory'}",
          "{'assertion_type': 'file content', 'expected_result': 'The contents of pyproject.toml match the expected format'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 117,
          "total_tokens": 240
        },
        "why_needed": "To ensure that the `batch_max_tests` feature is correctly loaded from the PyProject file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 131,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396-398, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005113627000014276,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'File existence and content', 'condition': 'pyproject.toml exists and contains the expected file structure'}",
          "{'assertion_type': 'JSON structure consistency', 'condition': 'the JSON object in pyproject.toml is consistent with the expected structure'}"
        ],
        "scenario": "Load batch parametrized tests",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 123,
          "total_tokens": 232
        },
        "why_needed": "Optimize Pytest configuration by reducing unnecessary test loading"
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402-404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00426269100000809,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context compression is enabled', 'value': 'True'}",
          "{'name': 'Context compression is disabled', 'value': 'False'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 117,
          "total_tokens": 221
        },
        "why_needed": "To ensure that the context compression feature is properly loaded and enabled in the PyPI token optimization process."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404-405, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005168641000011576,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context line padding is applied correctly', 'expected_value': '\\n# This is a test\\n# with line padding', 'actual_value': '```python\\nThis is a test\\nwith line padding\\n```'}",
          "{'name': 'Context line padding is not applied when using the default value', 'expected_value': '\\nThis is a test\\n', 'actual_value': '```python\\nThis is a test\\n\\n```'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 117,
          "total_tokens": 287
        },
        "why_needed": "To ensure that the `context_line_padding` option in the PyProject is correctly loaded and applied to context lines."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392-393, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004586467999985189,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"The 'prompt_tier' key in the PyProject file should be present and contain a list of tier names.\"}",
          "{'name': 'prompt_tier value', 'expected': \"The value of the 'prompt_tier' option should be a list of tier names.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 117,
          "total_tokens": 258
        },
        "why_needed": "To ensure that the `prompt_tier` option is correctly loaded from the PyProject file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007767079999894122,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert errors are not empty', 'message': \"Expected no error messages, but got 'batch_max_tests must be at least 1'\", 'type': 'error'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 135,
          "total_tokens": 248
        },
        "why_needed": "This test is necessary because it checks if the `batch_max_tests` configuration option allows for too few tests to be validated."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007944710000060695,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding must be 0 or positive', 'type': 'assertionError'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 129,
          "total_tokens": 217
        },
        "why_needed": "To ensure that the context line padding is correctly validated and raises an error when it's negative."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000778480999997555,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Invalid context_compression', 'type': 'AssertionError'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 124,
          "total_tokens": 202
        },
        "why_needed": "To test the validation of a scenario with an invalid context_compression setting."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008027160000096956,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': 'Invalid prompt_tier', 'value': \"['Invalid prompt_tier']\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 125,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the validation of invalid `prompt_tier` values does not produce any errors."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 124,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378-380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603-605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0027079429999901095,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg is an instance of Config', 'description': 'The function should check if cfg is indeed an instance of Config.'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 119,
          "total_tokens": 199
        },
        "why_needed": "To ensure the config has safe defaults."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007736920000240843,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytestconfig is not None', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 108,
          "total_tokens": 182
        },
        "why_needed": "The test requires that markers exist in the plugin configuration."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 91,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09688120799998501,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should exist after running the test.",
          "The `report.html` file should exist after running the test.",
          "Both files should have the expected contents before running the test.",
          "The `report.json` file should contain the correct data.",
          "The `report.html` file should not be empty or contain unexpected content.",
          "The plugin should correctly generate both JSON and HTML reports for the test function."
        ],
        "scenario": "Test generates both JSON and HTML reports for a test function.",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 279,
          "total_tokens": 425
        },
        "why_needed": "This test prevents regression in cases where the plugin is used to generate both JSON and HTML outputs."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_both_json_and_html_outputs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06213147900001559,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert data['run_meta']['collected_count'] == 3\", 'expected_value': 3, 'message': 'Expected collected count to be 3'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 198,
          "total_tokens": 294
        },
        "why_needed": "pytest_collection_finish counts items (line 378)"
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 11,
          "line_ranges": "70-71, 73-75, 77, 79, 142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.058940660999979855,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `nested` directory should be created in the report.json file.",
          "The `report.json` file should exist in the specified directory path.",
          "The `nested` directory should have been created recursively if it does not already exist.",
          "The plugin should create a nested directory structure for reports even when output directories are missing.",
          "The test should fail if the report.json file is not created or exists outside of the specified directory path."
        ],
        "scenario": "Test that output directories are created if missing.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 247,
          "total_tokens": 389
        },
        "why_needed": "This test prevents a regression where the plugin might not create nested directory structure for reports."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_creates_nested_directory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 50,
          "line_ranges": "78-79, 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330, 332, 334-335, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06500686500001507,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'error' key in the report should be set to 1 when there is an error.",
          "The 'summary' section of the report should contain an 'error' key with value 1.",
          "The 'error' value should match the number of times the test was run (in this case, once).",
          "The 'summary' section of the report should not be empty.",
          "The 'report.json' file should exist in the expected location."
        ],
        "scenario": "Test that fixture errors are captured in report.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 286,
          "total_tokens": 435
        },
        "why_needed": "This test prevents a regression where the error from a failed plugin hook is not properly reported."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_fixture_error_captured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 59,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 114,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.17385363000002485,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the report includes 'passed', 'failed', and 'skipped' outcomes.",
          "The test asserts that there are at least three types of outcomes in the report.",
          "The test checks if the report has a valid JSON structure with 'tests' and 'outcome' keys.",
          "The test ensures that the report does not contain any missing or invalid data.",
          "The test verifies that the report includes all test names (i.e., 'test_pass', 'test_fail', and 'test_skip').",
          "The test checks if the report has a correct order of outcomes (i.e., 'passed' before 'failed' and 'skipped').",
          "The test verifies that the report does not contain any duplicate outcomes."
        ],
        "scenario": "Test pytest_runtest_makereport captures all outcomes.",
        "token_usage": {
          "completion_tokens": 209,
          "prompt_tokens": 335,
          "total_tokens": 544
        },
        "why_needed": "This test prevents a potential regression where the report does not capture all outcomes of the tests."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_makereport_captures_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 250,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403-404, 558-559, 562-563, 566-568, 579, 583, 602-603, 619-620"
        }
      ],
      "duration": 0.05651765299998601,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'report_path.exists()', 'expected': {'status': 0, 'message': ''}, 'actual': {'status': 1, 'message': 'Report file not found'}}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 150,
          "total_tokens": 250
        },
        "why_needed": "The test is failing because the generated report exists."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.6173468859999787,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test should run without any errors or warnings indicating that the plugin was disabled.",
          "The test should exit with code 0 (success) if the plugin was successfully enabled.",
          "The test should verify that the JSON output is generated even when not asked for a report.",
          "The test should verify that passing only --llm-pdf works to trigger the plugin logic.",
          "The test should verify that the plugin key validation passes without any issues."
        ],
        "scenario": "Test that --llm-pdf option enables the plugin.",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 435,
          "total_tokens": 581
        },
        "why_needed": "To prevent a regression where the plugin is not enabled due to an error in Playwright."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_pdf_option_enables_plugin",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.060433451999983845,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'start_time' key should exist in the run_meta dictionary.",
          "The value of the 'start_time' key should be present in the run_meta dictionary.",
          "The 'start_time' value should match the current time when the test runs.",
          "The start time should be recorded correctly even if the test is not executed successfully.",
          "The report path should contain a 'start_time' key with a non-empty string value.",
          "The 'run_meta' dictionary should have a 'start_time' key with a datetime object value.",
          "The 'start_time' value should be in seconds since the epoch.",
          "The start time should not be affected by the test's execution status (e.g., failed, skipped)."
        ],
        "scenario": "Test that the pytest_sessionstart records start time is recorded correctly when running Pytest with --llm-report-json.",
        "token_usage": {
          "completion_tokens": 225,
          "prompt_tokens": 276,
          "total_tokens": 501
        },
        "why_needed": "This test prevents a regression where the start time of the session might not be recorded correctly if the report is generated without it."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_session_start_records_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007498880000014196,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 243,
          "prompt_tokens": 4096,
          "total_tokens": 4339
        },
        "why_needed": ""
      },
      "llm_context_override": "balanced",
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007691240000156085,
      "file_path": "tests/test_plugin_integration.py",
      "llm_opt_out": true,
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008190970000043762,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The requirement marker is used correctly."
        ],
        "scenario": "tests/test_plugin_integration.py",
        "token_usage": {
          "completion_tokens": 46,
          "prompt_tokens": 90,
          "total_tokens": 136
        },
        "why_needed": "To ensure that the requirement marker does not cause any errors."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
      "outcome": "passed",
      "phase": "call",
      "requirements": [
        "REQ-001",
        "REQ-002"
      ]
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 136,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04576336699997796,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the report.json file exists at the specified path.",
          "Check if the summary section of the report.json file contains the correct total and passed counts.",
          "Assert that the test_a.py nodeid is present in the HTML report.",
          "Verify that the test_b.py nodeid is also present in the HTML report.",
          "Check if the duration of each test case is correctly reported.",
          "Assert that the error message associated with test_b.py's failed outcome is 'AssertionError'.",
          "Verify that the total count of passed tests is 1, and the individual counts are correct."
        ],
        "scenario": "Test the integration of report writer with pytest_llm_report models and ReportWriter.",
        "token_usage": {
          "completion_tokens": 184,
          "prompt_tokens": 417,
          "total_tokens": 601
        },
        "why_needed": "This test prevents a regression where the report generation flow fails due to missing or corrupted JSON report files."
      },
      "nodeid": "tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586-587"
        }
      ],
      "duration": 0.001380230999984633,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytest_collectreport', 'expected_result': 'Mock object session.config.stash.get was not called'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 151,
          "total_tokens": 241
        },
        "why_needed": "This test is necessary to ensure that the collectreport plugin behaves correctly when it is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 12,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586, 590-592"
        }
      ],
      "duration": 0.0017022150000229885,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_report.called_once_with(mock_report)', 'description': 'The handle_collection_report method should be called once with mock_report as an argument.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 204,
          "total_tokens": 302
        },
        "why_needed": "To test the functionality of collecting reports when pytest_collectreport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0009509639999976116,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_type': 'Exception', 'message': 'pytest_collectreport is not available in this environment.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 138,
          "total_tokens": 219
        },
        "why_needed": "Because the plugin requires a valid session to function correctly."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0008949200000074597,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytest_collectreport', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 134,
          "total_tokens": 212
        },
        "why_needed": "Because the test is checking if collectreport skips when session is None."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0030539919999910126,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'LLM is not disabled', 'value': 'True'}",
          "{'name': 'LLM is enabled', 'value': 'True'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 143,
          "total_tokens": 251
        },
        "why_needed": "LLM is currently disabled by default. This test checks if the warning is raised when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 135,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 25,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-358, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002704806999986431,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected a UsageError to be raised', 'expected': 'UsageError', 'actual': 'pytest_llm_report.plugin'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 134,
          "total_tokens": 224
        },
        "why_needed": "Validation errors are raised when the plugin is configured with invalid values."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 17,
          "line_ranges": "328-330, 332-334, 336-338, 342-343, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012689309999984744,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_config.addinivalue_line.called', 'expected': 1, 'message': 'Expected mock_config.addinivalue_line to be called once'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 170,
          "total_tokens": 275
        },
        "why_needed": "To ensure that the configure function skips on xdist workers and does not attempt to add markers for them."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.003061365000007754,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_cfg.validate.asserts.return_value == []",
          "# Verify that validate() method returns an empty list when Config.load is missing"
        ],
        "scenario": "Test fallback to load_config if Config.load is missing.",
        "token_usage": {
          "completion_tokens": 317,
          "prompt_tokens": 747,
          "total_tokens": 1064
        },
        "why_needed": "This test prevents a potential bug where the plugin would attempt to load configuration from an empty Config object, causing an error."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 122,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002504501000004211,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'pyproject.toml contents with CLI options overridden'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 140,
          "total_tokens": 228
        },
        "why_needed": "CLI options override in pyproject.toml is necessary for plugin functionality."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 112,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.11577765299998077,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'File extension', 'value': '.pyproject.toml'}"
        ],
        "scenario": "Load configuration from PyProject.toml",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 136,
          "total_tokens": 204
        },
        "why_needed": "To test the plugin's ability to load configuration from a specific file format."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 9,
          "line_ranges": "399, 403-404, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013752710000005663,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mocked stash.get() with _enabled_key set to False",
          "mocked stash.get() with _enabled_key set to True"
        ],
        "scenario": "Test that terminal summary skips when plugin is disabled.",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 281,
          "total_tokens": 359
        },
        "why_needed": "Prevents regression in case the plugin is disabled and terminal summary is used."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "399-400, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009883039999749599,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 164,
          "total_tokens": 251
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 69,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0042843610000034005,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option is set to `"
        ],
        "scenario": "Test config loading from pytest objects (CLI) for maximal plugin functionality.",
        "token_usage": {
          "completion_tokens": 383,
          "prompt_tokens": 639,
          "total_tokens": 1022
        },
        "why_needed": "This test prevents regression in the maximal plugin's ability to load configuration files correctly, ensuring that the plugin can properly use pytest objects (CLI)."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::testload_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 7,
          "line_ranges": "558-559, 562-563, 566-568"
        }
      ],
      "duration": 0.001631461999977546,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Mocking', 'expected_mock': ['mock_item', 'mock_call'], 'actual_result': {'config': {'stash': {}}}}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 220,
          "total_tokens": 335
        },
        "why_needed": "The test is necessary because the `pytest_runtest_makereport` hookwrapper is not properly handling the generator when makereport is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0019707690000245748,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mock_collector` should be called with the provided `mock_report` when `pytest_runtest_makereport` is called.",
          "The `mock_item.config.stash.get(_enabled_key)` method should return `True` when `_enabled_key` is present in the stash.",
          "The `mock_item.config.stash.get(_collector_key)` method should return `mock_collector` when `_collector_key` is present in the stash.",
          "The `mock_call.send(mock_outcome)` call should not raise an exception if `mock_outcome.get_result.return_value` is `None`.",
          "The `mock_collector.handle_runtest_logreport` method should be called with the provided `mock_report` and `mock_item` when `pytest_runtest_makereport` is called."
        ],
        "scenario": "Test makereport calls collector when enabled.",
        "token_usage": {
          "completion_tokens": 220,
          "prompt_tokens": 371,
          "total_tokens": 591
        },
        "why_needed": "This test prevents a potential regression where the collector is not called when makereport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 602-603"
        }
      ],
      "duration": 0.0012918250000097942,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 358,
          "prompt_tokens": 149,
          "total_tokens": 507
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 602, 606-608"
        }
      ],
      "duration": 0.0017233539999779168,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_finish was called once with correct arguments', 'expected_args': ['[MagicMock(), MagicMock()]'], 'actual_args': [], 'assertion_type': 'method_call'}"
        ],
        "scenario": "TestPluginSessionHooks",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 219,
          "total_tokens": 309
        },
        "why_needed": "To test that collection_finish calls collector when enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 619-620"
        }
      ],
      "duration": 0.0013976930000012544,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_session.get.call_count', 'expected_value': 1, 'message': 'Expected mock_session.get to be called once.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 157,
          "total_tokens": 252
        },
        "why_needed": "To ensure that the pytest_sessionstart hook is properly disabled in the stash configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 11,
          "line_ranges": "558-559, 562, 566-568, 619, 623, 626, 628-629"
        }
      ],
      "duration": 0.0010864289999972243,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The key '_collector_key' should exist in the mock stash.",
          "The key '_start_time_key' should exist in the mock stash.",
          "A MagicMock instance with config.stash set to a MockStash instance should be created and assigned to pytest_sessionstart's config.",
          "The _collector_key and _start_time_key should both be present in the mock stash.",
          "The collector should have been successfully initialized when pytest_sessionstart is called on a session with enabled collectives.",
          "A KeyError or other exception should not be raised if pytest_sessionstart is called on a session without enable_collectives set to True."
        ],
        "scenario": "Test that sessionstart initializes collector when enabled and the collector is properly created.",
        "token_usage": {
          "completion_tokens": 191,
          "prompt_tokens": 335,
          "total_tokens": 526
        },
        "why_needed": "This test prevents a potential regression where the collector might not be initialized or created correctly if pytest_sessionstart is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002518436999991991,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')",
          "group.addoption.call_args_list[0][0].startswith('--llm-report')",
          "group.addoption.call_args_list[1][0].startswith('--llm-coverage-source')"
        ],
        "scenario": "Test pytest_addoption adds expected arguments to the parser.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 293,
          "total_tokens": 416
        },
        "why_needed": "This test prevents a bug where pytest_addoption does not add required arguments to the parser, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0025155320000180836,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'parser.addini was not called', 'expected_result': [], 'actual_result': 'parser.addini.called'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 140,
          "total_tokens": 226
        },
        "why_needed": "pytest_addoption no longer adds INI options"
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 53,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00311959499998693,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option is set to 'out.html'.",
          "The `stash` dictionary contains the correct stash data.",
          "The `Coverage` class is mocked correctly with a valid coverage report.",
          "The `MockStash` class is created and its `load` method is called.",
          "The `report` method of the `Coverage` class is called once.",
          "The `patched_pathlib.Path.exists` function returns True when it should return False.",
          "The `patched_coverage.Coverage.report` method is called once with a valid coverage report.",
          "The `pytest_llm_report.coverage_map.CoverageMapper` class is patched correctly."
        ],
        "scenario": "Test coverage percentage calculation logic for terminal summary.",
        "token_usage": {
          "completion_tokens": 189,
          "prompt_tokens": 395,
          "total_tokens": 584
        },
        "why_needed": "Prevents regression in coverage reporting when terminal summaries are generated."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 66,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-486, 491-494, 497, 499, 502-504, 512-514, 516, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0030440330000089943,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that `pytest_terminal_summary` is called once with correct configuration.",
          "Verify that `pytest_terminal_summary` passes a valid configuration to `llm.annotator.annotate_tests`.",
          "Verify that `pytest_llm_report.llm.annotator.annotate_tests` returns True for the provided configuration."
        ],
        "scenario": "Test that terminal summary with LLM enabled runs annotations correctly when provider is 'ollama'.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 477,
          "total_tokens": 598
        },
        "why_needed": "This test prevents regression where the plugin does not run annotations when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 45,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0019495690000042032,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The stash object passed to pytest_terminal_summary() does not contain any _enabled_key or _config_key.",
          "The stash object passed to pytest_terminal_summary() contains True for _enabled_key but False for _config_key.",
          "The stash object passed to pytest_terminal_summary() is empty.",
          "_enabled_key is present in the stash object, but its value is False.",
          "The stash object passed to pytest_terminal_summary() does not contain any coverage mapper.",
          "The stash object passed to pytest_terminal_summary() contains a CoverageMapper object with an empty map.",
          "The stash object passed to pytest_terminal_summary() has no _enabled_key or _config_key.",
          "_enabled_key is present in the stash object, but its value is False and _config_key is not present."
        ],
        "scenario": "Test terminal summary creates collector if missing.",
        "token_usage": {
          "completion_tokens": 204,
          "prompt_tokens": 391,
          "total_tokens": 595
        },
        "why_needed": "Prevents regression where terminal summary is not collecting metrics."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 21,
          "line_ranges": "399, 403, 407, 410-411, 413-414, 417-418, 420, 422-426, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0032675920000144743,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` option should be set to `/agg` for aggregation to work correctly.",
          "The `get()` method of the stash should return a report when aggregation is enabled.",
          "The `[]` index should also return a report when aggregation is enabled.",
          "The `ReportWriter` class should write JSON and HTML files when aggregation is enabled.",
          "The `Aggregator` class should aggregate reports correctly when aggregation is enabled.",
          "The `aggregate()` method of the Aggregator class should be called once with no arguments when aggregation is enabled.",
          "The `get()` method of the stash should return a report when aggregation is enabled and the stash has both `_enabled_key` and `_config_key` set to True.",
          "The `write_json()` and `write_html()` methods of the ReportWriter class should be called once with no arguments when aggregation is enabled."
        ],
        "scenario": "Test terminal summary with aggregation enabled.",
        "token_usage": {
          "completion_tokens": 225,
          "prompt_tokens": 441,
          "total_tokens": 666
        },
        "why_needed": "Prevents regression in aggregation functionality when terminal summary is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 52,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 476-479, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00519750500001237,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `load` method of `CoverageMapper` raises an `OSError` with message 'Disk full'.",
          "A warning is raised when trying to compute coverage percentage without loading the coverage map.",
          "The `report_writer` does not raise any warnings or errors during execution.",
          "The `pytest_terminal_summary` function returns a mock object that matches the expected behavior.",
          "The `MagicMock()` instance passed as an argument to `pytest_terminal_summary` is not modified by the test.",
          "The `mock_config.stash` dictionary contains the correct key-value pairs for the plugin configuration.",
          "The `mock_cov_cls.return_value` attribute of the mock object returns a new instance of `CoverageMapper` with the expected behavior.",
          "The `mock_cov.load.side_effect` attribute is set to an instance of `OSError` with the specified message."
        ],
        "scenario": "Test coverage calculation error when loading coverage map during terminal summary.",
        "token_usage": {
          "completion_tokens": 235,
          "prompt_tokens": 389,
          "total_tokens": 624
        },
        "why_needed": "This test prevents a regression where the plugin fails to calculate coverage percentages due to an OSError during load."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 63,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.006729460000002518,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'utils.py' file should be present in the assembled source code.",
          "The 'def util()' function should be found in the 'utils.py' file within the assembled source code.",
          "The ContextAssembler should assemble a balanced context configuration for the given test result."
        ],
        "scenario": "Tests the ContextAssembler with a balanced context configuration.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 331,
          "total_tokens": 448
        },
        "why_needed": "This test prevents regressions where the ContextAssembler is used with an unbalanced context configuration, which can lead to incorrect coverage metrics."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139-140, 268-272"
        }
      ],
      "duration": 0.0009623859999976503,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'def test_1():\\n    pass\\n'}"
        ],
        "scenario": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 176,
          "total_tokens": 271
        },
        "why_needed": "To assemble a complete context for the test 'test_a.py::test_1' and verify that it contains the expected code."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 30,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0010156159999894498,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The source of the assembled context should contain only the specified test function.",
          "The context object should be empty (i.e., no additional code or variables are present).",
          "The 'test_1' assertion should be present in the source code of the assembled context."
        ],
        "scenario": "Verifies that the ContextAssembler can assemble a minimal context for a test file with a single test function.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 267,
          "total_tokens": 397
        },
        "why_needed": "This test prevents regression when using the 'minimal' llm_context_mode, as it ensures that only necessary code is included in the assembly."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 46,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.001087620999982164,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'f1.py' file is present in the assembled context.",
          "The length of the 'f1.py' file does not exceed 40 bytes (20 bytes + truncation message)."
        ],
        "scenario": "Verify that the ContextAssembler does not exceed the specified context limits when assembling a test file.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 335,
          "total_tokens": 444
        },
        "why_needed": "This test prevents potential issues where the ContextAssembler exceeds the specified context limit, causing unexpected behavior or errors in the assembly process."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 50,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0010806180000031418,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file content is preserved and can be accessed through the assembled context.",
          "The 'truncated' key in the context does not exist.",
          "The file path of the content is correct (i.e., 'f1.py')",
          "The length of the content matches its original value (20 bytes)",
          "The coverage report includes all lines and counts as expected",
          "The test result indicates that the assembly was successful (outcome='passed')"
        ],
        "scenario": "Test that 'complete' mode does not truncate long files despite a small context size limit.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 361,
          "total_tokens": 527
        },
        "why_needed": "This test prevents a bug where the ContextAssembler truncates long files when the context size is too small, potentially leading to incorrect results or data loss."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_complete_context_limits_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 26,
          "line_ranges": "33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0010578859999839096,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_get_test_source` returns an empty string for the given input.",
          "The function `_get_test_source` does not raise any exceptions for the given input."
        ],
        "scenario": "Verify the function `_get_test_source` returns an empty string when given a non-existent file.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 275,
          "total_tokens": 386
        },
        "why_needed": "This test prevents a potential bug where the function `_get_test_source` throws a `ValueError` or raises an exception when given a non-existent file path."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 5,
          "line_ranges": "33, 284-287"
        }
      ],
      "duration": 0.0014659909999750198,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert assembler._should_exclude('test.pyc') is True",
          "assert assembler._should_exclude('secret/key.txt') is True",
          "assert assembler._should_exclude('public/readme.md') is False"
        ],
        "scenario": "The test verifies that the ContextAssembler should exclude certain files from the llm context.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 227,
          "total_tokens": 337
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly excludes files that are not actually present in the llm context."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_should_exclude",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        }
      ],
      "duration": 0.0009532689999787181,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context_files == {}",
          "def test_foo() in test_source"
        ],
        "scenario": "Test assemble in minimal mode returns no context files.",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 298,
          "total_tokens": 369
        },
        "why_needed": "This test prevents regression where the assemble function does not generate any context files when run in minimal mode."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_minimal_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 62,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0012276440000107414,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the assembler uses the specified context override.",
          "Check if the module file is included in the context files.",
          "Ensure that the coverage entry points are correct for the module file.",
          "Verify that the llm_context_override is respected and not overridden by other factors.",
          "Test that the assembly process correctly overrides the default mode with the specified override.",
          "Confirm that the test passes even when the llm_context_override is set to 'balanced' but the assembly process overrides it with a different mode."
        ],
        "scenario": "Test assemble respects llm_context_override from test.",
        "token_usage": {
          "completion_tokens": 168,
          "prompt_tokens": 362,
          "total_tokens": 530
        },
        "why_needed": "This test prevents regression that occurs when llm_context_override is set to 'balanced' but the assembly process overrides it with a different mode."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_with_context_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 20,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163-164, 201, 284-286"
        }
      ],
      "duration": 0.00107891599998311,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'secret_config.py' should not be included in the balanced context.",
          "No files should be matched against the exclude pattern '*secret*'.",
          "The coverage entry for 'secret_config.py' should have a line count of 1, indicating it is only executed once.",
          "The LLM context mode 'balanced' should exclude all files matching the specified exclude patterns.",
          "No files should match any glob patterns in the LLM context mode 'balanced'.",
          "The coverage entry for the test file should not have any line ranges or line counts.",
          "The LLM context mode 'balanced' should only include files that do not match any exclude patterns."
        ],
        "scenario": "Test balanced context excludes files matching exclude patterns.",
        "token_usage": {
          "completion_tokens": 197,
          "prompt_tokens": 331,
          "total_tokens": 528
        },
        "why_needed": "This test prevents a regression where the LLM context mode 'balanced' would include files that match the specified exclude patterns."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_excludes_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 16,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-161, 201"
        }
      ],
      "duration": 0.0008423610000249937,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context is empty', 'expected': {}, 'actual': {}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 201,
          "total_tokens": 286
        },
        "why_needed": "To ensure that the ContextAssembler correctly handles cases where a balanced context file does not exist."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.014066901000006737,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The content of the source file is truncated to prevent excessive memory usage.",
          "A message indicating truncation ('truncated') is included in the context.",
          "The total length of the content does not exceed the specified limit (120 bytes).",
          "The 'large_module.py' file itself contains a message indicating truncation ('truncated')."
        ],
        "scenario": "Test that balanced context respects max bytes limit.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 405,
          "total_tokens": 531
        },
        "why_needed": "Prevents a potential memory leak or unexpected behavior due to the large module size exceeding the allocated bytes limit."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_max_bytes_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 139-140"
        }
      ],
      "duration": 0.000832733999999391,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'context is empty', 'expected': {}, 'actual': {}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 162,
          "total_tokens": 244
        },
        "why_needed": "To ensure that the ContextAssembler can correctly assemble a balanced context with no coverage."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 35,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-157, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0012705149999874266,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context should be either empty (truncated) or contain only one file (non-truncated).",
          "context should not exceed the specified max_bytes limit.",
          "context should not contain more than llm_context_file_limit files. If it does, the test should fail with an appropriate error message."
        ],
        "scenario": "Test that loop exits when max bytes is reached before processing file and the context is truncated.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 409,
          "total_tokens": 548
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler exceeds the maximum allowed bytes in the LLM context without encountering any errors, potentially leading to incorrect coverage metrics or unexpected behavior."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_reaches_max_bytes_before_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0010754689999998845,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'includes', 'expression': 'module.py', 'value': 'True'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 211,
          "total_tokens": 309
        },
        "why_needed": "Complete context delegates to balanced because it requires a full context for all node assertions, which can be complex and error-prone."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 9,
          "line_ranges": "33, 78-79, 82-83, 86-89"
        }
      ],
      "duration": 0.0008835590000160209,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The result of the test should be an empty string.', 'expected_result': '', 'type': 'assertion'}"
        ],
        "scenario": "Test _get_test_source with empty nodeid returns empty string",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 148,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the function correctly handles an empty nodeid and returns an empty string."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_empty_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.0009446820000107436,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected source extraction to stop at next function definition', 'description': 'The code should contain a `def` statement immediately after a test function.', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 129,
          "total_tokens": 246
        },
        "why_needed": "To ensure that source extraction stops at the next function definition, even if it's not immediately following a test function."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 6,
          "line_ranges": "33, 78-79, 82-84"
        }
      ],
      "duration": 0.0008167229999855863,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The result of _get_test_source is an empty string when the input file does not exist.', 'expected_result': ''}"
        ],
        "scenario": "Tests for the _get_test_source method",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 142,
          "total_tokens": 229
        },
        "why_needed": "To ensure that the _get_test_source method handles cases where a test source file does not exist."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.0009534100000223589,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'indentation': 4, 'expected_lines': ['def foo(): pass']}, 'actual': {'indentation': 1, 'expected_lines': []}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 118,
          "total_tokens": 224
        },
        "why_needed": "This test is necessary to ensure that the _get_test_source function correctly extracts functions with proper indentation."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000799991000008049,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': ['1-3'], 'actual': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 106,
          "total_tokens": 175
        },
        "why_needed": "To ensure that consecutive lines are compressed correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0007806050000169762,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equals', 'expected_value': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 107,
          "total_tokens": 175
        },
        "why_needed": "To handle duplicate ranges in the input data."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "29-30"
        }
      ],
      "duration": 0.0007684619999963616,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'the function should return an empty string for an empty list', 'expected_result': '', 'message': 'Expected the function to return an empty string, but got [insert actual result here]'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 92,
          "total_tokens": 201
        },
        "why_needed": "The test is necessary because the current implementation of `compress_ranges` does not handle an empty input list correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0007840220000048248,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3, 5, 10-12, 15', 'actual': '1-3, 5, 10-12, 15'}",
          "{'expected': 'True', 'actual': 'True'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 130,
          "total_tokens": 238
        },
        "why_needed": "To test the ability of the `compress_ranges` function to handle mixed ranges and singles."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007611590000067281,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': '1, 3, 5', 'actual_result': '1, 3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 113,
          "total_tokens": 193
        },
        "why_needed": "Non-consecutive lines should be comma-separated."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "29, 33, 35-37, 39, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007750439999938408,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '5', 'actual_value': 'True'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_single_line",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 96,
          "total_tokens": 174
        },
        "why_needed": "This test ensures that the `compress_ranges` function does not attempt to compress a single line of numbers."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000789701999991621,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert compress_ranges([1, 2]) == '1-2'"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 103,
          "total_tokens": 168
        },
        "why_needed": "To test that two consecutive lines are compressed to a single range."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.000764433999989933,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3, 5', 'actual': '1-3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 110,
          "total_tokens": 187
        },
        "why_needed": "To ensure the function correctly handles unsorted input ranges."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "81-82"
        }
      ],
      "duration": 0.0007867070000031617,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert expand_ranges returns an empty list for an empty string', 'description': 'The output of the `expand_ranges` function should be an empty list when given an empty string as input.'}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 90,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the `expand_ranges` function handles empty strings correctly."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 11,
          "line_ranges": "81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007856639999772597,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3, 5, 10, 11, 12], 'actual': ['1', '2', '3', '5', '10', '11', '12']}",
          "{'expected': [], 'actual': []}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_mixed",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 121,
          "total_tokens": 240
        },
        "why_needed": "This test is necessary because the current implementation of `expand_ranges` only handles ranges and does not handle singles correctly."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_mixed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "81, 84-91, 95"
        }
      ],
      "duration": 0.0007572619999791641,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3], 'actual': '1-3'}",
          "{'expected': ['1', '2', '3'], 'actual': '[1, 2, 3]'}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_range",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 99,
          "total_tokens": 192
        },
        "why_needed": "The range function should expand to a list of integers."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_range",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 27,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95"
        }
      ],
      "duration": 0.000759364999993295,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'original and compressed should be equal', 'expected_value': [1, 2, 3, 5, 10, 11, 12, 15], 'actual_value': [1, 2, 3, 5, 10, 11, 12, 15]}"
        ],
        "scenario": "test_roundtrip",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 134,
          "total_tokens": 262
        },
        "why_needed": "To ensure that `compress_ranges` and `expand_ranges` are inverses."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 7,
          "line_ranges": "81, 84-87, 93, 95"
        }
      ],
      "duration": 0.0007611579999888818,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': [5], 'actual_value': ['5']}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_single_number",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 95,
          "total_tokens": 170
        },
        "why_needed": "The function `expand_ranges` is expected to handle a single input, producing a single output."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_single_number",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65, 67"
        }
      ],
      "duration": 0.0007637530000010884,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return '500ms' when input is 0.5 seconds.",
          "The function should return '1ms' when input is 0.001 seconds.",
          "The function should return '0ms' when input is 0.0 seconds."
        ],
        "scenario": "Test the format_duration function for milliseconds when input is less than 1 second.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 211,
          "total_tokens": 331
        },
        "why_needed": "Prevents a potential bug where the function does not correctly format durations in milliseconds when input is less than 1 second."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65-66"
        }
      ],
      "duration": 0.0007971459999964736,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert format duration of 1.23s returns '1.23s'\", 'expected': '1.23s', 'actual': 'format_duration(1.23)'}",
          "{'name': \"assert format duration of 60.0s returns '60.00s'\", 'expected': '60.00s', 'actual': 'format_duration(60.0)'}"
        ],
        "scenario": "tests/test_render.py::TestFormatDuration::test_seconds",
        "token_usage": {
          "completion_tokens": 153,
          "prompt_tokens": 116,
          "total_tokens": 269
        },
        "why_needed": "To ensure the `format_duration` function correctly formats durations in seconds for values greater than or equal to 1 second."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0007711279999966791,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "outcome-to-css-class mapping is correct for each outcome.",
          "outcome-to-css-class mapping preserves the original outcome value.",
          "outcome-to-css-class mapping handles skipped outcomes correctly.",
          "outcome-to-css-class mapping handles xfailed and xpassed outcomes correctly.",
          "outcome-to-css-class mapping handles error outcome correctly.",
          "outcome-to-css-class mapping preserves the correct CSS class for passed, failed, and skipped outcomes."
        ],
        "scenario": "Test that all outcomes map to CSS classes.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 263,
          "total_tokens": 393
        },
        "why_needed": "Prevents regression in rendering CSS classes for different outcomes."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0008259000000236938,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'outcome-unknown', 'actual': 'outcome-unknown'}"
        ],
        "scenario": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 102,
          "total_tokens": 175
        },
        "why_needed": "Unknown outcomes are not handled correctly and may cause unexpected styling."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65-67, 79-85, 87, 121-124, 126-127, 131-132, 155-157, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008454570000253625,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert '<!DOCTYPE html>' in html",
          "assert 'Test Report' in html",
          "assert 'test::passed' in html",
          "assert 'test::failed' in html",
          "assert 'PASSED' in html",
          "assert 'FAILED' in html",
          "assert 'Plugin:</strong> v0.1.0' in html",
          "assert 'Repo:</strong> v1.2.3' in html"
        ],
        "scenario": "Test renders basic report with fallback HTML.",
        "token_usage": {
          "completion_tokens": 153,
          "prompt_tokens": 426,
          "total_tokens": 579
        },
        "why_needed": "This test prevents a rendering issue where the full HTML document is not rendered correctly due to plugin or repository version issues."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-129, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008072050000009767,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report includes the file \"src/foo.py\" which is part of the test.",
          "The assertion checks that there are exactly 5 lines in the rendered HTML.",
          "The assertion verifies that the file name 'src/foo.py' is present in the rendered HTML.",
          "The assertion ensures that the number of lines in the rendered HTML matches the coverage report."
        ],
        "scenario": "Test renders coverage for fallback HTML test.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 288,
          "total_tokens": 409
        },
        "why_needed": "Prevents rendering of non-coverage files and ensures accurate coverage reporting."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 64,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 140-142, 144, 147, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008125350000227627,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Tests login flow",
          "Prevents auth bypass",
          "Confidence: 85%",
          "Expected HTML content includes the following strings: 'Tests login flow', 'Prevents auth bypass', and 'Confidence: 85%'."
        ],
        "scenario": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 317,
          "total_tokens": 427
        },
        "why_needed": "This test prevents LLM annotation rendering from failing due to missing confidence scores."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 68,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-178, 180-186, 191, 206, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008598039999867524,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Source Coverage summary should be present in the rendered HTML.",
          "Source code file path should be included in the HTML.",
          "Percentage of covered statements should be displayed in the HTML.",
          "Percentage of missed statements should be displayed in the HTML.",
          "Covered ranges should be correctly formatted in the HTML.",
          "Missed ranges should be correctly formatted in the HTML."
        ],
        "scenario": "The test verifies that the source coverage summary is included in the rendered HTML.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 331,
          "total_tokens": 465
        },
        "why_needed": "This test prevents a regression where the source coverage information is not displayed in the rendered HTML."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 55,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008224640000094041,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string 'XFailed' should be present in the rendered HTML.",
          "The string 'XPassed' should be present in the rendered HTML.",
          "Both 'XFailed' and 'XPassed' summary entries should be included in the rendered HTML."
        ],
        "scenario": "The test verifies that the 'XFailed' and 'XPassed' summary entries are included in the rendered HTML.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 283,
          "total_tokens": 410
        },
        "why_needed": "This test prevents a regression where the xfailed/xpassed summary is missing from the rendered HTML, potentially misleading users about the test results."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007864759999733906,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Hashes should be different', 'expected': {'hash1': '...'}}",
          "{'name': 'Hashes should be different', 'expected': {'hash2': '...'}}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 115,
          "total_tokens": 215
        },
        "why_needed": "To ensure that different content produces different hashes."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007587739999905807,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected hash to be equal', 'expected_value': '', 'actual_value': ''}",
          "{'message': 'Expected length of hash to be 64', 'expected_value': 64, 'actual_value': 64}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 129,
          "total_tokens": 233
        },
        "why_needed": "To ensure that the compute_sha256 function produces consistent hashes for empty byte sequences."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 72,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307"
        }
      ],
      "duration": 0.009719281000002411,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert meta.duration == 60.0: The duration of the build should be 60 seconds.",
          "assert meta.pytest_version: The pytest version should have a value.",
          "assert meta.plugin_version == __version__: The plugin version should match the current version.",
          "assert meta.python_version: The python version should have a value."
        ],
        "scenario": "Test ReportWriter::test_build_run_meta verifies that the build run meta includes version info.",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 318,
          "total_tokens": 450
        },
        "why_needed": "This test prevents regression where the report writer does not include version information in the build run meta."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_run_meta",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 19,
          "line_ranges": "156-158, 319, 321-322, 324-335, 337"
        }
      ],
      "duration": 0.0008427520000111599,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of outcomes should match the sum of passed, failed, skipped, xfailed, xpassed, error outcome types.",
          "Each outcome type (passed, failed, skipped, xfailed, xpassed, error) should be correctly counted in the summary.",
          "The `x` prefix for each outcome type indicates that it has an unknown number of tests.",
          "If a test is not included in any outcome type, its result should still be reported as 'error'.",
          "If all tests are skipped or ignored, the total count should remain 0."
        ],
        "scenario": "Test verifies that the `build_summary` method correctly counts all outcome types and their corresponding tests.",
        "token_usage": {
          "completion_tokens": 182,
          "prompt_tokens": 336,
          "total_tokens": 518
        },
        "why_needed": "This test prevents regression where a test might be skipped or ignored due to an incorrect count of outcome types."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 319, 321-322, 324-329, 337"
        }
      ],
      "duration": 0.0008030870000084178,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of tests should be equal to 4 (tests1, tests2, tests3, and tests4).",
          "The number of passed tests should be 2 (tests1 and tests3).",
          "The number of failed tests should be 1 (test3).",
          "The number of skipped tests should be 1 (test4)."
        ],
        "scenario": "Test that the `build_summary_counts` method counts outcomes correctly.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 283,
          "total_tokens": 420
        },
        "why_needed": "This test prevents a regression where the total count of passed, failed, and skipped tests is not accurate."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_counts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "156-158"
        }
      ],
      "duration": 0.0007650560000058704,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` attribute of the `ReportWriter` instance should be set to the provided `Config` object.",
          "The `warnings` attribute of the `ReportWriter` instance should be an empty list.",
          "The `artifacts` attribute of the `ReportWriter` instance should be an empty list."
        ],
        "scenario": "Test that a new ReportWriter instance initializes correctly with its configuration.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 199,
          "total_tokens": 317
        },
        "why_needed": "This test prevents a potential bug where the writer's configuration is not properly initialized."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_create_writer",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337"
        }
      ],
      "duration": 0.00959163200002422,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the report.tests list should be equal to 2.",
          "The value of report.summary.total should be equal to 2."
        ],
        "scenario": "Test writes a report that includes all tests.",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 255,
          "total_tokens": 336
        },
        "why_needed": "This test prevents regression by ensuring the ReportWriter can write reports with at least two tests."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.010380010999995193,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'report.summary.coverage_total_percent == 85.5', 'description': 'The total coverage percentage of the report should be equal to 85.5%'}"
        ],
        "scenario": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 132,
          "total_tokens": 237
        },
        "why_needed": "The test is necessary to ensure that the ReportWriter class correctly includes the total coverage percentage in the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 97,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.009380475000000388,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `report.source_coverage` should be 1.",
          "The file path of the first `source_coverage` entry in `report.source_coverage` should match `"
        ],
        "scenario": "Test ReportWriter::test_write_report_includes_source_coverage verifies that the report includes source coverage summary.",
        "token_usage": {
          "completion_tokens": 237,
          "prompt_tokens": 291,
          "total_tokens": 528
        },
        "why_needed": "This test prevents a regression where the report does not include source coverage information, which is crucial for understanding the code's quality and maintainability."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 99,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337"
        }
      ],
      "duration": 0.009458210000019562,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report should have at least one coverage entry for the first test.",
          "The file path of the first coverage entry should match the file path of the first test.",
          "Each coverage entry in the report should be a single object with 'file_path' and 'coverage' attributes."
        ],
        "scenario": "Test ReportWriter::test_write_report_merges_coverage verifies that the report writer merges coverage into tests.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 285,
          "total_tokens": 410
        },
        "why_needed": "This test prevents a regression where the report writer does not merge coverage from individual tests to the overall report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513-514, 516-519, 522-523"
        }
      ],
      "duration": 0.010693316999976332,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file `report.json` should exist at the specified path.",
          "Any warning messages written by the ReportWriterWithFiles class should have code 'W203'."
        ],
        "scenario": "Test that the ReportWriterWithFiles class falls back to direct write if atomic write fails and writes warnings.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 276,
          "total_tokens": 386
        },
        "why_needed": "The test prevents a regression where the atomic write operation fails, causing the report writer to fall back to direct write and potentially writing unnecessary or misleading warnings."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 128,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.011128468000009661,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Directory exists after writing report', 'expected': True, 'actual': 'exists'}"
        ],
        "scenario": "Test case 'tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing'",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 171,
          "total_tokens": 258
        },
        "why_needed": "The test writer should create an output directory if it does not exist."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 12,
          "line_ranges": "156-158, 477-480, 487-491"
        }
      ],
      "duration": 0.0012295069999765929,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer.warnings` list contains any warnings with code 'W201' (indicating permission denied error).",
          "The `writer.warnings` list does not contain any warnings with code 'W100' (indicating directory creation failure).",
          "The `writer.warnings` list is empty if the directory cannot be created.",
          "The `writer.warnings` list contains at least one warning with code 'W201'."
        ],
        "scenario": "Verify that the `test_ensure_dir_failure` test captures a warning when creating a non-existent directory.",
        "token_usage": {
          "completion_tokens": 158,
          "prompt_tokens": 278,
          "total_tokens": 436
        },
        "why_needed": "This test prevents a potential bug where the report writer fails to capture warnings for directories that cannot be created."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.001193830999994816,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_git_info()` should return an empty string for SHA and dirty flags if the `git` command fails.",
          "The function `get_git_info()` should not raise an exception if the `git` command is successful but returns no Git repository information.",
          "The function `get_git_info()` should handle the case where the `git` command fails with a specific error message ('Git not found') and return None for SHA and dirty flags.",
          "The function `get_git_info()` should handle the case where the `git` command fails but returns an empty string for SHA and dirty flags (e.g., when running on a system without Git installed).",
          "The function `get_git_info()` should not raise an exception if the `git` command is successful but returns no Git repository information, indicating that it cannot retrieve any information.",
          "The function `get_git_info()` should be able to handle different types of errors raised by the `git` command (e.g., permission issues, invalid options).",
          "The test should be able to pass even if the `git` command fails and no Git repository information can be retrieved due to external system limitations."
        ],
        "scenario": "Test 'Should handle git command failures gracefully.'",
        "token_usage": {
          "completion_tokens": 297,
          "prompt_tokens": 231,
          "total_tokens": 528
        },
        "why_needed": "To prevent a test failure when the `git` command fails and no Git repository information can be retrieved."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 120,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.0455059959999744,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report.html file should exist.",
          "The report.html file should contain the expected content (test1, test2, PASSED, FAILED, Skipped, XFailed, XPassed, Errors).",
          "Test1 and Test2 should be present in the HTML content.",
          "PASSED and FAILED should be present in the HTML content.",
          "Skipped and XFailed/XPassed should be present in the HTML content.",
          "Errors should be present in the HTML content."
        ],
        "scenario": "Test verifies that the report writer creates an HTML file with expected content.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 366,
          "total_tokens": 530
        },
        "why_needed": "This test prevents a regression where the report writer fails to create an HTML file even when there are tests that fail or skip."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-333, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04546242399999301,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test should find the string 'XFAILED' and 'XFailed' in the HTML content.",
          "The test should find the string 'XPASSED' and 'XPassed' in the HTML content.",
          "The test should assert that both 'XFAILED' and 'XFailed' are present in the HTML content.",
          "The test should assert that both 'XPASSED' and 'XPassed' are present in the HTML content.",
          "The test should verify that the report writer correctly includes xfail outcomes in the HTML summary."
        ],
        "scenario": "Test 'test_write_html_includes_xfail_summary' verifies that the report writer includes xfail outcomes in the HTML summary.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 308,
          "total_tokens": 489
        },
        "why_needed": "This test prevents regression of the issue where xfail outcomes are not included in the HTML summary."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.010343522999988863,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file should exist at the expected path.",
          "At least one artifact should be tracked in the JSON file."
        ],
        "scenario": "Test verifies that the `ReportWriter` creates a JSON file with the specified configuration.",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 265,
          "total_tokens": 354
        },
        "why_needed": "This test prevents regression where the `ReportWriter` fails to create a JSON file even when the report has an artifact."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-430, 441-442, 444-450, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.048783588999981475,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.pdf` path should be created in the temporary directory.",
          "Any artifacts written to the `report.pdf` path should match its original path.",
          "The `report.pdf` path should exist after the test completes.",
          "The `report.pdf` path should not be empty or None.",
          "The `writer.artifacts` list should contain any artifact with a matching path.",
          "Any artifacts written to the temporary directory should have a matching path in the `report.pdf` path."
        ],
        "scenario": "Test that `write_pdf` creates a PDF file when Playwright is available.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 478,
          "total_tokens": 642
        },
        "why_needed": "This test prevents regression where the report writer does not create a PDF file even if Playwright is available."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 103,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408-412, 415"
        }
      ],
      "duration": 0.010335637999986602,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pdf_path` should exist after writing the report.",
          "At least one warning with code `W204_PDF_PLAYWRIGHT_MISSING` should be present in the `writer.warnings` list."
        ],
        "scenario": "Test verifies that a warning is raised when Playwright is missing for PDF output.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 311,
          "total_tokens": 416
        },
        "why_needed": "This test prevents a potential bug where the report writer does not warn users about missing Playwright for PDF output."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.0026556139999911466,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function should return None for a nonexistent path.', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 123,
          "total_tokens": 210
        },
        "why_needed": "To ensure that the report writer can handle cases where git info is not available."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 16,
          "line_ranges": "67-74, 76-81, 83-84"
        }
      ],
      "duration": 0.009545294999981024,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is None or isinstance(sha, str)', 'expected_result': {'type': 'NoneType', 'message': 'Expected `get_git_info` to return None or a string'}, 'actual_result': {'type': 'AssertionError'}}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 159,
          "total_tokens": 289
        },
        "why_needed": "To ensure the `get_git_info` function returns a valid Git SHA or an error message when running in a non-Git repository."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0010185820000003787,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `sha` variable will be either None (if the fallback succeeds) or an empty string (if the fallback fails).",
          "The `dirty` variable will always be False because it is not modified during the test.",
          "The `get_plugin_git_info()` function will return a string representation of a Git hash if `_git_info` import fails and the plugin's `get_plugin_git_info()` function returns an empty string or a string representation of a Git hash.",
          "The `sha` variable will be None after calling `get_plugin_git_info()` because it is not modified during this test.",
          "The `dirty` variable will always be False because it is not modified during the test.",
          "The `get_plugin_git_info()` function will return an empty string or a string representation of a Git hash if `_git_info` import fails and the plugin's `get_plugin_git_info()` function returns None.",
          "The `sha` variable will be either None (if the fallback succeeds) or an empty string (if the fallback fails).",
          "The `dirty` variable will always be False because it is not modified during the test.",
          "The `get_plugin_git_info()` function will return a string representation of a Git hash if `_git_info` import fails and the plugin's `get_plugin_git_info()` function returns an empty string or a string representation of a Git hash."
        ],
        "scenario": "Tests that a fallback occurs when `_git_info` import fails and the plugin's `get_plugin_git_info()` function returns an empty string or a string representation of a Git hash.",
        "token_usage": {
          "completion_tokens": 373,
          "prompt_tokens": 253,
          "total_tokens": 626
        },
        "why_needed": "This test prevents regression where a fallback is not executed correctly when `_git_info` import fails, potentially causing unexpected behavior in the application."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0007846620000009352,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'returning a non-empty string for sha', 'description': 'The function should return either None or a non-empty string for the sha value.', 'expected_value': 'None'}",
          "{'name': 'returning a valid string for dirty', 'description': 'The function should return a valid string for the dirty value.', 'expected_value': 'False'}"
        ],
        "scenario": "test_get_plugin_git_info",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 141,
          "total_tokens": 274
        },
        "why_needed": "To ensure that the plugin's Git info returns some values."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_returns_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.010988763999989715,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The report file exists after writing', 'expected_result': 'True'}",
          "{'name': 'The report file is not empty after writing', 'expected_result': 'False'}"
        ],
        "scenario": "Test atomic write fallback",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 181,
          "total_tokens": 274
        },
        "why_needed": "To ensure the report writer can handle errors and still produce a valid report."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterAtomicWrite::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 28,
          "line_ranges": "156-158, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.12187301300002673,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer.warnings` list should contain a warning with code 'W201'",
          "The `writer.warnings` list should contain a warning with code 'W202'",
          "The `writer.warnings` list should contain a warning with code 'W203'",
          "The `writer.warnings` list should contain a warning with code 'W204'",
          "The `writer.warnings` list should contain a warning with code 'W205'",
          "The `writer.warnings` list should contain a warning with code 'W206'"
        ],
        "scenario": "Test PDF generation when playwright raises exception (lines 424-432) and expected a warning about PDF failure",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 356,
          "total_tokens": 531
        },
        "why_needed": "Prevents regression where PDF generation fails due to playwright exception without raising a warning"
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "156-158, 408-412, 415"
        }
      ],
      "duration": 0.0012265510000020186,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test should have a warning indicating that playwright is not installed.",
          "Any of the warnings in the report should contain 'W204'.",
          "The report should not be created because playwright is missing.",
          "The file 'report.pdf' should not exist at the expected location."
        ],
        "scenario": "Test that when playwright is not installed, a warning is generated about missing playwright and the report does not get created.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 293,
          "total_tokens": 416
        },
        "why_needed": "To prevent a potential bug where the PDF generation fails due to playwright not being installed."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 11,
          "line_ranges": "156-158, 455, 460, 462, 465-469"
        }
      ],
      "duration": 0.03539662299999691,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path created by the _resolve_pdf_html_source method exists and has the correct suffix (.html).",
          "The path is not empty.",
          "The suffix of the path is correctly set to .html."
        ],
        "scenario": "Test _resolve_pdf_html_source creates temp file when no HTML source is provided.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 265,
          "total_tokens": 374
        },
        "why_needed": "Prevents a potential issue where the report writer does not create a temporary PDF file if there are no HTML sources to resolve."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_creates_temp",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 455-457, 460, 462, 465-469"
        }
      ],
      "duration": 0.03596535099998732,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path to the resolved PDF report is not None and exists.",
          "The path to the resolved PDF report is not in the same directory as the temporary file.",
          "The resolved PDF report has the correct filename.",
          "The resolved PDF report does not contain any HTML content.",
          "The resolved PDF report is a valid PDF file with the correct metadata.",
          "The resolved PDF report is not corrupted or damaged."
        ],
        "scenario": "Test _resolve_pdf_html_source when configured HTML doesn't exist.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 270,
          "total_tokens": 415
        },
        "why_needed": "Prevents a bug where the test fails due to an incorrect assumption about the existence of a missing HTML file."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_missing_html_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 7,
          "line_ranges": "156-158, 455-458"
        }
      ],
      "duration": 0.0010208559999966837,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path to the existing HTML file is correctly set to `html_path`.",
          "The report writer correctly resolves the PDF source using `report`.",
          "The resolved path matches the expected value of `html_path`.",
          "The report is not created as a temporary file (`is_temp` is False)."
        ],
        "scenario": "Test _resolve_pdf_html_source uses existing HTML file.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 266,
          "total_tokens": 389
        },
        "why_needed": "Prevents a potential bug where the report writer does not use an existing HTML file when resolving the PDF source."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_uses_existing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008858930000030796,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert schema.scenario == 'Verify login'",
          "assert schema.why_needed == 'Catch auth bugs'",
          "assert schema.key_assertions == ['assert 200', 'assert token']",
          "assert schema.confidence == 0.95"
        ],
        "scenario": "Test that `AnnotationSchema.from_dict` correctly creates an annotation from a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 276,
          "total_tokens": 395
        },
        "why_needed": "Prevents regression in case of missing or malformed input data, ensuring the application can handle invalid inputs without crashing."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 8,
          "line_ranges": "90-92, 94-98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007960140000022875,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in data",
          "assert 'why_needed' in data",
          "assert 'key_assertions' in data",
          "assert isinstance(data['scenario'], str)",
          "assert isinstance(data['why_needed'], str)",
          "assert all(isinstance(x, str) for x in data['key_assertions'])"
        ],
        "scenario": "Should convert to dictionary with all fields.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 273,
          "total_tokens": 385
        },
        "why_needed": "Prevent regression in schema validation when using to_dict method for complex schemas."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09481003999999871,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file path for the report is correct and exists.",
          "The content of the report contains the expected string '<html'.",
          "The name 'test_simple' is present in the report content."
        ],
        "scenario": "The HTML report is created and exists as expected.",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 264,
          "total_tokens": 363
        },
        "why_needed": "This test prevents a potential bug where the HTML report might not be generated correctly or exist even after running the tests."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 69,
          "line_ranges": "78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-335, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.13463038599999777,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `assert_summary(labels: list[str], expected: int)` checks if each label in the `labels` list matches the corresponding count in the HTML report.",
          "It also checks for any missing summary labels and raises an assertion error with a descriptive message.",
          "For example, it will check if 'Total Tests' is present in the HTML report and its count matches the expected value of 6.",
          "Similarly, it will verify that each label (e.g., 'Passed', 'Failed', etc.) has a corresponding count in the HTML report.",
          "If any label is missing or does not match its expected count, it raises an assertion error with a message indicating which labels are missing and why.",
          "The test ensures that all statuses (including 'XFailed' and 'XPassed') are included in the summary counts.",
          "It also checks for any errors or exceptions raised during the execution of the `test_error` function.",
          "In case of an error, it raises an assertion error with a descriptive message indicating where the error occurred."
        ],
        "scenario": "test_html_summary_counts_all_statuses verifies that the HTML summary counts include all statuses.",
        "token_usage": {
          "completion_tokens": 280,
          "prompt_tokens": 621,
          "total_tokens": 901
        },
        "why_needed": "This test prevents regression where the HTML summary does not include all statuses, potentially leading to incorrect reporting or misleading results."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 55,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06970863999998755,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report path exists and contains the expected data.",
          "The schema version of the report matches the expected value.",
          "The total number of tests passed is correct (2 in this case).",
          "At least one test failed, which is correctly reported as such.",
          "All test names are included in the summary."
        ],
        "scenario": "The JSON report is created successfully.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 295,
          "total_tokens": 412
        },
        "why_needed": "This test prevents a potential bug where the report generation fails due to missing or incorrect configuration files."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 55,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 43,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 103,
          "line_ranges": "130-133, 135-137, 139, 141, 143, 190, 194-199, 201, 203, 205, 207, 210, 212-214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419-437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-506, 509, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06178270399999519,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_pass()` returns True.",
          "The 'why_needed' assertion checks if the annotation is present and prevents regressions.",
          "The 'key_assertions' list includes assertions related to the annotation presence.",
          "The 'choices' attribute of the mock completion function contains a message that matches the expected annotation content.",
          "The 'content' attribute of the chosen message in the mock completion function matches the expected annotation content.",
          "The 'message' attribute of the chosen message in the mock completion function is set to the expected annotation content.",
          "The 'choices' attribute of the mock completion function returns a list that includes an instance with the expected annotation content.",
          "The test passes if all assertions pass, preventing regressions."
        ],
        "scenario": "Verify that LLM annotations are included in the report when a provider is enabled.",
        "token_usage": {
          "completion_tokens": 208,
          "prompt_tokens": 385,
          "total_tokens": 593
        },
        "why_needed": "Prevent regressions by ensuring LLM annotations are present in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 12,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 100,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 44,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-507, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 111,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09846024099999795,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test case `test_llm_error_is_reported` verifies that the LLM error is reported in the HTML output.",
          "The `pytester.makepyfile` function creates a test function with an assertion.",
          "The `pytester.makeconftest` function patches the `litellm.completion` module to raise an error.",
          "The `pytester.makefile` function creates a pyproject.toml file with the `[tool.pytest_llm_report]` configuration.",
          "The test function `test_pass()` is called within the `makepyfile` function.",
          "The `pytester.makepyfile` function includes the `mock_completion` function in the test code.",
          "The `pytester.makeconftest` function sets the `litellm.completion` module to `mock_completion` before running the tests."
        ],
        "scenario": "Verify that LLM errors are surfaced in HTML output.",
        "token_usage": {
          "completion_tokens": 224,
          "prompt_tokens": 313,
          "total_tokens": 537
        },
        "why_needed": "Prevent regression where LLM errors are not reported correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.060884852000015144,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the LLM opt-out marker is correctly recorded.",
          "The test asserts that the LLM opt-out marker is enabled for a single test.",
          "The test checks if the 'llm_opt_out' attribute of the first test in the report is set to True.",
          "The test verifies that the LLM opt-out marker is not present in the tests list.",
          "The test asserts that the number of tests in the report is 1.",
          "The test checks if the 'llm_opt_out' attribute of each test is set to False.",
          "The test verifies that the 'llm_opt_out' attribute of the first test matches the expected value."
        ],
        "scenario": "Test the LLM opt-out marker.",
        "token_usage": {
          "completion_tokens": 184,
          "prompt_tokens": 290,
          "total_tokens": 474
        },
        "why_needed": "Prevents regression in LLM opt-out functionality."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05830635099999881,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest.mark.requirement` decorator should be applied to each test function that requires specific requirements.",
          "The `pytester.makepyfile()` method should create a file with the required pytest configuration.",
          "The `report_path` variable should be created and populated with the correct report path.",
          "The JSON data from the report path should contain a single test with the specified requirements.",
          "The 'requirements' key in the test's metadata should contain the expected requirements.",
          "The 'REQ-001' and 'REQ-002' strings should be present in the list of required requirements for each test.",
          "The `json.loads()` method should correctly parse the JSON data from the report path."
        ],
        "scenario": "Test the requirement marker functionality.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 307,
          "total_tokens": 500
        },
        "why_needed": "This test prevents a regression where the requirement marker is not recorded for tests with multiple requirements."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06512916500000188,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Multiple xfailed tests are recorded in the report.",
          "Each xfailed test is counted only once.",
          "The count of xfailed tests matches the number of tests with xfailed outcomes.",
          "No xfailed tests are missed due to missing test outcomes.",
          "The test verifies that each xfailed test contributes to the total count."
        ],
        "scenario": "The test verifies that multiple xfailed tests are recorded in the report.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 317,
          "total_tokens": 444
        },
        "why_needed": "This test prevents regression of a scenario where multiple xfailed tests are not recorded in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 43,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.058776591000025746,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key under 'skipped' should contain an integer value equal to 1.",
          "The 'summary' key under 'skipped' should not contain any other values or strings.",
          "The number of skipped tests should be exactly 1 as per the pytester's configuration.",
          "The test skip reason is correctly set to 'test skip'.",
          "The test file name and path are correctly recorded in the report.json file."
        ],
        "scenario": "Verify that skipped tests are recorded and their count is accurate.",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 264,
          "total_tokens": 416
        },
        "why_needed": "This test prevents a regression where the number of skipped tests might not be correctly reported in the LLM report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06480111900000907,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the JSON report should contain the correct number of xfailed tests (1).",
          "The value of the 'xfailed' key under the 'summary' section should be equal to 1.",
          "If no xfailed tests are recorded, the 'summary' section should not have a 'xfailed' key or its value should be 0."
        ],
        "scenario": "Test that xfailed tests are recorded and counted correctly in the report.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 264,
          "total_tokens": 398
        },
        "why_needed": "This test prevents a regression where xfailed tests are not properly reported in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203-205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.062313040999981695,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'test_param' function is called with the correct argument 'x'.",
          "The assert statement inside the function checks if x is greater than 0.",
          "The total number of test cases is 3 as expected.",
          "The number of passed tests is also 3 as expected.",
          "The report.json file contains a summary with total and passed counts."
        ],
        "scenario": "Test parameterized tests are recorded separately.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 290,
          "total_tokens": 417
        },
        "why_needed": "This test prevents regression in parametrized tests where the same input can produce different outputs."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.05234981999998922,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result.stdout.fnmatch_lines', 'expected_result': ['*Example:*--llm-report*'], 'actual_result': [1, 2, 3]}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 123,
          "total_tokens": 228
        },
        "why_needed": "This test is necessary to ensure that the CLI help text includes usage examples for the plugin registration feature."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.04723160499997903,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"Markers should be registered with pytester.runpytest('--markers')\", 'expected_result': 'Markers should be registered'}",
          "{'description': 'Markers should match the expected lines in stdout.fnmatch_lines', 'expected_result': ['*llm_opt_out*', '*llm_context*', '*requirement*']}"
        ],
        "scenario": "TestPluginRegistration test",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 142,
          "total_tokens": 256
        },
        "why_needed": "To ensure that LLM markers are registered correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.052769748999992316,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'plugin registration', 'expected_result': 'The plugin was successfully registered.'}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 118,
          "total_tokens": 190
        },
        "why_needed": "To verify that the plugin is registered correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09864250300000776,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string '<html' should be present in the report HTML.",
          "The report path should exist and contain an 'html' substring.",
          "The content of the report path should contain the string 'html'.",
          "The nodeid field should not cause the Pytest reporter to crash or produce invalid reports."
        ],
        "scenario": "Test verifies that special characters in nodeid are handled correctly.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 288,
          "total_tokens": 415
        },
        "why_needed": "This test prevents a bug where special characters in the nodeid field cause the Pytest reporter to crash or produce invalid HTML reports."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007814969999913046,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected result', 'value': '1m 0.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 106,
          "total_tokens": 186
        },
        "why_needed": "To ensure the `format_duration` function can correctly format a duration of exactly one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007376140000019404,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The result should contain '\u03bcs' to indicate microsecond format.\", 'expected_value': '\u03bcs'}",
          "{'description': 'The result should be equal to the expected value.', 'expected_result': '500\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 121,
          "total_tokens": 237
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-millisecond durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007622710000134703,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"result == '500.0ms'\", 'expected_result': '500.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 119,
          "total_tokens": 202
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-second durations as milliseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007932780000032835,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"result contains 'm'\", 'expected': '1m 30.5s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_minutes_format",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 124,
          "total_tokens": 207
        },
        "why_needed": "To ensure the `format_duration` function correctly formats durations over a minute, including minutes and seconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_minutes_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007247199999937948,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '3m 5.0s', 'actual': '3m 5.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 112,
          "total_tokens": 196
        },
        "why_needed": "To ensure the `format_duration` function correctly formats multiple minutes into a human-readable string."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007338679999975284,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '1.00s', 'actual_value': '1.0'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_one_second",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 101,
          "total_tokens": 186
        },
        "why_needed": "The function `format_duration` should return a string representation of exactly one second in the format 'X.Xss'."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_one_second",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007431049999979678,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert 's' in result\", 'expected_result': \"'s'\", 'actual_result': '5.50s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_seconds_format",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 110,
          "total_tokens": 194
        },
        "why_needed": "To ensure that the seconds function correctly formats numbers under a minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_seconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007592750000071646,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '1.0ms', 'actual_value': '1.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 111,
          "total_tokens": 189
        },
        "why_needed": "To ensure the `format_duration` function correctly formats small millisecond durations."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007248599999911676,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'value': '1\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 116,
          "total_tokens": 193
        },
        "why_needed": "To ensure that the `format_duration` function correctly handles very small durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007871569999906569,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The formatted string should be in the format 'YYYY-MM-DDTHH:MM:SS+HH:MM:SS'\", 'expected_value': '2024-01-15T10:30:45+00:00'}"
        ],
        "scenario": "Test ISO Format with UTC",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 143,
          "total_tokens": 241
        },
        "why_needed": "To test the correct formatting of datetime objects with UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007398079999916263,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '2024-06-20T14:00:00', 'actual': '2024-06-20T14:00:00'}"
        ],
        "scenario": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 136,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the naive datetime (no timezone) is correctly formatted."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007787110000094799,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Result contains 12 digits', 'expected': '12345678901234567890', 'actual': '123456'}"
        ],
        "scenario": "Tests time module",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 133,
          "total_tokens": 206
        },
        "why_needed": "To test the format of datetime objects with microseconds"
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_with_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.000769042999991143,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result.tzinfo is not None', 'expected': 'True'}",
          "{'name': 'result.tzinfo == UTC', 'expected': 'True'}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 109,
          "total_tokens": 204
        },
        "why_needed": "To ensure the datetime object has a valid UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007734819999996034,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'type': 'assertion', 'value': 'datetime.datetime(2023, 1, 1, 0, 0, tzinfo=TimezoneInfo.get_default())'}"
        ],
        "scenario": "Tests that the `is_current_time` function returns a valid JSON response when called with an empty input.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 116,
          "total_tokens": 255
        },
        "why_needed": "The test is necessary because the `utc_now()` function does not return a valid datetime object if no UTC time has been set. This can cause issues in tests that rely on this function returning a specific value."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_is_current_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0008170940000127302,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Type of result', 'expected': 'datetime', 'actual': 'isinstance(result, datetime)'}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_returns_datetime",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 94,
          "total_tokens": 175
        },
        "why_needed": "To ensure that the `utc_now()` function returns a datetime object."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_returns_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008738999999877706,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` in `TokenRefresher` raises a `TokenRefreshError` with the correct message 'Authentication failed'.",
          "The error message returned by `get_token()` includes the string 'exit 1', which is expected for command failure.",
          "The test asserts that the error message contains the string 'Authentication failed' to verify its correctness."
        ],
        "scenario": "When TokenRefresher raises an error on command failure, it should return a TokenRefreshError with the correct message.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 310,
          "total_tokens": 460
        },
        "why_needed": "This test prevents a potential bug where TokenRefresher does not handle command failures correctly and may silently fail or produce incorrect results."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_command_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-109, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008335249999902317,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'empty output' in str(exc_info.value).lower()",
          "assert exc_info.value.value == 'empty output'",
          "assert isinstance(exc_info.value.value, str)",
          "assert len(exc_info.value.value) == 0",
          "# Check if the string is empty"
        ],
        "scenario": "The test verifies that the TokenRefresher raises an error when given an empty output.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 297,
          "total_tokens": 424
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not raise an error for an empty output, potentially masking a regression."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_empty_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008481619999827217,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` instance returns a new token after force refresh.",
          "The output of the `get_token()` method contains the expected token value.",
          "The number of calls to the `get_token()` method increases by one after force refresh.",
          "The `call_count` variable is updated correctly with each call to `get_token()`.",
          "The `token1` and `token2` variables are assigned the correct values based on the output of `get_token()`.",
          "The `force=True` parameter does not affect the return value of `get_token()`."
        ],
        "scenario": "Test that 'force_refresh' bypasses cache and returns a new token when called with the same command.",
        "token_usage": {
          "completion_tokens": 199,
          "prompt_tokens": 346,
          "total_tokens": 545
        },
        "why_needed": "This test prevents a bug where the TokenRefresher does not return a new token when force_refresh is called with the same command."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008698119999905884,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token` variable should be equal to 'custom-key-token'.",
          "The `json_key` parameter passed to `TokenRefresher.get_token()` should match the expected value.",
          "The `subprocess.run()` function returns a CompletedProcess object with the correct JSON output.",
          "The `stdout` attribute of the `CompletedProcess` object is set to 'custom-key-token'.",
          "The `stderr` attribute of the `CompletedProcess` object is empty.",
          "The `json.dumps()` function correctly converts the expected JSON string to a Python dictionary.",
          "The `returncode` attribute of the `CompletedProcess` object is 0 (indicating successful execution)."
        ],
        "scenario": "Test the `TokenRefresher` class with a custom JSON key.",
        "token_usage": {
          "completion_tokens": 209,
          "prompt_tokens": 303,
          "total_tokens": 512
        },
        "why_needed": "This test prevents a bug where the `TokenRefresher` class does not properly handle custom JSON keys in the `get_token()` method."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_custom_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008841990000121314,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token` key in the JSON output is set to 'json-token-value'.",
          "The `expires_in` value in the JSON output is set to 3600 (1 hour).",
          "The `output_format` parameter is set to 'json'.",
          "The `json_key` parameter is set to 'token'.",
          "The extracted token matches the expected string 'json-token-value'."
        ],
        "scenario": "Verify that the `TokenRefresher` extracts a JSON token from the expected output.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 308,
          "total_tokens": 457
        },
        "why_needed": "This test prevents a potential bug where the `get-token` command returns an incorrect or malformed JSON response."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000917111999996223,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "token == 'my-secret-token'",
          "stdout contains 'INFO: Processing...' and 'my-secret-token'",
          "stderr does not contain any relevant information",
          "the `get_token()` method returns the expected token value"
        ],
        "scenario": "The test verifies that the `TokenRefresher` extracts the correct token from the provided text output.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 298,
          "total_tokens": 412
        },
        "why_needed": "This test prevents a potential bug where the token is not extracted correctly due to an incorrect or incomplete text output."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_text_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-134, 149-150"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008864630000005036,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method of the `TokenRefresher` instance should raise a `TokenRefreshError` exception when receiving an invalid JSON string.",
          "The error message returned by the `get_token` method should contain the word 'json' in lowercase.",
          "A non-empty string containing only whitespace characters or special characters (except for quotes) should be considered as an invalid JSON input.",
          "The `subprocess.CompletedProcess` result should include a returncode of 1, indicating that the command failed with a non-zero exit status.",
          "The `stdout` and `stderr` attributes of the `subprocess.CompletedProcess` result should contain strings containing 'not valid json' and '', respectively.",
          "The `TokenRefreshError` exception raised by the `get_token` method should be an instance of the `Exception` class.",
          "The error message returned by the `get_token` method should not be empty (i.e., it should contain at least one non-whitespace character)."
        ],
        "scenario": "Test that TokenRefresher raises an error when receiving invalid JSON from the command.",
        "token_usage": {
          "completion_tokens": 272,
          "prompt_tokens": 299,
          "total_tokens": 571
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher class does not handle invalid input correctly."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008127450000188219,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `invalidate()` of the `TokenRefresher` class clears the cache.",
          "A new token is generated and stored in the cache after calling `invalidate()`.",
          "The number of tokens in the cache increases by one after calling `invalidate()`.",
          "The cached token values are different from the actual token values obtained using `get_token()`.",
          "The function `invalidate()` does not return an error or exception when called with invalid arguments (e.g., no command, refresh interval, or output format).",
          "The function `invalidate()` clears the cache after a successful refresh, even if there are still tokens in the cache.",
          "The cached token values are different from the actual token values obtained using `get_token()` even after calling `invalidate()` multiple times."
        ],
        "scenario": "Test TokenRefresher.invalidate() clears cache and verifies it is invalidated after refresh.",
        "token_usage": {
          "completion_tokens": 230,
          "prompt_tokens": 340,
          "total_tokens": 570
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not invalidate its cache after a successful refresh, leading to stale token values being returned."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalidate",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139-141, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010693579999951908,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'token' key should be present in the output of the get_token() method.",
          "The 'not found' string should be present in the error message.",
          "The error message should include the required JSON key ('token').",
          "The error message should not contain any other keys or values that are relevant to the test case.",
          "The 'token' key should be present in the output of the get_token() method even if it is missing from the input data.",
          "The 'not found' string should be present in the output of the get_token() method even if the required JSON key ('token') is not present in the input data.",
          "The error message should include a clear indication that the 'token' key was not found, without including any other relevant information."
        ],
        "scenario": "Test that TokenRefresher raises an error when the JSON key is missing.",
        "token_usage": {
          "completion_tokens": 244,
          "prompt_tokens": 325,
          "total_tokens": 569
        },
        "why_needed": "To prevent a potential bug where the TokenRefresher does not raise an error when the required JSON key is missing, allowing the test to fail with a meaningful error message."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_missing_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.05178559800000926,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "All threads should acquire the lock before accessing the token.",
          "The first thread to acquire the lock should receive the same token as all other threads.",
          "If any thread acquires the lock after the initial call to get_token(), it should not affect the result of get_token() for subsequent calls.",
          "The output of get_token() should be consistent across all threads, with no variation in the token string.",
          "Subsequent calls to get_token() should return the same token as the first call made by any thread.",
          "If a thread acquires the lock before calling get_token(), it should not affect the results of subsequent calls to get_token().",
          "The output of subprocess.CompletedProcess() should be consistent across all threads, with no variation in the command string or return code."
        ],
        "scenario": "Test TokenRefresher thread safety by verifying that all threads receive the same token after concurrent execution.",
        "token_usage": {
          "completion_tokens": 229,
          "prompt_tokens": 427,
          "total_tokens": 656
        },
        "why_needed": "This test prevents a potential bug where multiple threads may acquire the lock and access different tokens, leading to inconsistent results."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_thread_safety",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 16,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001068455000023505,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'get_token()' method of the TokenRefresher instance raises a TokenRefreshError with the message 'timed out' when the 'get-token' command takes longer than 30 seconds to complete.",
          "The 'get_token()' method does not raise a TokenRefreshError when the 'get-token' command completes within the specified timeout period (30 seconds in this case).",
          "The 'get_token()' method correctly raises a TokenRefreshError with the message 'timed out' when the 'get-token' command takes longer than 30 seconds to complete.",
          "The 'refresh_interval' attribute of the TokenRefresher instance is set to 3600 seconds (1 hour), which allows the 'get-token' command to timeout within this interval.",
          "The 'output_format' attribute of the TokenRefresher instance is set to 'text', which does not affect the timeout handling behavior of the 'get-token' command."
        ],
        "scenario": "The test verifies that the TokenRefresher handles command timeout correctly by raising a TokenRefreshError when the 'get-token' command takes longer than 30 seconds to complete.",
        "token_usage": {
          "completion_tokens": 275,
          "prompt_tokens": 279,
          "total_tokens": 554
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not handle command timeouts properly, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_timeout_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008143990000064605,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get-token` command should be the same for both `token1` and `token2` after caching.",
          "The `call_count` variable should only increment once when the function is called with a cached token.",
          "Both `token1` and `token2` should have the same value after caching, indicating that they are the same token.",
          "The output of the `get-token` command for both calls should be identical to prevent multiple calls to the command.",
          "If the function is called again with a different cached token, it should not call the command again and return an empty string or a specific error message.",
          "The `subprocess.CompletedProcess` object returned by the fake run function should have an `stdout` field that contains the expected output for both calls to the `get-token` command."
        ],
        "scenario": "Test Token Refreshing with Caching to verify that the function does not call the command again after caching.",
        "token_usage": {
          "completion_tokens": 254,
          "prompt_tokens": 353,
          "total_tokens": 607
        },
        "why_needed": "This test prevents a potential bug where the `TokenRefresher` calls the `get-token` command multiple times if it is cached, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_token_caching",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008433619999834718,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should raise a `TokenRefreshError` when executed with a non-zero return code.",
          "The error message 'exit 1' should be printed to stderr.",
          "The string 'No error output' should be present in the error message."
        ],
        "scenario": "Test that a command failure results in an exception being raised and the expected error message is printed to stderr.",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 322,
          "total_tokens": 445
        },
        "why_needed": "To ensure that TokenRefresher correctly handles command failures with no stderr output, preventing unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_command_failure_no_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008694519999892236,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Expected a TokenRefreshError to be raised with the message 'empty' when the command string is empty.\", 'type': 'assertion_error'}"
        ],
        "scenario": "TokenRefresherEdgeCases",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 151,
          "total_tokens": 226
        },
        "why_needed": "Test handling of empty command string."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_empty_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-88, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009544309999967027,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `TokenRefresher` instance is not created with an invalid command string.",
          "The `refresh_interval` parameter is set to 3600 seconds.",
          "The `output_format` parameter is set to 'text'.",
          "An exception of type `TokenRefreshError` is raised when calling `get_token()` on the `refresher` instance.",
          "The error message contains the string 'Invalid command string'."
        ],
        "scenario": "Test the test_invalid_command_string function to verify it handles an invalid command string (shlex parse error).",
        "token_usage": {
          "completion_tokens": 156,
          "prompt_tokens": 251,
          "total_tokens": 407
        },
        "why_needed": "Prevent a TokenRefreshError due to shlex parse errors when handling invalid shell syntax in the command."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_invalid_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 27,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-137, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009033959999840135,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'output' argument of the get_token method should be a dict.",
          "The 'expected' key in the output should contain 'list'.",
          "The 'token' key in the expected dictionary should also be present and equal to 'array'."
        ],
        "scenario": "Test verifies that TokenRefresher raises a TokenRefreshError when the output is not a dictionary.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 328,
          "total_tokens": 445
        },
        "why_needed": "This test prevents regression where the function does not raise an error for non-dict JSON outputs."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_not_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009001589999968473,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `json.dumps({"
        ],
        "scenario": "Test handling when token value is an empty string.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 324,
          "total_tokens": 444
        },
        "why_needed": "Prevents potential TokenRefreshError due to invalid input."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001071951999989551,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `json.dumps()` function is called with an integer argument instead of a string.",
          "The `stdout` attribute contains an empty string.",
          "The `stderr` attribute is empty.",
          "An exception is raised with the message 'empty or not a string'.",
          "The `TokenRefreshError` is correctly raised."
        ],
        "scenario": "Test verifying that the `get_token` method raises a `TokenRefreshError` when the token value is not a string.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 326,
          "total_tokens": 462
        },
        "why_needed": "This test prevents regression where the `get_token` method incorrectly handles non-string token values."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_not_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 19,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113, 115-118"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008839289999968969,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method raises a `TokenRefreshError` with the message 'Failed to execute'.",
          "The `refresh_interval` parameter has no effect on the behavior of the `get_token()` method when an OSError occurs.",
          "The `output_format` parameter does not affect the handling of OSError during execution.",
          "The `fake_run()` function raises an `OSError` with a message 'Command not found'.",
          "The `TokenRefresher` instance is created with a command that returns an error.",
          "The `get_token()` method attempts to execute the command and raise an exception if it fails."
        ],
        "scenario": "Test the TokenRefresher's handling of OSError when executing a command.",
        "token_usage": {
          "completion_tokens": 184,
          "prompt_tokens": 280,
          "total_tokens": 464
        },
        "why_needed": "Prevent regressions where the TokenRefresher fails to handle OSError during execution."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_oserror_on_execution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 4,
          "line_ranges": "132, 153-155"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000800392000002148,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output should be empty after stripping blank lines.",
          "The output should contain non-whitespace wrapper lines but no whitespace content lines.",
          "The error message should indicate that there are no non-empty lines in the output."
        ],
        "scenario": "Test that a TokenRefresher fails when the output has only blank lines after initial strip, but still contains non-whitespace wrapper lines.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 376,
          "total_tokens": 504
        },
        "why_needed": "Prevents a potential bug where a TokenRefresher incorrectly handles text output with only whitespace lines after an initial strip, potentially leading to incorrect token refresh results."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_text_only_whitespace_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008441739999796027,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should raise a `TokenRefreshError` with the message 'empty' when passed an empty whitespace-only command string.",
          "The error message should contain the word 'empty'.",
          "The test should fail when running on a system where the input command is empty.",
          "The test should not pass when running on a system where the input command contains non-whitespace characters.",
          "The function `get_token()` should raise a `TokenRefreshError` with a specific error message even if the input command is empty.",
          "The error message should contain the word 'empty'.",
          "The test should fail when running on a system where the input command does not contain any whitespace characters."
        ],
        "scenario": "Test the test_whitespace_only_command to ensure it raises a TokenRefreshError when given an empty whitespace-only command string.",
        "token_usage": {
          "completion_tokens": 211,
          "prompt_tokens": 236,
          "total_tokens": 447
        },
        "why_needed": "Prevent regression by ensuring the test covers the case where the input command is empty."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_whitespace_only_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 73,
          "line_ranges": "399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-487, 491-494, 497, 499, 502-506, 509, 512-514, 516-521, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.005174791999991157,
      "file_path": "tests/test_token_usage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total input tokens should be 30 and the total output tokens should be 15.",
          "The number of annotations should be 2.",
          "The total tokens should be 45.",
          "The annotation count should match the number of test cases."
        ],
        "scenario": "Test token usage aggregation for multiple test cases",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 775,
          "total_tokens": 876
        },
        "why_needed": "Prevents regression in token usage reporting when aggregating results from multiple tests."
      },
      "nodeid": "tests/test_token_usage.py::test_token_usage_aggregation",
      "outcome": "passed",
      "phase": "call"
    }
  ]
}