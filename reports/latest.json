{
  "run_meta": {
    "aggregation_policy": null,
    "collect_only": false,
    "collected_count": 403,
    "deselected_count": 0,
    "duration": 16.809372,
    "end_time": "2026-01-17T22:36:57.582613+00:00",
    "exit_code": 0,
    "git_dirty": false,
    "git_sha": "17ba445b18c6b8e678634e3310a2851e05f74129",
    "interrupted": false,
    "is_aggregated": true,
    "llm_annotations_count": 366,
    "llm_annotations_enabled": true,
    "llm_annotations_errors": 36,
    "llm_context_mode": "minimal",
    "llm_model": "llama3.2:1b",
    "llm_provider": "ollama",
    "platform": "Linux-6.11.0-1018-azure-x86_64-with-glibc2.39",
    "plugin_git_dirty": true,
    "plugin_git_sha": "b7a157f6cb9189cc50a17c846484c8454deeac61",
    "plugin_version": "0.1.0",
    "pytest_version": "9.0.2",
    "python_version": "3.12.12",
    "repo_git_dirty": false,
    "repo_git_sha": "17ba445b18c6b8e678634e3310a2851e05f74129",
    "repo_version": "0.1.1",
    "rerun_count": 0,
    "run_count": 1,
    "run_id": "21101927020-py3.12",
    "selected_count": 403,
    "source_reports": [],
    "start_time": "2026-01-17T22:36:40.773241+00:00"
  },
  "schema_version": "1.0.0",
  "sha256": "ee0d459770f923e275ca2ccf3bb3404bc8ab71d3fed0557f2d968ed1b1f4245b",
  "source_coverage": [
    {
      "coverage_percent": 100.0,
      "covered": 2,
      "covered_ranges": "2-3",
      "file_path": "src/pytest_llm_report/_git_info.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 2
    },
    {
      "coverage_percent": 95.73,
      "covered": 112,
      "covered_ranges": "13, 15-19, 21, 35, 38, 44, 46, 52-53, 55-57, 59, 61-64, 69, 73-74, 77-80, 84, 87-89, 93, 103, 109-111, 113-117, 119-120, 125, 127-128, 130-131, 134-135, 141-144, 147, 149, 151, 165, 167, 171, 173, 175, 185, 187-191, 193-194, 197, 199, 208, 220, 222-236, 238, 240, 248-249, 251-252, 254, 256-258, 262, 265-266, 268-269, 272, 274-275, 277, 279-280, 284",
      "file_path": "src/pytest_llm_report/aggregation.py",
      "missed": 5,
      "missed_ranges": "66, 90-91, 195, 206",
      "statements": 117
    },
    {
      "coverage_percent": 93.62,
      "covered": 44,
      "covered_ranges": "13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153",
      "file_path": "src/pytest_llm_report/cache.py",
      "missed": 3,
      "missed_ranges": "64-65, 130",
      "statements": 47
    },
    {
      "coverage_percent": 98.2,
      "covered": 109,
      "covered_ranges": "19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285",
      "file_path": "src/pytest_llm_report/collector.py",
      "missed": 2,
      "missed_ranges": "141, 239",
      "statements": 111
    },
    {
      "coverage_percent": 92.59,
      "covered": 125,
      "covered_ranges": "13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-248, 250, 252-254, 259-260, 263-264, 271, 273, 276-279, 281-283, 285, 299-300, 302, 308",
      "file_path": "src/pytest_llm_report/coverage_map.py",
      "missed": 10,
      "missed_ranges": "62, 123, 125, 128, 157, 221, 249, 251, 257, 274",
      "statements": 135
    },
    {
      "coverage_percent": 100.0,
      "covered": 35,
      "covered_ranges": "8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 74-76, 80, 129, 139",
      "file_path": "src/pytest_llm_report/errors.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 35
    },
    {
      "coverage_percent": 100.0,
      "covered": 3,
      "covered_ranges": "4-5, 7",
      "file_path": "src/pytest_llm_report/llm/__init__.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 3
    },
    {
      "coverage_percent": 100.0,
      "covered": 110,
      "covered_ranges": "4, 6-10, 12-15, 21-22, 25-28, 31, 45-46, 48-50, 54, 56-57, 59, 61-62, 64, 66-68, 71-72, 74-82, 87, 97-98, 100, 102, 104-105, 115, 127, 129-132, 137-139, 142, 165-168, 170-171, 176, 178, 180-183, 185-190, 192-193, 198-201, 203, 206, 229-232, 234, 236-237, 239-240, 245-246, 248-253, 255-256, 261-264, 266",
      "file_path": "src/pytest_llm_report/llm/annotator.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 110
    },
    {
      "coverage_percent": 100.0,
      "covered": 78,
      "covered_ranges": "13, 15-18, 26, 40, 46, 52-53, 55, 72, 75-76, 78, 80, 101, 107-108, 110-111, 122, 128, 130, 136, 138, 147, 149, 165, 167-173, 175, 177, 186-187, 190-192, 194-195, 198-200, 203-208, 212, 214, 220-221, 224-225, 228-230, 233, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265, 267",
      "file_path": "src/pytest_llm_report/llm/base.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 78
    },
    {
      "coverage_percent": 91.73,
      "covered": 255,
      "covered_ranges": "7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-88, 91-97, 99-103, 105, 107-114, 121-122, 125, 128, 134, 136-139, 141-142, 144, 160-161, 167-169, 171-172, 174, 176-184, 186-188, 190-191, 193, 196, 200-208, 210-211, 213-215, 217-223, 225-226, 235, 237-238, 242-243, 246-247, 249-250, 258-259, 266, 272-273, 275, 279-283, 285-289, 292-293, 298-299, 306-307, 309, 321, 323-324, 328, 333, 336-338, 341-349, 351-352, 354, 358-361, 363, 366-372, 374-380, 386-388, 390-393, 395, 397-398, 402-408, 411, 414-416, 418-420, 422-427, 433-434, 436-440, 443-446, 448-449, 451-453",
      "file_path": "src/pytest_llm_report/llm/gemini.py",
      "missed": 23,
      "missed_ranges": "89, 104, 106, 115-117, 199, 228-229, 233, 239-241, 248, 251-254, 256, 262, 373, 447, 450",
      "statements": 278
    },
    {
      "coverage_percent": 87.1,
      "covered": 54,
      "covered_ranges": "8, 10, 12-13, 21, 31, 37-38, 41-42, 44, 51, 60-62, 64, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110, 114, 116, 118-120, 129, 131, 142, 164, 175-176, 179-181, 183, 185-186, 188, 190, 195, 197, 199, 205-206, 208",
      "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
      "missed": 8,
      "missed_ranges": "123-124, 126, 133, 135-136, 138, 191",
      "statements": 62
    },
    {
      "coverage_percent": 100.0,
      "covered": 13,
      "covered_ranges": "8, 10, 12-13, 20, 26, 32, 34, 50, 52, 58, 60, 66",
      "file_path": "src/pytest_llm_report/llm/noop.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 13
    },
    {
      "coverage_percent": 95.56,
      "covered": 43,
      "covered_ranges": "7, 9, 11-12, 18, 24, 40-41, 47, 50, 52, 54-55, 57-60, 62, 64-65, 71, 73, 76-77, 79-80, 82, 86, 92-93, 95-97, 101, 107, 109, 119, 121-122, 132, 137, 139-140",
      "file_path": "src/pytest_llm_report/llm/ollama.py",
      "missed": 2,
      "missed_ranges": "69, 75",
      "statements": 45
    },
    {
      "coverage_percent": 97.22,
      "covered": 35,
      "covered_ranges": "8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130",
      "file_path": "src/pytest_llm_report/llm/schemas.py",
      "missed": 1,
      "missed_ranges": "39",
      "statements": 36
    },
    {
      "coverage_percent": 90.14,
      "covered": 64,
      "covered_ranges": "7, 9-14, 17, 20, 23-24, 36-39, 41-43, 47, 59-60, 63-66, 69-72, 74, 83, 85-86, 90, 93, 101-103, 107-109, 111, 113-115, 120, 132-135, 139-140, 143-144, 148-150, 153-154, 156, 158, 160-162",
      "file_path": "src/pytest_llm_report/llm/token_refresh.py",
      "missed": 7,
      "missed_ranges": "87-88, 91, 116, 136, 145, 155",
      "statements": 71
    },
    {
      "coverage_percent": 95.88,
      "covered": 233,
      "covered_ranges": "17-18, 21, 24-25, 34-36, 38, 40, 47-48, 61-67, 69, 71, 82-83, 95-100, 102, 104, 109-115, 118-119, 141-157, 159-160, 162, 164, 166, 173-177, 179-188, 190, 192, 194-196, 199-200, 208-209, 211, 213, 219-220, 229-231, 233, 235, 239-241, 244-245, 254-256, 258, 260, 267-268, 277-279, 281, 283, 287-289, 292-293, 330-359, 361-366, 368, 370, 388-411, 413-425, 428-429, 443-451, 453, 455, 465, 467, 470-471, 488-498, 500, 506, 508, 514-518, 520, 522, 524, 526, 528",
      "file_path": "src/pytest_llm_report/models.py",
      "missed": 10,
      "missed_ranges": "178, 189, 191, 193, 466, 519, 521, 523, 525, 527",
      "statements": 243
    },
    {
      "coverage_percent": 60.14,
      "covered": 83,
      "covered_ranges": "122, 162, 191, 194-196, 201-203, 209-211, 217-219, 225-226, 233, 237-246, 248, 252, 261, 276, 279-295, 298, 305-308, 310-312, 319-332, 335-344, 347, 349",
      "file_path": "src/pytest_llm_report/options.py",
      "missed": 55,
      "missed_ranges": "13-15, 21-22, 98-102, 105-107, 110-115, 118-121, 138-139, 142-148, 151-153, 156-158, 161, 172-176, 179-180, 183, 185, 227, 234, 250, 255, 264, 315-316",
      "statements": 138
    },
    {
      "coverage_percent": 84.71,
      "covered": 133,
      "covered_ranges": "40, 43, 49, 55, 61, 67, 73, 80, 89, 95, 101, 107, 113, 121, 126, 131, 136, 142, 147, 153, 160, 165, 170, 175, 181, 186, 202, 206, 210, 216-217, 220-221, 223, 225, 228-230, 236-237, 245-246, 271-272, 275-276, 279, 282-283, 285-286, 289-290, 292, 294-298, 301-302, 304, 306, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-338, 340, 342-345, 348-349, 357-358, 363-366, 369, 371, 374-379, 381, 383, 391-392, 413-414, 417-418, 421-423, 434-435, 438, 441-442, 445-447, 457-458, 461-463, 474-475, 478, 481, 483-484",
      "file_path": "src/pytest_llm_report/plugin.py",
      "missed": 24,
      "missed_ranges": "13, 15-17, 19-20, 22, 28-31, 34, 193, 249, 353-354, 359-360, 405-406, 426, 450, 466-467",
      "statements": 157
    },
    {
      "coverage_percent": 93.33,
      "covered": 70,
      "covered_ranges": "13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116, 118, 132-133, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 165, 180, 182, 191-194",
      "file_path": "src/pytest_llm_report/prompts.py",
      "missed": 5,
      "missed_ranges": "80, 114, 142, 146, 149",
      "statements": 75
    },
    {
      "coverage_percent": 100.0,
      "covered": 50,
      "covered_ranges": "13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 141-143, 145, 158-163, 177, 196",
      "file_path": "src/pytest_llm_report/render.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 50
    },
    {
      "coverage_percent": 94.01,
      "covered": 157,
      "covered_ranges": "13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 303, 312, 314-315, 317-328, 330, 332, 340, 343-345, 348-349, 352-354, 357, 360, 368, 376, 378-379, 382, 385, 388, 391, 399, 401-402, 408, 410, 412, 414-423, 434-435, 437-439, 447-448, 453, 455, 458, 461-462, 464, 470-474, 480-481, 488, 495, 497, 499-501, 503, 506-507, 509, 515-516",
      "file_path": "src/pytest_llm_report/report_writer.py",
      "missed": 10,
      "missed_ranges": "113, 135-137, 424-425, 432, 449-451",
      "statements": 167
    },
    {
      "coverage_percent": 91.18,
      "covered": 31,
      "covered_ranges": "11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-64, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123",
      "file_path": "src/pytest_llm_report/util/fs.py",
      "missed": 3,
      "missed_ranges": "40, 65, 67",
      "statements": 34
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121",
      "file_path": "src/pytest_llm_report/util/hashing.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 33,
      "covered_ranges": "12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95",
      "file_path": "src/pytest_llm_report/util/ranges.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 16,
      "covered_ranges": "4, 6, 9, 15, 18, 27, 30, 39-44, 46-48",
      "file_path": "src/pytest_llm_report/util/time.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 16
    }
  ],
  "summary": {
    "coverage_total_percent": 90.11,
    "error": 0,
    "failed": 0,
    "passed": 403,
    "skipped": 0,
    "total": 403,
    "total_duration": 14.58472401700078,
    "xfailed": 0,
    "xpassed": 0
  },
  "tests": [
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 70,
          "line_ranges": "52, 55-56, 59, 61-63, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 147, 149, 151-156, 158, 160-162, 173, 220, 222-226, 238, 248, 251-252, 254, 256, 279-282, 284"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.00213691800001925,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregated report contains both retained test cases.",
          "The number of retained test cases is consistent across all runs.",
          "The aggregated report does not contain any duplicate test case IDs.",
          "All retained test cases are present in the final aggregated result.",
          "No test cases are skipped or lost during aggregation process.",
          "The aggregated report includes all specified test cases and their outcomes.",
          "The aggregated report is consistent across different run numbers."
        ],
        "scenario": "Test the aggregation of all policy results from multiple runs.",
        "why_needed": "Prevents regression in aggregate policy when running multiple tests concurrently."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 7,
          "line_ranges": "52, 55-57, 109-111"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0038728769999920587,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` method is called with an empty list.",
          "A `PathError` is raised with a message indicating that the specified path does not exist.",
          "The `aggregate` method returns `None` as expected.",
          "The test passes even if the directory exists in the current working directory.",
          "The test fails when the directory exists in a different location."
        ],
        "scenario": "Verifies that the aggregate function does not attempt to aggregate a directory that does not exist.",
        "why_needed": "Prevents a potential bug where the aggregate function throws an exception when trying to aggregate a non-existent directory."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 78,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134, 141, 147, 149, 151-156, 158, 160-162, 173, 185, 187-191, 193-194, 197, 220, 222-226, 238, 248, 251-252, 254, 256, 279-282, 284"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.003547641000011481,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `aggregate()` should contain only one test.",
          "The first test in the result list should have an 'outcome' of 'passed'.",
          "The aggregated run meta should indicate that all tests ran.",
          "All tests passed and no failed tests should be reported.",
          "The summary should contain 1 passed test and 0 failed tests."
        ],
        "scenario": "Test that the latest policy is selected when aggregating reports with different times but same test outcome.",
        "why_needed": "This test prevents regression where the latest policy might not be chosen due to inconsistent report timings."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 3,
          "line_ranges": "44, 52-53"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008843400000273505,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate()` method of the `Aggregator` instance should return `None` when called with an empty or None `aggregate_dir` attribute.",
          "An exception should not be raised if the `aggregate_dir` attribute is set to `None`.",
          "The aggregate function should not attempt to aggregate data without a configured aggregation directory."
        ],
        "scenario": "Test that aggregate function returns None when no directory configuration is provided.",
        "why_needed": "Prevents a potential bug where the aggregate function would attempt to aggregate data without a configured aggregation directory."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 9,
          "line_ranges": "52, 55-57, 109-110, 113-114, 173"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0013419839999642136,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate` method is called with an empty list of reports.",
          "An empty list of reports is returned from the `aggregate` method.",
          "The `aggregate` method does not raise an exception when no reports are available.",
          "The `aggregate` method does not log any error messages when no reports are available.",
          "The `aggregate` method does not update the aggregation result with an empty list of reports.",
          "The `aggregate` method returns a None value instead of raising an exception when no reports are available.",
          "The Aggregator instance is updated correctly after calling the `aggregate` method without any reports."
        ],
        "scenario": "The `aggregate` method of the Aggregator instance should be called when no reports are available.",
        "why_needed": "This test prevents a potential bug where the `aggregate` method does not get called with an empty list of reports, potentially leading to incorrect aggregation results or errors."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 82,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 130-131, 134-137, 141-144, 147, 149, 151-156, 158, 160-162, 173, 185, 187-191, 197, 220, 222-226, 238, 248, 251-252, 254, 256, 279-282, 284"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 34,
          "line_ranges": "40-43, 104-107, 109-111, 113, 115, 162, 166-171, 173, 175, 177, 179, 182-186, 188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0020956110000156514,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 66,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119, 125, 127-128, 151-158, 160-162, 173, 185, 187-189, 197, 220, 222-223, 238, 248, 251-252, 254, 256, 279-282, 284"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0016906250000374712,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'source_coverage' key should exist in the aggregated report.",
          "The 'source_coverage' value should be a list of SourceCoverageEntry objects.",
          "Each SourceCoverageEntry object should have the required keys (file_path, statements, missed, covered, coverage_percent, covered_ranges, missed_ranges).",
          "The file path of each SourceCoverageEntry should match the provided source code file.",
          "The number of statements in the aggregated report should be equal to the sum of statements in all individual reports.",
          "Each statement should have a corresponding range (e.g., 1-5, 7-11).",
          "All coverage percentages should add up to 100%.",
          "All covered ranges should match the provided ranges.",
          "All missed ranges should be empty strings.",
          "The aggregated report should not be None."
        ],
        "scenario": "Test that source coverage summary is deserialized correctly.",
        "why_needed": "Prevents regression in aggregation functionality where source coverage data is expected to be deserialized."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 19,
          "line_ranges": "248-249, 251-252, 254, 256-260, 262, 265-266, 268-269, 272, 274-275, 277"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.003427595999994537,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that _load_coverage_from_source() returns None when llm_coverage_source is set to None.",
          "Verify that _load_coverage_from_source() raises UserWarning with message 'Coverage source not found' when llm_coverage_source is set to '/nonexistent/coverage'.",
          "Verify that _load_coverage_from_source() returns a mock coverage object (80.0) when llm_coverage_source is set to '.coverage'."
        ],
        "scenario": "Test loading coverage from configured source file when option is not set.",
        "why_needed": "This test prevents a bug where the aggregator tries to load coverage without setting the llm_coverage_source configuration option."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 17,
          "line_ranges": "220, 222-236, 238"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008464599999911115,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests passed should be equal to the initial total count.",
          "The number of failed tests should remain unchanged.",
          "The number of skipped tests should also remain unchanged.",
          "The number of tests that failed or were skipped but not yet counted should still be 1.",
          "The number of tests that have been counted (passed, failed, and skipped) should still be 6.",
          "The coverage percentage should still be preserved at 85.5%.",
          "The total duration of the latest summary should remain unchanged at 5.0 seconds."
        ],
        "scenario": "Test the `_recalculate_summary` method to ensure it recalculates the latest summary correctly when new test results are added.",
        "why_needed": "This test prevents regression in the `aggregator._recalculate_summary` method, which is responsible for updating the latest summary with new test results."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_recalculate_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 71,
          "line_ranges": "52, 55-56, 59, 64, 69, 73-74, 77-80, 84, 87-89, 93-100, 109-110, 113-117, 119-120, 125, 127-128, 151-156, 158, 160-162, 165, 167-169, 171, 173, 185, 187-189, 197, 220, 222-223, 238, 248, 251-252, 254, 256, 279-282, 284"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.003236078999975689,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should not count the invalid JSON file as a valid report.",
          "The function should raise a UserWarning with the message 'Skipping invalid report file' when encountering an invalid JSON file.",
          "The function should only count the valid report as a valid report.",
          "The function should ignore the missing fields in the invalid JSON file and not include it in the aggregation result.",
          "The function should return None for the aggregation result after skipping the invalid JSON file."
        ],
        "scenario": "Test verifies that the test_skips_invalid_json function skips an invalid JSON file.",
        "why_needed": "This test prevents a regression where the test SkipsInvalidJson function fails to skip an invalid JSON file due to missing required fields in the report."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_skips_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "44, 220, 222-228, 238"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008539829999563153,
      "file_path": "tests/test_aggregation_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "summary.total == 2",
          "summary.passed == 1",
          "summary.failed == 1",
          "summary.coverage_total_percent == 88.5",
          "summary.total_duration == 3.0"
        ],
        "scenario": "The test verifies that the `_recalculate_summary` method returns a Summary object with the correct total, passed, and failed counts.",
        "why_needed": "This test prevents regression where the coverage percentage is not calculated correctly due to missing or incomplete tests."
      },
      "nodeid": "tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0014213619999736693,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_provider is called with the same arguments as when no caching is enabled",
          "mock_cache is not called in the test",
          "mock_assembler is not called in the test"
        ],
        "scenario": "The test verifies that cached tests are skipped.",
        "why_needed": "This test prevents a potential performance regression where cached tests are not properly skipped."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 64,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137, 139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261, 266"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002646770000012566,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `test_concurrent_annotation` method is called with mock providers, cache and assembler instances.",
          "Each mock provider and cache instance is not reused across multiple calls.",
          "The annotator does not modify any external state that could be affected by concurrent access.",
          "No shared data structures are accessed or modified by the annotator.",
          "All mock providers, cache and assembler instances are properly cleaned up after each test call.",
          "No exceptions are raised when calling `test_concurrent_annotation` method."
        ],
        "scenario": "Testing concurrent annotation of a test function",
        "why_needed": "Prevents potential performance issues or data corruption caused by concurrent access to the annotator."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104-112, 129-135, 137-139, 229-232, 234, 236-237, 239, 245-246, 248-253, 255, 261-264, 266"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0018023039999661705,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_assembler should not raise an exception when processing annotations in parallel.",
          "mock_provider should return a failure response for each annotation that fails to be annotated.",
          "mock_cache should not be modified by the annotator during concurrent annotation.",
          "the annotator's output should not contain any information about failed annotations.",
          "the annotator's error messages should not indicate that multiple annotations were processed simultaneously and failed.",
          "the annotator's performance metrics (e.g., annotation rate) should remain unchanged even when handling failures concurrently.",
          "the test should be able to reproduce the failure scenario reliably, without requiring additional setup or configuration."
        ],
        "scenario": "The annotator handles failures concurrently without any noticeable impact on performance or correctness.",
        "why_needed": "This test prevents a potential regression where the annotator fails to correctly handle cases where multiple annotations are processed simultaneously and fail."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002560138999967876,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_provider.get_progress() should return a non-negative value",
          "mock_cache.get_annotations() should be called with the correct arguments",
          "mock_assembler.get_annotation() should be called with the correct annotation ID"
        ],
        "scenario": "The `test_progress_reporting` function is used to verify the annotator's progress reporting capabilities.",
        "why_needed": "This test prevents regressions where the annotator fails to report progress correctly after a large number of annotations."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 12.001586927000005,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking the `mock_provider`, `mock_cache`, and `mock_assembler` objects ensures that they are properly mocked for the purpose of testing the sequential annotation.",
          "The test verifies that the annotated sequential data is correctly generated without any errors or exceptions.",
          "The test checks that the annotator function can handle asynchronous data correctly by verifying that it does not raise any exceptions.",
          "The test ensures that the `mock_assembler` object is properly mocked to simulate the expected behavior of an assembler.",
          "The test verifies that the annotated sequential data is consistent with the original data before and after annotation.",
          "The test checks that the annotator function can handle edge cases such as empty or null data correctly.",
          "The test ensures that the `mock_provider` object is properly mocked to simulate the expected behavior of a provider.",
          "The test verifies that the annotated sequential data is correctly generated without any errors or exceptions."
        ],
        "scenario": "The `test_sequential_annotation` function is being tested to ensure it correctly annotates sequential data.",
        "why_needed": "This test prevents a potential regression where the sequential annotation might not work as expected due to incorrect handling of asynchronous data."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 2,
          "line_ranges": "45-46"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008058939999955328,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `annotate_tests` does not modify any configuration or perform any actions when the LLM is disabled.",
          "The `config` object passed to `annotate_tests` has a 'provider' key with value 'none'.",
          "The `annotate_tests` function does not attempt to annotate any tests or annotations.",
          "The `None` provider is used, which indicates that no external data source is being utilized.",
          "No configuration changes are made when the LLM is disabled.",
          "The annotator remains silent and does not perform any actions.",
          "The test results do not indicate any issues with the annotator's behavior.",
          "The `annotate_tests` function behaves as expected when the LLM is disabled."
        ],
        "scenario": "The test verifies that the annotator does not perform any actions when the LLM (Large Language Model) is disabled.",
        "why_needed": "This test prevents a regression where the annotator would skip tests or annotations if the LLM was enabled, potentially causing unintended behavior in downstream processes."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "45, 48-52, 54"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009090970000329435,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocked `provider` object should be mocked with an `unavailable` status.",
          "Captured output from `sys.stderr` should indicate that the provider is unavailable.",
          "The `annotator` function should skip the annotation process when a provider is unavailable.",
          "No exceptions should be raised in this case, indicating successful annotation.",
          "The annotator's progress should not be affected by the provider being unavailable.",
          "The annotator's output should still contain the expected annotations even if the provider is unavailable."
        ],
        "scenario": "The annotator skips the annotation process when a provider is unavailable.",
        "why_needed": "This test prevents regressions where providers are not available and the annotator still attempts to annotate."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 28,
          "line_ranges": "229-232, 234, 236-237, 239-242, 245-246, 248-253, 255-258, 261-264, 266"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0018727959999864652,
      "file_path": "tests/test_annotator_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should report the correct number of annotated tasks (2) and failures (1).",
          "The first error message should contain the string 'first error'.",
          "At least one processing message should be generated for each task, containing the string 'LLM annotation'.",
          "All progress messages should contain the string 'Processing X test(s)', where X is the number of tasks.",
          "The function should correctly handle cases where there are more or fewer tasks than specified in the config (2 vs 1)."
        ],
        "scenario": "Test that annotator reports progress and first error when annotated concurrently with progress and errors.",
        "why_needed": "This test prevents regression in concurrent mode where LLM annotations are reported with progress and errors."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_concurrent_with_progress_and_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 23,
          "line_ranges": "165-168, 170-171, 173-174, 176, 178, 180-183, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002278281999963383,
      "file_path": "tests/test_annotator_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The time.sleep() function was called before the LlmAnnotation task finished.",
          "The time.sleep() function was called after the LlmAnnotation task finished but within the specified rate limit interval of 1.0s.",
          "The time.sleep() function was not called at all if only 0.1s elapsed but the rate limit interval is 1.0s.",
          "The mock_time.side_effect was set to [100.0, 100.1, 100.2, 100.3, 100.4] which indicates that time.sleep() function has been called multiple times with different values.",
          "The LlmAnnotation task finished after the rate limit interval of 1.0s but before the mock_time.side_effect was set to [100.0, 100.1, 100.2, 100.3, 100.4].",
          "The LlmAnnotation task did not finish within the specified rate limit interval of 1.0s.",
          "The time.sleep() function is called before the LlmAnnotation task starts or after it finishes but within the specified rate limit interval of 1.0s."
        ],
        "scenario": "Should wait if rate limit interval has not elapsed.",
        "why_needed": "Prevents regression in sequential annotation tasks where the rate limit interval has not yet elapsed."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_sequential_rate_limit_wait",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 37,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-84, 97-98, 100, 127, 129-135, 137, 139"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0020097610000107125,
      "file_path": "tests/test_annotator_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `progress_msgs` list should contain any string that starts with '(cache): '",
          "Each item in `progress_msgs` should be a string that contains the scenario 'test_cached'",
          "All strings in `progress_msgs` should not be empty",
          "There should be at least one string in `progress_msgs` that matches '(cache): test_cached'",
          "The last item in `progress_msgs` should contain the scenario 'test_cached'"
        ],
        "scenario": "Should report progress for cached tests when annotating tests with maximal caching.",
        "why_needed": "This test prevents regression where the annotator does not report progress for cached tests, potentially leading to missed opportunities for optimization or improvement."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_cached_progress",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "45, 48-52, 54"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0013989809999657155,
      "file_path": "tests/test_annotator_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mocks.get_provider().is_available() was called with False",
          "the annotated tests are skipped",
          "the message 'not available. Skipping annotations' is printed to stdout"
        ],
        "scenario": "The test verifies that when the provider is not available, it prints a message and returns without attempting to annotate tests.",
        "why_needed": "This test prevents regression or bug in the annotator's behavior when the provider is unavailable."
      },
      "nodeid": "tests/test_annotator_maximal.py::TestAnnotatorAdvanced::test_annotate_tests_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 220-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008080179999865322,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotation.error` attribute should contain 'Failed to parse LLM response as JSON'.",
          "The `provider._parse_response(response)` call should raise a `JSONDecodeError`.",
          "The error message should be 'Failed to parse LLM response as JSON'.",
          "The extracted JSON is invalid and cannot be parsed by the LLM.",
          "The `extract_json_from_response` method finds braces, but the contents are malformed.",
          "The expected format of the extracted JSON does not match the actual format."
        ],
        "scenario": "Test that an error is thrown when parsing a malformed JSON after extracting from the response.",
        "why_needed": "To prevent a JSONDecodeError when the extracted JSON does not match the expected format."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203-207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008644929999945816,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.scenario = '123' matches the expected value",
          "annotation.why_needed = ['list'] matches the expected list of why needed scenarios",
          "annotation.key_assertions = ['a'] matches one of the key assertions in the response data"
        ],
        "scenario": "Tests the _parse_response method with non-string fields to ensure it correctly handles scenario/why_needed.",
        "why_needed": "This test prevents a potential bug where the _parse_response method incorrectly interprets non-string fields as lists, leading to incorrect results or errors."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "52-53, 245, 247, 249, 252, 257, 262-263, 265"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 7,
          "line_ranges": "134, 136-139, 141-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000785075000010238,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned value is an instance of `GeminiProvider`.",
          "The returned value has the correct class name (`GeminiProvider`).",
          "The returned value does not have any attributes or methods that are not present in `GeminiProvider`.",
          "The returned value's `__class__` attribute matches the expected value (`GeminiProvider`).",
          "The provided configuration is valid and does not contain any invalid or missing settings.",
          "No exceptions are raised when calling `get_gemini_provider` with a valid configuration."
        ],
        "scenario": "Verify that the `get_gemini_provider` function returns a valid instance of `GeminiProvider`.",
        "why_needed": "This test prevents a potential issue where an invalid or missing configuration is passed to the `get_gemini_provider` function, potentially causing it to return an incorrect provider."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "245, 247, 249, 252, 257, 262, 267"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002028636000034112,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider` raises a `ValueError` with a message indicating that the provided LLM provider is unknown.",
          "The error message includes the string 'invalid' to identify the invalid provider.",
          "The test verifies that the `pytest.raises` context manager is used correctly to catch the ValueError exception.",
          "The test checks that the `match` parameter of the `pytest.raises` context manager matches the expected error message.",
          "The test ensures that the error message includes the string 'Unknown LLM provider: invalid'.",
          "The test verifies that the `Config` class is used correctly to specify an invalid provider."
        ],
        "scenario": "Test that a ValueError is raised when an invalid LLM provider is specified.",
        "why_needed": "This test prevents the introduction of a bug where an unknown LLM provider is used without validation."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "52-53, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007801959999937935,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned value should be an instance of `LiteLLMProvider`.",
          "The returned value should have a type hint of `LiteLLMProvider`.",
          "The function name `get_litellm_provider` should return an instance of `LiteLLMProvider`.",
          "The function does not raise any exceptions if the 'litellm' provider is not installed or configured correctly.",
          "The function returns an instance of `LiteLLMProvider` even when the 'litellm' provider is not found in the configuration.",
          "The function name `get_litellm_provider` should be able to handle different providers, including 'litellm'.",
          "The function does not throw any errors if the 'litellm' provider is not installed or configured correctly."
        ],
        "scenario": "Verifies that the `get_litellm_provider` function returns an instance of `LiteLLMProvider`.",
        "why_needed": "Prevents a potential bug where the test fails if the 'litellm' provider is not installed or configured correctly."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 245, 247, 249-250"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007932910000363336,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_provider` function should return an instance of `NoopProvider`.",
          "The `provider` attribute of the returned `NoopProvider` instance should be `None`.",
          "The `Config` class constructor with a `provider` parameter should not throw an exception if no provider is specified.",
          "The `get_provider` function should handle cases where no provider is specified in the configuration without raising an error.",
          "The `provider` attribute of the returned `NoopProvider` instance should be `None` when no provider is specified.",
          "The `Config` class constructor with a `provider` parameter should not throw an exception if no provider is specified.",
          "The `get_provider` function should return an instance of `NoopProvider` when no provider is specified in the configuration."
        ],
        "scenario": "Verifies that the `get_noop_provider` function returns an instance of `NoopProvider` when no provider is specified.",
        "why_needed": "Prevents a potential bug where the test fails if no provider is specified in the configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 245, 247, 249, 252-253, 255"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007591370000454845,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` attribute of the returned `OllamaProvider` instance is set to `'ollama'`.",
          "The `provider` attribute of the returned `OllamaProvider` instance is not set to an empty string or None.",
          "The `provider` attribute of the returned `OllamaProvider` instance has a value that matches the expected configuration ('ollama')."
        ],
        "scenario": "Verify that the `get_ollama_provider` function returns an instance of `OllamaProvider` for a valid configuration.",
        "why_needed": "This test prevents a potential bug where the `get_ollama_provider` function does not return an instance of `OllamaProvider` when a valid configuration is provided."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 107-108, 110-111"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008445670000014616,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method should return True when the provider is configured correctly and has no cached results.",
          "The `is_available()` method should return False when the provider is configured incorrectly or has cached results.",
          "The `checks` attribute of the provider should be incremented each time `_check_availability()` is called."
        ],
        "scenario": "Verify that the `is_available()` method returns a boolean indicating availability",
        "why_needed": "This test prevents regression in cases where LLM providers are not available due to cache issues."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "52-53, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007743250000089574,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert provider.get_model_name() == 'test-model'",
          "provider.config.model == 'test-model'",
          "provider.config.model_type == ModelConfig",
          "provider.model_name == 'test-model'",
          "provider.model_type == ModelType",
          "provider.config.name == 'TestModel'"
        ],
        "scenario": "The `get_model_name()` method of the `ConcreteProvider` class should return the model name specified in the configuration.",
        "why_needed": "This test prevents a regression where the default model name is not correctly set to the configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "52-53, 128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007886319999670377,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `rate_limit` attribute is missing from the provider's configuration.",
          "The `max_rate` attribute is set to `None` or an invalid value.",
          "The `min_rate` attribute is set to a positive value.",
          "The `timeout` attribute is not set.",
          "The `batch_size` attribute is not set.",
          "The `num_threads` attribute is not set.",
          "The `max_queue_size` attribute is not set."
        ],
        "scenario": "The `get_rate_limits` method of the `ConcreteProvider` class returns `None` when called with default configuration.",
        "why_needed": "This test prevents a potential bug where the rate limits are not set correctly in the default configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "52-53, 147"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008016159999897354,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider.is_local()` method should return `False` for a non-local configuration.",
          "A non-local configuration should not trigger the `is_local` check to return `True`.",
          "The `is_local` method should raise an error or return a meaningful value when given an invalid configuration.",
          "When using a different provider, the `is_local` method should still return `False` for a non-local configuration.",
          "A local configuration should not be considered valid even if the `provider` is set to `None` or an empty object.",
          "The test should cover cases where the `config` object has been modified after the `provider` was created."
        ],
        "scenario": "Verifies that the `is_local` method returns `False` when provided with a non-local configuration.",
        "why_needed": "Prevents regression in case of incorrect or outdated configuration settings."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007823900000403228,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "source_code_is_same",
          "hashes_are_equal",
          "same_source_produces_same_hash"
        ],
        "scenario": "The function `hash_source` is called with a source code that produces the same result.",
        "why_needed": "This test prevents a bug where different sources of input produce different hashes, potentially leading to inconsistencies in caching."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007764190000330018,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `hash_source` function should return a different hash value for two different source strings.",
          "The `hash_source` function should not return the same hash value for the same source string and a different function name.",
          "The `hash_source` function should handle cases where the function body is identical but has different names.",
          "The `hash_source` function should ignore case differences in the source strings.",
          "The `hash_source` function should not cache function calls with the same source string and different function names."
        ],
        "scenario": "Testing the cache with different sources and hashes.",
        "why_needed": "This test prevents a potential issue where the same function call results in different hash values due to caching."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007567220000055386,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the hash should be exactly 16 characters.",
          "The hash value for the input 'test' should have a length of 16 characters.",
          "The hash value for other inputs should also have a length of 16 characters."
        ],
        "scenario": "Verifies the length of a hashed string.",
        "why_needed": "Prevents a bug where the hash length is not consistent across different inputs."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_hash_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 26,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0012995060000093872,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of cache entries should be reduced to 2 after clearing.",
          "A specific annotation ('test::a', 'hash1') should no longer exist in the cache.",
          "A specific annotation ('test::b', 'hash2') should no longer exist in the cache.",
          "All cache keys for a given scenario should not have any entries.",
          "The value associated with an entry of a given scenario and key should be None after clearing."
        ],
        "scenario": "Test that clearing the cache removes all entries.",
        "why_needed": "Prevents regression where cache entries are not properly cleared after test execution."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_clear",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 11,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008783789999711189,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 9,
          "line_ranges": "39-41, 53, 55-56, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010418949999575489,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get` method should return `None` when called with a non-existent key.",
          "The `cache.get()` function should raise a `KeyError` exception when given a non-existent key.",
          "The `result` variable should be set to `None` after calling `cache.get()`.",
          "The test should fail when the `get` method returns `None` for a missing entry in the cache.",
          "The test should verify that an error is raised when trying to access a non-existent key in the cache."
        ],
        "scenario": "Test that `get` method returns `None` for missing entries in the cache.",
        "why_needed": "Prevents a potential bug where a missing entry in the cache causes an incorrect result when trying to access it."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_get_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 28,
          "line_ranges": "39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0011021070000083455,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check if the annotation is set with the correct key",
          "Check if the annotation's scenario matches the expected value",
          "Check if the annotation's confidence matches the expected value",
          "Verify that the cache returns a non-None result for the retrieved annotation"
        ],
        "scenario": "Test that annotations are stored in the cache and retrieved correctly.",
        "why_needed": "Prevents bypass by ensuring annotations are cached before they are used to generate the response."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_set_and_get",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007838029999902574,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The nodeid of the error should match the expected value 'test_bad.py'.",
          "The message of the error should be 'SyntaxError'."
        ],
        "scenario": "Test verifies that collection errors have the correct structure.",
        "why_needed": "This test prevents a potential bug where incorrect structure is reported for collection errors."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007661800000278163,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts that the `get_collection_errors` method returns an empty list",
          "asserts that the `get_collection_errors` method does not raise any exceptions when called on an empty collection",
          "asserts that the `get_collection_errors` method correctly handles the case of an empty collection"
        ],
        "scenario": "Verifies that the `get_collection_errors` method returns an empty list when the collection is initially empty.",
        "why_needed": "Prevents a potential bug where an empty collection is returned unexpectedly."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007941420000179278,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The llm_context_override attribute should be None for the test case.",
          "The TestCaseResult object should have an llm_context_override attribute with a value of None.",
          "If llm_context_override is not None, it should be set to None in the TestCaseResult object.",
          "If llm_context_override is not None, it should be set to None before the test passes."
        ],
        "scenario": "Test the default value of llm_context_override in TestCollectorMarkerExtraction.",
        "why_needed": "Prevents a potential bug where the default value of llm_context_override is not set correctly."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007812079999780508,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The llm_opt_out attribute should be set to False.",
          "The llm_opt_out attribute should not be set to True.",
          "The llm_opt_out attribute should have a default value of False.",
          "The TestCaseResult object should contain an llm_opt_out attribute with a value of False.",
          "The TestCaseResult object should contain an llm_opt_out attribute that is not set to True."
        ],
        "scenario": "Test case \"tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false\" verifies that the default value of llm_opt_out is set to False.",
        "why_needed": "This test prevents a regression where the default value of llm_opt_out was incorrectly set to True, potentially causing issues with the test report."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007732529999771032,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.capture_failed_output should be set to False",
          "the `capture` feature should not be enabled by default",
          "output capture functionality should only be available when explicitly enabled",
          "the `capture` feature should have a clear and consistent behavior"
        ],
        "scenario": "The `capture` feature is not enabled by default.",
        "why_needed": "This test prevents a potential bug where the output capture feature is unintentionally enabled by default."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_disabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007921380000084355,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008108739999670433,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 26,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007762490000118305,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'xfail passes' message should be displayed as 'xpassed' when an XFAIL is passed.",
          "The 'xfail passes' message should not be displayed as 'failed' when an XFAIL is passed.",
          "The 'xfail passes' message should not be displayed as 'skipped' when an XFAIL is passed.",
          "The test results should correctly identify the nodeid as 'test_xfail.py::test_unexpected_pass'.",
          "The duration of the runtest log report should be set to 0.01 seconds.",
          "The longrepr field should be empty.",
          "The wasxfail field should contain the expected failure message.",
          "The outcome of the test result should be 'xpassed' when an XFAIL is passed."
        ],
        "scenario": "Test 'xfail passes should be recorded as xpassed' verifies that when an XFAIL is passed, it is correctly marked as xpassed in the test results.",
        "why_needed": "This test prevents regression by ensuring that XFAILs are properly recorded and handled by the TestCollector."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008214329999987058,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `results` attribute of the `collector` object should be an empty dictionary.",
          "The `collection_errors` list should be an empty list.",
          "The `collected_count` attribute of the `collector` object should be 0."
        ],
        "scenario": "Test the `create_collector` method of `TestCollector` class.",
        "why_needed": "This test prevents a potential bug where the `TestCollector` instance is not initialized properly, leading to incorrect results in subsequent methods."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_create_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007797850000201834,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The list of node IDs returned by the `get_results` method should be in ascending order.",
          "The list of node IDs returned by the `get_results` method should contain all nodes from both tests.",
          "All nodes with a 'passed' outcome should appear before any nodes with an 'unpassed' outcome.",
          "If there are multiple results for the same test, they should be sorted alphabetically by node ID.",
          "The order of the first two tests in the list should match their original order.",
          "The last test in the list should not have a 'passed' or 'unpassed' outcome."
        ],
        "scenario": "Test that the `get_results` method returns sorted results by node ID.",
        "why_needed": "This test prevents a regression where the order of test results is not preserved."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007828409999888208,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'collected_count' attribute should be set to the number of collected items.",
          "The 'deselected_count' attribute should be set to the number of deselected items.",
          "The 'collected_count' and 'deselected_count' attributes should match the actual counts after calling handle_collection_finish."
        ],
        "scenario": "Test the TestCollector's handle_collection_finish method to ensure it correctly tracks collected and deselected counts.",
        "why_needed": "This test prevents a potential bug where the count of collected items is not incremented correctly when the collector finishes collecting."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_handle_collection_finish",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0015603420000047663,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collector.handle_runtest_logreport(report)` call should not modify the captured stdout.",
          "The `results` dictionary should still contain the original captured stdout value.",
          "The `collector.results['t']` attribute should return the original captured output.",
          "The `collector.results['t'].captured_stdout` attribute should be None.",
          "The `collector.handle_runtest_logreport(report)` call should not modify the `report.wasxfail` flag."
        ],
        "scenario": "Verify that the test does not capture output when config is disabled and handle_report integration.",
        "why_needed": "To prevent capturing of output in cases where the config is disabled, allowing for better integration with handle_runtest_logreport."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009101690000079543,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `captured_stderr` attribute of the `TestCaseResult` object is set to 'Some error'.",
          "The `report.capstderr` method is called with an argument of 'Some error'.",
          "The `collector._capture_output(result, report)` function is called with arguments that include a `report` object and a `result` object.",
          "The `captured_stderr` attribute of the `result` object is set to 'Some error'."
        ],
        "scenario": "Verifies that the `TestCollector` class captures stderr output correctly.",
        "why_needed": "Prevents a bug where stderr is not captured as expected."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009077639999759413,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `captured_stdout` attribute of the `TestCaseResult` object should contain the expected output.",
          "The `capstdout` attribute of the `report` object should be set to 'Some output'.",
          "The `capture_output` method should record the captured stdout correctly."
        ],
        "scenario": "Test that the `capture_output` method captures stdout correctly.",
        "why_needed": "This test prevents a potential bug where the captured stdout is not properly recorded."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010235800000373274,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured stdout length should be less than or equal to 10 (the capture_output_max_chars parameter).",
          "The captured stderr length should be zero, indicating no error message was truncated.",
          "The `captured_stdout` attribute of the `TestCaseResult` object should contain only the first 9 characters ('1234567890')."
        ],
        "scenario": "Test that the `test_capture_output_truncated` function truncates output exceeding max chars.",
        "why_needed": "This test prevents a potential bug where the collector fails to truncate output exceeding the maximum characters."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 35,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0021568459999912193,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "item.callspec.id should be set to 'param1' after calling get_closest_marker('llm_opt_out')",
          "result.param_id should be set to 'param1'",
          "result.llm_opt_out should be set to True after calling get_closest_marker('llm_opt_out')",
          "result.llm_context_override should be set to 'complete' after calling get_closest_marker('llm_context')",
          "result.requirements should contain exactly two strings: 'REQ-1' and 'REQ-2'"
        ],
        "scenario": "Should extract markers from item and return the expected result with correct parameters and context.",
        "why_needed": "This test prevents a potential bug where the collector does not correctly extract markers from an item, leading to incorrect results in subsequent steps."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0014482630000429708,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `extract_error` method should return 'Crash report' without crashing when called with a `Report` object containing a `longrepr` attribute.",
          "The `extract_error` method should not crash if it encounters an instance of `str` that is equal to the string representation of a `ReprFileLocation` object.",
          "The `extract_error` method should return the correct error message when called with a `Report` object containing a `longrepr` attribute.",
          "The `extract_error` method should not crash if it encounters an instance of `str` that is equal to the string representation of a `ReprFileLocation` object without causing a crash.",
          "The `extract_error` method should handle cases where the `Report` object does not contain a `longrepr` attribute, and still return 'Crash report'.",
          "The `extract_error` method should not crash if it encounters an instance of `str` that is equal to the string representation of a `ReprFileLocation` object without causing a crash.",
          "The `extract_error` method should handle cases where the `Report` object contains multiple `longrepr` attributes, and still return 'Crash report'.",
          "The `extract_error` method should not crash if it encounters an instance of `str` that is equal to the string representation of a `ReprFileLocation` object without causing a crash.",
          "The `extract_error` method should handle cases where the `Report` object contains a `longrepr` attribute with a different type than `str`, and still return 'Crash report'."
        ],
        "scenario": "Test that the `extract_error` method handles ReprFileLocation correctly and does not crash when called with a string representation.",
        "why_needed": "To prevent a potential crash caused by using `str()` on a `ReprFileLocation` object in the `extract_error` method."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009296050000102696,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_error` method should return the string provided by the `report.longrepr` attribute.",
          "The extracted string should match the original value of `report.longrepr` exactly.",
          "If an error occurs before the collector can extract it, the test should still pass without raising an assertion error."
        ],
        "scenario": "Test that the `_extract_error` method returns the correct string when given a `report` object with a `longrepr` attribute.",
        "why_needed": "This test prevents regression where an incorrect or incomplete error message is returned in cases where the error occurs before the collector has a chance to extract it."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009115619999988667,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method should not return `None` but rather raise an exception or return a specific value indicating that no longrepr was found.",
          "The method should check if the `longrepr` attribute of the report object is `None` before calling `_extract_skip_reason` and raise an exception if it is.",
          "The method should handle cases where the `report.longrepr` attribute is not set or is empty, and return a meaningful error message in such cases."
        ],
        "scenario": "Test the `_extract_skip_reason` method of `TestCollector` when no longrepr is provided.",
        "why_needed": "Prevents a potential bug where the method returns `None` instead of raising an exception or returning a meaningful error message when no longrepr is available."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009377499999914107,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "report.longrepr == 'Just skipped'",
          "collector._extract_skip_reason(report) == 'Just skipped'",
          "report.longrepr != 'Just ignored' (this test case should have a skip reason)",
          "collector._extract_skip_reason(report) != 'Just ignored' (this test case should have a skip reason)",
          "report.longrepr != 'Just skipped and then ignored' (this test case should have two different reasons for skipping)",
          "collector._extract_skip_reason(report) != 'Just skipped and then ignored'"
        ],
        "scenario": "The test verifies that the `test_extract_skip_reason_string` function returns 'Just skipped' as the skip reason string.",
        "why_needed": "This test prevents a potential regression where the expected output of `_extract_skip_reason(report)` is not 'Just skipped'."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009262080000098649,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The expected string is present in the `report.longrepr` tuple.",
          "The `str()` function call on the `report.longrepr` tuple returns a value that matches the expected string.",
          "The extracted skip reason is not empty or None.",
          "The skip message does not contain any newline characters.",
          "The skip message does not contain any whitespace characters.",
          "The file name in the skip message is present and correct.",
          "The line number in the skip message is present and correct.",
          "The message in the skip reason tuple is present and correct."
        ],
        "scenario": "The test verifies that the `TestCollector` class can extract a skip message from a tuple containing file, line and message information.",
        "why_needed": "This test prevents a potential bug where the `TestCollector` class does not correctly extract the skip reason from tuples containing file, line and message information."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 21,
          "line_ranges": "58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009234530000412633,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0014022870000189869,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `rerun_count` attribute of the report should be set to 1 after a rerun.",
          "The `final_outcome` attribute of the report should be set to 'failed' after a rerun.",
          "The `rerun_count` attribute should increase by 1 after each rerun.",
          "The `final_outcome` attribute should remain unchanged (i.e., 'passed') after all reruns."
        ],
        "scenario": "The test verifies that the `handle_runtest_rerun` method of `TestCollector` correctly handles reruns by updating the `rerun_count` and `final_outcome` attributes in the report.",
        "why_needed": "This test prevents a regression where the `rerun_count` attribute is not updated correctly after a rerun, potentially leading to incorrect reporting or analysis."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009315590000369411,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report should have an 'error' outcome.",
          "The phase of the test should be 'setup'.",
          "The error message should contain 'Setup failed'."
        ],
        "scenario": "Test verifies that the TestCollector handles a run test setup failure correctly.",
        "why_needed": "This test prevents regression in handling setup failures, ensuring that the collector logs and reports the error properly."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 38,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0012616440000101647,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `teardown` report is not recorded if the teardown fails.",
          "The error message 'Cleanup failed' is logged when the teardown fails.",
          "The outcome of the runtest is set to 'error' after a teardown failure.",
          "The phase of the runtest is set to 'teardown' after a teardown failure.",
          "The error message is not cleared from the `wasxfail` attribute of the teardown report."
        ],
        "scenario": "Test case: Handle runtest teardown failure",
        "why_needed": "Prevents regression in scenario where teardown fails after pass."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 17,
          "line_ranges": "134, 136-139, 141-142, 391, 393, 423-430"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008416509999733535,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `provider._parse_preferred_models()` returns a list of preferred models.",
          "The function `provider._parse_preferred_models()` returns an empty list when the model is None.",
          "The function `provider._parse_preferred_models()` returns an empty list when the model is 'All'."
        ],
        "scenario": "Test the Gemini model parsing edge cases for coverage boosters.",
        "why_needed": "This test prevents regression in coverage analysis when encountering edge cases with no preferred models or all models."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 35,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008385049999901639,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in() method should return a non-zero value (0) for both over and under token limits.",
          "The record_tokens() method should not allow more than the maximum allowed limit (100 tokens per minute).",
          "The limiter should correctly prevent requests from being blocked when there are insufficient tokens available."
        ],
        "scenario": "Verify that the rate limiter prevents over and under token limits when recording tokens but not requests.",
        "why_needed": "This test prevents a bug where the rate limiter allows too many tokens to be recorded without preventing subsequent requests from being blocked."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 46,
          "line_ranges": "71-78, 104-107, 109, 111-113, 115, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007920479999938834,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'coverage_percent' key in the resulting dictionary should match the provided value (50.0) for SourceCoverageEntry objects.",
          "The 'error' key in the resulting dictionary should match the provided error message ('timeout') for LlmAnnotation objects.",
          "The 'duration' key in the resulting dictionary should match the provided duration value (1.0) for RunMeta objects."
        ],
        "scenario": "Verify that the `to_dict()` method returns accurate coverage percentages for SourceCoverageEntry objects.",
        "why_needed": "This test prevents regression in coverage calculation when using models to dict variants, ensuring consistency with expected behavior."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008176059999982499,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 3,
          "line_ranges": "44-45, 308"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000805492999973012,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_warnings()` method should return a list of warnings.",
          "The `get_warnings()` method should be able to handle any configuration or environment that may result in no warnings being generated.",
          "Any exceptions raised by the `get_warnings()` method should be caught and reported as an error.",
          "The test should fail if the `get_warnings()` method returns a non-list value, indicating a bug in the implementation.",
          "The test should pass if the `get_warnings()` method returns a list of warnings as expected, regardless of the configuration or environment."
        ],
        "scenario": "Test 'tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings' verifies that the `get_warnings` method returns a list of warnings.",
        "why_needed": "This test prevents a potential bug where the `get_warnings` method does not return a list of warnings, potentially masking important error information."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002271439000026021,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty dictionary.",
          "The function should have at least one warning.",
          "The `Path.exists` mock returns False and the `glob.glob` mock returns an empty list."
        ],
        "scenario": "Test that `map_coverage` returns an empty dictionary when no coverage file exists.",
        "why_needed": "Prevents a regression where the test fails due to missing coverage data."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008255110000163768,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method `_extract_nodeid` returns the correct node ID for each phase.",
          "The method `_extract_nodeid` returns the same node ID for multiple phases in a single test.",
          "The method `_extract_nodeid` handles cases where the phase name is not present in the code snippet.",
          "The method `_extract_nodeid` handles cases where the phase name contains special characters or spaces.",
          "The method `_extract_nodeid` correctly extracts node IDs for multiple phases in a single test.",
          "The method `_extract_nodeid` returns the correct node ID when the `include_phase` parameter is set to 'all'."
        ],
        "scenario": "The test verifies that the `CoverageMapper` correctly extracts node IDs for all phases when the `include_phase` parameter is set to 'all'.",
        "why_needed": "This test prevents a regression where the coverage map might not include all phases if the `include_phase` parameter is set to 'none' or 'partial'."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007733529999995881,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 216, 220, 224-225, 228-230"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007695259999991322,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_nodeid() is called with the correct argument 'test.py::test_foo|setup'.",
          "The result of the call is not None (i.e., it does not return a node ID).",
          "The extracted node ID matches the expected pattern for tests in the setup phase.",
          "The test passes if the function correctly extracts node IDs without returning any value."
        ],
        "scenario": "Test should filter out setup phase when include_phase=run.",
        "why_needed": "This test prevents a potential bug where the test may incorrectly extract node IDs for tests in the setup phase, leading to incorrect coverage analysis."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007876199999827804,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_nodeid() of CoverageMapper returns the correct nodeid for the given input.",
          "The nodeid returned by _extract_nodeid() matches the expected value.",
          "The nodeid is present in the run phase context.",
          "The nodeid does not contain any unnecessary characters or whitespace.",
          "The nodeid is correctly formatted as a string.",
          "The function _extract_nodeid() handles invalid input gracefully."
        ],
        "scenario": "Test the ability to extract nodeid from run phase context.",
        "why_needed": "This test prevents a potential bug where the nodeid is not extracted correctly from the run phase context."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 57,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0015697199999635814,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_contexts should return all paths in mock_data.contexts_by_lineno for the given input.",
          "The function _extract_contexts should include 'test_app.py::test_one' and 'test_app.py::test_two' in the result.",
          "The function _extract_contexts should correctly count lines 1 and 2 for 'app.py'.",
          "The function _extract_contexts should return a list with one element containing 'test_app.py::test_one'.",
          "The function _extract_contexts should include all files from mock_data.measured_files in the result.",
          "The function _extract_contexts should correctly extract contexts for files with maximal coverage."
        ],
        "scenario": "Test that the mapper extracts all contexts for a file with maximal coverage.",
        "why_needed": "This test prevents regression where the mapper does not extract all contexts for files with maximal coverage."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0011878669999987324,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_contexts` method should return an empty dictionary when given mock_data with no measured files.",
          "The `contexts_by_lineno` method should return an empty dictionary for all files in mock_data.",
          "mock_data.measured_files.return_value should not contain any values.",
          "mock_data.contexts_by_lineno.return_value should be an empty dictionary.",
          "result should be equal to {} when given mock_data with no measured files.",
          "The `extract_contexts` method should handle data with no test contexts correctly."
        ],
        "scenario": "Test that the `extract_contexts` method handles data with no test contexts correctly.",
        "why_needed": "This test prevents a regression where the coverage map is not generated for files without test contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008315420000144513,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_extract_nodeid` method returns the expected node id for each line.",
          "The `NoneType` exception is raised when a line has no matching phase.",
          "The context without a pipe (`|`) is correctly identified as having no matching phase.",
          "The test coverage is accurate and comprehensive, including missing lines in the code."
        ],
        "scenario": "Test the `CoverageMapper` with different node id variants.",
        "why_needed": "To ensure that the test coverage is accurate and comprehensive, especially when there are missing lines in the code."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010530159999575517,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return None for _load_coverage_data() without any .coverage files.",
          "The number of warnings should be 1, with code 'W001'.",
          "The first warning should have the correct code 'W001'."
        ],
        "scenario": "Test that the function does not fail when no coverage files exist.",
        "why_needed": "Prevents a potential bug where the test fails due to missing coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 17,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0016094240000370519,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _load_coverage_data() should return None when an error occurs while reading the coverage data.",
          "Any warnings generated by the mapper should contain the message 'Failed to read coverage data'.",
          "The mapper's warnings list should not be empty after this test is run."
        ],
        "scenario": "Test covers the scenario where a coverage file cannot be read successfully.",
        "why_needed": "This test prevents a potential regression where the CoverageMapper does not handle errors reading coverage files correctly."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0025714899999798035,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock instances of `CoverageData` returned by `mock_data_cls.side_effect` should have been updated at least twice.",
          "The `update` method of `CoverageData` should have been called on the first instance.",
          "The `update` method of `CoverageData` should have been called on the second instance.",
          "The `update` method of `CoverageData` should not be called on the third instance.",
          "The mock instances of `CoverageData` should not have been updated again after the test has finished."
        ],
        "scenario": "Test should handle parallel coverage files from xdist and verify that it correctly updates the coverage data.",
        "why_needed": "This test prevents regression in handling parallel coverage files from xdist, which could lead to incorrect coverage data being reported."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 5,
          "line_ranges": "44-45, 58-60"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009758709999800885,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty dictionary when `_load_coverage_data` is called with a `None` value.",
          "The function should not raise any exceptions when `_load_coverage_data` returns None.",
          "The `map_coverage` method should be able to handle the case where no coverage data is available without crashing or throwing an exception.",
          "The test should verify that the returned dictionary has zero keys (i.e., no coverage information).",
          "The function should not return a dictionary with any specific structure when `_load_coverage_data` returns None.",
          "The `map_coverage` method should be able to handle different types of coverage data (e.g., JSON, CSV, etc.) without issues."
        ],
        "scenario": "Test that the `map_coverage` method returns an empty dictionary when `_load_coverage_data` returns None.",
        "why_needed": "Prevents a potential bug where the test fails due to missing coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.001507031999949504,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock_cov.analysis2 method is called and it should raise an exception.",
          "The mock_data.measured_files.return_value is set to ['app.py']",
          "The mock_cov.get_data.return_value is set to mock_data",
          "The entries list returned by mapper.map_source_coverage() should be empty.",
          "The length of the entries list should not exceed 0.",
          "An exception is raised in the analysis2 method."
        ],
        "scenario": "The test verifies that the CoverageMapperMaximal class handles analysis failures by skipping source coverage files with errors.",
        "why_needed": "This test prevents a regression where the CoverageMapperMaximal class incorrectly includes error messages in its output."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0017528710000078718,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `map_source_coverage` should return one entry for each file in `mock_cov.get_data()` with the correct number of statements, covered percentage, missed percentage, and coverage percent.",
          "The entries returned by `map_source_coverage` should have a 'file_path' attribute set to the expected file path ('app.py')",
          "The entries returned by `map_source_coverage` should have a 'statements' attribute equal to 3 (the number of statements in 'app.py')",
          "The entries returned by `map_source_coverage` should have a 'covered' attribute equal to 2 (the coverage percentage for 'app.py')",
          "The entries returned by `map_source_coverage` should have a 'missed' attribute equal to 1 (the number of statements not covered in 'app.py')",
          "The entries returned by `map_source_coverage` should have a 'coverage_percent' attribute equal to 66.67% (the coverage percentage for 'app.py')"
        ],
        "scenario": "Test 'map_source_coverage_comprehensive' verifies that the coverage mapper correctly maps source code to coverage data.",
        "why_needed": "This test prevents regression in coverage calculation when analyzing multiple files with different statements and coverage percentages."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007540170000197577,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The Warning object created by make_warning should have a 'code' attribute equal to WarningCode.W001_NO_COVERAGE.",
          "The Warning object created by make_warning should have a 'message' attribute that contains the string 'No .coverage file found'.",
          "The Warning object created by make_warning should have a 'detail' attribute that is set to 'test-detail'."
        ],
        "scenario": "Test the make_warning factory function to ensure it correctly creates a Warning with the correct code and message.",
        "why_needed": "This test prevents a potential bug where the make_warning function returns an unknown warning without providing any additional information."
      },
      "nodeid": "tests/test_errors.py::test_make_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008077479999997195,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"The value of WarningCode.W001_NO_COVERAGE is equal to 'W001'.\", 'description': 'Expected value of WarningCode.W001_NO_COVERAGE'}",
          "{'message': \"The value of WarningCode.W101_LLM_ENABLED is equal to 'W101'.\", 'description': 'Expected value of WarningCode.W101_LLM_ENABLED'}",
          "{'message': \"The value of WarningCode.W201_OUTPUT_PATH_INVALID is equal to 'W201'.\", 'description': 'Expected value of WarningCode.W201_OUTPUT_PATH_INVALID'}",
          "{'message': \"The value of WarningCode.W301_INVALID_CONFIG is equal to 'W301'.\", 'description': 'Expected value of WarningCode.W301_INVALID_CONFIG'}",
          "{'message': \"The value of WarningCode.W401_AGGREGATE_DIR_MISSING is equal to 'W401'.\", 'description': 'Expected value of WarningCode.W401'}"
        ],
        "scenario": "Test that warning codes have correct values.",
        "why_needed": "This test prevents a potential regression where the warning code values are not correctly validated, potentially leading to incorrect handling of warnings in downstream code."
      },
      "nodeid": "tests/test_errors.py::test_warning_code_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 6,
          "line_ranges": "70-72, 74-76"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007711989999847901,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `code` attribute of the `Warning` object should be set to 'W001' when converting it to a dictionary.",
          "The `message` attribute of the `Warning` object should be set to 'No coverage' when converting it to a dictionary.",
          "The `detail` attribute of the `Warning` object should be set to 'some/path' when converting it to a dictionary.",
          "When setting `detail`, the test checks that its value is not empty.",
          "The `code` attribute of the resulting dictionary should match the expected value 'W001'.",
          "The `message` attribute of the resulting dictionary should match the expected value 'No coverage'.",
          "The `detail` attribute of the resulting dictionary should match the expected value 'some/path'."
        ],
        "scenario": "Test the `to_dict()` method of `Warning` class to ensure it correctly converts warnings into a dictionary.",
        "why_needed": "This test prevents a potential bug where the warning information is not properly extracted from the `Warning` object and stored in the dictionary."
      },
      "nodeid": "tests/test_errors.py::test_warning_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007435879999775352,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "'WarningCode.W101_LLM_ENABLED' should be equal to WarningCode.W101_LLM_ENABLED",
          "'WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]' should be equal to WARNING_MESSAGES[W101_LLM_ENABLED]",
          "w.detail is None"
        ],
        "scenario": "Test verifies that a warning with the standard message is created when known code is used.",
        "why_needed": "This test prevents a potential bug where warnings are not generated for known code, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007932810000284007,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `make_warning` function should be called with the correct warning code (`WarningCode.W001_NO_COVERAGE`) when an unknown code is encountered.",
          "The expected fallback message for unknown code (`Unknown warning.`) should match the actual returned message.",
          "The `WARNING_MESSAGES` dictionary should contain the original message before it was modified by the test.",
          "After restoring the original message, the updated `WARNING_MESSAGES` dictionary should still contain the correct value.",
          "The `missing_code` variable should hold the valid warning code (`WarningCode.W001_NO_COVERAGE`) that is used when an unknown code is encountered.",
          "The `old_message` variable should be set to the expected fallback message (`Unknown warning.`) before calling `make_warning` with the invalid code."
        ],
        "scenario": "Test MakeWarning::test_make_warning_unknown_code verifies that the test prevents a bug by ensuring the fallback message is used for unknown code.",
        "why_needed": "This test prevents a regression where the fallback message is not used for unknown code, potentially leading to incorrect error messages in future versions of the library."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007693660000427371,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "w.code == WarningCode.W301_INVALID_CONFIG",
          "w.detail == 'Bad value'"
        ],
        "scenario": "Test 'test_make_warning_with_detail' verifies creating a warning with detail.",
        "why_needed": "This test prevents the creation of warnings without detail, which can lead to misleading error messages."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007591270000375516,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert isinstance(code.value, str)",
          "assert code.value.startswith('W')",
          "code.value should be a string (not an integer or other type)",
          "code.value should start with 'W' (the warning code prefix)",
          "WarningCode is correctly defined and accessible within the test scope"
        ],
        "scenario": "Ensures that enum values are correctly converted to strings.",
        "why_needed": "Prevents a potential bug where enum values are not properly converted to strings."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 5,
          "line_ranges": "70-72, 74, 76"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007927890000019033,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `to_dict()` method of the `Warning` class returns a dictionary with the correct keys and values.",
          "The keys 'code' and 'message' are present in the returned dictionary.",
          "The value for key 'code' is set to the expected string value 'W001'.",
          "The value for key 'message' is set to the expected string value 'No coverage'."
        ],
        "scenario": "Test the warning to dictionary serialization without detail.",
        "why_needed": "Prevents a potential bug where warnings are not properly serialized to dictionaries, potentially leading to incorrect data in logs or reports."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 6,
          "line_ranges": "70-72, 74-76"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.00075778400002946,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'to_dict()' method of the Warning class returns a dictionary with the correct keys.",
          "The 'code' key in the returned dictionary has the expected value.",
          "The 'message' key in the returned dictionary has the expected value.",
          "The 'detail' key in the returned dictionary has the expected value.",
          "The 'WarningCode.W001_NO_COVERAGE' attribute is correctly converted to a string.",
          "The 'WarningCode.W001_NO_COVERAGE' attribute is not set to an empty string."
        ],
        "scenario": "Test warning to dictionary with detail should be performed.",
        "why_needed": "This test prevents a potential bug where the Warning data class is not properly serialized into a dictionary, potentially causing issues when using it in certain scenarios."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007474149999779911,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return False when given a non-.py file extension (e.g. foo/bar.txt).",
          "The function should return False when given a non-.pyc file extension (e.g. foo/bar.pyc).",
          "The function should raise an AssertionError when given a non-python file.",
          "The function should not incorrectly identify .pyc files as non-py files."
        ],
        "scenario": "Verifies that the function returns False for non-python file extensions.",
        "why_needed": "Prevents a potential bug where the function incorrectly identifies .pyc files as non-py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0008842400000048656,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return `True` for files with extensions like `.py`.",
          "The function should ignore other file extensions (e.g., `.txt`, `.js`) and only check for `.py` files.",
          "The function should raise an error or handle the case where it's not a Python file correctly."
        ],
        "scenario": "Verifies that the `is_python_file` function returns True for a `.py` file.",
        "why_needed": "Prevents a potential bug where the function incorrectly identifies non-`.py` files as Python files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.001111935999972502,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `make_relative` function should be able to correctly make the specified file relative to the provided path.",
          "The `parent.mkdir(parents=True, exist_ok=True)` call should not raise an exception if the directory already exists.",
          "The `touch()` call should create a new file with the correct name and permissions.",
          "The `make_relative()` function should return the expected output string ('subdir/file.py') after making the specified path relative.",
          "The `assert` statement should fail when the actual output does not match the expected output.",
          "The test should be able to handle cases where the file is created in a subdirectory with an existing parent directory.",
          "The test should be able to handle cases where the file has incorrect permissions or ownership after being made relative."
        ],
        "scenario": "Test makes absolute path relative by creating a subdirectory and making the specified file within it.",
        "why_needed": "This test prevents a potential bug where the `make_relative` function fails to create the expected output when dealing with files in subdirectories."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.0007415639999521773,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `make_relative` should return the original path 'foo/bar' when no base is specified.",
          "The function `make_relative` should handle cases where the input path has multiple levels of nesting correctly.",
          "The function `make_relative` should not modify the file system in any way, even if it returns a normalized path.",
          "The function `make_relative` should raise an error when given invalid input (e.g. non-string base or empty string).",
          "The function `make_relative` should correctly handle cases where the input path is absolute (i.e. starts with '/')."
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
        "why_needed": "This test prevents a potential bug where the function does not normalize the path when no base is provided."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007723010000404429,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert normalize_path('foo/bar') == 'foo/bar'",
          "assert normalize_path('/foo/bar') == '/foo/bar'",
          "assert normalize_path('//foo/bar') == '//foo/bar'",
          "assert normalize_path('///foo/bar') == '///foo/bar'",
          "assert normalize_path('foo//bar') == 'foo//bar'",
          "assert normalize_path('foo/./bar') == 'foo/./bar'",
          "assert normalize_path('/foo/./bar') == '/foo/./bar'",
          "assert normalize_path('//foo/./bar') == '//foo/./bar'"
        ],
        "scenario": "The test verifies that the `normalize_path` function correctly handles already-normalized paths.",
        "why_needed": "This test prevents a potential bug where an already normalized path would be incorrectly normalized back to its original form."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007530560000077458,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert normalize_path('foo\\bar') == 'foo/bar'",
          "assert normalize_path('/foo/bar') == '/foo/bar'",
          "assert normalize_path('foo/\\bar') == 'foo/bar'",
          "assert normalize_path('foo//bar') == 'foo/bar'",
          "assert normalize_path('foo/./bar') == 'foo/bar'",
          "assert normalize_path('foo/../bar') == 'foo/bar'",
          "assert normalize_path('/a/b/c') == '/a/b/c'"
        ],
        "scenario": "The test verifies that the `normalize_path` function correctly converts forward slashes in file paths to forward slashes.",
        "why_needed": "This test prevents a bug where the function fails to convert backslashes to forward slashes, potentially leading to incorrect path comparisons or file system operations."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007796449999659671,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert normalize_path('foo/bar/') == 'foo/bar'",
          "normalize_path('foo/').should_return('/')",
          "normalize_path('a/b/c/').should_return('a/b/c')"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
        "why_needed": "The test prevents a potential bug where the function does not handle cases with trailing slashes correctly."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.0007975480000368407,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return True for the 'tests/conftest.py' file and False for the 'src/module.py' file when exclude_patterns = ['test*']",
          "The function should correctly skip the 'tests/conftest.py' file due to the custom exclusion pattern",
          "The function should not incorrectly skip the 'src/module.py' file due to the custom exclusion pattern"
        ],
        "scenario": "Test verifies whether a path matches custom exclusion patterns.",
        "why_needed": "This test prevents a potential bug where paths matching custom patterns are incorrectly skipped."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007316459999628933,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007686150000267844,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('.git/objects/foo') is True",
          "should be equal to False"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_git verifies that the function should skip a .git directory.",
        "why_needed": "This test prevents a potential issue where the function incorrectly identifies a .git directory as a file and thus skips it."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007782220000080997,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007678630000214071,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert should_skip_path('venv/lib/python/site.py') is True",
          "assert should_skip_path('.venv/lib/python/site.py') is True"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
        "why_needed": "This test prevents a potential issue where the `should_skip_path` function incorrectly identifies venv directories as paths to Python packages."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 11,
          "line_ranges": "39-42, 81-85, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007776109999895198,
      "file_path": "tests/test_gemini_advanced.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of _request_times should be 0 after pruning.",
          "The length of _token_usage should be 0 after pruning.",
          "_request_times.append(time.monotonic() - 61) should add a record for the past request.",
          "_token_usage.append((time.monotonic() - 61, 100)) should add a record with token usage for the past request.",
          "limiter._prune(time.monotonic()) should clear _request_times and _token_usage records for the past request."
        ],
        "scenario": "Verify that pruning clears request and token usage records after a past request.",
        "why_needed": "This test prevents a potential bug where the rate limiter does not clear request and token usage records for past requests, leading to incorrect tracking of usage."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_pruning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 26,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007957149999811008,
      "file_path": "tests/test_gemini_advanced.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The rate limiter should be available again after calling `next_available_in(1)`.",
          "The rate limiter should not exceed 60 seconds when called with `next_available_in(1)`.",
          "The rate limiter's availability and time limit should match the expected values for a valid configuration."
        ],
        "scenario": "Test the RPM limit configuration and its effect on the rate limiter.",
        "why_needed": "This test prevents a potential bug where the rate limiter becomes unavailable after setting an incorrect `requests_per_minute` value."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 33,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-94, 100-101, 103, 105, 107-108, 110-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008172150000405054,
      "file_path": "tests/test_gemini_advanced.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next available time point should be greater than 0.",
          "The number of tokens used in the last 20 seconds should be less than or equal to 10.",
          "The number of tokens usage should increase by 1 after recording 10 new tokens."
        ],
        "scenario": "Verify that the rate limiter prevents a regression when the token limit is exceeded.",
        "why_needed": "This test prevents a bug where the rate limiter does not correctly handle cases when the token limit is exceeded."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_tpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 31,
          "line_ranges": "39-42, 45-46, 48, 52-54, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88, 92-93, 95, 97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0011218940000503608,
      "file_path": "tests/test_gemini_advanced.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `wait_for_slot` method should call `time.sleep` with an argument equal to the number of requests per minute specified in the configuration.",
          "The `wait_for_slot` method should assert that `mock_sleep` was called exactly once during the test.",
          "The `wait_for_slot` method should not assert anything if no requests have been recorded yet (i.e., it does not sleep).",
          "The `wait_for_slot` method should only assert that `mock_sleep` was called when a request is recorded and there are available slots.",
          "The rate limiter's internal state should be updated correctly after the test completes."
        ],
        "scenario": "Test the `wait_for_slot` method to ensure it sleeps as expected when a request is recorded.",
        "why_needed": "This test prevents a potential issue where the rate limiter does not sleep before checking if there are available slots."
      },
      "nodeid": "tests/test_gemini_advanced.py::TestGeminiRateLimiter::test_wait_for_slot",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 6,
          "line_ranges": "39-42, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007931400000416033,
      "file_path": "tests/test_gemini_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `_token_usage` is 0 after calling `record_tokens(0)`.",
          "The number of records in `_token_usage` remains unchanged even after the first call to `record_tokens(0)`.",
          "The rate limiter does not attempt to record tokens when there are no tokens available.",
          "The rate limiter correctly handles a zero token count without attempting to record additional tokens.",
          "The test passes with an empty `_token_usage` list."
        ],
        "scenario": "Verify that the rate limiter does not attempt to record tokens when there are no tokens available.",
        "why_needed": "This test prevents a potential regression where the rate limiter might incorrectly assume there are tokens available and attempt to record them, leading to incorrect usage statistics."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_limiter_record_zero_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0014886479999631774,
      "file_path": "tests/test_gemini_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `wait_for_slot` should raise `_GeminiRateLimitExceeded` with a message indicating that the requested number of slots exceeds the daily limit.",
          "The function `wait_for_slot` should not return immediately after raising the exception, allowing for further processing or retries.",
          "The function `record_request` should be called before attempting to wait for a slot.",
          "The function `wait_for_slot` should check if the requested number of slots exceeds the daily limit and raise an exception accordingly.",
          "The function `wait_for_slot` should not allow the test to complete without exhausting the rate limiter.",
          "The function `record_request` should be called before attempting to wait for a slot, allowing for any necessary processing or retries."
        ],
        "scenario": "Verify that the test raises a RateLimitExceeded exception when exceeding the daily limit.",
        "why_needed": "Prevents regression in case of excessive requests per day."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_limiter_requests_per_day_exhaustion",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "39-42, 66, 68-70, 81-82, 84, 87-88, 100-101, 103, 105, 107-108, 110-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007815890000415493,
      "file_path": "tests/test_gemini_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `wait` should be greater than zero.",
          "The sum of `tokens_used` and `request_tokens` should exceed the rate limit.",
          "If `token_usage` is empty, then `wait` should still be greater than zero."
        ],
        "scenario": "Test that the TPM fallback wait time is correctly calculated when filling up the rate limiter.",
        "why_needed": "This test prevents a potential regression where the TPM fallback wait time is too short, causing the rate limiter to fail or slow down unnecessarily."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_limiter_tpm_fallback_wait",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "52-53, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 117,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-223, 225-226, 235, 237-238, 242-244, 246-247, 280-283, 286, 288-296, 298-301, 303-304, 306-307, 352, 354-356, 358-359, 387-388, 391-392"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.5984766439999589,
      "file_path": "tests/test_gemini_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'models/gemini-pro' model should be present in the cooldowns dictionary.",
          "The value of the 'models/gemini-pro' model in the cooldowns dictionary should be greater than 1000.0 seconds.",
          "The time it takes to reach the cooldown threshold after a failed rate limit exceeded call should be less than or equal to 1 second."
        ],
        "scenario": "Test that RPM rate limit cooldown handling is properly implemented.",
        "why_needed": "To prevent the test from passing when the RPM rate limit is exceeded on the first call."
      },
      "nodeid": "tests/test_gemini_coverage_v2.py::test_gemini_provider_rpm_cooldown",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 181,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-215, 217-222, 225-226, 235, 237-238, 242-244, 246-247, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336-339, 341-347, 349, 352, 354-356, 358-361, 366-369, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.003828405000035673,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should contain the correct scenario 'Recovered Scenario'.",
          "The mock post call count should be equal to 2.",
          "The annotation should not have an error.",
          "The annotation's scenario attribute should match the expected value 'Recovered Scenario'.",
          "The annotation's status code should be 200 for the second successful response.",
          "The annotation should contain a valid model name 'models/m1'."
        ],
        "scenario": "Test that the GeminiProvider annotates a rate limit retry scenario correctly.",
        "why_needed": "This test prevents regression in the GeminiProvider's ability to handle rate limit retries."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 173,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-226, 235, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-349, 352, 354-356, 358-361, 366-369, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.005379178999987744,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.scenario == 'Success Scenario'",
          "assert not annotation.error",
          "assert mock_parse.return_value.scenario == 'Success Scenario'",
          "assert mock_parse.return_value.text == 'Success Scenario'",
          "assert mock_parse.return_value.tokens == []",
          "assert mock_get.return_value.json.return_value['models'][0]['name'] == 'models/gemini-1.5-flash'",
          "assert mock_get.return_value.json.return_value['rateLimits'][0]['value'] == 15",
          "assert mock_post.return_value.json.return_value['candidates'][0]['content']['parts'][0]['text'] == 'Success Scenario'"
        ],
        "scenario": "Verify that _annotate_internal returns the correct LlmAnnotation object when _call_gemini returns text and tokens.",
        "why_needed": "This test prevents a regression where _parse_response might expect an incorrect format of response from _call_gemini."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 10,
          "line_ranges": "134, 136-139, 141-142, 272-273, 275"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002564065000001392,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider._check_availability() should return False when environment variable GEMINI_API_TOKEN is not provided.",
          "provider._check_availability() should return True when environment variable GEMINI_API_TOKEN is provided with a valid API token."
        ],
        "scenario": "Tests the availability of the Gemini provider when environment variables are set.",
        "why_needed": "The test prevents a potential bug where the provider's availability is incorrectly reported as available when it should be unavailable due to an incorrect API token."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_availability",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007675120000385505,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008786899999790876,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method returns 0.0 before the third request is recorded.",
          "The next_available_in method returns a value between 0 and 60.0 for the third request.",
          "The wait time after the first two requests is less than or equal to 60.0 seconds."
        ],
        "scenario": "Verify that the rate limiter does not block requests for a short period after initial requests.",
        "why_needed": "This test prevents a potential issue where the rate limiter blocks subsequent requests immediately after the first two, potentially causing unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0008002240000450911,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config1 and config2 should have different hashes",
          "The hash of config1 should not be equal to the hash of config2",
          "The hash of config1 should not be equal to a hash generated by another configuration provider (ollama)",
          "The hash of config2 should not be equal to a hash generated by another configuration provider (none)"
        ],
        "scenario": "Test that different configuration providers produce different hashes.",
        "why_needed": "This test prevents a bug where the same configuration provider is used with different inputs, resulting in identical hashes."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007796250000069449,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the computed hash should be exactly 16 characters.",
          "The computed hash should not exceed 15 characters due to padding.",
          "No leading zeros are present in the computed hash.",
          "All non-zero digits are present in the computed hash.",
          "The first character is a zero (0).",
          "The second character is also a zero (0).",
          "All other characters are non-zero and distinct.",
          "There are no duplicate characters in the computed hash."
        ],
        "scenario": "Verifies the length of the computed hash is 16 characters.",
        "why_needed": "This test prevents a potential issue where the hash might be too long and cause performance issues or security vulnerabilities."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 6,
          "line_ranges": "32, 44-48"
        }
      ],
      "duration": 0.0009078049999970972,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The computed SHA-256 hash of the file should be equal to its content hash.",
          "The content hash of the file should be equal to the computed SHA-256 hash.",
          "The computed file hash should match the content hash when the same file is used for both computations.",
          "The computed SHA-256 hash of a file with different content than its content hash should not match the content hash.",
          "The computed file hash should be consistent across multiple runs of the test.",
          "The content hash of a file with different content than its content hash should also be consistent across multiple runs of the test."
        ],
        "scenario": "Test that the computed SHA-256 hash of a file matches its content hash when the same file is used for both computations.",
        "why_needed": "This test prevents a bug where the computed file hash does not match the content hash due to differences in file system or storage implementation."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "44-48"
        }
      ],
      "duration": 0.000911820999988322,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the computed SHA-256 hash should be exactly 64 bytes.",
          "The hash value should match the expected output provided by `compute_file_sha256(path)`.",
          "Any errors or exceptions raised during the computation process should not affect the overall test result."
        ],
        "scenario": "Verify that the test hashes a file correctly.",
        "why_needed": "This test prevents a potential bug where the hash is not calculated correctly due to incorrect file contents."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007977790000381901,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The computed HMAC signature should be different for two different keys.",
          "The computed HMAC signature should not be equal to the expected signature for key1.",
          "The computed HMAC signature should not be equal to the expected signature for key2.",
          "The computed HMAC signature should have a different value when compared to the expected signature for key1.",
          "The computed HMAC signature should have a different value when compared to the expected signature for key2.",
          "The computed HMAC signature should be different from the expected signature for both keys.",
          "The computed HMAC signature should not match the expected signature for key1 and key2."
        ],
        "scenario": "Test 'test_different_key' verifies that different keys produce different signatures.",
        "why_needed": "This test prevents a potential issue where the same key produces the same signature for different inputs."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_different_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.00077599899998404,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the computed HMAC should be 64 bytes.",
          "The computed HMAC should not be empty.",
          "The computed HMAC should contain all characters from the input data.",
          "The computed HMAC should contain the correct padding.",
          "The computed HMAC should include the secret key.",
          "The computed HMAC should have a length of 64 bytes."
        ],
        "scenario": "Verifies the computation of HMAC using a secret key.",
        "why_needed": "Prevents a potential issue where an attacker could manipulate the signature by tampering with the input data or the secret key."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_with_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.000776830000006612,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The two computed hashes should be equal.",
          "The hash values should have the same length (32 bytes).",
          "The first character of each hash should match.",
          "The second character of both hashes should match.",
          "All characters in both hashes except for the last one should match.",
          "The last character of both hashes should match.",
          "The hash values should be identical when compared using a hashing algorithm like SHA-256."
        ],
        "scenario": "Test that the computed SHA-256 hashes are consistent for the same input.",
        "why_needed": "This test prevents a bug where different inputs produce different hashes, potentially leading to unexpected behavior or data corruption."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_consistent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0008034390000375424,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the computed hash should be exactly 64 hexadecimal characters.",
          "The computed hash should not have any leading zeros.",
          "All non-zero hexadecimal digits in the hash should appear consecutively.",
          "No whitespace or punctuation should be present in the hash.",
          "The hash should not contain any duplicate bytes.",
          "All hexadecimal digits should be either 0-9, A-F, a-f, or +/.",
          "The hash should start with an even number of hexadecimal digits for SHA-256."
        ],
        "scenario": "Verify the length of the computed SHA-256 hash is 64 characters.",
        "why_needed": "This test prevents a potential bug where the hash length may be less than 64 characters, potentially causing issues with certain applications or libraries that require this length."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.08111415399997668,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'pytest' package should be present in the 'dependency' section of the snapshot.",
          "The 'pytest' package should be listed as an item in the 'dependencies' section of the snapshot.",
          "The 'pytest' package should be included in the output of the `get_dependency_snapshot` function.",
          "The presence of 'pytest' in the dependency snapshot is required for the test to pass.",
          "Including 'pytest' in the dependency snapshot ensures accurate testing and reproducibility of the code.",
          "Without 'pytest', the test would fail due to missing dependencies."
        ],
        "scenario": "Verifies that the `get_dependency_snapshot` function includes the 'pytest' package as part of its output.",
        "why_needed": "This test prevents a regression where the 'pytest' package is not included in the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.08232573599997295,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "snapshot is of type dict",
          "snapshot has correct keys (e.g. 'dependencies', 'versions')",
          "snapshot does not contain any missing keys",
          "snapshot contains only valid package information",
          "snapshot is a dictionary with the expected structure",
          "snapshot can be converted to a dictionary using `json.dumps()`",
          "snapshot is not empty or None"
        ],
        "scenario": "The test verifies that the `get_dependency_snapshot()` function returns a dictionary.",
        "why_needed": "This test prevents a potential bug where the function might return an incorrect data type or format."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "73, 76-77, 80-81"
        }
      ],
      "duration": 0.0009067020000088633,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `hmac_key_file` attribute of the `Config` object should point to the expected file location.",
          "The `load_hmac_key` function should be able to successfully load the HMAC key from the specified file.",
          "The loaded HMAC key should match the expected value 'my-secret-key'.",
          "The configuration object `config` should have a `hmac_key_file` attribute pointing to the correct file location.",
          "An error message indicating that the file was not found or could not be read should not be raised when loading the HMAC key.",
          "The loaded HMAC key should contain only the expected bytes 'my-secret-key'.",
          "The configuration object `config` should have a `hmac_key_file` attribute pointing to the correct file location and the HMAC key should match it."
        ],
        "scenario": "Test 'test_loads_key' verifies that the HMAC key can be loaded from a file.",
        "why_needed": "This test prevents a bug where the HMAC key is not properly loaded from a file due to incorrect file path or missing file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 4,
          "line_ranges": "73, 76-78"
        }
      ],
      "duration": 0.0008245590000228731,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `load_hmac_key` should return `None` if the `hmac_key_file` parameter is set to `nonexistent.key`.",
          "The test should fail when a missing key file is provided.",
          "The expected output of `load_hmac_key` should be `None` in this case.",
          "The function call with an empty string as a key file path should raise a `ValueError`.",
          "A `KeyError` exception should be raised when trying to load the HMAC key from a non-existent file.",
          "The test should not fail if the `hmac_key_file` parameter is set to a valid file path but the key is missing.",
          "The function call with a valid key file path and an empty string as a key file should return the loaded HMAC key."
        ],
        "scenario": "Test that a missing key file returns None when provided.",
        "why_needed": "This test prevents a potential bug where the HMAC key is not loaded due to an empty or non-existent key file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 2,
          "line_ranges": "73-74"
        }
      ],
      "duration": 0.0007615509999823189,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` object passed to `load_hmac_key` is not None.",
          "The `key` variable is None after calling `load_hmac_key(config).",
          "No exception is raised when no key file is specified in the configuration.",
          "The function does not throw an error or raise a meaningful exception when given an empty or missing key file configuration.",
          "The `load_hmac_key` function's behavior changes unexpectedly without providing clear documentation or warnings.",
          "The test does not verify that the function throws a `ConfigError` with a specific message when no key file is specified.",
          "The test does not cover all possible edge cases, such as a missing key file in a configuration object with other settings.",
          "The test does not provide any information about what happens to the configuration object or the loaded HMAC key after calling `load_hmac_key(config).'"
        ],
        "scenario": "Verify that the `load_hmac_key` function returns `None` when no key file is specified.",
        "why_needed": "Prevents a potential bug where the function does not handle the case of an empty or missing key file configuration."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010533859999668493,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.aggregate_dir should be None.",
          "config.aggregate_policy should be 'latest'.",
          "config.aggregate_include_history should be False."
        ],
        "scenario": "Test the default aggregation configuration.",
        "why_needed": "Prevents a regression where aggregation defaults to an empty directory and does not include historical data."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007733630000075209,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `capture_failed_output` field in the test configuration is set to `False`.",
          "The value of `capture_failed_output` in the test configuration matches the expected default value.",
          "The captured output does not contain any failed messages when `capture_failed_output` is False."
        ],
        "scenario": "Verify that the default capture failed output is set to False.",
        "why_needed": "Prevents a regression where the default capture failed output was accidentally set to True."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007947539999690889,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function get_default_config() returns an LLMContextMode object with value 'minimal'.",
          "The llm_context_mode attribute of config.llm is equal to 'minimal'.",
          "config.llm_context_mode is a string and its value is 'minimal'.",
          "get_default_config().llm_context_mode is not None.",
          "config.llm_context_mode is an object with contextMode property set to minimal."
        ],
        "scenario": "Tests the default context mode for minimal configuration.",
        "why_needed": "This test prevents a potential regression where the context mode is not set to minimal by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 4,
          "line_ranges": "123, 163, 252, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007935909999901014,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_llm_enabled()` method returns False.",
          "The `get_default_config()` function returns an instance with `is_llm_enabled()` set to True.",
          "The `config` object has a non-zero value for `llm_enabled` attribute.",
          "The `config` object does not have any attributes that could cause it to be enabled by default.",
          "The `get_default_config()` function returns an instance with a different configuration than expected.",
          "The `is_llm_enabled()` method is called on the `config` object without checking its current state first.",
          "The `is_llm_enabled()` method checks for the presence of certain attributes or values in the `config` object."
        ],
        "scenario": "Verify that LLM is not enabled by default in the configuration.",
        "why_needed": "Prevent regression where LLM is enabled by default due to a bug or change in the configuration."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000755319999996118,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `omit_tests_from_coverage` attribute of the configuration object returned by `get_default_config()` is set to `True`.",
          "The value of `omit_tests_from_coverage` is indeed `True` for the given test case.",
          "A test is not being omitted from coverage analysis due to an incorrect or missing configuration setting."
        ],
        "scenario": "The `TestConfigDefaults` class's `test_omit_tests_default_true` method verifies that the `omit_tests_from_coverage` attribute of the configuration object returned by `get_default_config()` is set to `True`.",
        "why_needed": "This test prevents a potential issue where tests are not being omitted from coverage analysis due to an incorrect or missing configuration setting."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008117849999962345,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.provider should be None.",
          "assert config.provider == 'none' after calling get_default_config()."
        ],
        "scenario": "Tests the default provider setting when it is set to 'none'.",
        "why_needed": "This test prevents a potential bug where the provider setting is not correctly handled when it defaults to 'none'."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007946740000193131,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'secret' keyword is present in any glob pattern used to exclude files.",
          "The '.env' file is also excluded from the list of patterns.",
          "Any other secret files that may have been added by external tools are not included in the exclusion list."
        ],
        "scenario": "Test that secret files are excluded by default from the LLM context.",
        "why_needed": "This test prevents a potential bug where secret files might be inadvertently included in the LLM context."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 117,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.00689458800002285,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 67,
          "line_ranges": "235-237, 239, 241, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.006140731000016331,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of tests in the report should be zero.",
          "The summary section of the report should have a 'total' key with a value of zero.",
          "The data from the report.json file should contain a 'summary' section with a 'total' key equal to zero."
        ],
        "scenario": "Test that an empty test suite produces a valid report.",
        "why_needed": "This test prevents regression where the full pipeline is not producing any reports."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.035327876999986074,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The HTML file exists at the specified path.",
          "The HTML file contains the expected content with the test result.",
          "The test result string 'test_pass' is present in the HTML file."
        ],
        "scenario": "The full pipeline generates an HTML report that includes the test results.",
        "why_needed": "This test prevents a potential issue where the HTML report is missing or incorrect due to a bug in the ReportWriter class."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/_git_info.py",
          "line_count": 2,
          "line_ranges": "2-3"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 133,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05904678699999977,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report writer is configured with a path to a 'report.json' file.",
          "The report is written successfully and exists at the specified path.",
          "The schema version of the report matches the expected value.",
          "The summary statistics (total, passed, failed, skipped) are correctly reported.",
          "The 'tests/test_a.py::test_one', 'tests/test_a.py::test_two', and 'tests/test_b.py::test_skip' nodes have been successfully executed with their respective outcomes.",
          "The test has not been skipped due to a timeout or other error."
        ],
        "scenario": "Verify that the test generates a valid JSON report for the full pipeline.",
        "why_needed": "This test prevents regression where the full pipeline fails to generate a valid JSON report due to incorrect configuration or missing dependencies."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009711829999901056,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' field is present in the data.",
          "The 'run_meta' field is present in the data.",
          "The 'summary' field is present in the data.",
          "The 'tests' field is present in the data."
        ],
        "scenario": "Test that the `ReportRoot` class has a report root with required fields.",
        "why_needed": "This test prevents a regression where the schema version, run meta, summary, and tests are missing from the report root."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007821789999979956,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "is_aggregated is present in data",
          "run_count is present in data"
        ],
        "scenario": "Verify that `RunMeta` has aggregation fields when it's not aggregated.",
        "why_needed": "This test prevents regression where `RunMeta` does not have aggregation fields even if it's not aggregated."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007680040000082045,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status_fields_exist",
          "exit_code_field_exists",
          "interrupted_field_exists",
          "collect_only_field_exists",
          "collected_count_field_exists",
          "selected_count_field_exists"
        ],
        "scenario": "The test verifies that RunMeta has status fields.",
        "why_needed": "This test prevents a potential bug where the run metadata does not contain all necessary status fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007547390000013365,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The schema version should be defined.",
          "The schema version should contain at least one dot (.) character.",
          "The schema version should be in a valid semver-like format (e.g., '1.2.3')."
        ],
        "scenario": "Verify that the schema version is defined and matches a semver-like format.",
        "why_needed": "This test prevents regression where the schema version is not defined or does not match a semver-like format."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007860970000024281,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field should be present in the `data` dictionary.",
          "The 'outcome' field should be present in the `data` dictionary.",
          "The 'duration' field should be present in the `data` dictionary."
        ],
        "scenario": "Test verifies that `TestCaseResult` has required fields.",
        "why_needed": "This test prevents a potential bug where the `TestCaseResult` object is missing some required fields, potentially leading to incorrect analysis or reporting."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "52-53, 245, 247, 249, 252, 257, 262-263, 265"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 7,
          "line_ranges": "134, 136-139, 141-142"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007998630000543017,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "provider.__class__ == 'GeminiProvider'",
          "provider.model == 'gemini-1.5-flash'",
          "provider.name == 'GeminiProvider'"
        ],
        "scenario": "The test verifies that the `get_provider` function returns an instance of GeminiProvider when the configuration is set to 'gemini'.",
        "why_needed": "This test prevents a potential bug where the `get_provider` function does not return a valid provider."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "52-53, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007838430000219887,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` attribute of the returned `LiteLLMProvider` instance should be set to 'litellm'.",
          "The `__class__.__name__` attribute of the returned `LiteLLMProvider` instance should match 'LiteLLMProvider'."
        ],
        "scenario": "The test verifies that the `get_provider` function returns an instance of LiteLLMProvider when a specific provider is specified.",
        "why_needed": "This test prevents a potential bug where the correct provider is not returned due to incorrect configuration or missing dependencies."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 245, 247, 249-250"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000782411000045613,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function get_provider() should be able to create a NoopProvider instance when the 'provider' parameter is set to 'none'.",
          "The returned value of get_provider(config) should be an instance of NoopProvider.",
          "The assertion isinstance(provider, NoopProvider) should pass for the correct NoopProvider instance."
        ],
        "scenario": "test_get_provider_with_none_provider returns NoopProvider.",
        "why_needed": "This test prevents a bug where the LLM is not properly initialized with a None provider."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 245, 247, 249, 252-253, 255"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007814479999979085,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function get_provider() returns an instance of OllamaProvider.",
          "The class name of the returned provider matches 'OllamaProvider'.",
          "The method __class__ checks if the returned provider is indeed an instance of OllamaProvider.",
          "The type of the provider is correctly identified as OllamaProvider.",
          "The configuration object passed to get_provider() has a valid provider value ('provider=ollama')."
        ],
        "scenario": "The test verifies that OllamaProvider is returned when 'provider='ollama' in the configuration.",
        "why_needed": "This test prevents a potential bug where the correct provider type (OllamaProvider) is not detected when using the 'provider=ollama' configuration."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "245, 247, 249, 252, 257, 262, 267"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008249800000044161,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_provider(config)` should be called with a valid provider.",
          "The function `Config(provider='unknown')` should be created with an invalid provider.",
          "A ValueError should be raised when trying to get a provider from the unknown config.",
          "The error message 'unknown' should contain the string 'unknown' in lowercase.",
          "An AssertionError should be raised if the test is run without raising a ValueError."
        ],
        "scenario": "Test 'test_unknown_raises' verifies that an unknown provider raises a ValueError.",
        "why_needed": "This test prevents the regression where an unknown provider is used without raising a ValueError."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_unknown_raises",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007585660000017924,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "should have required methods",
          "should be able to annotate",
          "should be able to check if available",
          "should have model name attribute",
          "should have config attribute"
        ],
        "scenario": "Test that NoopProvider implements LlmProvider interface.",
        "why_needed": "Prevents a bug where NoopProvider does not implement required methods of LlmProvider."
      },
      "nodeid": "tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000804853000033745,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation is of type LlmAnnotation",
          "annotation scenario is an empty string",
          "annotation why_needed is an empty string",
          "annotation key_assertions are an empty list"
        ],
        "scenario": "The test verifies that the NoopProvider returns an empty annotation when no annotation is provided.",
        "why_needed": "This test prevents a regression where the NoopProvider does not return any annotation for certain scenarios."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 66"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000763956000014332,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 107, 110-111"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 58"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008469210000043859,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method should return `True`.",
          "The `is_available()` method should not raise an exception.",
          "The `is_available()` method should be able to be called without raising any errors.",
          "The provider's availability can be checked using the `is_available()` method.",
          "The provider's availability is a critical aspect of its functionality and should not be compromised.",
          "The test should fail if the provider is not available, indicating a bug or regression."
        ],
        "scenario": "Verify that the NoopProvider instance is always available.",
        "why_needed": "To ensure the provider's availability, which is crucial for its functionality."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_is_available",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 65,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0010895039999923029,
      "file_path": "tests/test_llm_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_annotate_tests_emits_summary` should print 'Annotated 1 test(s) via litellm' in the captured output.",
          "The provider `FakeProvider(LlmAnnotation)` should be used instead of the actual LLM annotation provider.",
          "The `get_provider` method of the `llm.annotator` module should return a `FakeProvider` instance when called with a configuration object."
        ],
        "scenario": "The test verifies that the annotation summary is printed when annotations run.",
        "why_needed": "This test prevents regression where the annotation summary is not printed."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_emits_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0010579850000453916,
      "file_path": "tests/test_llm_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The correct message should be printed when starting LLM annotations for a test.",
          "The correct message should include the name of the test that started the annotation.",
          "Each annotation should have its own unique message.",
          "The message should indicate the number of tests being annotated.",
          "The message should contain the provider used to annotate the test.",
          "The message should be printed before starting any annotations.",
          "Any error messages from annotations should not be printed."
        ],
        "scenario": "Test that the progress of LLM annotations is reported correctly.",
        "why_needed": "This test prevents regression where the progress of LLM annotations is not reported."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_reports_progress",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 65,
          "line_ranges": "45, 48-49, 56-57, 59, 61-62, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0010628840000208584,
      "file_path": "tests/test_llm_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'tests/test_a.py::test_a' node should be called with the provider.",
          "The 'tests/test_b.py::test_b' node should have an LLM annotation set to None.",
          "The 'tests/test_c.py::test_c' node should not have an LLM annotation set to None.",
          "The number of LLM annotations for all tests should be 1.",
          "The provider function should call the fake provider with the correct configuration.",
          "The calls made by the fake provider should only include the 'tests/test_a.py::test_a' node."
        ],
        "scenario": "Test that LLM annotations respect opt-out and limit settings.",
        "why_needed": "This test prevents regression by ensuring LLM annotations do not skip opt-out tests or exceed the maximum number of tests."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_respects_opt_out_and_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 68,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-173, 176, 178, 180-183, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.001268697999989854,
      "file_path": "tests/test_llm_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider's calls to `llm.annotator.get_provider` should be ['tests/test_a.py::test_a', 'tests/test_b.py::test_b']",
          "The sleep_calls list should contain [2.0] as the expected value"
        ],
        "scenario": "The test verifies that the LLM annotator respects the requests-per-minute rate limit.",
        "why_needed": "This test prevents a potential bug where the annotator exceeds the allowed number of requests per minute, potentially causing performance issues or errors."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_respects_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "45, 48-52, 54"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008852919999640108,
      "file_path": "tests/test_llm_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'is_available' method returns False for the unavailable provider.",
          "The annotation process should not proceed without checking the provider's availability.",
          "A clear error message should be displayed indicating that the provider is unavailable.",
          "The test should fail when an unavailable provider is encountered during annotation.",
          "The test should provide a descriptive error message explaining why the annotation skipped.",
          "The test should include the provider name in the failure message to identify the issue."
        ],
        "scenario": "Test that annotation skips unavailable providers with a clear message.",
        "why_needed": "To prevent skipping annotation of tests when an unavailable provider is detected."
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_skips_unavailable_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 30,
          "line_ranges": "39-41, 53, 55-56, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-67, 71-72, 74-81, 87-92, 97-98, 100, 102, 104, 115-122, 127, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 12,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-84"
        }
      ],
      "duration": 0.0013319860000251538,
      "file_path": "tests/test_llm_annotator.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_annotator.py::test_annotate_tests_uses_cache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007705579999992551,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in required",
          "assert 'why_needed' in required"
        ],
        "scenario": "The test verifies that the schema requires both \"scenario\" and \"why_needed\" fields.",
        "why_needed": "This test prevents a regression where the schema is not enforced, potentially allowing invalid data to pass through."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000797599000009086,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "checks password",
          "checks username"
        ],
        "scenario": "Test that AnnotationSchema.from_dict() correctly parses a dictionary with required keys.",
        "why_needed": "Prevents dict-based schema creation from bypassing authentication mechanisms."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007946840000272459,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "schema.scenario = \"\" (empty string)",
          "schema.why_needed = \"\" (empty string)\"\" (empty string)",
          "schema.scenario in \"\"\" (empty string, list of strings)"
        ],
        "scenario": "The test verifies that the AnnotationSchema class correctly handles an empty input.",
        "why_needed": "This test prevents a potential bug where the AnnotationSchema class may throw an error or produce incorrect results when given an empty dictionary as input."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007658289999881163,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008022669999832033,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']"
        ],
        "scenario": "The test verifies that the `ANNOTATION_JSON_SCHEMA` contains required fields.",
        "why_needed": "This test prevents a potential bug where the schema is missing essential fields, potentially leading to errors or inconsistencies."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "90-92, 94-96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007631550000155585,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assertion 1: The 'scenario' key in the data dictionary should match the provided scenario string.",
          "assertion 2: The 'why_needed' key in the data dictionary should match the provided why_needed string.",
          "assertion 3: The 'key_assertions' key in the data dictionary is present and contains the expected list of assertions."
        ],
        "scenario": "Test AnnotationSchema::test_schema_to_dict verifies that the annotation schema is correctly serialized to a dictionary.",
        "why_needed": "This test prevents regression by ensuring that the annotation schema can be properly converted into a JSON-like format."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "52-53, 245, 247, 249-250"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000768063999998958,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The config object passed to get_provider() has the correct value for the 'provider' key.",
          "The provider instance returned by get_provider() is an instance of NoopProvider.",
          "The assert isinstance() call passes with a NoopProvider instance as expected."
        ],
        "scenario": "The factory function should return a NoopProvider instance when the 'provider' parameter is set to 'none'.",
        "why_needed": "This test prevents a potential bug where a NoopProvider is not returned for the 'none' provider."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008149710000111554,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` variable should be an instance of `LlmProvider`.",
          "The `provider` variable should not have any additional attributes or methods outside of the `LlmProvider` interface.",
          "The `provider` variable should not inherit from `Config` directly, but rather through its intended class hierarchy.",
          "The `NoopProvider` class should be correctly implemented to only provide a no-op service without any additional functionality.",
          "Any assertions made within the test should focus on verifying that the `LlmProvider` interface is respected by the `NoopProvider` implementation."
        ],
        "scenario": "Verify that the `NoopProvider` class correctly inherits from `LlmProvider`.",
        "why_needed": "This test prevents a potential bug where the `NoopProvider` class is incorrectly implemented as an instance of `LlmProvider` instead of its intended interface."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000770599000020411,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result.scenario == \"\" (empty string)",
          "assert result.why_needed == \"\" (empty string)",
          "assert result.key_assertions == [] (no key assertions performed)"
        ],
        "scenario": "The NoopProvider should return an empty annotation when no tests are provided.",
        "why_needed": "This test prevents a regression where the NoopProvider returns an empty annotation for no tests."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007869890000051782,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result has a 'scenario' attribute",
          "The result has a 'why_needed' attribute",
          "The result has a 'key_assertions' attribute"
        ],
        "scenario": "The test verifies that the annotate method of the NoopProvider returns an LlmAnnotation-like object with the required key assertions.",
        "why_needed": "This test prevents a potential regression where the annotate method does not return the expected annotations, potentially causing issues downstream in the testing pipeline."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008122760000333074,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "...",
          "...",
          "...",
          "...",
          "...",
          "...",
          "...",
          "...",
          "..."
        ],
        "scenario": "The test verifies that the ProviderContract handles an empty code by returning a non-empty result.",
        "why_needed": "This test prevents potential bugs where an empty code would cause the contract to fail or return an error."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 50"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007934000000204833,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result is not None after calling `provider.annotate(test, 'code', None)`.",
          "The provider can handle None context without raising an exception or producing unexpected results.",
          "The annotate method does not raise an error when given a None value for the 'code' field.",
          "The provider's behavior changes when given a None context for the annotation 'code'.",
          "The test case passes even if the provider encounters None context during execution.",
          "The result is not None after calling `provider.annotate(test, 'code', None)` with a valid value.",
          "The annotate method does not raise an error when given a non-None value for the 'code' field."
        ],
        "scenario": "Test the provider's handling of None context when annotating a TestCaseResult.",
        "why_needed": "This test prevents potential bugs where the provider might fail or produce incorrect results when given a None context for an annotation."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 15,
          "line_ranges": "52-53, 245, 247, 249-250, 252-253, 255, 257-258, 260, 262-263, 265"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 7,
          "line_ranges": "134, 136-139, 141-142"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008056639999836079,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider has an attribute named 'annotate'.",
          "The provider is callable.",
          "The provider has a method named 'annotate'.",
          "The annotate method is defined on the provider object.",
          "The annotate method is correctly called when invoked.",
          "The annotate method is not None.",
          "The annotate method does not throw any exceptions."
        ],
        "scenario": "All providers should have an annotate method.",
        "why_needed": "This test prevents regression in the contract where providers are not annotated with a method."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 153,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-221, 237, 249-250, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 352, 354-356, 358-361, 366-369, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423-424, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008923659999595657,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotate` method should not be called with a context that exceeds the maximum allowed size.",
          "The `annotate` method should raise an exception when called with an invalid context.",
          "The `annotate` method should log a warning or error message when called with an invalid context.",
          "The `annotate` method should update the annotation metadata correctly even if the context is too large.",
          "The `annotate` method should not modify the original context but create a new one instead.",
          "A test case should be added to verify this scenario and prevent it from occurring in the future."
        ],
        "scenario": "The `annotate` method of the `GeminiProvider` class is called with a context that is too large.",
        "why_needed": "This test prevents a potential memory leak or performance issue caused by annotating contexts larger than expected."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134, 136-139, 141-142, 160-164"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008129470000426409,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'litellm not installed. Install with: pip install litellm'",
          "provider.annotate(test, 'def test_case(): assert True')",
          "test_case() should raise an exception when the provider cannot annotate it due to a missing dependency"
        ],
        "scenario": "The LiteLLMProvider annotates the missing dependency correctly.",
        "why_needed": "This test prevents a potential bug where the provider does not report an error for a missing dependency."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134, 136-139, 141-142, 160-161, 167-169"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008165539999822613,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `GEMINI_API_TOKEN` environment variable should be set before running the test.",
          "The `GEMINI_API_TOKEN` environment variable should be present in the configuration object.",
          "The annotation function should raise an error when the `GEMINI_API_TOKEN` is not set."
        ],
        "scenario": "Test that a missing API token prevents the annotation of a case with an annotated function.",
        "why_needed": "This test verifies that setting the GEMINI_API_TOKEN environment variable before running tests will prevent the annotation of cases with annotated functions when the provider is not configured to use it."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 183,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-223, 225-226, 235, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-349, 352, 354-356, 358-361, 366-372, 374, 376-377, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009478789999661785,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'status ok' assertion is included in the response metadata.",
          "The totalTokenCount key contains the correct value of 123.",
          "The candidates list includes a single record with text containing the expected JSON data.",
          "The usageMetadata key contains the correct rate limits information.",
          "The limiter object is not None, indicating that tokens were recorded correctly.",
          "The limiter._token_usage list has one element with the correct token count and value.",
          "The limiter._token_usage[0][1] equals 123, which matches the expected value."
        ],
        "scenario": "Verify that tokens are recorded correctly by the Gemini provider.",
        "why_needed": "Prevents regressions and ensures accurate token usage tracking."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 181,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-226, 235, 237-238, 242-244, 246-247, 280-283, 286-289, 292, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336-339, 341-347, 349, 352, 354-356, 358-361, 366-372, 374-375, 380-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010030720000031579,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The LLM provider should retry the annotation process after a rate limit is exceeded.",
          "The LLM provider should re-annotate the model with the updated parameters after a rate limit is exceeded.",
          "The LLM provider should update the model's annotations to reflect the new parameters after a rate limit is exceeded.",
          "The LLM provider should not retry the annotation process if the rate limit is not exceeded.",
          "The LLM provider should respect the rate limits set by the client and not retry the annotation process.",
          "The LLM provider should update the model's annotations with the correct parameters after a rate limit is exceeded, even if it's not explicitly retried."
        ],
        "scenario": "Verify that the LLM provider annotates retries on rate limits correctly.",
        "why_needed": "This test prevents a potential regression where the LLM provider does not handle rate limit retrials correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 177,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210, 213-214, 217-222, 225-226, 235, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374, 376, 378-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425-426, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010576239999977588,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `rotate_models_on_daily_limit` method is called with an empty dictionary as its argument.",
          "A new model is created and added to the list of models.",
          "The `rotate_models_on_daily_limit` method is called again with a non-empty dictionary as its argument, which should not rotate any existing models.",
          "No exception is raised when rotating models on a daily limit.",
          "The number of models in the list remains unchanged after rotation on a daily limit.",
          "Models are rotated only once per day, regardless of the test duration.",
          "The `rotate_models_on_daily_limit` method is called with an empty dictionary as its argument during the first test run."
        ],
        "scenario": "The `annotate` method of the `GeminiProvider` class rotates models on a daily limit during testing.",
        "why_needed": "This test prevents regression in the LLM model rotation feature by ensuring that models are rotated only once per day."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 184,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-208, 210-211, 213-214, 217-222, 225-226, 235, 258-260, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374, 376, 378-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010285100000260172,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotate` method should not be called with more than one annotation per day.",
          "The `annotate` method should skip any annotation that is already being processed or has been skipped in the past day.",
          "The `annotate` method should raise an error if it tries to annotate a new annotation when the daily limit is exceeded.",
          "The `annotate` method should not be called with annotations that are not valid (e.g. invalid types, missing required arguments).",
          "Any annotations that have been skipped in the past day should still be skipped by the `annotate` method even if they are re-added.",
          "The `annotate` method should correctly handle cases where multiple annotations need to be annotated at once (e.g. multiple entities being processed)",
          "The `annotate` method should not silently ignore or crash when it encounters an annotation that is already being processed or has been skipped in the past day."
        ],
        "scenario": "The test verifies that the `annotate` method skips annotations when the daily limit is exceeded.",
        "why_needed": "This test prevents a regression where the `annotate` method fails to skip annotations due to an incorrect implementation of the daily limit check."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 177,
          "line_ranges": "39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-226, 235, 280-283, 286-289, 292, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374-383, 387-388, 391-393, 397-398, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010350119999884555,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation contains the correct scenario and why-needed information.",
          "The annotation includes the expected key assertions.",
          "The annotation has a non-zero confidence level.",
          "The captured model is correctly set for the test case.",
          "The system role of the message is 'system' as expected.",
          "The test login function is present in the response messages.",
          "The def test_login() function is present in the response messages."
        ],
        "scenario": "Test that the annotate method correctly annotates a successful response with mock data.",
        "why_needed": "Prevents regressions caused by missing or incorrect annotations in responses from LiteLLM providers."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 190,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-188, 190-191, 193-194, 196, 200-208, 210-211, 213-214, 217-222, 225-226, 235, 258-260, 280-283, 286-289, 292-296, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374, 376, 378-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0011083989999747246,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The recovered model should have the same accuracy as before exhaustion.",
          "The recovered model's inference time should be less than or equal to 24 seconds.",
          "The recovered model's memory usage should decrease by at least 50% compared to before exhaustion.",
          "The recovered model's computation time should decrease by at least 30% compared to before exhaustion.",
          "The recovered model's latency should decrease by at least 20% compared to before exhaustion.",
          "The recovered model's memory allocation should be reduced by at least 75% compared to before exhaustion.",
          "The recovered model's garbage collection should complete within the first 24 seconds of inference."
        ],
        "scenario": "The test verifies that the exhausted model recovers after 24 hours.",
        "why_needed": "This test prevents a regression where the model does not recover from exhaustion within 24 hours."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 65,
          "line_ranges": "134, 136-139, 141-142, 286, 288-289, 292-296, 298-301, 303-304, 306-307, 352, 354-356, 358-361, 366-369, 380-383, 391, 393, 397-398, 402-408, 411, 414-416, 418-420, 423-424, 434, 436-438, 441-442"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008452879999936158,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `fetch_available_models` method should return an empty list when there are no available models.",
          "The `fetch_available_models` method should raise a `GeminiProviderError` with a suitable error message when there are no available models.",
          "The `fetch_available_models` method should not return any results when there are no available models and the `GEMINIMODELS` environment variable is set to `None` or an empty string.",
          "The `fetch_available_models` method should raise a `GeminiProviderError` with a suitable error message when the `GEMINIMODELS` environment variable is not set or is an invalid value.",
          "The `fetch_available_models` method should return a list of available models with a length greater than 0 when there are available models and the `GEMINIMODELS` environment variable is set to a valid value.",
          "The `fetch_available_models` method should raise a `GeminiProviderError` with a suitable error message when the `GEMINIMODELS` environment variable is not set or is an invalid value, but there are available models."
        ],
        "scenario": "The `fetch_available_models` method of the `GeminiProvider` class raises an error when there are no available models.",
        "why_needed": "This test prevents a potential regression where the `fetch_available_models` method returns incorrect results or raises an exception due to an incomplete set of available models."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 169,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134, 136-139, 141-142, 160-161, 167-168, 171-172, 174, 176-184, 186-187, 200-202, 206-208, 210, 213-214, 217-222, 225-226, 235, 280-283, 286-289, 292, 298-301, 303-304, 306-307, 321, 323-326, 328-331, 333-334, 336, 341-347, 349, 352, 354-356, 358-361, 366-372, 374-375, 380-383, 387-388, 391-393, 397-399, 402-405, 407-408, 411, 414-416, 418-420, 423, 425, 427-430, 434, 436-440, 443-446, 448-449, 451-453"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.001087970000014593,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `refresh_interval` attribute of the `GeminiProvider` instance is set to the expected value before each test call.",
          "After refreshing the model list, all models are present and up-to-date within the specified time frame.",
          "The `refresh_interval` attribute is updated correctly after each test call to reflect the new interval.",
          "No exceptions are raised when refreshing the model list with an invalid or outdated interval value."
        ],
        "scenario": "The model list should refresh after a specified interval.",
        "why_needed": "This test prevents regression that may occur when the interval between model updates is changed."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 47,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110-111, 114, 116, 118-121, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009818730000006326,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the provider correctly sets the `litellm_token_refresh_command` attribute.",
          "Verify that the provider correctly sets the `litellm_token_refresh_interval` attribute.",
          "Verify that the provider attempts to refresh the token after the first failed attempt.",
          "Verify that the second successful attempt does not result in a 401 error.",
          "Verify that the correct tokens are captured and stored for future reference.",
          "Verify that the `call_count` variable is incremented correctly for each call to `fake_completion`.",
          "Verify that the `token_count` variable is incremented correctly for each call to `fake_run`.",
          "Verify that the `config` object contains the correct settings for the provider."
        ],
        "scenario": "Test that LiteLLM provider retries on 401 after refreshing token.",
        "why_needed": "Reason: The current implementation does not handle the case where the user's token is refreshed, leading to a 401 error."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_401_retry_with_token_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 32,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110, 114, 129, 131, 164-168, 170-171, 175, 179-180, 183"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008287769999810735,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should contain an error message indicating a completion error.",
          "The error message should be 'boom'.",
          "The annotation should include the line number and function name of the completion error.",
          "The annotation should include the string 'boom' as the error message.",
          "The annotation should not ignore the completion error when it occurs in the test case.",
          "The annotation should surface the completion error in the output of the LiteLLMProvider.",
          "The annotation should indicate that the completion error is a result of the test case being executed."
        ],
        "scenario": "The test verifies that the LiteLLMProvider annotates completion errors correctly.",
        "why_needed": "This test prevents a regression where the LiteLLMProvider does not surface completion errors in annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 32,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 190, 195"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008734699999877193,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'response_data' parameter must be a dictionary.",
          "The 'json.dumps(response_data)' function should raise a TypeError if it cannot serialize the data.",
          "The 'response_data' dictionary should contain a 'key_assertions' key with a list of strings.",
          "The 'litellm' module should have a 'litellm' attribute that is a callable.",
          "The 'fake_litellm_response' function should return an instance of the 'FakeLiteLLMResponse' class.",
          "The 'json.dumps(response_data)' function should raise a TypeError if it cannot serialize the data.",
          "The 'response_data' dictionary should contain a 'key_assertions' key with a list of strings."
        ],
        "scenario": "Test that LiteLLMProvider rejects invalid key_assertions payloads.",
        "why_needed": "This test prevents the provider from silently failing when receiving an invalid key_assertions payload, allowing for better debugging and error messages."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 8,
          "line_ranges": "37-38, 41, 80-84"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008004339999843069,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation message is set to 'litellm not installed. Install with: pip install litellm'.",
          "The annotation indicates that the dependency was not found.",
          "The annotation provides a clear and concise error message.",
          "The test verifies that the provider reports an error for missing dependencies correctly."
        ],
        "scenario": "Test that the LiteLLMProvider annotates a missing dependency correctly.",
        "why_needed": "This test prevents a potential bug where the provider does not report an error for missing dependencies."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 31,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 197"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008921749999899475,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation has the correct scenario 'Checks login'.",
          "The annotation has the correct why_needed 'Stops regressions'.",
          "The annotation has the correct key_assertions ['status ok', 'redirect']."
        ],
        "scenario": "Test that the annotate method returns an LlmAnnotation object with the correct scenario, why_needed, and key_assertions.",
        "why_needed": "Prevents a regression where LiteLLMProvider is not correctly annotating successful responses from mock responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 32,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175-176, 179-180, 183, 185-186, 188, 197"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008638129999667399,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `api_base` attribute of the `LiteLLMProvider` instance is set to 'https://proxy.corp.com/v1'.",
          "Check if the `litellm_api_base` configuration option is set to a valid URL.",
          "Verify that the `FakeLiteLLMResponse` object returned by `fake_completion` has an `api_base` attribute equal to 'https://proxy.corp.com/v1'."
        ],
        "scenario": "Test the LiteLLM provider's API base passthrough functionality.",
        "why_needed": "This test prevents regression in case the API base is not passed through correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 32,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008440550000159419,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The API key should be set to `TEST_KEY` when using the 'litellm' provider.",
          "The API key should be retrieved from environment variable `TEST_KEY` if it's not provided in the config.",
          "The API key should be passed through to the completion call of LiteLLM provider."
        ],
        "scenario": "Test: tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough",
        "why_needed": "This test prevents a bug where the API key is not passed through to the completion call of LiteLLM provider."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 107, 110-111"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 6,
          "line_ranges": "37-38, 41, 205-206, 208"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008059340000272641,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method of the `LiteLLMProvider` class should return `True` when the `litellm` module is available in the system's modules.",
          "When the `litellm` module is installed, the provider should be able to detect it and set its `is_available()` attribute accordingly.",
          "The provider should not raise any exceptions or errors if the `litellm` module is already loaded and available.",
          "If the `litellm` module is not installed, the provider should still be able to detect it correctly.",
          "The provider's behavior should change when the `litellm` module is removed from the system's modules.",
          "When the `litellm` module is imported dynamically using `import litellm`, the provider should also detect its presence.",
          "If the `litellm` module is not available in the system's modules, the provider should still be able to set its `is_available()` attribute correctly."
        ],
        "scenario": "Test that the LiteLLM provider detects installed module.",
        "why_needed": "Prevents a potential bug where the provider does not detect the installed module."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 38,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-181, 183, 185-186, 188, 197"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010799059999726524,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `litellm_token_refresh_command` is set to 'get-token'.",
          "The `litellm_token_refresh_interval` is set to 3600 seconds.",
          "The `api_key` is captured and verified as 'dynamic-token-789' after the test case execution.",
          "A successful token refresh is expected within 1 hour (3600 seconds)."
        ],
        "scenario": "The test verifies that the LiteLLM provider uses TokenRefresher to refresh tokens in a dynamic token scenario.",
        "why_needed": "This test prevents regression when the provider fails to refresh tokens due to insufficient or expired tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_token_refresh_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 33,
          "line_ranges": "52-53, 72, 75-76, 78, 165, 167-173, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 15,
          "line_ranges": "40-41, 47, 50, 52, 54-55, 57-60, 62, 64-65, 71"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008918750000361797,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `annotate` handles the case when the context is too short and returns a fallback value.",
          "The function `annotate` handles the case when the context is too long and raises an error with a meaningful message.",
          "The function `annotate` correctly propagates the error up the call stack if it occurs due to insufficient context length.",
          "The function `annotate` provides a clear indication of the reason for the error (insufficient context) in its error message.",
          "The function `annotate` does not silently ignore the error and instead raises an exception with more information about the context."
        ],
        "scenario": "Verifies that the annotate fallbacks on context length error are handled correctly.",
        "why_needed": "Prevents a potential regression where the annotation fails due to an insufficient context length."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "40-41, 47, 50, 52, 54-55, 57-59, 73, 76-77, 79-80, 82-83"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008418410000103904,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should include the reason for failure as 'Failed after 2 retries. Last error: boom'.",
          "The annotation should not include any additional information about the cause of the failure.",
          "The annotation should only include the last error that occurred during the call to the function.",
          "The annotation should use the correct syntax and formatting for error messages in Python.",
          "The annotation should be able to handle different types of exceptions raised by the function being annotated, including but not limited to Exception.",
          "The annotation should not ignore or suppress any errors that occur during the execution of the function being annotated.",
          "The annotation should provide a clear and concise description of what went wrong when the function was called.",
          "The annotation should be able to handle cases where the function being annotated is not actually failing, but instead returning an error code or exception."
        ],
        "scenario": "The test verifies that the OllamaProvider annotates the provided function with an error message when a call to the function raises a call error.",
        "why_needed": "This test prevents regression in handling call errors, ensuring that the annotation of functions with call errors is accurate and informative."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "52-53, 72, 75, 80"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "40-44"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007927490000270154,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'httpx not installed. Install with: pip install httpx'",
          "provider.annotate(test, 'def test_case(): assert True')",
          "test.nodeid == 'tests/test_sample.py::test_case'"
        ],
        "scenario": "The Ollama provider should report an error when annotating a function without the required httpx dependency.",
        "why_needed": "This test prevents potential issues where the provider incorrectly reports missing dependencies or fails to provide useful information."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 26,
          "line_ranges": "52-53, 72, 75, 80, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 29,
          "line_ranges": "40-41, 47, 50, 52, 54-55, 57-60, 62, 71, 119, 121-128, 132-135, 137, 139-140"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008856530000116436,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "119, 121-128, 132-135, 137, 139-140"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008100420000118902,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_call_ollama` of the `OllamaProvider` class returns a JSON response with the correct model and prompt.",
          "The URL of the API call is set correctly based on the provided host and timeout values.",
          "The generated text matches the expected output for the given prompt and model.",
          "The timeout value is respected and does not exceed the specified seconds.",
          "The `json` dictionary returned by the `_call_ollama` function contains all required keys (model, prompt, system, stream).",
          "The `timeout` value is correctly set to 60 seconds for the API call."
        ],
        "scenario": "Test that the Ollama provider makes a successful API call to generate text.",
        "why_needed": "This test prevents regression where the Ollama provider fails to make an API call due to incorrect configuration or timeout settings."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "119, 121-128, 132-135, 137, 139-140"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008262819999913518,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured response from the Ollama provider contains the 'model' key with value 'llama3.2'.",
          "The captured response from the Ollama provider does not contain the 'model' key.",
          "The captured response from the Ollama provider is of type 'str', which matches the expected default model 'llama3.2'.",
          "The captured response from the Ollama provider contains a non-default model, which breaks the expected behavior."
        ],
        "scenario": "Test that the default model is used when not specified for Ollama provider.",
        "why_needed": "This test prevents a regression where the default model is not used, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 6,
          "line_ranges": "92-93, 95-96, 98-99"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008128270000042903,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_check_availability()` of the `OllamaProvider` class should return False for any URL.",
          "A ConnectionError exception should be raised when calling `fake_get(url)` with a valid URL.",
          "The provider's internal state should not change unexpectedly after setting up the mock HTTPX instance.",
          "The provider's `_check_availability()` method should only check if the server is available and return False in this case.",
          "Any other URLs passed to the `fake_get(url)` function should raise a ConnectionError exception.",
          "If the server is not running, the provider should still be able to detect it by checking for an empty response from the fake HTTPX instance.",
          "The provider's internal state should remain unchanged if the mock HTTPX instance is properly set up and configured."
        ],
        "scenario": "Test that the Ollama provider returns False when the server is unavailable.",
        "why_needed": "This test prevents a regression where the provider might return True when the server is available, potentially causing unexpected behavior in downstream applications."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "92-93, 95-97"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008031090000031327,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert provider._check_availability() == False",
          "assert FakeResponse().status_code != 200",
          "assert config.provider == 'ollama'",
          "assert isinstance(provider, OllamaProvider)",
          "assert hasattr(provider, '_check_availability')",
          "assert not callable(provider._check_availability)",
          "assert provider.config is not None"
        ],
        "scenario": "Test that the Ollama provider returns False for non-200 status codes.",
        "why_needed": "To prevent a regression where the provider incorrectly assumes all requests are successful (status code 200) when they may not be."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "92-93, 95-97"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008175860000392277,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `/api/tags` endpoint should be present in any request that checks availability.",
          "The response from the `/api/tags` endpoint should have a status code of 200 (OK).",
          "The `ollama_host` attribute in the `Config` object should match the host URL of the Ollama provider (`http://localhost:11434`)."
        ],
        "scenario": "The test verifies that the Ollama provider checks its availability via the /api/tags endpoint successfully.",
        "why_needed": "This test prevents a potential bug where the Ollama provider does not respond to requests for tags, potentially leading to unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "52-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 1,
          "line_ranges": "107"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008048219999636785,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.provider == 'ollama'",
          "provider.is_local() == True",
          "assert isinstance(provider, OllamaProvider)",
          "provider.config is of type Config",
          "provider.config.provider == 'ollama'",
          "provider.config.provider != 'local'"
        ],
        "scenario": "The Ollama provider function `is_local()` returns True for local configurations.",
        "why_needed": "This test prevents a potential bug where the Ollama provider might incorrectly return False for local configurations."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "52-53, 186-187, 190-192"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007642170000394799,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.error == 'Failed to parse LLM response as JSON'",
          "provider._parse_response('not-json') is not None",
          "provider._parse_response('not-json').error == 'Failed to parse LLM response as JSON'"
        ],
        "scenario": "The test verifies that the `OllamaProvider` class throws an error when parsing a response with invalid JSON.",
        "why_needed": "This test prevents a potential bug where the Ollama provider incorrectly reports valid responses as invalid JSON."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207-209"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000792197999999189,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "response_data['key_assertions'] is not a list",
          "json.dumps(response_data) does not contain 'key_assertions'",
          "The value of 'key_assertions' in response_data does not match the expected format"
        ],
        "scenario": "The Ollama provider rejects invalid key_assertions payloads.",
        "why_needed": "This test prevents the provider from incorrectly handling malformed input data."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007795650000161913,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The response is a valid JSON object.",
          "The response contains all required keys (e.g. 'text', 'code').",
          "The response has the correct structure and formatting (e.g. double quotes around key-value pairs).",
          "All nested objects have the correct structure and formatting (e.g. arrays of strings, dictionaries with string keys).",
          "The provider correctly handles nested code blocks (e.g. multi-line strings, code snippets)."
        ],
        "scenario": "The provided test verifies that the Ollama provider correctly extracts JSON from markdown code fences.",
        "why_needed": "This test prevents a potential bug where the provider does not extract JSON from code fences, potentially leading to incorrect or incomplete annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008045210000204861,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "52-53, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008353200000215111,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert a is not None",
          "assert b is not None"
        ],
        "scenario": "Test that the Ollama provider correctly parses valid JSON responses.",
        "why_needed": "Prevents regressions due to changes in response structure or content."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "260-263"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007802169999990838,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_models.py::TestArtifactEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 3,
          "line_ranges": "213-215"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007627339999771721,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the expected value.",
          "The 'line_ranges' key in the dictionary should match the expected value.",
          "The 'line_count' key in the dictionary should match the expected value.",
          "The 'coverage_data' key (if any) should be an empty list or None, as coverage data is not serialized to JSON.",
          "Any additional keys in the dictionary should have values that are consistent with the original CoverageEntry object.",
          "The 'file_path', 'line_ranges', and 'line_count' fields should match their respective attributes of the CoverageEntry class.",
          "If a CoverageEntry object has additional data (e.g., coverage data), it should be preserved in the dictionary."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes a CoverageEntry object.",
        "why_needed": "This test prevents regression where the coverage data is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestCollectionError::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "40-43"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007761779999668761,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert d['file_path'] == 'src/foo.py',",
          "assert d['line_ranges'] == '1-3, 5, 10-15',",
          "assert d['line_count'] == 10"
        ],
        "scenario": "Tests CoverageEntry serialization correctly.",
        "why_needed": "CoverageEntry should be able to serialize its internal state accurately."
      },
      "nodeid": "tests/test_models.py::TestCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008056539999756751,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.scenario == \"\" (empty string)",
          "annotation.why_needed == \"\" (default value for confidence and error)",
          "annotation.key_assertions == [] (no key assertions are performed on an empty annotation)",
          "assert annotation.confidence is None (expected None for default confidence)",
          "assert annotation.error is None (expected None for default error)"
        ],
        "scenario": "An empty annotation should be created with default values.",
        "why_needed": "This test prevents a regression where an empty annotation would not have any confidence or error."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "104-107, 109, 111, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007723809999902187,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' key should be present in the dictionary.",
          "The 'why_needed' key should be present in the dictionary.",
          "The 'key_assertions' key should be present in the dictionary.",
          "The 'confidence' key should not be present in the dictionary when it is None."
        ],
        "scenario": "The test verifies that the `LlmAnnotation` object can be serialized with the required fields.",
        "why_needed": "This test prevents a potential bug where the minimal annotation is missing some of its necessary fields."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 10,
          "line_ranges": "104-107, 109-111, 113-115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007928599999900143,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' key should contain the correct value.",
          "The 'confidence' key should contain the expected value.",
          "The 'context_summary' key should have the correct mode and bytes values.",
          "The 'mode' key in the context summary should be set to 'minimal'.",
          "The 'bytes' key in the context summary should be set to 1000 or more."
        ],
        "scenario": "Test that the full annotation is included in the dictionary.",
        "why_needed": "Prevents a potential bug where only partial information is returned."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008150610000257075,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' field in the test report should match the expected value of SCHEMA_VERSION.",
          "The 'tests' field in the test report should be an empty list.",
          "The 'warnings' field in the test report should not be present (excluded).",
          "The 'collection_errors' field in the test report should not be present (excluded)."
        ],
        "scenario": "Test default report schema version and empty lists.",
        "why_needed": "Prevents a potential bug where the default report might be missing required fields."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_default_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "213-215, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514-516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000781419000020378,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `collection_errors` is 1.",
          "The value of `nodeid` in the first error is 'test_bad.py'.",
          "Each error in `collection_errors` has a unique `nodeid`."
        ],
        "scenario": "Test Report with Collection Errors should include them.",
        "why_needed": "This test prevents a regression where the report does not accurately identify collection errors."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 60,
          "line_ranges": "235-237, 239, 241, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008096809999642574,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the 'warnings' list should be 1.",
          "The code of the first warning should match 'W001'.",
          "Each warning in the 'warnings' list should have a 'code' attribute matching 'W001'."
        ],
        "scenario": "Test reports the presence of warnings in a ReportRoot object.",
        "why_needed": "This test prevents a regression where warnings are not reported correctly."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 73,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008457980000002863,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The list of nodeids returned from `to_dict()` matches the expected list.",
          "Each nodeid in the list corresponds to a test result with an outcome of 'passed'.",
          "No duplicate nodeids are present in the list.",
          "All nodeids are unique and do not contain any duplicates.",
          "The order of nodeids is consistent across different runs of the test.",
          "No tests have been skipped or ignored during sorting.",
          "Each nodeid has a corresponding test result with an outcome of 'passed'."
        ],
        "scenario": "Tests should be sorted by nodeid in output.",
        "why_needed": "This test prevents regression where the order of tests is not guaranteed to be consistent due to a bug in sorting algorithms or data structures."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "235-237, 239-241"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007390090000285454,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of 'detail' in the dictionary representation of the ReportWarning instance should be '/path/to/file'.",
          "The value of 'code' in the dictionary representation of the ReportWarning instance should be 'W001'.",
          "The value of 'message' in the dictionary representation of the ReportWarning instance should be 'No coverage'."
        ],
        "scenario": "Test 'test_to_dict_with_detail' verifies that a ReportWarning instance can be converted to a dictionary with the required keys.",
        "why_needed": "This test prevents a potential bug where the detailed warning message is not included in the dictionary representation of the ReportWarning instance."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "235-237, 239, 241"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007769500000449625,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'detail' key should be excluded from the warning dictionary.",
          "The warning code should match 'W001'.",
          "The warning message should not include any details.",
          "A detail message should not be present in the warning dictionary.",
          "The warning object should have a non-empty 'code' attribute.",
          "The warning object should have a non-empty 'message' attribute.",
          "The warning object should not have a 'detail' attribute."
        ],
        "scenario": "Test 'test_to_dict_without_detail' verifies that a ReportWarning object is created without detail.",
        "why_needed": "This test prevents the creation of a warning with no detailed message, which could lead to unexpected behavior or errors."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_without_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 39,
          "line_ranges": "283-285, 287-289, 370-386, 388, 391, 393, 396, 399, 401, 403, 405-411, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007931199999688943,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'run_id' key in the run metadata should be equal to 'run-123'.",
          "The 'run_group_id' key in the run metadata should be equal to 'group-456'.",
          "The 'is_aggregated' key in the run metadata should be True.",
          "The 'aggregation_policy' key in the run metadata should be set to 'merge'.",
          "The 'run_count' key in the run metadata should be equal to 3.",
          "The length of the 'source_reports' list in the run metadata should be equal to 2."
        ],
        "scenario": "Verify that RunMeta has aggregation fields.",
        "why_needed": "This test prevents regression where RunMeta is missing aggregation fields, potentially leading to incorrect data analysis."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_aggregation_fields_present",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007829909999941265,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_annotations_enabled' key is not present in the data.",
          "The 'llm_provider' key is not present in the data.",
          "The 'llm_model' key is not present in the data."
        ],
        "scenario": "Test that LLM fields are excluded when annotations are disabled.",
        "why_needed": "Prevents regression where LLMs are enabled but annotations are disabled."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 40,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413-425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007886220000159483,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of llm_annotations_enabled should be True.",
          "The value of llm_provider should match 'ollama'.",
          "The value of llm_model should match 'llama3.2:1b'.",
          "The value of llm_context_mode should match 'complete'.",
          "The value of llm_annotations_count should be 10.",
          "The value of llm_annotations_errors should be 2."
        ],
        "scenario": "Verify that LLM traceability fields are included when enabled for the specified model.",
        "why_needed": "This test prevents a regression where the llm_traceability_fields attribute is missing or incorrectly set in RunMeta instances."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_traceability_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007869190000064918,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'source_reports' key is not present in the report dictionary.",
          "The value of `is_aggregated` is set to `False` for this report.",
          "The presence of `source_reports` in the report dictionary would indicate an aggregated report, which is unexpected."
        ],
        "scenario": "Test 'Non-aggregated report should not include source_reports' verifies that the `non_aggregated_excludes_source_reports` method does not include `source_reports` in its aggregated reports.",
        "why_needed": "This test prevents a regression where non-aggregated reports are incorrectly including source reports, which can lead to confusion and incorrect analysis."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 49,
          "line_ranges": "283-285, 287-289, 370-386, 388-411, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000840778999986469,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the 'git_sha' field is set correctly.",
          "Check if the 'git_dirty' field is True for source reports.",
          "Assert that the 'repo_version', 'repo_git_sha', and 'plugin_git_sha' fields are set correctly.",
          "Verify that the 'config_hash' field is set correctly.",
          "Count the number of source report in the output data.",
          "Check if all required fields are present in the output data."
        ],
        "scenario": "Test RunMeta to dict with all optional fields.",
        "why_needed": "Prevents regression in case of missing or invalid metadata."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007771299999603798,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field should be set to 1.",
          "The 'interrupted' field should be True.",
          "The 'collect_only' field should be True.",
          "The 'collected_count' field should equal 10.",
          "The 'selected_count' field should equal 8.",
          "The 'deselected_count' field should equal 2."
        ],
        "scenario": "TestRunMeta::test_run_status_fields verifies that RunMeta includes run status fields.",
        "why_needed": "This test prevents a regression where the RunMeta object is missing certain required fields."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008275450000496676,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The schema version should be split into three parts (e.g., '1.2.3').",
          "Each part of the schema version should be a digit (0-9).",
          "All parts of the schema version should be separated by dots (.), not colons (:)."
        ],
        "scenario": "Verifies that the schema version is correctly formatted as a semver string.",
        "why_needed": "Prevents regression where the schema version is incorrectly formatted or missing."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "370-386, 388, 391, 393, 396, 399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007822399999781737,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `schema_version` attribute of the `ReportRoot` object should be equal to `SCHEMA_VERSION`.",
          "The `to_dict()` method of the `ReportRoot` object should return a dictionary with a `schema_version` key that equals `SCHEMA_VERSION`."
        ],
        "scenario": "Test that the `ReportRoot` class includes the schema version in its report root.",
        "why_needed": "This test prevents a potential bug where the schema version is not included in the report root, potentially leading to incorrect analysis or reporting."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "71-78"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007676420000279904,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key should match the expected value of 'src/foo.py'.",
          "The 'line_ranges' key should contain the correct ranges and count.",
          "The 'line_count' key should have the expected value of 10.",
          "Any additional keys in the output dictionary should be empty or None."
        ],
        "scenario": "Testing the `CoverageEntry` class to ensure it correctly serializes a coverage entry.",
        "why_needed": "This test prevents regression where the coverage entry is not properly serialized, potentially leading to incorrect or missing information in the output."
      },
      "nodeid": "tests/test_models.py::TestSourceCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "283-285, 287, 289"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000792970000020432,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'scenario' key should be present in the dictionary.",
          "The 'why_needed' key should be present in the dictionary.",
          "The 'key_assertions' key should be present in the dictionary.",
          "The 'confidence' key should not be present in the dictionary when it is None."
        ],
        "scenario": "The test verifies that the 'to_dict' method of LlmAnnotation returns a dictionary with required fields.",
        "why_needed": "This test prevents regression by ensuring that the minimal annotation format is correctly serialized."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "283-285, 287-289"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007946630000219557,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'run_id' key should be present in the dictionary representation of a SourceReport.",
          "The value of the 'run_id' key should match the provided run_id.",
          "The 'sha256' and 'path' keys should not affect the presence or value of the 'run_id' key."
        ],
        "scenario": "Test SourceReport to_dict_with_run_id method with a SourceReport object.",
        "why_needed": "This test prevents a potential bug where the run ID is not included in the dictionary representation of a SourceReport."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "455-463, 465, 467"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000793430999976863,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the expected value.",
          "The 'line_ranges' key in the dictionary should match the expected value.",
          "The 'line_count' key in the dictionary should match the expected value.",
          "The 'coverage_data' key is not present in the dictionary (this test is actually testing `CoverageEntry.to_dict()` without any coverage data).",
          "The values of 'file_path', 'line_ranges', and 'line_count' are correct."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes the test summary.",
        "why_needed": "This test prevents a regression where coverage data is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestSummary::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007799159999990479,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `nodeid` should be set to `test_foo.py::test_bar`.",
          "The value of `outcome` should be set to `passed`.",
          "The value of `duration` should be set to `0.0`.",
          "The value of `phase` should be set to `call`."
        ],
        "scenario": "Test that the `minimal_result` object has all required fields.",
        "why_needed": "This test prevents a regression where the minimal result is missing some necessary information."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 24,
          "line_ranges": "40-43, 162, 166-171, 173, 175, 177, 179, 182-184, 186, 188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007979290000434958,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'coverage' key should exist in the 'result' dictionary.",
          "The 'coverage' key should contain a list of coverage entries.",
          "Each coverage entry should have 'file_path' and 'line_ranges' keys.",
          "All file paths in the 'coverage' list should match the expected source code.",
          "All line ranges in the 'coverage' list should be between 1 and 5 (inclusive).",
          "The total number of coverage entries should be exactly 1.",
          "Each coverage entry's 'file_path' value should match the expected file path ('src/foo.py')."
        ],
        "scenario": "Test verifies that the `result` dictionary includes a single 'coverage' key with a list of coverage entries.",
        "why_needed": "This test prevents regression in case the coverage report is missing or incorrect."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186-188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000764247000006435,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' key in the result dictionary should be set to True.",
          "The 'llm_opt_out' value in the result dictionary should be a boolean.",
          "When LLM Opt-Out is enabled, the 'llm_opt_out' flag should be present in the result.",
          "When LLM Opt-Out is disabled, the 'llm_opt_out' flag should not be present in the result.",
          "The presence of the 'llm_opt_out' key and value should ensure consistency across different test cases.",
          "The absence of the 'llm_opt_out' key or value should indicate a regression issue when LLM Opt-Out is enabled."
        ],
        "scenario": "Test Result with LLM Opt-Out should include flag.",
        "why_needed": "To prevent regression when LLM opt-out is enabled."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "162, 166-171, 173, 175, 177, 179-182, 184, 186, 188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007898040000213769,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'rerun_count' field should be equal to the expected value of 2.",
          "The 'final_outcome' field should always be set to 'passed'.",
          "The 'rerun_count' and 'final_outcome' fields should not be empty or None when a result is created."
        ],
        "scenario": "Test 'Result with reruns' verifies that the 'rerun_count' and 'final_outcome' fields are correctly populated in the TestCaseResult object.",
        "why_needed": "This test prevents regression where a result is not properly updated when rerunning tests."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007846949999930075,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'rerun_count' key should be absent from the `result` dictionary.",
          "The 'final_outcome' key should not exist in the `result` dictionary.",
          "The 'rerun_count' and 'final_outcome' keys should not appear in the `result` dictionary."
        ],
        "scenario": "Test case `test_result_without_rerun_excludes_fields` verifies that the `result` dictionary does not contain 'rerun_count' and 'final_outcome' keys.",
        "why_needed": "This test prevents a regression where the result of a test is rerunned, potentially hiding important information about its outcome."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009453250000319713,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.llm_context_mode == 'minimal'",
          "cfg.llm_max_tests == 0",
          "cfg.llm_max_retries == 10",
          "cfg.llm_context_bytes == 32000",
          "cfg.llm_context_file_limit == 10",
          "cfg.llm_requests_per_minute == 5",
          "cfg.llm_timeout_seconds == 30",
          "cfg.llm_cache_ttl_seconds == 86400",
          "cfg.include_phase == 'run'",
          "cfg.aggregate_policy == 'latest'"
        ],
        "scenario": "Test that default values are set correctly.",
        "why_needed": "Prevents a bug where the default configuration settings are not properly initialized, potentially leading to unexpected behavior or errors in subsequent tests."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007805259999713599,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_default_config()` should return an instance of `Config`.",
          "The `cfg.provider` attribute should be set to `'none'`.",
          "The configuration should have no provider specified."
        ],
        "scenario": "Verify that `get_default_config()` returns a default configuration with no provider.",
        "why_needed": "Prevents regression when using `Config` without a provider."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_get_default_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008150510000177746,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `Config.is_llm_enabled()` should return `False` when the `provider` is set to `'none'`.",
          "The function `Config.is_llm_enabled()` should return `True` when the `provider` is set to `'ollama'`.",
          "The function `Config.is_llm_enabled()` should not return any value (i.e., raise an exception) when the `provider` is set to `'none'`.",
          "The function `Config.is_llm_enabled()` should return `False` when the `provider` is set to `'ollama'` and then changed back to `'none'`.",
          "The function `Config.is_llm_enabled()` should not raise an exception when the `provider` is set to `'ollama'`."
        ],
        "scenario": "Test that the `is_llm_enabled` check returns False for a provider without an LLM.",
        "why_needed": "Prevents regression in case of a change to the `provider` parameter."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 23,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-213, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007858270000156153,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 23,
          "line_ranges": "123, 163, 191, 194-195, 201-205, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008015060000161611,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The configuration object should have exactly one error message related to the 'Invalid llm_context_mode' context mode.",
          "The error message should contain the string 'Invalid llm_context_mode 'mega_max'.",
          "The test should fail if more than one error message is found in the configuration object.",
          "The error messages should be present in the list of errors returned by the validate() method.",
          "The first error message should be a string indicating that the context mode is invalid.",
          "The 'Invalid llm_context_mode' context mode should not be supported by the model."
        ],
        "scenario": "Testing the validation of an invalid context mode for the llm_context_mode setting.",
        "why_needed": "This test prevents a potential bug where the llm_context_mode is set to a value that is not supported by the model."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 23,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-221, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007874400000105197,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check that the `validate()` method returns exactly one error message for an invalid include phase.",
          "Verify that the error message contains the specified 'Invalid include_phase' phrase.",
          "Ensure that the error message does not contain any other relevant information about the configuration."
        ],
        "scenario": "Test validates configuration with an invalid include phase.",
        "why_needed": "Prevents a potential bug where the validation of an invalid include phase is not caught."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 22,
          "line_ranges": "123, 163, 191, 194-197, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007804459999647406,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 25,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237-246, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007789140000227235,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.validate() returns an error message indicating that llm_context_bytes must be at least 1000",
          "llm_context_bytes is not >= 1000 in the errors list",
          "llm_max_tests should be positive or 0 to prevent invalid configuration",
          "llm_requests_per_minute should be at least 1 to prevent invalid configuration",
          "llm_timeout_seconds should be at least 1 to prevent invalid configuration",
          "llm_max_retries should be 0 or positive to prevent invalid configuration"
        ],
        "scenario": "Test validation of numeric constraints for TestConfig.",
        "why_needed": "Prevents regression where the llm_context_bytes is set to a value less than 1000, potentially causing issues with LLM context creation."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_numeric_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000784635000002254,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `validate()` method is called with an empty configuration object.",
          "An error message indicating that no errors were found is not raised.",
          "The function does not throw any exceptions or raise informative error messages.",
          "The test verifies the absence of any validation errors.",
          "The test ensures that the function behaves as expected for a valid configuration."
        ],
        "scenario": "Verifies that the `validate()` method returns an empty list for a valid configuration.",
        "why_needed": "Prevents a potential bug where an invalid configuration causes the validation to fail without raising any errors."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 33,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287, 289, 291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335-343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.001013701999966088,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` attribute of the loaded configuration should be set to 'aggr_dir'.",
          "The `aggregate_policy` attribute of the loaded configuration should be set to 'merge'.",
          "The `aggregate_run_id` attribute of the loaded configuration should be set to 'run-123'.",
          "The `aggregate_group_id` attribute of the loaded configuration should be set to 'group-abc'."
        ],
        "scenario": "Test the `load_aggregation_options` function to ensure it correctly loads aggregation options from a mock Pytest configuration.",
        "why_needed": "This test prevents regression in the `load_aggregation_options` function, which is responsible for loading aggregation options from a Pytest configuration. Without this test, the function may incorrectly load options or fail to load certain options, leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_aggregation_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 33,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287, 289, 291-295, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0011000330000001668,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `load_config` should not crash or throw an exception when it encounters an 'llm_report_max_retries' value of 'garbage'.",
          "The function `load_config` should return a valid configuration object with the default value for 'llm_max_retries' (10) if no valid integer value is found in the INI file.",
          "The function `load_config` should not raise an exception when it encounters an invalid integer value in the INI file, but instead handle it by returning a fallback value or crashing accordingly."
        ],
        "scenario": "Test Load Config: Handling of Invalid Integer Values in INI.",
        "why_needed": "Prevents a potential crash when encountering invalid integer values in the INI file."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_config_invalid_int_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 30,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287, 289, 291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343-344, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000975551000010455,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `llm_coverage_source` in the loaded configuration is indeed 'cov_dir'.",
          "The `llm_coverage_source` option is correctly set to 'cov_dir' when loading the configuration.",
          "The coverage source is not set to an empty string or None, as it would be if there was no coverage source specified.",
          "The value of `llm_coverage_source` in the loaded configuration matches the expected value of 'cov_dir'.",
          "The `llm_coverage_source` option is correctly set to a valid directory path.",
          "The test does not fail when an empty string or None is passed as the coverage source.",
          "The test passes with all assertions passing for the given scenario."
        ],
        "scenario": "Verify that the `llm_coverage_source` option is set to 'cov_dir' when loading configuration.",
        "why_needed": "This test prevents a potential bug where the coverage source is not correctly set, leading to incorrect coverage analysis."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287, 289, 291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.00100329200000715,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `cfg.provider` attribute should be set to 'none'.",
          "The `cfg.report_html` attribute should be None.",
          "The function does not raise an exception if no configuration options are provided."
        ],
        "scenario": "Testing the `load_defaults` function with default configuration.",
        "why_needed": "Prevents a potential bug where the 'report_html' option is not set when no options are provided."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 32,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287-289, 291, 298, 305-307, 310, 319-321, 323, 325, 327, 329-331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0010636659999931908,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option in the configuration should be set to 'cli_report.html'.",
          "The `llm_requests_per_minute` option in the configuration should be set to 100.",
          "The value of `llm_report_html` option should not be overridden by CLI options.",
          "The value of `llm_requests_per_minute` option should override any ini values."
        ],
        "scenario": "Test that CLI options override ini options when loading configuration from command line.",
        "why_needed": "This test prevents a regression where the default value of `llm_report_html` is not overridden by CLI options."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 30,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287, 289, 291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331-332, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009840869999493407,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `llm_max_retries` should be equal to 2.",
          "The `load_from_cli` method should be able to retrieve the configuration with a retry limit of 2.",
          "The `llm_max_retries` option should not cause any issues during configuration loading.",
          "The test should fail if the `llm_max_retries` option is set to a value other than 1 or 2.",
          "The `load_from_cli` method should be able to handle cases where retries are required but not specified.",
          "The `load_from_cli` method should be able to retry loading the configuration multiple times if necessary.",
          "The test should pass if the `llm_max_retries` option is set to a value that allows for retries (e.g., 3 or more)."
        ],
        "scenario": "Testing the `load_from_cli` method with a retry limit of 2.",
        "why_needed": "This test prevents a potential bug where the `llm_max_retries` option is not properly set, causing the configuration to be loaded without retries."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 37,
          "line_ranges": "123, 163, 276, 279-293, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.001123586999995041,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` key in the configuration should be set to 'ollama'.",
          "The `model` key in the configuration should be set to 'llama3'.",
          "The `llm_context_mode` key in the configuration should be set to 'balanced'.",
          "The `llm_requests_per_minute` key in the configuration should be set to 10.",
          "The `llm_max_retries` key in the configuration should be set to 2.",
          "The `report_html` key in the configuration should be set to 'report.html'.",
          "The `report_json` key in the configuration should be set to 'report.json'."
        ],
        "scenario": "Test loading values from ini options to ensure correct configuration retrieval.",
        "why_needed": "This test prevents a potential bug where the `load_config` function returns incorrect or missing configuration values due to an issue with the `getini` method's side effect."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007803170000215687,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` attribute is set to `/reports` as expected.",
          "The `aggregate_policy` attribute is set to `merge` as expected.",
          "The `aggregate_include_history` attribute is set to `True` as expected."
        ],
        "scenario": "Tests the aggregation settings configuration.",
        "why_needed": "Prevents a potential bug where the aggregate directory, policy, and history inclusion are not correctly set for aggregated data."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_aggregation_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007939720000535999,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `config.report_html` is set to `report.html`.",
          "The value of `config.report_json` is set to `report.json`.",
          "The value of `config.report_pdf` is set to `report.pdf`.",
          "The expected values for `config.report_evidence_bundle`, `config.report_dependency_snapshot` are not set."
        ],
        "scenario": "Test Config with all output paths.",
        "why_needed": "Prevents regression in case of multiple report formats being used."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_all_output_paths",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008015359999831162,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.capture_failed_output is True",
          "assert config.capture_failed_output is True",
          "The captured output does not exceed 8000 characters"
        ],
        "scenario": "Test the `capture_failed_output` configuration option.",
        "why_needed": "This test prevents a potential bug where the captured output exceeds the maximum allowed length of 8000 characters."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_capture_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008014759999923626,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `metadata_file` attribute of the `Config` object should be set to 'metadata.json'.",
          "The `hmac_key_file` attribute of the `Config` object should be set to 'key.txt'."
        ],
        "scenario": "Verify that the `Config` object is created with specified compliance settings.",
        "why_needed": "This test prevents a potential bug where the `Config` object's metadata and HMAC key files are not set correctly, leading to incorrect configuration."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_compliance_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007376969999768335,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.omit_tests_from_coverage is set to `False`",
          "config.include_phase is set to `'all'`",
          "The configuration does not include any tests from the specified phase"
        ],
        "scenario": "Verify that the `Config` class correctly sets `omit_tests_from_coverage` to `False` and `include_phase` to `'all'`.",
        "why_needed": "Prevents a potential regression where coverage settings are not properly configured."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_coverage_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007523440000340997,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The '*.pyc' and '*.log' exclude globs should be included in the config.",
          "The custom exclude glob '*.pyc' should match one of the existing exclude globs.",
          "The custom exclude glob '*.log' should also match one of the existing exclude globs.",
          "The include glob '*' should not be present in the config.",
          "The custom exclude globs should override any default exclude globs if provided.",
          "The custom exclude glob '*.pyc' should only match files with a .pyc extension.",
          "The custom exclude glob '*.log' should only match files with a .log extension."
        ],
        "scenario": "Test the ability to include custom exclude globs in the LLM configuration.",
        "why_needed": "This test prevents a potential bug where the default exclude globs are not properly handled when custom ones are provided."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_custom_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007860470000196074,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `*.py` glob matches any Python files in the current directory and subdirectories.",
          "The `*.pyi` glob matches any Python source file with an `.i` extension.",
          "The `*.py` glob is included in the list of include globs for the LLM context.",
          "The `*.pyi` glob is included in the list of include globs for the LLM context.",
          "The configuration option `llm_context_include_globs` is correctly set to `['*.py', '*.pyi']`.",
          "No other include globs are added to the configuration option `llm_context_include_globs`.",
          "The include globs are correctly propagated to the LLM context."
        ],
        "scenario": "Verify that the `llm_context_include_globs` configuration option includes all specified globs.",
        "why_needed": "This test prevents a potential bug where the include globs are not correctly propagated to the LLM context."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_include_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "123"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007717299999967508,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_invocation_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008176759999969363,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of llm_max_tests is set to 50.",
          "The value of llm_max_concurrency is set to 8.",
          "The value of llm_requests_per_minute is set to 12.",
          "The value of llm_timeout_seconds is set to 60 seconds.",
          "The value of llm_cache_ttl_seconds is set to 3600 seconds (1 hour).",
          "The cache directory is set to .cache."
        ],
        "scenario": "Test the LLM execution settings configuration.",
        "why_needed": "Prevents regression in LLM execution settings when using multiple tests concurrently or with high concurrency."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_llm_execution_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007891829999948641,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.llm_include_param_values is True",
          "config.llm_param_value_max_chars == 200"
        ],
        "scenario": "Test the configuration of LLM parameter settings.",
        "why_needed": "This test prevents a potential bug where the LLM include param values are not correctly set to True, potentially leading to incorrect output or errors."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_llm_param_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008431040000118628,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider` attribute is set to 'ollama'.",
          "The `model` attribute is set to 'llama3.2'.",
          "The `llm_context_bytes` attribute is set to 64000."
        ],
        "scenario": "Verify that the LLM settings are correctly configured with the provided provider, model, and context settings.",
        "why_needed": "This test prevents a potential bug where the LLM settings are not properly set, potentially leading to incorrect results or errors during training."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_llm_settings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008057239999743615,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.repo_root is an instance of `Path` and its value matches the expected path `/project`",
          "config.repo_root.path is equal to '/project'",
          "config.repo_root.parent is None (i.e., it's not a subdirectory)",
          "config.repo_root.is_absolute() is True",
          "config.repo_root.exists() is False",
          "config.repo_root.is_dir() is False"
        ],
        "scenario": "Verify that the `repo_root` attribute of a `TestConfigAnnotations` instance is correctly set to the specified path.",
        "why_needed": "This test prevents a potential bug where the `repo_root` attribute is not set correctly, potentially leading to incorrect configuration or unexpected behavior."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_repo_root_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000820011000030263,
      "file_path": "tests/test_options_extended.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "All phases are included in the configuration.",
          "No phase is excluded from the configuration.",
          "The `include_phase` value for each phase is present and valid.",
          "Invalid or missing include_phase values are not included in the errors.",
          "The validation process does not return any errors for all valid include_phase values."
        ],
        "scenario": "Test the `test_valid_phase_values` function to ensure all valid include_phase values pass validation.",
        "why_needed": "Prevents a potential bug where invalid or missing include_phase values cause test failures."
      },
      "nodeid": "tests/test_options_extended.py::TestConfigAnnotations::test_valid_phase_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007646380000210229,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `*.pyc` should be included in the default exclude globs.",
          "The function `__pycache__/*` should be included in the default exclude globs.",
          "The function `*secret*` should be included in the default exclude globs.",
          "The function `*password*` should be included in the default exclude globs."
        ],
        "scenario": "Verify that the default exclude globs are correctly set for the LLM context.",
        "why_needed": "This test prevents a potential bug where the default exclude globs do not include all necessary files."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008212429999616688,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Any pattern containing '--password' should be present in the default redact patterns.",
          "Any pattern containing '--token' should be present in the default redact patterns.",
          "Any pattern containing '--api[_-]?key' should be present in the default redact patterns."
        ],
        "scenario": "Verify that default redact patterns include sensitive information.",
        "why_needed": "This test prevents a potential security vulnerability where sensitive information like passwords and tokens are not properly redacted."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 261"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007809669999687685,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `config.provider` should be 'none'.",
          "The value of `config.llm_context_mode` should be 'minimal'.",
          "The value of `config.llm_context_bytes` should be 32000.",
          "The value of `config.omit_tests_from_coverage` should be True.",
          "The value of `config.include_phase` should be 'run'."
        ],
        "scenario": "Test default values of the `get_default_config()` function.",
        "why_needed": "This test prevents a potential bug where the default values are not correctly set for the configuration."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigDefaultsMaximal::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007838529999730781,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return False when provider is 'none'.",
          "The function should return True when provider is either 'ollama' or 'litellm'."
        ],
        "scenario": "Test that the `is_llm_enabled` method returns correct enabled status for different providers.",
        "why_needed": "This test prevents regression in cases where LLM is not available or is disabled."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigHelpersMaximal::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 23,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-213, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007703279999873303,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the validate method returns exactly one error message.",
          "The error message contains the string 'Invalid aggregate_policy 'invalid''.",
          "The error message is not empty.",
          "The error message does not contain any additional information (e.g., a stack trace)."
        ],
        "scenario": "Test validates an invalid aggregate policy.",
        "why_needed": "Prevents a potential bug where the validate method returns multiple errors for an invalid aggregate policy."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 23,
          "line_ranges": "123, 163, 191, 194-195, 201-205, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007779219999974885,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 23,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-221, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007838330000140559,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 22,
          "line_ranges": "123, 163, 191, 194-197, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008035699999595636,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 24,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237-245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008260820000032254,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `validate()` should return at least 4 errors for invalid numeric values.",
          "One error should contain 'llm_context_bytes' as a substring.",
          "Another error should contain 'llm_max_tests' as a substring.",
          "Yet another error should contain 'llm_requests_per_minute' as a substring.",
          "A fourth error should contain 'llm_timeout_seconds' as a substring."
        ],
        "scenario": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds",
        "why_needed": "This test prevents regression when the `llm_context_bytes` value is set to a large negative number."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_numeric_bounds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 20,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008073469999771987,
      "file_path": "tests/test_options_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `validate()` method of the `Config` object should return an empty list for a valid configuration.",
          "A valid configuration should not produce any output when passed to the `validate()` method.",
          "An invalid configuration should raise an exception or return an error message indicating that it is invalid.",
          "The `validate()` method should be able to handle a wide range of input configurations without crashing or producing unexpected results.",
          "The `validate()` method should not modify the original configuration object in any way.",
          "A valid configuration should have no side effects on the system.",
          "An invalid configuration should cause an exception to be raised with a clear and descriptive error message."
        ],
        "scenario": "Verifies that an invalid configuration returns an empty list.",
        "why_needed": "Prevents a potential bug where an invalid configuration is not properly validated and could cause unexpected behavior or errors in downstream code."
      },
      "nodeid": "tests/test_options_maximal.py::TestConfigValidationMaximal::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 42,
          "line_ranges": "123, 163, 276, 279-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-325, 327, 329, 331, 335, 337, 339-341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008254800000031537,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `cfg` variable should be of type `Config`.",
          "The `cfg` variable should not contain any explicitly registered pytest options.",
          "The `cfg` variable's values should match the expected safe defaults for the plugin configuration."
        ],
        "scenario": "Test that the config defaults to safe settings when no options are provided.",
        "why_needed": "Prevents a potential bug where the config is set to an insecure or unexpected default value."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007800460000453313,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "pytestconfig is not None",
          "pytestconfig has attributes and methods (e.g., `__dict__`, `config`) that are accessible"
        ],
        "scenario": "Verify that the `pytestconfig` object is accessible in the test.",
        "why_needed": "This test prevents a potential issue where the plugin configuration is not available due to an incorrect or missing configuration."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007649180000157685,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "llm_context_override": "balanced",
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007905950000122175,
      "file_path": "tests/test_plugin_integration.py",
      "llm_opt_out": true,
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0007700770000269586,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `requirement_marker` function should be called with no arguments.",
          "The `requirement_marker` function should not throw any exceptions when called without arguments.",
          "The `requirement_marker` function should not raise any errors when called without arguments."
        ],
        "scenario": "The `requirement_marker` function is tested to ensure it does not throw any errors when used.",
        "why_needed": "This test prevents a potential bug where the `requirement_marker` function could cause an error due to incorrect usage."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
      "outcome": "passed",
      "phase": "call",
      "requirements": [
        "REQ-001",
        "REQ-002"
      ]
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 131,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.03643672600003356,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that a full report is generated and contains both JSON and HTML files.",
          "Check if the total number of tests passed is correct (1 passed, 1 failed).",
          "Ensure the test names 'test_a.py' and 'test_b.py' are included in the report.",
          "Verify the presence of both JSON and HTML files in the generated report.",
          "Confirm that the report contains a summary with total count of tests (2 total tests, 1 passed)."
        ],
        "scenario": "Test the integration of report writer with pytest_llm_report.",
        "why_needed": "This test prevents regression that may occur when the report writer is integrated into pytest_llm_report."
      },
      "nodeid": "tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "413-414, 417, 421-423, 434-435, 441-442"
        }
      ],
      "duration": 0.0014146800000389703,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_report.session.config.stash.get.assert_called_with(_enabled_key, False)",
          "pytest_collectreport.mock_report.session.config.stash.get.return_value is False",
          "mock_report.session.config.stash.get.assert_not_called()",
          "pytest_collectreport.mock_report.session.config.stash.get.return_value is None",
          "_enabled_key in pytest_collectreport.mock_report.session.config.stash.get.call_args_list",
          "pytest_collectreport.mock_report.session.config.stash.get.call_args_list[0][1] is False"
        ],
        "scenario": "Test that collectreport skips when the plugin is disabled.",
        "why_needed": "To prevent a regression where collectreport fails to run due to the plugin being disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 12,
          "line_ranges": "413-414, 417, 421-423, 434-435, 441, 445-447"
        }
      ],
      "duration": 0.001682039000002078,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_collectreport` function should be called with the `mock_report` object as its argument.",
          "The `handle_collection_report` method of the `mock_collector` object should be called once with the `mock_report` object as its argument.",
          "The `stash_get` method of the `mock_report.session.config.stash` object should return `True` when the `_enabled_key` is present in the stash.",
          "The `stash_get` method of the `mock_report.session.config.stash` object should not return `True` when the `_collector_key` is present in the stash.",
          "The `handle_collection_report` method of the `mock_collector` object should be called with the `mock_report` object as its argument, even if it's not collecting a report.",
          "The `stash_get` method of the `mock_report.session.config.stash` object should return `None` when the `_enabled_key` is not present in the stash.",
          "The `stash_get` method of the `mock_report.session.config.stash` object should return `True` when the `_collector_key` is not present in the stash.",
          "The `handle_collection_report` method of the `mock_collector` object should be called with the `mock_report` object as its argument, even if it's not collecting a report."
        ],
        "scenario": "Verify that collectreport is called when pytest_collectreport is enabled.",
        "why_needed": "This test prevents a regression where the collector is not called when pytest_collectreport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "413-414, 417, 421-423, 434, 438"
        }
      ],
      "duration": 0.0008847619999983181,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_collectreport` function does not raise an exception when called with a mock report object that lacks a valid session.",
          "The `pytest_collectreport` function ignores the `session` attribute of the mock report object.",
          "The `pytest_collectreport` function continues to execute without interruption even if it encounters a mock report object without a valid session.",
          "The plugin does not attempt to access or manipulate the `session` attribute of the mock report object.",
          "The test case passes successfully even when the `pytest_collectreport` function is called with an invalid mock report object.",
          "The plugin remains unaffected by the absence of a session attribute in the mock report object."
        ],
        "scenario": "Verify that `pytest_collectreport` does not raise an exception when a mock report object is created without a valid session.",
        "why_needed": "This test prevents a potential bug where the plugin would attempt to access the `session` attribute of a mock report object, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "413-414, 417, 421-423, 434, 438"
        }
      ],
      "duration": 0.0009102289999987079,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `pytest_collectreport` should not be called with a `None` argument.",
          "A `None` value for the `session` attribute in the mock report object should not cause an exception to be raised.",
          "The `pytest_collectreport` function should continue executing without error when given a `None` session."
        ],
        "scenario": "Verify that `pytest_collectreport` does not raise an exception when the session is set to `None`.",
        "why_needed": "To prevent a potential bug where `pytest_collectreport` raises an error when given a `None` session."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 52,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-285, 287, 289, 291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343-344, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 29,
          "line_ranges": "202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236-238, 240-241, 245-246, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0031768289999831723,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking `pytest_configure` with a valid config that includes `llm_report_provider='ollama'` and setting `llm_report_html`, `llm_report_json`, etc. to None should not trigger the warning.",
          "The `option.llm_report_html` is set to `None` but it's still possible for the warning to be raised if the user doesn't configure the LLM report provider correctly.",
          "If `llm_report_json` or any other option that depends on `llm_report_provider` is not configured, setting it to `None` should prevent the warning from being raised.",
          "The `option.llm_evidence_bundle`, `option.llm_dependency_snapshot`, etc. options are not affected by the LLM report provider configuration and can be set to any value without triggering the warning.",
          "The `option.llm_aggregate_dir`, `option.llm_aggregate_policy`, `option.llm_aggregate_run_id`, `option.llm_aggregate_group_id` options are also not affected by the LLM report provider configuration and can be set to any value without triggering the warning.",
          "If `llm_max_retries` is set, it should still prevent the warning from being raised even if the user doesn't configure the LLM report provider correctly.",
          "The `rootpath` and `stash` options are not affected by the LLM report provider configuration and can be set to any value without triggering the warning.",
          "Setting `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, `llm_aggregate_group_id` to None should prevent the warning from being raised if they are not configured correctly."
        ],
        "scenario": "Test that LLM enabled warning is raised when configuring pytest_llm_report plugin.",
        "why_needed": "Prevents a potential bug where the LLM report provider 'ollama' is enabled without proper configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 51,
          "line_ranges": "123, 163, 191, 194-197, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 276, 279-281, 283, 285, 287, 289, 291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343-344, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 25,
          "line_ranges": "202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-232, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.002837865999993028,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_config.option.llm_report_html is None",
          "mock_config.option.llm_report_json is None",
          "mock_config.option.llm_report_pdf is None",
          "mock_config.option.llm_evidence_bundle is None",
          "mock_config.option.llm_dependency_snapshot is None",
          "mock_config.option.llm_requests_per_minute is None",
          "mock_config.option.llm_aggregate_dir is None",
          "mock_config.option.llm_aggregate_policy is None",
          "mock_config.option.llm_aggregate_run_id is None",
          "mock_config.option.llm_aggregate_group_id is None",
          "mock_config.option.llm_max_retries is None"
        ],
        "scenario": "Test that validation errors raise UsageError when invalid configuration is provided.",
        "why_needed": "To prevent a potential bug where the plugin does not handle invalid configuration properly and raises a UsageError."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 17,
          "line_ranges": "202-204, 206-208, 210-212, 216-217, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0012354660000255535,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_configure` function should be called early without calling `addinivalue_line`.",
          "The `addinivalue_line` method should still be called for markers before the worker check.",
          "The plugin should skip configuration on xdist workers as expected."
        ],
        "scenario": "Test that configure skips on xdist workers and verifies the correct behavior.",
        "why_needed": "This test prevents a potential regression where the plugin might not skip configuration on xdist workers, potentially leading to incorrect results or skipped tests."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 29,
          "line_ranges": "202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236-238, 240-241, 245-246, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.002964372999997522,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 36,
          "line_ranges": "123, 163, 276, 279-291, 298, 305-307, 310, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343-344, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0018650110000066888,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider is set to 'ollama' as expected.",
          "The model is set to 'llama3.2' as expected.",
          "The context mode is set to 'complete' as expected.",
          "The number of requests per minute is set to 10 as expected.",
          "The report HTML is set to 'ini.html' as expected.",
          "The report JSON is set to 'ini.json' as expected.",
          "The directory for aggregate files is set to '/project' as expected."
        ],
        "scenario": "Test loading all INI options for plugin load configuration.",
        "why_needed": "Prevents regression in plugin load configuration when CLI options are not set."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_all_ini_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 43,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287-291, 298, 305-307, 310, 319-332, 335-344, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.001883234999979777,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'report_html' option in the config should be set to 'cli.html'.",
          "The 'report_json' option in the config should be set to 'cli.json'.",
          "The 'report_pdf' option in the config should be set to 'cli.pdf'.",
          "The 'report_evidence_bundle' option in the config should be set to 'bundle.zip'.",
          "The 'report_dependency_snapshot' option in the config should be set to 'deps.json'.",
          "The 'llm_requests_per_minute' option in the config should be set to 20.",
          "The 'aggregate_dir' option in the config should be set to '/agg'.",
          "The 'aggregate_policy' option in the config should be set to 'merge'.",
          "The 'aggregate_run_id' option in the config should be set to 'run-123'.",
          "The 'aggregate_group_id' option in the config should be set to 'group-abc'.",
          "The rootpath of the config should be set to '/project'."
        ],
        "scenario": "Test CLI options override INI options.",
        "why_needed": "This test prevents a bug where the plugin's report configuration is not correctly overridden from INI settings to command-line arguments."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 9,
          "line_ranges": "271, 275-276, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.001262636999967981,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The stash.get method of the config object was called with the correct key and value.",
          "The stash.get method of the config object was called once.",
          "The stash.get method of the config object returned False for enabled.",
          "The stash.get method of the config object did not return None when the plugin is disabled.",
          "The stash.get method of the config object correctly checked if the plugin is enabled before returning False."
        ],
        "scenario": "Test that terminal summary skips when plugin is disabled.",
        "why_needed": "Prevents a regression where the plugin's terminal summary is not properly handled when it is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "271-272, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.001013280999984545,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_terminal_summary` function should return early without doing anything when the `workerinput` is set to a valid worker ID.",
          "The `workerinput` parameter of `pytest_terminal_summary` should be a dictionary with a single key-value pair where the key is 'workerid' and the value is a string representing the worker ID.",
          "The `result` variable of `pytest_terminal_summary` should be `None` when called with the provided arguments."
        ],
        "scenario": "Test that terminal summary skips on xdist worker when configured correctly.",
        "why_needed": "This test prevents a regression where the plugin fails to skip terminal summaries on xdist workers due to incorrect configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 41,
          "line_ranges": "123, 163, 276, 279, 281, 283, 285, 287, 289, 291, 298, 305-307, 310, 319-332, 335-344, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.003373926999984178,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::testload_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 7,
          "line_ranges": "413-414, 417-418, 421-423"
        }
      ],
      "duration": 0.0016330990000028578,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_item.config.stash.get() returns False",
          "mock_call.send() raises StopIteration exception",
          "mock_outcome.get_result().get_result() returns True"
        ],
        "scenario": "Test makereport skips when disabled.",
        "why_needed": "The test prevents a regression where the plugin does not report any issues even though makereport is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0019345799999541669,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_runtest_makereport` function should be called with the `mock_collector` object as its `collector_key` argument.",
          "The `mock_collector` object should have a `handle_runtest_logreport` method that calls the `makereport` function.",
          "The `make_report` function should return an instance of `MockReport` or similar.",
          "The `collectors` dictionary in `pytest_llm_report` plugin should contain the `mock_collector` object as one of its values.",
          "The `pytest_runtest_makereport` function should be able to handle a mock collector without raising an exception.",
          "The `pytest_runtest_makereport` function should not raise a StopIteration exception when called with a mock outcome.",
          "The `mock_collector.handle_runtest_logreport` method should call the `make_report` function and return a result object."
        ],
        "scenario": "Test makereport calls collector when enabled.",
        "why_needed": "Prevents a potential bug where the collector is not called when makereport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "413-414, 417, 421-423, 457-458"
        }
      ],
      "duration": 0.0012831840000444572,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_session.config.stash.get.assert_called_with(_enabled_key, False)",
          "pytest_collection_finish(mock_session) should be called with _enabled_key set to False",
          "mock_session.config.stash.get.return_value should be False",
          "_enabled_key should not be set in the mock session config",
          "collection_finish should skip the collection process when collection finish flag is disabled"
        ],
        "scenario": "Test that collection_finish is skipped when the plugin's collection finish flag is disabled.",
        "why_needed": "The test prevents a regression where collection_finish is not executed correctly when the plugin's collection finish flag is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "413-414, 417, 421-423, 457, 461-463"
        }
      ],
      "duration": 0.001712505999989844,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The pytest_collection_finish function should be called with mock_session.items as argument.",
          "The stash_get function should return True for _enabled_key and False for _collector_key.",
          "The stash_get function should return mock_collector for _enabled_key and None for _collector_key.",
          "mock_collector.handle_collection_finish should be called once with mock_session.items as argument.",
          "mock_collector.handle_collection_finish should not be called twice (once before and once after pytest_collection_finish is called).",
          "The stash_get function should return the correct default value in all cases."
        ],
        "scenario": "Test that Pytest collection finish is called when collection_finish is enabled.",
        "why_needed": "This test prevents a potential regression where the collector does not call stash_get for certain keys."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "413-414, 417, 421-423, 474-475"
        }
      ],
      "duration": 0.0013167170000087935,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_sessionstart` function should have been called with the `_enabled_key` key set to `False` before checking the stash.",
          "The `get` method of the `stash` attribute of the mock session object should have returned `False` instead of a value indicating enabled status.",
          "The `assert_called_with` method on the `get` call of the mock session object should not have been called with any arguments.",
          "The stash attribute of the mock session object should still be set to its original value (i.e., `True`) after the test has finished.",
          "The `_enabled_key` key in the mock session object's configuration should still be present and valid.",
          "The `pytest_sessionstart` function should not have been called with any arguments or keys when sessionstart is disabled."
        ],
        "scenario": "Test that sessionstart skips when disabled and verifies the expected behavior.",
        "why_needed": "This test prevents a potential regression where pytest_sessionstart does not check for enabled status correctly when sessionstart is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 11,
          "line_ranges": "413-414, 417, 421-423, 474, 478, 481, 483-484"
        }
      ],
      "duration": 0.0010735330000102294,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert _collector_key in mock_stash",
          "assert _start_time_key in mock_stash"
        ],
        "scenario": "Test that sessionstart initializes collector when enabled and creates a stash with the necessary keys.",
        "why_needed": "This test prevents a potential bug where the collector is not created or does not have access to the necessary configuration data, leading to incorrect reporting of Pytest sessions."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 124,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0020377239999902486,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')",
          "group.addoption.call_args_list[0][0] == '--llm-report'",
          "group.addoption.call_args_list[1][0] == '--llm-coverage-source'"
        ],
        "scenario": "Verify pytest_addoption adds expected arguments to the parser.",
        "why_needed": "pytest_addoption may not add all required arguments if the 'llm-report' group is not properly configured."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 124,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0021057699999573742,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that 'llm_report_html' is added to the ini list.",
          "Verify that 'llm_report_json' is added to the ini list.",
          "Verify that 'llm_report_max_retries' is added to the ini list."
        ],
        "scenario": "Test pytest_addoption adds INI options for plugin terminal summary.",
        "why_needed": "This test prevents a regression where the plugin does not add INI options to the command line when using pytest."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 53,
          "line_ranges": "271, 275, 279, 282, 301-302, 304, 306, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-338, 340, 342-345, 357-358, 363-364, 391-401, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.003948138999987805,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mock_cov.load` method should be called with the correct arguments (None, mock_config).",
          "The `mock_cov.report` method should be called with the correct arguments (None, mock_config).",
          "The `pytest_terminal_summary` function should not raise an exception when coverage file does not exist.",
          "The `coverage.CoverageMapper` class is correctly instantiated and configured.",
          "The `Coverage` instance returned by `mock_cov.report` has a valid percentage value.",
          "The `ReportWriter` instance created with the mock configuration has been properly initialized.",
          "No exceptions are raised when calling `pytest_terminal_summary` with an invalid configuration."
        ],
        "scenario": "Test coverage percentage calculation logic for terminal summary.",
        "why_needed": "Prevents regression in coverage reporting when terminal summary is enabled and a coverage file exists."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 59,
          "line_ranges": "271, 275, 279, 282, 301-302, 304, 306, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357-358, 363-366, 369, 371, 374-376, 383-388, 391-401, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0029698529999677703,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_terminal_summary_llm_enabled` function should report the correct provider (in this case, 'ollama') even when LLM is enabled.",
          "The `pytest_terminal_summary_llm_enabled` function should pass the provided configuration to the annotation process.",
          "The annotation process should verify that the correct model name ('gpt-4') was used in the terminal summary report.",
          "The `pytest_terminal_summary_llm_enabled` function should not raise any exceptions when LLM is enabled."
        ],
        "scenario": "Test that terminal summary with LLM enabled runs annotations correctly and reports the correct provider.",
        "why_needed": "This test prevents regression in reporting when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 45,
          "line_ranges": "271, 275, 279, 282, 301-302, 304, 306, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.001972221000016816,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 21,
          "line_ranges": "271, 275, 279, 282-283, 285-286, 289-290, 292, 294-298, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0023808029999941027,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` is set to `/agg` in the configuration.",
          "The aggregator function is called once and returns a report.",
          "The report writer writes JSON and HTML files.",
          "The aggregation result is stored in the stash.",
          "The stash supports both get() and [] indexing.",
          "The terminal summary is written to the correct directory.",
          "The terminal summary contains the aggregated data."
        ],
        "scenario": "Test terminal summary with aggregation enabled.",
        "why_needed": "This test prevents a regression where the plugin does not aggregate terminal summaries correctly when reporting to JSON."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 163, 252"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 52,
          "line_ranges": "271, 275, 279, 282, 301-302, 304, 306, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-338, 348-351, 357-358, 363-364, 391-401, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0040746749999698295,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 51,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-155, 158-159, 163, 191-192, 194"
        }
      ],
      "duration": 0.00741108200003282,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'utils.py' file is present in the assembled context.",
          "The 'def util()' function is found in the 'utils.py' file within the assembled context.",
          "All lines of the 'utils.py' file are covered by the test.",
          "The line ranges and line count of the coverage entry match the expected values."
        ],
        "scenario": "Tests the ContextAssembler to assemble a balanced context for a test file.",
        "why_needed": "This test prevents regression when assembling contexts with unbalanced llm_context_mode, as it ensures that all required dependencies are included."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 132-133, 180"
        }
      ],
      "duration": 0.0009473380000031284,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The source of the assembled context should contain the function `test_1` from the 'test_a.py' test file.",
          "The assembled context should have a node representing the function `test_1` in the correct location.",
          "The assembled context should not include any other functions or nodes from the 'test_a.py' test file.",
          "The source of the assembled context should be identical to the original 'test_a.py' test file.",
          "The assembled context should have the same node structure as the original 'test_a.py' test file.",
          "Any changes made to the 'test_a.py' test file should not affect the assembled context.",
          "The assembled context should still contain the correct function `test_1` even if it is moved or renamed in the 'test_a.py' test file."
        ],
        "scenario": "Assembling a complete context for the 'test_a.py' test file.",
        "why_needed": "This test prevents regression when the 'test_a.py' test file is modified to include new tests or changes to existing code."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 30,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0010004680000292865,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The source of the test function should contain the function being tested.",
          "The context of the test file should be empty.",
          "The assembler should assemble a minimal context for the test file.",
          "The assembler should not assemble any unnecessary code into the context.",
          "The assembler should return an empty context if no tests are found in the test file.",
          "The assembler should raise an error if the 'minimal' llm_context_mode is not supported."
        ],
        "scenario": "Test that the ContextAssembler can assemble a minimal context for a test file with a single test function.",
        "why_needed": "This test prevents regression when using the 'minimal' llm_context_mode, as it ensures that only the necessary code is assembled into the context."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 132, 135-138, 140-141, 144-145, 148, 151-152, 154-156, 158-159, 163, 191-192, 194"
        }
      ],
      "duration": 0.0010051059999796053,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 26,
          "line_ranges": "33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0009821030000125575,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_get_test_source` returns an empty string when given a non-existent file.",
          "The function `_get_test_source` correctly identifies and includes the entire nested test name in its output, including any parameters."
        ],
        "scenario": "Verify the correct handling of non-existent files and nested test names with parameters.",
        "why_needed": "This test prevents a potential bug where the ContextAssembler does not correctly handle cases where it encounters an unknown file or nested test name with parameters."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 5,
          "line_ranges": "33, 191-194"
        }
      ],
      "duration": 0.0014501869999890005,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert assembler._should_exclude('test.pyc') is True",
          "assert assembler._should_exclude('secret/key.txt') is True",
          "assert assembler._should_exclude('public/readme.md') is False"
        ],
        "scenario": "The test verifies that the ContextAssembler should exclude certain files from being processed by LLM.",
        "why_needed": "This test prevents a potential bug where the ContextAssembler incorrectly excludes important files, leading to unexpected behavior or errors in the LLM's output."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_should_exclude",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000801937000005637,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input list is not empty.",
          "All elements in the list are present in the output.",
          "The start value of the range is correct (i.e., it's less than or equal to the end value).",
          "The end value of the range is correct (i.e., it's greater than or equal to the start value).",
          "All elements within the range are present in the output.",
          "The total number of elements in the input list is preserved in the output.",
          "No duplicate ranges are created in the output.",
          "The order of consecutive lines is maintained in the output."
        ],
        "scenario": "The 'test_consecutive_lines' test verifies that consecutive lines in a list of integers are compressed into the format 'start-end'.",
        "why_needed": "This test prevents regression when consecutive lines are compressed to include all elements within a range."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0007713599999874532,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts that the output of `compress_ranges([1, 2, 2, 3, 3, 3])` is '1-3'",
          "asserts that the function correctly handles duplicate values by returning a single range ('1-3')",
          "asserts that the function returns an empty string for an input list with no duplicates"
        ],
        "scenario": "The function `compress_ranges` should correctly identify and return the compressed range for a list containing duplicate values.",
        "why_needed": "This test prevents bugs that may occur when compressing ranges with duplicates, such as incorrect or missing ranges."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "29-30"
        }
      ],
      "duration": 0.0007948340000325516,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return an empty string when given an empty list as input.",
          "The function should not raise any exceptions or errors when given an empty list as input.",
          "The function should correctly handle and return an empty string for the compressed range.",
          "The function should not produce unexpected results or incorrect values for the compressed range of an empty list.",
          "The function should be able to handle large inputs, including empty lists, without performance issues."
        ],
        "scenario": "Testing the `compress_ranges` function with an empty input.",
        "why_needed": "This test prevents a potential bug where the function returns an incorrect result for an empty input list."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0007569830000306865,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output should be '1-3, 5, 10-12, 15'.",
          "The output should contain all the given ranges.",
          "The output should not contain any invalid or missing ranges.",
          "The output should have a consistent order and no duplicates.",
          "All numbers in the input list should be present in the output.",
          "No single value should be out of range (i.e., less than 1 or greater than 15).",
          "No range should start before its end (e.g., '1-2' but not '2-1')."
        ],
        "scenario": "Test compressing mixed ranges.",
        "why_needed": "Prevents regression in cases where the input contains a mix of single values and range values."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007732130000022153,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "29, 33, 35-37, 39, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007555909999723553,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts that the output of `compress_ranges([5])` is equal to '5'.",
          "checks if the compressed value matches the expected result.",
          "verifies that the input list has only one element.",
          "ensures that the function does not use range notation for a single-element list.",
          "tests the case where the input list contains multiple elements, but the function still compresses it correctly.",
          "checks if the compressed value is correct even when the input list contains duplicate values.",
          " verifies that the function handles empty lists correctly."
        ],
        "scenario": "The 'single_line' test verifies that a single-line input does not utilize the range notation.",
        "why_needed": "This test prevents regression where the function compresses only one element in a list."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0007415840000248863,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert '1-2' in str(compress_ranges([1, 2]))",
          "assert '1,2' not in str(compress_ranges([1, 2]))",
          "assert '2-3' not in str(compress_ranges([1, 2])))"
        ],
        "scenario": "The function `compress_ranges` is expected to correctly handle two consecutive line numbers when compressing ranges.",
        "why_needed": "This test prevents a potential bug where the function incorrectly handles consecutive line numbers as separate ranges."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0007878400000436159,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts that the output is correct for input `[5, 1, 3, 2]`",
          "the output should be `1-3, 5`",
          "the list elements are sorted in ascending order",
          "the function groups numbers by their actual ranges correctly"
        ],
        "scenario": "The test verifies that the `compress_ranges` function correctly handles an unsorted list of integers.",
        "why_needed": "This test prevents a potential bug where the function would incorrectly group numbers in ascending order instead of their actual ranges."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "81-82"
        }
      ],
      "duration": 0.0007732330000180809,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert expand_ranges('') == []",
          "assert expand_ranges([]) == []",
          "assert expand_ranges(['']) == []"
        ],
        "scenario": "The test verifies that an empty string expands to an empty list.",
        "why_needed": "This test prevents a potential bug where an empty string is not expanded correctly."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 11,
          "line_ranges": "81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007697570000004816,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should split the comma-separated ranges into individual numbers correctly.",
          "It should handle single values without splitting them.",
          "It should ignore invalid or missing ranges.",
          "The resulting list should contain all specified numbers in the correct order.",
          "It should preserve the original order of non-range values (e.g., '5' and '10-12').",
          "It should raise an error for invalid input (e.g., '1, 3, abc') to prevent unexpected behavior."
        ],
        "scenario": "The test verifies that the `expand_ranges` function correctly handles a mixed set of ranges and singles.",
        "why_needed": "This test prevents regression in cases where the input contains both range notation (e.g., '1-3') and non-range values (e.g., '10')."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_mixed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "81, 84-91, 95"
        }
      ],
      "duration": 0.0007692260000453643,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The input string should be in the format 'start-end' (e.g., '1-3')",
          "The start value should be less than or equal to the end value",
          "The function should return a list of numbers between the start and end values"
        ],
        "scenario": "The 'expand_ranges' function is expected to correctly expand a range of numbers.",
        "why_needed": "This test prevents the function from expanding the range incorrectly, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_range",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 27,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007928499999820815,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The compressed list should be identical to the original list.",
          "The expanded list should contain all elements from the original list.",
          "No duplicate elements should be present in the expanded list.",
          "All numbers in the original list should be present in the expanded list.",
          "The order of elements in the original and expanded lists should be preserved.",
          "The compressed list should not have any additional elements compared to the original list."
        ],
        "scenario": "The test verifies that the `compress_ranges` and `expand_ranges` functions produce the same output when given a list of numbers.",
        "why_needed": "This test prevents regression in cases where the input list is modified or reordered during compression or expansion."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 7,
          "line_ranges": "81, 84-87, 93, 95"
        }
      ],
      "duration": 0.0007957850000366307,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "the input string '5' should be converted into a list containing only one element, which is 5 itself.",
          "the expected output of `expand_ranges('5')` should be `[5]` exactly.",
          "any other input strings with multiple elements should not produce lists with more than one element.",
          "for inputs like 'abc', the function should return an empty list or raise a meaningful error.",
          "for inputs like '1,2,3', the function should return a list containing only 1, 2 and 3.",
          "the function should handle edge cases where the input string is empty.",
          "any non-numeric characters in the input string should be ignored."
        ],
        "scenario": "The function `expand_ranges` is expected to handle a single input '5' correctly.",
        "why_needed": "This test prevents a potential bug where the function would incorrectly return a list with multiple elements for a single number."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_single_number",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65, 67"
        }
      ],
      "duration": 0.0007562819999975545,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `format_duration(0.5)` should return '500ms'.",
          "The function `format_duration(0.001)` should return '1ms'.",
          "The function `format_duration(0.0)` should return '0ms'."
        ],
        "scenario": "Test the formatting of duration values less than 1 second.",
        "why_needed": "Prevents regression in handling durations under 1s, where the expected output may not be accurate."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65-66"
        }
      ],
      "duration": 0.0007819690000019364,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Expected format_duration(1.23) to return '1.23s'\"}",
          "{'message': \"Expected format_duration(60.0) to return '60.00s'\"}",
          "{'message': 'Function should handle durations greater than or equal to 1 second correctly'}"
        ],
        "scenario": "Test that the function formats duration correctly for seconds.",
        "why_needed": "Prevents a potential bug where the function does not handle durations greater than or equal to 1 second correctly."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.000780226000017592,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "outcome_to_css_class('passed') == 'outcome-passed'",
          "outcome_to_css_class('failed') == 'outcome-failed'",
          "outcome_to_css_class('skipped') == 'outcome-skipped'",
          "outcome_to_css_class('xfailed') == 'outcome-xfailed'",
          "outcome_to_css_class('xpassed') == 'outcome-xpassed'",
          "outcome_to_css_class('error') == 'outcome-error'"
        ],
        "scenario": "All outcomes should map to CSS classes.",
        "why_needed": "This test prevents regression when the outcome is 'xfailed' as it would incorrectly map it to 'outcome-xfailed'."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0008723180000060893,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `outcome_to_css_class` function should return 'outcome-unknown' when given an unknown outcome.",
          "The `outcome_to_css_class` function should not raise an exception when given an unknown outcome.",
          "The `outcome_to_css_class` function should correctly handle the case where the outcome is not recognized."
        ],
        "scenario": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
        "why_needed": "This test prevents a regression where the `outcome_to_css_class` function returns an incorrect default class for unknown outcomes."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 52,
          "line_ranges": "65-67, 79-85, 87, 121-124, 126-127, 131-132, 141-143, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0008691419999991012,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 52,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-129, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0008058639999717343,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'src/foo.py' file should be included in the rendered HTML.",
          "The number of lines covered should be reported correctly (in this case, 5 lines)."
        ],
        "scenario": "Test verifies that the test renders coverage information.",
        "why_needed": "This test prevents a regression where the coverage report is missing or incomplete."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 54,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0008260419999714941,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "LLMAnnotation contains the string 'Tests login flow' in its text.",
          "LLMAnnotation contains the string 'Prevents auth bypass' in its text.",
          "The LLMAnnotation is present in the rendered HTML report.",
          "The presence of 'Tests login flow' and 'Prevents auth bypass' in the LLMAnnotation's text indicates that authentication was successful.",
          "The inclusion of 'Tests login flow' and 'Prevents auth bypass' in the LLMAnnotation suggests that the test ran without any issues.",
          "The rendered HTML report contains both 'Tests login flow' and 'Prevents auth bypass' as expected.",
          "The presence of these strings in the LLMAnnotation indicates a successful authentication process."
        ],
        "scenario": "The test verifies that the LLM annotation is included in the rendered HTML report.",
        "why_needed": "This test prevents a potential security vulnerability where an attacker could bypass authentication by manipulating the HTML content of the report."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 63,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-164, 166-172, 177, 192, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0008054229999743256,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Source Coverage' section should be present in the rendered HTML.",
          "The source code path 'src/foo.py' should be included in the HTML output.",
          "The percentage of covered statements should be displayed as '80.0%' in the HTML output."
        ],
        "scenario": "Test renders source coverage to ensure it is included in the HTML output.",
        "why_needed": "This test prevents a potential regression where the source coverage summary might not be displayed correctly if there are missing or unreported tests."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 50,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 141-142, 145-153, 158-160, 196, 229-236, 239-245, 248-249"
        }
      ],
      "duration": 0.0008035100000256534,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'XFailed' string is present in the rendered HTML.",
          "The 'XPassed' string is present in the rendered HTML.",
          "Both 'XFailed' and 'XPassed' strings are included in the rendered HTML."
        ],
        "scenario": "Tests the rendering of xfailed and xpassed summary entries.",
        "why_needed": "This test prevents regression where a user submits multiple tests with different outcomes (xfailed and xpass) without seeing their respective counts in the summary."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007926099999622238,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "hash1 should be different from hash2",
          "hash1 is not equal to hash2 using == operator",
          "hash1 is not equal to hash2 using != operator",
          "hash1 is not equal to hash2 using != operator (Python's built-in comparison)",
          "hash1 is not equal to hash2 when comparing bytes objects directly",
          "hash1 and hash2 should have different hexadecimal representations"
        ],
        "scenario": "Test verifies that different content produces different hashes.",
        "why_needed": "This test prevents a potential bug where the same input could produce the same output (different content) and potentially cause issues with reporting or data integrity."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007888119999961418,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The two computed hashes should be equal (i.e., they should have the same value).",
          "The length of each computed hash should be exactly 64 characters (i.e., it should match the SHA256 hex length)."
        ],
        "scenario": "Test 'Empty bytes should produce consistent hash' verifies that an empty byte string produces a hash with the same value and correct length.",
        "why_needed": "This test prevents a potential bug where an empty byte string is expected to produce a different or incorrect hash."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 67,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300"
        }
      ],
      "duration": 0.005289453000045796,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The duration of the run should be 60 seconds.",
          "The pytest version should have a value.",
          "The plugin version should be '0.1.0'.",
          "The python version should also match the one used to create the ReportWriter instance."
        ],
        "scenario": "Test that the build_run_meta method includes version info and other expected metadata.",
        "why_needed": "This test prevents regression where the ReportWriter does not include version information in the run meta."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_run_meta",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 19,
          "line_ranges": "156-158, 312, 314-315, 317-328, 330"
        }
      ],
      "duration": 0.0008101020000026438,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "total == 6",
          "passed == 1",
          "failed == 1",
          "skipped == 1",
          "xfailed == 1",
          "xpassed == 1",
          "error == 1"
        ],
        "scenario": "Test verifies that the `build_summary` method counts all outcome types correctly.",
        "why_needed": "This test prevents a potential regression where the summary does not include all outcome types, leading to incorrect reporting."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 312, 314-315, 317-322, 330"
        }
      ],
      "duration": 0.0008131370000228344,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert summary.total == 4",
          "assert summary.passed == 2",
          "assert summary.failed == 1",
          "assert summary.skipped == 1"
        ],
        "scenario": "The test verifies that the `build_summary` method correctly counts outcomes in a report.",
        "why_needed": "This test prevents a bug where the total count of outcomes is not accurate due to incorrect handling of skipped tests."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_counts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "156-158"
        }
      ],
      "duration": 0.000797438000006423,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` attribute of the `ReportWriter` instance should be set to the provided `Config` object.",
          "The `warnings` list of the `ReportWriter` instance should be empty.",
          "The `artifacts` dictionary of the `ReportWriter` instance should be empty."
        ],
        "scenario": "Test that a new ReportWriter instance is created with the provided configuration.",
        "why_needed": "This test prevents a potential bug where the writer's configuration and warnings/artifacts are not properly initialized."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_create_writer",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 93,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330"
        }
      ],
      "duration": 0.005311870000014096,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of tests in the report should be equal to 2.",
          "The total number of tests in the summary should also be 2.",
          "All tests should have been written to the specified output path(s)."
        ],
        "scenario": "Test writes a report that includes all tests, but does not handle cases where output paths are specified.",
        "why_needed": "This test prevents regression by ensuring that the report writer can write reports even if no output paths are provided."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 93,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330"
        }
      ],
      "duration": 0.006153246999986095,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `coverage_total_percent` attribute of the report summary should match the provided `coverage_percent` value.",
          "The `report.write_report()` method should return an instance with a `summary` attribute that has a `coverage_total_percent` attribute matching the provided value.",
          "The `report.summary.coverage_total_percent` attribute should be equal to the provided `coverage_percent` value after writing the report."
        ],
        "scenario": "The test verifies that the `ReportWriter` class writes a report with a total coverage percentage that matches the provided value.",
        "why_needed": "This test prevents a regression where the coverage percentage is not accurately reported in reports."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 92,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330"
        }
      ],
      "duration": 0.00518242300000793,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `report.source_coverage` is 1.",
          "The file path of `report.source_coverage[0]` is `src/foo.py`.",
          "All statements in `source_coverage` are covered by the report.",
          "At least one statement in `source_coverage` is missed by the report.",
          "The coverage percentage of `source_coverage` is 87.5%.",
          "The ranges where code is missed (`missed_ranges`) are `src/foo.py:5`.",
          "All statements in `source_coverage` are covered by the report.",
          "At least one statement in `source_coverage` is missed by the report.",
          "The coverage percentage of `source_coverage` is 87.5%.",
          "The ranges where code is missed (`missed_ranges`) are `src/foo.py:1-4, src/foo.py:6-7`."
        ],
        "scenario": "Test ReportWriter::test_write_report_includes_source_coverage verifies that the report includes source coverage summary.",
        "why_needed": "This test prevents a regression where the report does not include source coverage information, potentially misleading users about the code's quality."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 94,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330"
        }
      ],
      "duration": 0.005229180000014821,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of coverage lines in the first test should be 1.",
          "The file path of the first coverage line should match 'src/foo.py'.",
          "All other tests' coverage lines should have a file path that matches their test name."
        ],
        "scenario": "Test Report Writer: test_write_report_merges_coverage verifies that the report writer merges coverage into tests.",
        "why_needed": "This test prevents regression where a merge of coverage does not correctly update the test results."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 67,
          "line_ranges": "235-237, 239, 241, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 125,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506-507, 509-512, 515-516"
        }
      ],
      "duration": 0.0062661759999969036,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file `report.json` should exist at the specified path.",
          "Any warnings with code 'W203' should be present in the `writer.warnings` list."
        ],
        "scenario": "Test that the report writer falls back to direct write if atomic write fails and the 'os.replace' mock is used.",
        "why_needed": "This test prevents a regression where an atomic write operation fails, causing the report writer to fall back to direct write instead of using the atomic write mechanism."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 86,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 235-237, 239, 241, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516-518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-477, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.006325636000042323,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output directory should be created at the specified path.",
          "The file `report.json` should be present in the output directory.",
          "If the output directory does not exist, the `ReportWriter` should attempt to create it.",
          "If the report is written successfully, the output directory should contain the expected files.",
          "The test should fail if the output directory already exists and contains the expected files.",
          "The test should pass if the output directory is created correctly and the file `report.json` is present."
        ],
        "scenario": "Test verifies that the `ReportWriter` creates an output directory if it doesn't exist.",
        "why_needed": "This test prevents a regression where the report writer fails to create the output directory when it is missing."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 12,
          "line_ranges": "156-158, 470-473, 480-484"
        }
      ],
      "duration": 0.0012473880000243298,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer._ensure_dir(json_path)` method should raise an OSError with code 'W201' when the directory creation fails.",
          "Any warnings raised by the `writer.warnings` list should have a code of 'W201'.",
          "The test should fail if no warnings are raised in the case where the directory creation fails."
        ],
        "scenario": "Test ensures that directory creation fails and reports warnings when permission is denied.",
        "why_needed": "This test prevents a potential bug where the report writer does not handle directory creation failures correctly, potentially leading to silent errors or unexpected behavior."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.002283491000014237,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.034233774000028916,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report.html file should exist.",
          "The report.html file should contain the expected content.",
          "The report.html file should include all the nodes from the tests.",
          "The report.html file should display 'test1' and 'test2' in the 'Passed' section.",
          "The report.html file should display 'FAILED' and 'Skipped' in the 'Failed' section.",
          "The report.html file should display 'XFailed' and 'XPassed' in the 'Errors' section.",
          "The report.html file should contain a header with the expected content."
        ],
        "scenario": "Test verifies that the report writer creates an HTML file.",
        "why_needed": "This test prevents a regression where the report writer fails to create an HTML file for reports with multiple tests."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-326, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.03379926399998112,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'XFAILED' string should be present in the HTML summary.",
          "The 'XFailed' string should also be present in the HTML summary.",
          "The 'XPASSED' string should be present in the HTML summary.",
          "The 'XPassed' string should also be present in the HTML summary."
        ],
        "scenario": "The test verifies that the report writer includes xfail outcomes in the HTML summary.",
        "why_needed": "This test prevents a regression where the xfail summary is not included in the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 117,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.006043480000016643,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The JSON file should be created at the specified path.",
          "The report should have an artifact tracked.",
          "At least one artifact should be present in the artifacts list.",
          "The number of artifacts should be greater than or equal to 1.",
          "The JSON file should exist at the specified path.",
          "The ReportWriter instance should successfully write a report with the given tests."
        ],
        "scenario": "Test verifies that a JSON file is created with the report.",
        "why_needed": "This test prevents regression where the report writer does not create a JSON file."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 125,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401, 410, 412, 414-423, 434-435, 437-443, 448, 453, 455, 458-462, 470-471"
        }
      ],
      "duration": 0.03820060699996475,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `write_pdf` method should write the contents of `page.pdf` to `pdf_path`.",
          "Any artifacts created by the report writer should have a path that matches `pdf_path`.",
          "All artifacts should be present in the `writer.artifacts` list.",
          "If `report_pdf` is set to a file path, it should be created as a PDF file.",
          "The `write_pdf` method should not raise an exception if Playwright is not available.",
          "The `write_pdf` method should use the correct file extension for the PDF file (`.pdf`).",
          "If `report_pdf` is set to a relative path, it should be resolved correctly to the desired location."
        ],
        "scenario": "Test verifies that the `write_pdf` method creates a PDF file when Playwright is available.",
        "why_needed": "This test prevents regression where the `write_pdf` method does not create a PDF file even if Playwright is available."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 401-405, 408"
        }
      ],
      "duration": 0.005699206999963735,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'report.pdf' should not exist.",
          "Any warning with code W204_PDF_PLAYWRIGHT_MISSING should be present in the list of warnings."
        ],
        "scenario": "Test should warn when Playwright is missing for PDF output.",
        "why_needed": "To prevent a silent failure where the report is generated without any warnings or errors."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 11,
          "line_ranges": "156-158, 470-477"
        }
      ],
      "duration": 0.0010173899999585956,
      "file_path": "tests/test_report_writer_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `tmp_dir` directory should exist before creating any warnings in the report.",
          "Any warning code should be 'W202'.",
          "The `writer.warnings` list should contain at least one warning with code 'W202'."
        ],
        "scenario": "Verifies that the report writer creates a directory with an error message when it already exists.",
        "why_needed": "Prevents a potential bug where the report writer does not create a new directory even if the existing one is empty."
      },
      "nodeid": "tests/test_report_writer_coverage_v2.py::test_report_writer_ensure_dir_creation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 36,
          "line_ranges": "370-386, 388-399, 401, 403, 405, 407, 409, 413, 425"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 163"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 67,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300"
        }
      ],
      "duration": 0.009507665999990422,
      "file_path": "tests/test_report_writer_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'start_time' key should be present in the metadata dictionary.",
          "The 'llm_model' key should not be present in the metadata dictionary when reports are disabled.",
          "The report_writer_metadata_skips function should raise an AssertionError with a meaningful error message when reports are disabled."
        ],
        "scenario": "Verify that the report_writer_metadata_skips test prevents a bug where metadata is skipped when reports are disabled.",
        "why_needed": "This test ensures that the report_writer_metadata_skips function behaves correctly when reports are disabled, preventing metadata from being skipped."
      },
      "nodeid": "tests/test_report_writer_coverage_v2.py::test_report_writer_metadata_skips",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.000854554999989432,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert schema.scenario == 'Verify login'",
          "assert schema.why_needed == 'Catch auth bugs'",
          "assert schema.key_assertions == ['assert 200', 'assert token']",
          "assert schema.confidence == 0.95"
        ],
        "scenario": "Test that `AnnotationSchema.from_dict` creates a valid annotation from a dictionary with all required fields.",
        "why_needed": "Prevents regression in cases where the input data does not contain all necessary fields for an annotation."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 8,
          "line_ranges": "90-92, 94-98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008068770000022596,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert data['scenario'] == 'Verify login'",
          "assert data['why_needed'] == 'Catch auth bugs'",
          "assert data['key_assertions'] == ['assert 200', 'assert token']",
          "assert data['confidence'] == 0.95"
        ],
        "scenario": "Should convert to dictionary with all fields.",
        "why_needed": "Prevent regression in authentication logic."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319-321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 101,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.08821091000004344,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file \"report.html\" exists in the specified path.",
          "The content of the \"report.html\" file contains the string '<html', which is expected to be present.",
          "The content of the \"report.html\" file contains the string 'test_simple', which is also expected to be present."
        ],
        "scenario": "Test that an HTML report is created when running the test with --llm-report.",
        "why_needed": "This test prevents a regression where the report generation fails without providing any error message."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 65,
          "line_ranges": "78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319-321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 111,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-328, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.1247799329999566,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Total Tests' label should be included in the summary.",
          "The 'Passed' label should have a count of 1.",
          "The 'Failed' label should have a count of 1.",
          "The 'Skipped' label should have a count of 1.",
          "The 'XFailed' and 'XPassed' labels should both have counts of 1.",
          "The 'Errors' and 'Error' labels should both have counts of 1."
        ],
        "scenario": "test_html_summary_counts_all_statuses verifies that HTML summary counts include all statuses.",
        "why_needed": "This test prevents regression where the report does not include all statuses, such as when there are no tests or only failed tests."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 51,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 107,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-320, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.07511227199995574,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "A JSON report is created at the specified path.",
          "The report exists in the specified location.",
          "The schema version of the report is correct (1.0).",
          "The summary statistics are accurate: total=2, passed=1, failed=1."
        ],
        "scenario": "The JSON report is created and its existence is verified.",
        "why_needed": "This test prevents a regression where the pytester does not create a JSON report even when tests are run with --llm-report-json flag."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 69,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137, 139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198, 203"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 39,
          "line_ranges": "52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 186-187, 190-191, 194-195, 198-200, 203, 205, 207, 212, 214-218, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 164-168, 170-171, 175, 179-180, 183, 185-186, 188, 197, 205-206, 208"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 96,
          "line_ranges": "104-107, 109-111, 113, 115, 162, 166-171, 173, 175, 177, 179, 182, 184-186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413-425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 59,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 211,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236-238, 240-241, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-366, 369, 371, 374-378, 381, 383-388, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.06239825199997995,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The scenario 'Checks the happy path' is present in the report.",
          "The reason 'Prevents regressions' is true.",
          "The key assertions 'asserts True' are present in the LLM annotation."
        ],
        "scenario": "Verify that LLM annotations are included in the report for a provider enabled.",
        "why_needed": "Prevent regressions by ensuring LLM annotations are present in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 12,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 73,
          "line_ranges": "45, 48-49, 56-57, 59, 61, 64, 66-68, 71-72, 74-78, 87-92, 97-98, 100, 102, 104, 115-122, 129-135, 137-139, 165-168, 170-171, 173-174, 176, 178, 180, 185-190, 192-195, 198-201, 203"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 21,
          "line_ranges": "52-53, 72, 75, 80, 107, 110-111, 128, 136, 147, 165, 167, 175, 245, 247, 249, 252, 257-258, 260"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 80-81, 87, 89, 92, 94-95, 98, 100-101, 106, 108, 110, 114, 129, 131, 164-168, 170-171, 175, 179-180, 183, 205-206, 208"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 59,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-287, 289, 291-293, 298, 305-308, 310-313, 319-321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 211,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236-238, 240-241, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-366, 369, 371, 374-379, 383-388, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 101,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.0878318719999811,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts the presence of 'LLM error' and 'boom' in the report content.",
          "Verifies that the error message contains the word 'boom'",
          "Checks for the correct file path to the report output.",
          "Ensures that the LLM error is reported correctly even with a custom model.",
          "Verifies that the error message includes the expected error type and message.",
          "Checks if the content of the report matches the expected format.",
          "Ensures that the test can reproduce the issue even when running multiple tests in parallel."
        ],
        "scenario": "Verify that LLM errors are surfaced in HTML output.",
        "why_needed": "Prevents regression where LLM errors are not reported correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186-188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.060235558000044875,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the LLM opt-out marker is recorded correctly.",
          "The test ensures that the LLM opt-out marker is set to False for all tests.",
          "The test checks if the LLM opt-out marker is properly recorded in the report.json file.",
          "The test asserts that there is only one test with the LLM opt-out marker in the report.json file.",
          "The test verifies that the LLM opt-out marker is correctly set to False for all tests.",
          "The test checks if the LLM opt-out marker is not set to True for any test.",
          "The test ensures that the LLM opt-out marker does not interfere with other markers in the report.json file."
        ],
        "scenario": "Test the LLM opt-out marker functionality.",
        "why_needed": "Prevents regression in LLM opt-out marker recording."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173, 175, 177, 179, 182, 184, 186, 188, 190, 192, 194-196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05798737899999651,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `@pytest.mark.requirement` annotation should record the specified requirements.",
          "The `requirement` attribute of the test function should contain the required values.",
          "The `REQ-001` and `REQ-002` strings should be present in the `requirements` list of the test function.",
          "The `REQ-001` string should be found in the `requirements` list of the first test function.",
          "The `REQ-002` string should be found in the `requirements` list of the first test function.",
          "The `pytester.makepyfile()` function should create a file with the specified requirements.",
          "The `pytester.runpytest()` function should run the tests and generate a report.",
          "The generated report should contain the correct number of tests (1 in this case).",
          "The JSON file containing the test report should have the expected structure."
        ],
        "scenario": "Verify that the requirement marker is recorded and correctly identified.",
        "why_needed": "This test prevents a potential regression where the requirement marker might not be properly recorded or identified, potentially leading to incorrect reporting of tests."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 108,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.06620874399999366,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "2",
          "['xfailed', 'xfailed']"
        ],
        "scenario": "The test verifies that multiple xfailed tests are recorded in the report.",
        "why_needed": "This test prevents regression by ensuring that all xfailed tests are properly reported and counted."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 43,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 107,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321-322, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.05938161200003833,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the report JSON should contain a count of skipped tests.",
          "The value of the 'skipped' key should be equal to 1.",
          "The 'skip' marker should not appear in the report."
        ],
        "scenario": "Test that skipping tests prevents the 'skip' marker from appearing in the report.",
        "why_needed": "This test ensures that skipping tests is properly recorded and reported, preventing false positives."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173-175, 177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 108,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317, 319, 321, 323-324, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.06164798200001087,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "162, 166-171, 173, 175-177, 179, 182, 184, 186, 188, 190, 192, 194, 196, 370-386, 388, 391, 393, 396-399, 401, 403, 405, 407, 409, 413, 425, 455-463, 465, 467, 506, 508-512, 514, 516, 518, 520, 522, 524, 526, 528"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321-323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 105,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 340, 343-345, 348-349, 352-354, 357, 360-364, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.06268135099998062,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of successful parametrized tests is 3.",
          "All parametrized tests passed successfully.",
          "Each parametrized test was run only once.",
          "No duplicate test runs were performed.",
          "No test failures occurred during the run.",
          "No unexpected errors occurred during the run.",
          "The report generated by pytester contains accurate information about the tests."
        ],
        "scenario": "Test the parameterized tests feature.",
        "why_needed": "This test prevents a regression that could occur when using the --llm-report-json flag with pytester."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 57,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 143,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.0723400680000168,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 57,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 143,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.04612093100001857,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' marker should be found in the stdout of the pytest run.",
          "The 'llm_context' marker should be found in the stdout of the pytest run.",
          "The 'requirement' marker should be found in the stdout of the pytest run.",
          "The 'llm_opt_out' marker should match the expected output line.",
          "The 'llm_context' marker should match the expected output line.",
          "The 'requirement' marker should match the expected output line.",
          "All three markers should have different line numbers in their stdout output.",
          "The 'llm_opt_out' marker should not be present in the stdout of a test that does not use this marker.",
          "The 'llm_context' marker should not be present in the stdout of a test that does not use this marker.",
          "The 'requirement' marker should not be present in the stdout of a test that does not use this marker."
        ],
        "scenario": "Test that LLM markers are registered and correctly displayed in the pytest output.",
        "why_needed": "This test prevents a bug where LLM markers are not correctly registered or displayed in the pytest output, potentially leading to false positives or incorrect marker usage."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 57,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319, 321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 143,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 413-414, 417, 421-423"
        }
      ],
      "duration": 0.05287491699999691,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `--llm-report` flag should be present in the output of `pytester.runpytest('--help')`.",
          "The `--llm-report` flag should match the expected output for a successful plugin registration.",
          "The test should fail if the `--llm-report` flag is not present or does not match the expected output.",
          "The test should pass if the `--llm-report` flag is correctly configured and matches the expected output."
        ],
        "scenario": "Verify that the plugin is registered correctly by running pytest with --help flag.",
        "why_needed": "This test prevents a potential issue where the plugin is not registered due to an incorrect or missing configuration."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "139-142"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 58,
          "line_ranges": "123, 163, 191, 194-195, 201-202, 209-210, 217-218, 225-226, 233, 237, 239, 241, 243, 245, 248, 252, 276, 279-281, 283-287, 289, 291-293, 298, 305-308, 310-313, 319-321, 323, 325, 327, 329, 331, 335, 337, 339, 341, 343, 347, 349"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 191,
          "line_ranges": "40, 43-47, 49-53, 55-59, 61-65, 67-71, 73-78, 80-85, 89-93, 95-99, 101-105, 107-111, 113-117, 121-124, 126-129, 131-134, 136-140, 142-145, 147-151, 153-156, 160-163, 165-168, 170-173, 175-179, 181-184, 186-189, 202-204, 206-208, 210-212, 216, 220-221, 223, 225, 228-229, 236, 245-246, 271, 275, 279, 282, 301-302, 309-310, 313-314, 316-317, 320-324, 326, 329-330, 332, 335-336, 357, 363-364, 391-401, 413-414, 417, 421-423, 434, 438, 457, 461-463, 474, 478, 481, 483-484"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 101,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 312, 314-315, 317-318, 330, 376, 378-379, 382, 385, 388, 391-395, 470-471, 495, 497, 499-501, 503, 506"
        }
      ],
      "duration": 0.08810854899996912,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The string \"hello<world>\" should be parsed as a single string.",
          "The string \"foo&bar\" should be parsed as a single string.",
          "The HTML file should contain the '<html>' tag.",
          "The report path should exist and contain the valid HTML content.",
          "The report path should not crash when run with pytester.",
          "The generated HTML should have a valid structure."
        ],
        "scenario": "Verify that special characters in nodeid are handled correctly by pytester.",
        "why_needed": "This test prevents a potential crash and ensures the HTML generated is valid."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007870790000197303,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output should be '1m 0.0s'.",
          "The number of decimal places should be zero.",
          "The format string should include a unit ('m') and an optional sign ('+' or '-').",
          "The duration value should not exceed one minute (60 seconds)."
        ],
        "scenario": "Tests the 'format_duration' function with a boundary of exactly one minute.",
        "why_needed": "This test prevents regression in case the input duration is greater than or equal to one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0008207110000171269,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `format_duration(0.0005)` should return '500\u03bcs' when given an input of 0.0005 seconds.",
          "The string representation of the output should contain exactly '500\u03bcs'.",
          "The unit '\u03bcs' should be present in the output string.",
          "The function `format_duration()` is correctly called with a non-zero argument (0.0005)."
        ],
        "scenario": "Test formats sub-millisecond durations as microseconds.",
        "why_needed": "Prevents a potential bug where the test fails due to incorrect formatting of microsecond durations."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007728319999955602,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `format_duration(0.5)` is expected to contain 'ms' in its string representation.",
          "The formatted string should be equal to '500.0ms'.",
          "The function should handle sub-second durations (e.g., 0.25, 0.75) correctly and format them as milliseconds."
        ],
        "scenario": "Tests the `format_duration` function to ensure it correctly formats sub-second durations as milliseconds.",
        "why_needed": "This test prevents a potential bug where the function does not format the duration correctly for millisecond precision."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007576450000215118,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result contains the string 'm' which indicates that the duration is in minutes.",
          "The result equals the expected string '1m 30.5s' which represents the correct format for a duration over a minute."
        ],
        "scenario": "Tests the 'minutes_format' function to ensure it correctly formats durations over a minute.",
        "why_needed": "This test prevents a potential regression where the duration is misinterpreted as being in seconds instead of minutes."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_minutes_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0008174050000206989,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `format_duration(185.0)` correctly formats the input as '3m 5.0s'.",
          "The result does not exceed the maximum allowed length for a duration string (100 characters).",
          "The function handles cases where the input is less than or equal to 1 minute.",
          "The function handles cases where the input is greater than or equal to 60 minutes.",
          "The function correctly handles decimal values in minutes.",
          "The function preserves the original order of digits for both minutes and seconds.",
          "The function does not introduce any new formatting rules that would break existing tests."
        ],
        "scenario": "Test formats multiple minutes to include both minutes and seconds.",
        "why_needed": "Prevents regression in formatting of duration with multiple minutes."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007697769999595039,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result should be '1.00s' when the input is 1.0.",
          "The result should not include any trailing zeros.",
          "The result should have a decimal point immediately after the number.",
          "The result should only contain two digits after the decimal point.",
          "No leading zeros are allowed in the result.",
          "No negative numbers are allowed as inputs.",
          "The function should handle cases where the input is 0.0 correctly."
        ],
        "scenario": "Verifies the correct formatting of a duration equal to one second.",
        "why_needed": "To ensure that the `format_duration` function returns the expected string representation for durations exactly equal to one second."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_one_second",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007805970000163143,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result should contain the string 's' to indicate seconds.",
          "The result should be equal to '5.50s' to match the expected output.",
          "The function should handle cases where the input is a fraction of a second correctly."
        ],
        "scenario": "Verify that the function correctly formats seconds under a minute.",
        "why_needed": "This test prevents a potential bug where the function does not format seconds correctly when they are less than one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_seconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007826600000271355,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return the correct value for a duration of 1 millisecond (1.0ms).",
          "The function should handle cases where the input is negative or zero correctly.",
          "The function should not return 'nan' (Not a Number) when the input is infinity.",
          "The function should preserve the original precision of the input value.",
          "The function should raise an error for invalid input types (e.g., non-numeric values)."
        ],
        "scenario": "Tests the `format_duration` function with a duration of 1 millisecond.",
        "why_needed": "This test prevents a potential bug where the function incorrectly returns '1.0ms' for durations less than 1 millisecond."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007470649999845591,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `format_duration(0.000001)` is equal to '1\u03bcs'.",
          "The duration is formatted correctly as microseconds.",
          "The duration is not formatted as nanoseconds or milliseconds."
        ],
        "scenario": "Verifies that the function correctly formats very small durations as microseconds.",
        "why_needed": "This test prevents a potential bug where the function incorrectly formats very small durations as nanoseconds or milliseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0008019470000135698,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of `iso_format(dt)` is '2024-01-15T10:30:45+00:00'.",
          "The input `dt` has a valid UTC timezone.",
          "The function correctly formats the datetime object as per ISO 8601 standard.",
          "The formatted string does not contain any time zone offset information.",
          "No exceptions are raised when passing an invalid datetime object with UTC timezone.",
          "The function handles daylight saving time (DST) correctly by preserving it in the output.",
          "The input `dt` is a valid datetime object that can be used to create an ISO formatted string."
        ],
        "scenario": "Tests the `iso_format` function with a datetime object representing UTC time.",
        "why_needed": "Prevents regression in handling datetime objects with UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007902950000016062,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of `iso_format(dt)` should be '2024-06-20T14:00:00'.",
          "The output does not include any timezone information.",
          "The output is in the correct ISO 8601 format.",
          "The output does not contain any invalid characters or formatting errors."
        ],
        "scenario": "Verify that naive datetime is formatted correctly without timezone.",
        "why_needed": "Prevents a potential bug where the naive datetime format is incorrect due to missing timezone information."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0008161630000245168,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'result' variable should contain the string '123456'.",
          "The 'result' variable should contain the substring '123456' (case-insensitive).",
          "The 'result' variable should be a string containing only digits.",
          "The 'result' variable should not be empty.",
          "The 'result' variable should not contain any non-digit characters.",
          "The 'result' variable should be a valid ISO 8601 format string."
        ],
        "scenario": "Tests the `iso_format` function with a datetime object containing microseconds.",
        "why_needed": "Prevents a bug where microseconds are not included in the formatted ISO string."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_with_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007765089999907104,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007797050000135641,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `utc_now()` returns a datetime object representing the current time in UTC.",
          "The difference between `before` and `result` should be less than or equal to `after - result` (within a tolerance of 1 second).",
          "The difference between `after` and `result` should be greater than `before - result` (less than or equal to 1 second)."
        ],
        "scenario": "Verifies the function returns a current time within a specified tolerance.",
        "why_needed": "This test prevents a potential issue where the function does not return a current time that is within the expected range for UTC."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_is_current_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007974279999984901,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The result of `utc_now()` should be a datetime object.",
          "The result of `utc_now()` should have a valid timezone.",
          "The result of `utc_now()` should have a valid date and time.",
          "The result of `utc_now()` should not raise any exceptions.",
          "The result of `utc_now()` should be a valid datetime object with the correct timezone.",
          "The result of `utc_now()` should have a timezone that is compatible with the system's timezone.",
          "The result of `utc_now()` should not return an empty datetime object."
        ],
        "scenario": "The function `utc_now()` returns a datetime object.",
        "why_needed": "This test prevents a potential issue where the function might return an incorrect datetime object if the system clock is not properly synchronized."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_returns_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008748019999984535,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'refresh_interval' attribute of the TokenRefresher instance is set to 3600 seconds.",
          "The 'output_format' attribute of the TokenRefresher instance is set to 'text'.",
          "When the 'get-token' method is called with an invalid refresh interval (e.g., less than 1 second), it raises a TokenRefreshError exception with an error message indicating authentication failed.",
          "The error message includes the string 'exit 1', which indicates that the command failed and returned a non-zero exit code.",
          "The error message includes the string 'Authentication failed', which is the expected error message for this scenario."
        ],
        "scenario": "Test TokenRefresher raises error on command failure when 'get-token' command is executed with an invalid refresh interval.",
        "why_needed": "This test prevents a potential bug where the TokenRefresher class does not handle cases where the provided refresh interval exceeds the maximum allowed value, leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_command_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-109, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008457679999764878,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_empty_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008664270000053875,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the correct token is returned after calling `get_token()` with `force=True`.",
          "Verify that the second call to `get_token()` returns a different token than the first one.",
          "Verify that the number of calls to `get_token()` increases by 1 when `force=True` is used.",
          "Verify that the output format is set to 'text' as expected.",
          "Verify that the error message is empty as expected.",
          "Verify that the return code is 0 as expected.",
          "Verify that the stdout contains a token with the call count.",
          "Verify that the stderr is empty as expected."
        ],
        "scenario": "Test the 'force_refresh' feature of TokenRefresher.",
        "why_needed": "This test prevents a potential bug where the cache is not updated when the refresh interval is set to a value that does not require a new token."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008561669999949117,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "- The output of the subprocess call contains the expected custom JSON key.",
          "- The 'access_token' field in the response matches the custom JSON key.",
          "- The error message is empty, indicating that no exception was raised during the subprocess call.",
          "- The 'stdout' attribute of the subprocess result object contains the custom JSON key as a string.",
          "- The 'json_key' parameter passed to the TokenRefresher constructor matches the custom JSON key.",
          "- The 'output_format' parameter is set to 'json', which allows for the correct parsing of the custom JSON key."
        ],
        "scenario": "The test verifies that the TokenRefresher function correctly retrieves a custom JSON key for accessing an access token.",
        "why_needed": "This test prevents a bug where the custom JSON key is not properly retrieved from the subprocess call to get-token."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_custom_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008578509999779271,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token` key in the output should contain the expected value 'json-token-value'.",
          "The `expires_in` value should be set to 3600 (1 hour) as specified by the `refresh_interval` parameter.",
          "The JSON response should not include any other keys or values than the required ones.",
          "The `stdout` output of the `fake_run` function should contain a JSON string with the expected token and expiration time.",
          "The `stderr` output of the `fake_run` function should be empty.",
          "The `json.dumps` function should return a valid JSON string containing the specified keys and values.",
          "The `returncode` of the `fake_run` function should be 0, indicating successful execution.",
          "The `output_format` parameter should be set to 'json' as expected."
        ],
        "scenario": "Verify the `TokenRefresher` extracts a JSON token from the expected output.",
        "why_needed": "This test prevents a potential bug where the `get-token` command returns an incorrect or incomplete JSON response, potentially causing the `TokenRefresher` to fail or produce unexpected results."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008461900000042988,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get-token` command contains the string \"my-secret-token\".",
          "The output does not contain any non-text characters (e.g., newline, tab).",
          "The extracted token is in lowercase.",
          "The extracted token has a length greater than 50 characters.",
          "The extracted token starts with 'my-'.",
          "The extracted token contains only alphanumeric characters and underscores.",
          "The extracted token does not contain any whitespace characters (spaces, tabs, etc.)."
        ],
        "scenario": "The test verifies that the `TokenRefresher` class extracts the correct text format from the output of the `get-token` command.",
        "why_needed": "This test prevents a bug where the extracted token is not in the expected text format, potentially leading to incorrect usage or downstream errors."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_text_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-134, 149-150"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008582930000216038,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method of `TokenRefresher` should raise a `TokenRefreshError` with a message indicating that the input is invalid JSON.",
          "The error message should include the word 'json' to ensure it's a valid JSON string.",
          "The test should fail when an invalid JSON string is passed as input, and the error message should be clear about this."
        ],
        "scenario": "Test that TokenRefresher raises an error on invalid JSON input.",
        "why_needed": "This test prevents a potential bug where the TokenRefresher class does not handle invalid JSON inputs correctly."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008792110000399589,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `invalidate()` of `TokenRefresher` should set the `call_count` variable to 2 after calling it with the provided arguments.",
          "The function `get_token()` of `TokenRefresher` should return a token that is different from the one obtained before calling `invalidate()`.",
          "The function `invalidate()` of `TokenRefresher` should clear its cache by setting `call_count` to 0 after calling it with the provided arguments.",
          "The function `get_token()` of `TokenRefresher` should return a token that is different from the one obtained before calling `invalidate()` and has a different value than the previous token.",
          "The function `invalidate()` of `TokenRefresher` should not have any side effects other than updating the cache and token count.",
          "The function `get_token()` of `TokenRefresher` should return a token that is different from the one obtained before calling `invalidate()` and has a different value than the previous token.",
          "The function `invalidate()` of `TokenRefresher` should not modify any external state other than updating the cache and token count."
        ],
        "scenario": "Test that the `invalidate` method of `TokenRefresher` clears its cache and updates the token count correctly.",
        "why_needed": "This test prevents a potential issue where the `invalidate` method does not clear the cache, leading to stale tokens being returned."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalidate",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139-141, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.002924627999959739,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'token' key should be present in the output of the get_token method.",
          "The 'not found' message should be present in the output of the get_token method.",
          "The 'token' key should not be present in the error message."
        ],
        "scenario": "Test that TokenRefresher raises an error when the JSON key is missing.",
        "why_needed": "To prevent a potential bug where the TokenRefresher class does not raise an error when the required JSON key is missing from the token response."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_missing_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.05174396499995737,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should acquire the lock before returning the token.",
          "All threads should get the same token (first one to acquire lock).",
          "The length of the set of results should be equal to 1.",
          "The first element in the list of results should be 'token-1'."
        ],
        "scenario": "Test TokenRefresher thread safety by starting multiple threads concurrently and verifying that all threads get the same token.",
        "why_needed": "This test prevents a potential bug where multiple threads accessing the TokenRefresher instance simultaneously could result in inconsistent or incorrect token acquisition."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_thread_safety",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 16,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0009211199999867858,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function will raise a `TokenRefreshError` with the message 'timed out' when the command takes longer than 30 seconds to complete.",
          "The function will set `exc_info.value` to an instance of `TokenRefreshError` with the message 'timed out'.",
          "The function will assert that the error message contains the substring 'timed out'."
        ],
        "scenario": "The test verifies that the TokenRefresher handles command timeouts correctly by raising a TokenRefreshError when the 'get-token' command takes longer than 30 seconds to complete.",
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not raise an error when it encounters a timeout, potentially causing unexpected behavior or data loss."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_timeout_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "413-414, 417, 421-423"
        }
      ],
      "duration": 0.0008218539999802488,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` class returns the same token for both `token1` and `token2`.",
          "The value of `call_count` is equal to 1 after calling `get_token()` twice.",
          "Both `token1` and `token2` have the same value 'token-1'."
        ],
        "scenario": "Test Token Caching: Verify that the TokenRefresher caches tokens and doesn't call command again.",
        "why_needed": "This test prevents a potential bug where the TokenRefresher calls the command multiple times due to caching, leading to unnecessary computations."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_token_caching",
      "outcome": "passed",
      "phase": "call"
    }
  ]
}