{
  "run_meta": {
    "aggregation_policy": null,
    "collect_only": false,
    "collected_count": 623,
    "deselected_count": 0,
    "duration": 119.85032,
    "end_time": "2026-01-21T04:20:47.409653+00:00",
    "exit_code": 0,
    "git_dirty": false,
    "git_sha": "6ca9d0d9b8119ce18efb8514475229959a5a445b",
    "interrupted": false,
    "is_aggregated": true,
    "llm_annotations_count": 620,
    "llm_annotations_enabled": true,
    "llm_annotations_errors": 2,
    "llm_context_mode": "minimal",
    "llm_model": "llama3.2:1b",
    "llm_provider": "ollama",
    "llm_total_input_tokens": 131832,
    "llm_total_output_tokens": 75520,
    "llm_total_tokens": 207352,
    "platform": "Linux-6.11.0-1018-azure-x86_64-with-glibc2.39",
    "plugin_git_dirty": true,
    "plugin_git_sha": "a03dbe622cdc018f89b74731aed91adf1a582867",
    "plugin_version": "0.2.0",
    "pytest_version": "9.0.2",
    "python_version": "3.12.12",
    "repo_git_dirty": false,
    "repo_git_sha": "6ca9d0d9b8119ce18efb8514475229959a5a445b",
    "repo_version": "0.2.0",
    "rerun_count": 0,
    "run_count": 1,
    "run_id": "21197045261-py3.12",
    "selected_count": 623,
    "source_reports": [],
    "start_time": "2026-01-21T04:18:47.559333+00:00"
  },
  "schema_version": "1.1.0",
  "sha256": "579ab739aa076ddc244ea2847b712d5532828412f7a90cde3cfbdc6978cf2fbf",
  "source_coverage": [
    {
      "coverage_percent": 100.0,
      "covered": 2,
      "covered_ranges": "2-3",
      "file_path": "src/pytest_llm_report/_git_info.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 2
    },
    {
      "coverage_percent": 95.04,
      "covered": 115,
      "covered_ranges": "13, 15-19, 21, 36, 39, 45, 47, 53-54, 56-58, 60, 62-65, 70, 74-75, 78-81, 85, 88-90, 94, 104, 110, 113-115, 117-121, 123-124, 129, 131-132, 134-135, 138-139, 145-147, 149, 152, 155, 158, 160, 162, 176, 178, 182, 184, 186, 196, 198-202, 204-205, 208, 210, 219, 231, 233-247, 249, 251, 259-260, 262-263, 265, 267-269, 273, 276-277, 279-280, 283, 285-286, 288, 290-291, 295",
      "file_path": "src/pytest_llm_report/aggregation.py",
      "missed": 6,
      "missed_ranges": "67, 91-92, 111, 206, 217",
      "statements": 121
    },
    {
      "coverage_percent": 93.62,
      "covered": 44,
      "covered_ranges": "13, 15-19, 21, 27, 33, 39-41, 43, 53, 55-56, 58, 60-62, 68-69, 78, 86, 88, 90, 92, 94, 97, 103, 107, 118-119, 121, 123, 129, 132-136, 141, 144, 153",
      "file_path": "src/pytest_llm_report/cache.py",
      "missed": 3,
      "missed_ranges": "64-65, 130",
      "statements": 47
    },
    {
      "coverage_percent": 99.1,
      "covered": 110,
      "covered_ranges": "19, 21-22, 24, 26-27, 33-34, 45-50, 52, 58, 60-62, 69, 78-79, 81, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127-128, 130, 132-133, 135-137, 140-141, 143, 155, 163-164, 167-169, 171, 173, 181-182, 185-189, 191, 198-200, 202, 209-210, 212-214, 216, 218, 227-228, 230-236, 238, 241, 250-252, 254, 261, 264-265, 268-269, 271, 277, 279, 285",
      "file_path": "src/pytest_llm_report/collector.py",
      "missed": 1,
      "missed_ranges": "239",
      "statements": 111
    },
    {
      "coverage_percent": 94.34,
      "covered": 50,
      "covered_ranges": "13-15, 18, 27, 29-31, 33, 35-36, 38-41, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 72, 81-82, 86, 88-90, 93, 96, 108, 111, 124, 126-127, 129-130, 133, 135",
      "file_path": "src/pytest_llm_report/context_util.py",
      "missed": 3,
      "missed_ranges": "53, 83-84",
      "statements": 53
    },
    {
      "coverage_percent": 95.56,
      "covered": 129,
      "covered_ranges": "13, 15-17, 19-22, 30, 38, 44-45, 47, 58-60, 64, 72-73, 83, 86, 88-90, 92, 94-96, 98, 101-104, 106-108, 114, 116, 118, 121-122, 127-128, 131-135, 137-140, 144-146, 148, 150, 152-153, 156, 160-162, 165, 167-168, 173, 176, 178-184, 187-189, 191, 196, 199-200, 202, 204, 216-217, 220, 224-225, 228-234, 236, 239, 241, 243-244, 246-250, 252-254, 257, 259-260, 263-264, 271, 273-274, 276-279, 281-283, 285, 299-300, 302, 308",
      "file_path": "src/pytest_llm_report/coverage_map.py",
      "missed": 6,
      "missed_ranges": "62, 123, 125, 157, 221, 251",
      "statements": 135
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "8-9, 12, 25-28, 31-36, 39-42, 45-46, 49-51, 54-55, 64-66, 68, 70, 73, 77-79, 83, 132, 142",
      "file_path": "src/pytest_llm_report/errors.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 3,
      "covered_ranges": "4-5, 7",
      "file_path": "src/pytest_llm_report/llm/__init__.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 3
    },
    {
      "coverage_percent": 86.36,
      "covered": 133,
      "covered_ranges": "4, 6-10, 12-15, 21-22, 25-30, 33, 47-48, 50-52, 56, 58-59, 65, 67-68, 70, 73-74, 76, 84, 86-90, 95-96, 98-99, 106-107, 112-113, 116, 121-126, 130, 132, 134, 137, 144, 156, 181-182, 184, 186, 188-189, 199, 211, 213-216, 221-223, 226, 249-252, 254-255, 260, 262, 264-267, 269-270, 277-279, 281, 283-284, 289-290, 292-293, 298-301, 303, 306, 329-332, 334, 336, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381",
      "file_path": "src/pytest_llm_report/llm/annotator.py",
      "missed": 21,
      "missed_ranges": "77-81, 160-168, 173, 286-287, 345, 364-365, 371",
      "statements": 154
    },
    {
      "coverage_percent": 95.42,
      "covered": 125,
      "covered_ranges": "13, 15-18, 20, 30, 33, 47, 50, 53, 59, 65-66, 68, 87-88, 96, 101, 103, 105, 128, 134-135, 137-138, 149, 155, 157, 163, 165, 174, 176, 185-186, 188, 191-198, 200, 202, 212, 214-217, 219-222, 224, 232, 243, 245, 247, 264, 266-267, 270-272, 274-275, 277, 279, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314, 316, 325-326, 329-331, 333-334, 337-339, 342-347, 351, 353, 359-360, 363-364, 367-369, 372, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404, 406",
      "file_path": "src/pytest_llm_report/llm/base.py",
      "missed": 6,
      "missed_ranges": "91-92, 230, 284, 292, 296",
      "statements": 131
    },
    {
      "coverage_percent": 95.56,
      "covered": 86,
      "covered_ranges": "8, 10-13, 20, 23-24, 27-29, 31-32, 34, 36-37, 39, 44, 53-55, 58, 67-68, 70, 73, 92-93, 95, 97, 103-106, 108-110, 112, 122-123, 126-128, 136, 139, 156-157, 160, 162, 164-167, 170-176, 181-185, 187-188, 190, 192-194, 196-197, 203-206, 209-210, 213-214, 216-218, 222, 224",
      "file_path": "src/pytest_llm_report/llm/batching.py",
      "missed": 4,
      "missed_ranges": "158, 207, 211, 220",
      "statements": 90
    },
    {
      "coverage_percent": 97.85,
      "covered": 318,
      "covered_ranges": "7, 9-13, 15-16, 23-27, 30-34, 37-42, 44-46, 48-50, 52, 57-63, 65-70, 72-73, 75-78, 80-85, 87-89, 91-97, 99-114, 121-122, 125, 128, 134-135, 137-141, 143-144, 146, 164-166, 173-175, 178, 181-182, 184, 186-189, 191-192, 198-206, 208-210, 212-213, 215, 218, 221-230, 232-233, 235-237, 239-243, 246-247, 249-252, 254-255, 259, 261, 263, 268, 272-276, 279-281, 283, 288-293, 295, 299-305, 308-309, 311-312, 318-319, 322, 326, 332-333, 335, 339-343, 345-349, 352-353, 358-359, 366-367, 369, 383, 385-386, 390, 410, 413-415, 418-422, 424-427, 432, 434-435, 437, 441-444, 446, 449-463, 469, 471-473, 475-478, 480, 486, 488-491, 493, 495, 497-498, 502-508, 511, 514-516, 518-521, 523-528, 534, 537, 539-543, 547-548, 550-559, 562-564, 567-570, 574",
      "file_path": "src/pytest_llm_report/llm/gemini.py",
      "missed": 7,
      "missed_ranges": "115-117, 298, 310, 313-314",
      "statements": 325
    },
    {
      "coverage_percent": 98.7,
      "covered": 76,
      "covered_ranges": "8, 10, 12-13, 21, 31, 37-38, 41-42, 44, 51, 60-62, 64, 82-83, 89, 92, 95-96, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 122, 124-126, 129-130, 132, 135, 137, 139, 141-142, 144, 148, 170, 182-183, 186-188, 190, 192-193, 196-198, 204, 206, 211, 213, 215, 221-222, 224, 227-231, 234, 236, 242-243, 245",
      "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
      "missed": 1,
      "missed_ranges": "207",
      "statements": 77
    },
    {
      "coverage_percent": 100.0,
      "covered": 13,
      "covered_ranges": "8, 10, 12-13, 20, 26, 32, 34, 51, 53, 59, 61, 67",
      "file_path": "src/pytest_llm_report/llm/noop.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 13
    },
    {
      "coverage_percent": 98.61,
      "covered": 71,
      "covered_ranges": "7, 9, 11-12, 18, 24, 42-43, 49, 52-53, 55, 58, 60-61, 63-67, 70, 74-77, 83, 85-86, 92, 94, 96-98, 100-101, 103, 107, 113-114, 116-118, 122, 128, 130, 138, 140, 142-144, 149-150, 156, 158, 160-162, 165-167, 172-173, 178, 180, 190, 192-193, 204, 209, 211-212",
      "file_path": "src/pytest_llm_report/llm/ollama.py",
      "missed": 1,
      "missed_ranges": "90",
      "statements": 72
    },
    {
      "coverage_percent": 97.22,
      "covered": 35,
      "covered_ranges": "8, 10-12, 16, 22, 38, 42-44, 46-47, 50-53, 55, 58-59, 62-65, 67-68, 77, 84, 90, 94-98, 102, 130",
      "file_path": "src/pytest_llm_report/llm/schemas.py",
      "missed": 1,
      "missed_ranges": "39",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 71,
      "covered_ranges": "7, 9-14, 17, 20, 23-24, 36-39, 41-43, 47, 59-60, 63-66, 69-72, 74, 83, 85-88, 90-91, 93, 101-103, 107-109, 111, 113-116, 120, 132-136, 139-140, 143-145, 148-150, 153-156, 158, 160-162",
      "file_path": "src/pytest_llm_report/llm/token_refresh.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 71
    },
    {
      "coverage_percent": 93.94,
      "covered": 31,
      "covered_ranges": "4, 6, 9, 20, 23, 42-43, 46-47, 51-53, 55-56, 66, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90, 93-94, 96, 98",
      "file_path": "src/pytest_llm_report/llm/utils.py",
      "missed": 2,
      "missed_ranges": "48, 78",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 253,
      "covered_ranges": "17-18, 20, 23, 26-27, 36-38, 40, 42, 49-50, 59-61, 63, 65, 72-73, 86-92, 94, 96, 107-108, 120-126, 128, 130, 135-143, 146-147, 169-185, 187-188, 190, 192, 194, 201-224, 227-228, 236-237, 239, 241, 247-248, 257-259, 261, 263, 270-271, 280-282, 284, 286, 290-292, 295-296, 333-362, 364-372, 374, 376, 394-417, 419-437, 440-441, 455-463, 465, 467, 477-479, 482-483, 500-510, 512, 518, 520, 526-540",
      "file_path": "src/pytest_llm_report/models.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 253
    },
    {
      "coverage_percent": 78.73,
      "covered": 211,
      "covered_ranges": "122, 170, 199, 202-204, 209-211, 217-219, 225-227, 233-235, 241-242, 245-254, 257-259, 265-267, 271-274, 276, 284, 293, 308, 311-312, 320-325, 327, 332-337, 340-345, 348-349, 352-353, 356-357, 360-369, 372-375, 378-393, 396-397, 400-405, 408-409, 412-413, 416-421, 426-427, 430-431, 436-439, 444-447, 449, 451, 453, 460-461, 463-464, 466-467, 470-475, 479, 482-495, 498, 502-503, 507, 510, 514-515, 519-520, 524, 527, 531, 534-536, 540-541, 545-546, 550, 553, 557, 560, 564-565, 569, 572-574, 578, 581-584, 587, 591-592, 596, 599-608, 611, 613",
      "file_path": "src/pytest_llm_report/options.py",
      "missed": 57,
      "missed_ranges": "13-15, 21-22, 98-102, 105-107, 110-115, 118-121, 138-139, 142-149, 152-155, 158-160, 163-166, 169, 180-184, 187-188, 191, 193, 278, 287, 296",
      "statements": 268
    },
    {
      "coverage_percent": 86.81,
      "covered": 158,
      "covered_ranges": "41, 44, 50, 56, 62, 68, 74, 81, 90, 96, 102, 108, 114, 122, 128, 134, 142, 148, 155, 161, 169, 176, 185, 192, 199, 208, 215, 223, 229, 235, 241, 247, 254, 260, 268, 274, 283, 289, 297, 304, 311, 328, 332, 336, 342-343, 346-347, 349, 351, 354-356, 362-363, 371-372, 399-400, 403-404, 407, 410-411, 413-414, 417-418, 420, 422-426, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 476-477, 485-487, 491-494, 497, 499, 502-507, 509, 512-514, 516-521, 523, 534-535, 558-559, 562-563, 566-568, 579-580, 583, 586-587, 590-592, 602-603, 606-608, 619-620, 623, 626, 628-629",
      "file_path": "src/pytest_llm_report/plugin.py",
      "missed": 24,
      "missed_ranges": "13, 15-18, 20-21, 23, 29-32, 35, 319, 377, 481-482, 488, 548-549, 571, 595, 611-612",
      "statements": 182
    },
    {
      "coverage_percent": 97.27,
      "covered": 107,
      "covered_ranges": "13, 15-17, 24, 27, 33, 35, 49, 52, 55, 58-61, 63, 65, 67, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 114, 116, 118, 139-140, 142-144, 147, 152-153, 155-157, 159-161, 163-164, 166-167, 170-171, 173, 177, 180, 189, 192-194, 196-197, 201, 203, 216-217, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249, 251, 268, 275, 284-287",
      "file_path": "src/pytest_llm_report/prompts.py",
      "missed": 3,
      "missed_ranges": "80, 185, 233",
      "statements": 110
    },
    {
      "coverage_percent": 90.77,
      "covered": 59,
      "covered_ranges": "13, 15-16, 18, 24, 30-31, 34, 40, 42, 50-51, 53, 56, 65-67, 70, 79, 87, 90, 99, 101-102, 107, 110, 121-124, 126-129, 131-134, 140-142, 147, 155-157, 159, 172-177, 191, 210-211, 224, 267, 269, 285",
      "file_path": "src/pytest_llm_report/render.py",
      "missed": 6,
      "missed_ranges": "148-149, 212, 217-218, 222",
      "statements": 65
    },
    {
      "coverage_percent": 98.2,
      "covered": 164,
      "covered_ranges": "13, 15-25, 27-29, 46, 55, 58, 67-68, 76, 83-84, 89, 98-100, 102, 105-108, 110, 113, 116, 127-128, 130, 142, 150, 156-158, 160, 186-189, 192, 197-199, 202-203, 211, 222-223, 226-227, 230-231, 233, 235, 254, 256-259, 262-264, 266, 268, 310, 319, 321-322, 324-335, 337, 339, 347, 350-352, 355-356, 359-361, 364, 367, 375, 383, 385-386, 389, 392, 395, 398, 406, 408-409, 415, 417, 419, 421-432, 439, 441-442, 444-446, 454-458, 460, 462, 465, 468-469, 471, 477-481, 487-488, 495, 502, 504, 506-508, 510, 513-514, 516, 522-523",
      "file_path": "src/pytest_llm_report/report_writer.py",
      "missed": 3,
      "missed_ranges": "135-137",
      "statements": 167
    },
    {
      "coverage_percent": 97.06,
      "covered": 33,
      "covered_ranges": "11, 13-14, 17, 30, 33, 36, 39, 42, 45, 55-56, 58-60, 63-65, 67, 70, 79, 82, 100, 103, 111-113, 116-117, 119-121, 123",
      "file_path": "src/pytest_llm_report/util/fs.py",
      "missed": 1,
      "missed_ranges": "40",
      "statements": 34
    },
    {
      "coverage_percent": 100.0,
      "covered": 36,
      "covered_ranges": "12, 14-17, 23, 32, 35, 44-48, 51, 61, 64, 73-74, 76-78, 80-81, 86, 96, 103-104, 107, 113-114, 116-121",
      "file_path": "src/pytest_llm_report/util/hashing.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 36
    },
    {
      "coverage_percent": 100.0,
      "covered": 33,
      "covered_ranges": "12, 15, 29-30, 33, 35-37, 39-40, 42, 45-47, 50, 52, 55, 65-67, 70, 81-82, 84-91, 93, 95",
      "file_path": "src/pytest_llm_report/util/ranges.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 33
    },
    {
      "coverage_percent": 100.0,
      "covered": 16,
      "covered_ranges": "4, 6, 9, 15, 18, 27, 30, 39-44, 46-48",
      "file_path": "src/pytest_llm_report/util/time.py",
      "missed": 0,
      "missed_ranges": "",
      "statements": 16
    }
  ],
  "summary": {
    "coverage_total_percent": 93.04,
    "error": 0,
    "failed": 0,
    "passed": 623,
    "skipped": 0,
    "total": 623,
    "total_duration": 116.45716667899953,
    "xfailed": 0,
    "xpassed": 0
  },
  "tests": [
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008046389999947223,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test is complex, requiring multiple assertions to accurately estimate its complexity."
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 118,
          "total_tokens": 191
        },
        "why_needed": "To ensure that tests with mocks and multiple assertions are correctly scored by the complexity estimator."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_complex_test_high_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 185-186, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008135859999924833,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"assert provider._estimate_test_complexity('') == 0\", 'expected_value': 0, 'message': 'Expected _estimate_test_complexity to return 0 for an empty string'}",
          "{'description': 'assert provider._estimate_test_complexity(None) == 0', 'expected_value': 0, 'message': 'Expected _estimate_test_complexity to return 0 for None'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 136,
          "total_tokens": 299
        },
        "why_needed": "The test is necessary because it checks the behavior of the `Config` class when given an empty source."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_empty_source_zero_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 17,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002163695999996662,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'simple_test', 'description': 'The simple test should have low complexity score.'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 115,
          "total_tokens": 201
        },
        "why_needed": "This test is needed because it checks for simplicity of tests and their complexity scores."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestComplexityEstimation::test_simple_test_low_complexity",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007953709999952707,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Invalid prompt tier', 'expected_value': 'invalid', 'actual_value': 'None'}"
        ],
        "scenario": "Test invalid prompt tier",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 126,
          "total_tokens": 208
        },
        "why_needed": "To ensure that the `prompt_tier` field is validated correctly and raises an error when it's not a valid value."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007932769999996481,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'minimal', 'actual_value': ['minimal', 'standard', 'auto'], 'error_message': 'Invalid prompt tier. Must be one of: minimal, standard, auto'}",
          "{'expected_value': 'standard', 'actual_value': ['minimal', 'standard', 'auto'], 'error_message': 'Invalid prompt tier. Must be one of: minimal, standard, auto'}"
        ],
        "scenario": "Valid prompt tiers",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 142,
          "total_tokens": 283
        },
        "why_needed": "To ensure that the `prompt_tier` field is validated correctly and does not cause any issues."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestConfigValidation::test_valid_prompt_tiers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-220, 222, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007668869999974959,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'standard_prompt_for_complex_tests', 'description': 'The auto-tier should use the standard prompt for complex tests.'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 122,
          "total_tokens": 208
        },
        "why_needed": "Auto mode should use standard prompt for complex tests."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_complex_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 23,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008126240000052576,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'selected_prompt_type', 'expected_value': 'MINIMAL_SYSTEM_PROMPT'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 155,
          "total_tokens": 249
        },
        "why_needed": "To ensure that the auto-tiering system uses minimal prompts for simple tests, which can improve test execution speed and reduce memory usage."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_auto_tier_simple_test",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 212, 214-215, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000779911999998717,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': 'minimal'}"
        ],
        "scenario": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 122,
          "total_tokens": 196
        },
        "why_needed": "Config override to minimal should always use minimal prompt."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_minimal_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 212, 214, 216-217, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007613169999984848,
      "file_path": "tests/test_adaptive_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': 'STANDARD_SYSTEM_PROMPT'}"
        ],
        "scenario": "Config override to standard should always use standard prompt.",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 148,
          "total_tokens": 226
        },
        "why_needed": "To ensure consistent and reliable testing, it is essential to use the standard system prompt for all tests."
      },
      "nodeid": "tests/test_adaptive_prompts.py::TestPromptTierSelection::test_standard_tier_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 71,
          "line_ranges": "53, 56-57, 60, 62-64, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021550409999946396,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregate result should have two tests (both retained).",
          "The aggregate result should contain both report1 and report2.",
          "Both retained tests should be included in the aggregate result.",
          "No test should be removed from the aggregate result when aggregating multiple reports.",
          "All aggregated reports should retain their original order.",
          "The aggregate result should not be None.",
          "The number of tests in the aggregate result should match the number of reports."
        ],
        "scenario": "Test the aggregation function with all policy when aggregating multiple reports.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 364,
          "total_tokens": 514
        },
        "why_needed": "This test prevents regression in case of multiple aggregated reports and ensures that both tests are retained."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_all_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 8,
          "line_ranges": "53, 56-58, 110, 113-115"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0037696269999969445,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected exception to be raised', 'description': 'The `aggregate` method should raise a `FileNotFoundError` when the aggregation directory does not exist.'}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 104,
          "total_tokens": 208
        },
        "why_needed": "To ensure that the `aggregate` method of the Aggregator class raises an exception when the aggregation directory does not exist."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_dir_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 79,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138, 145, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 204-205, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004034713000010015,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that the `aggregate` method correctly picks the latest policy for each report.",
          "It checks that the `tests` attribute of the result is populated with a single test, and its outcome is 'passed'.",
          "The assertion on `result.run_meta.is_aggregated` ensures that the run was aggregated.",
          "The assertions on `result.run_meta.run_count` and `result.summary.passed` verify the correct number of runs and passed tests respectively.",
          "Finally, it checks that only one test is reported in the summary."
        ],
        "scenario": "Test: tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 477,
          "total_tokens": 643
        },
        "why_needed": "Prevents regression when comparing aggregate results across different times."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_latest_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 3,
          "line_ranges": "45, 53-54"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000807053000002611,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'agg is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 110,
          "total_tokens": 187
        },
        "why_needed": "To test that an aggregator function returns None when no directory configuration is provided."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_dir_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "53, 56-58, 110, 113-114, 117-118, 184"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001277845000004163,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate()` method should return `None` when called without any reports.",
          "No reports should be found in the directory.",
          "The absence of reports should prevent further aggregation.",
          "The `aggregate()` method should raise an exception or handle this case differently if it's not None.",
          "The test should fail when calling `aggregate()` with no reports.",
          "A test failure should occur when calling `aggregate()` without any reports."
        ],
        "scenario": "Test that `aggregate` returns None when there are no reports.",
        "token_usage": {
          "completion_tokens": 146,
          "prompt_tokens": 201,
          "total_tokens": 347
        },
        "why_needed": "Prevents a potential bug where the aggregate function does not handle cases with no reports."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_no_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 87,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 134-135, 138-141, 145-147, 149-150, 152-153, 155, 158, 160, 162-167, 169, 171-173, 184, 196, 198-202, 208, 231, 233-237, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 40,
          "line_ranges": "42-45, 65-68, 130-133, 135-137, 139, 141-143, 190, 194-199, 201, 203, 205, 207, 210-214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00237180599999931,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Coverage was correctly deserialized from the JSON report.",
          "LLM annotation was correctly deserialized from the JSON report, including scenario, why needed, key assertions, and confidence.",
          "Token usage was correctly serialized from the JSON report.",
          "LLM token usage could not be re-serialized without the correct configuration."
        ],
        "scenario": "Test that coverage and LLM annotations are properly deserialized and can be re-serialized.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 1002,
          "total_tokens": 1127
        },
        "why_needed": "Prevents regression in core functionality by ensuring accurate token usage and LLM annotation serialization."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_coverage_and_llm_annotations",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 67,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123, 129, 131-132, 162-169, 171-173, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017609610000022258,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `source_coverage` list within the report should contain exactly one SourceCoverageEntry object.",
          "The `SourceCoverageEntry` object should have the correct file path and coverage statistics.",
          "The `file_path` attribute of the `SourceCoverageEntry` object should match the expected value.",
          "The number of statements in the source code should be equal to the reported coverage percentage.",
          "The missed statements should be less than or equal to the total statements in the source code.",
          "The covered statements should be greater than or equal to the reported coverage percentage.",
          "The coverage percentage should be a valid floating-point number between 0 and 100.",
          "The coverage range should be correctly formatted as '1-5, 7-11'.",
          "The missed range should be correctly formatted as '6, 12'."
        ],
        "scenario": "Test that source coverage summary is deserialized correctly.",
        "token_usage": {
          "completion_tokens": 216,
          "prompt_tokens": 395,
          "total_tokens": 611
        },
        "why_needed": "Prevents regression in source coverage reporting functionality."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_aggregate_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 19,
          "line_ranges": "259-260, 262-263, 265, 267-271, 273, 276-277, 279-280, 283, 285-286, 288"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0033174289999919893,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that `_load_coverage_from_source()` returns None when `llm_coverage_source` is not set.",
          "Verify that a UserWarning is raised when `llm_coverage_source` is not set and `coverage.py` does not exist.",
          "Verify that successful loading of coverage data occurs with mock coverage data.",
          "Verify that the `CoverageMapper` returns the correct coverage percentage.",
          "Verify that the `cov.report()` method is called on the mock coverage object.",
          "Verify that the `map_source_coverage()` method is called on the mock mapper object.",
          "Verify that the mock coverage class is instantiated correctly with mock cov and mock mapper objects.",
          "Verify that the mock cov report returns 80.0 as expected."
        ],
        "scenario": "Test loading coverage from configured source file when option is not set.",
        "token_usage": {
          "completion_tokens": 206,
          "prompt_tokens": 584,
          "total_tokens": 790
        },
        "why_needed": "Prevents regression in case the `llm_coverage_source` option is not provided."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_load_coverage_from_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 17,
          "line_ranges": "231, 233-247, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009360849999922038,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count of all tests should remain unchanged.",
          "The passed count should be equal to the number of tests that were passed in the latest summary.",
          "The failed count should be equal to the number of tests that were failed in the latest summary.",
          "The skipped count should be equal to the number of tests that were skipped in the latest summary.",
          "The xfailed count should be equal to the number of tests that had an error in the latest summary.",
          "The xpassed count should be equal to the number of tests that passed in the latest summary.",
          "The error count should remain unchanged from the latest summary.",
          "The coverage percentage should not change when recalculating the summary."
        ],
        "scenario": "Test that the _recalculate_summary method preserves the latest summary's total, passed, failed, skipped, xfailed, xpassed, and error counts when recalculating the summary.",
        "token_usage": {
          "completion_tokens": 229,
          "prompt_tokens": 473,
          "total_tokens": 702
        },
        "why_needed": "This test prevents a regression where the latest summary is not correctly calculated if there are multiple tests with different outcomes."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_recalculate_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 72,
          "line_ranges": "53, 56-57, 60, 65, 70, 74-75, 78-81, 85, 88-90, 94-101, 110, 113-114, 117-121, 123-124, 129, 131-132, 162-167, 169, 171-173, 176, 178-180, 182, 184, 196, 198-200, 208, 231, 233-234, 249, 259, 262-263, 265, 267, 290-293, 295"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003138042999992763,
      "file_path": "tests/test_aggregation.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregator` instance is not skipped for the 'invalid.json' report.",
          "The `result` object contains a single run meta with a count of 1.",
          "The test asserts that the `result` object does not contain any additional run metadata.",
          "The test asserts that the `result` object has no additional keys or values.",
          "The test asserts that the `result` object is not `None`.",
          "The test verifies that only valid reports are counted in the aggregation result."
        ],
        "scenario": "Test that skipping an invalid JSON file prevents the test from counting it as a valid aggregation run.",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 352,
          "total_tokens": 522
        },
        "why_needed": "This test ensures that the aggregator correctly handles and skips reports containing invalid JSON data, preventing unnecessary runs."
      },
      "nodeid": "tests/test_aggregation.py::TestAggregator::test_skips_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/aggregation.py",
          "line_count": 10,
          "line_ranges": "45, 231, 233-239, 249"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008210290000079112,
      "file_path": "tests/test_aggregation_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total duration of all tests is 3.0 seconds.",
          "At least one test passed (outcome == 'passed')",
          "At least one test failed (outcome == 'failed')",
          "The total coverage percentage is 88.5%",
          "All tests have a unique nodeid ('t1', 't2')"
        ],
        "scenario": "The test verifies that the aggregator recalculates the summary correctly when there are multiple tests with different outcomes.",
        "token_usage": {
          "completion_tokens": 145,
          "prompt_tokens": 299,
          "total_tokens": 444
        },
        "why_needed": "This test prevents regression in case of multiple tests having different outcomes, as it ensures that the correct number of tests are included in the coverage calculation."
      },
      "nodeid": "tests/test_aggregation_maximal.py::TestAggregationMaximal::test_recalculate_summary_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-91, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001612201999989793,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 'Mocked provider instance'}",
          "{'name': 'mock_cache', 'expected_value': 'Mocked cache instance'}",
          "{'name': 'mock_assembler', 'expected_value': 'Mocked assembler instance'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 112,
          "total_tokens": 231
        },
        "why_needed": "To test the batch optimization message"
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_batch_optimization_message",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 50,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-128, 130, 134, 156, 181-182, 184, 211, 213-219, 221, 223"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010392279999962284,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked cache should be updated after each request', 'expected_result': {'mocked_cache': {}}, 'actual_result': {'mocked_cache': {}}}",
          "{'name': 'Cache is not overwritten by subsequent requests', 'expected_result': {'mocked_cache': {}}, 'actual_result': {'mocked_cache': {}}}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 101,
          "total_tokens": 242
        },
        "why_needed": "To ensure that the progress reporting is cached correctly and not overwritten by subsequent requests."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 95,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-124, 130, 132, 134, 137-141, 144-151, 156, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016757310000059533,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_cache.is_called_with', 'description': 'Mocked `is_called_with` method of mock_cache should be called with expected arguments.'}",
          "{'name': 'mock_assembler.is_called_with', 'description': 'Mocked `is_called_with` method of mock_assembler should be called with expected arguments.'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 102,
          "total_tokens": 237
        },
        "why_needed": "To ensure that cached tests are skipped when the test cache is enabled."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_cached_tests_are_skipped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 90,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221, 223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003108857000000853,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider', 'expected_value': 'Mock provider object'}",
          "{'name': 'mock_cache', 'expected_value': 'Mock cache object'}",
          "{'name': 'mock_assembler', 'expected_value': 'Mock assembler object'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 98,
          "total_tokens": 223
        },
        "why_needed": "To ensure that annotators can process multiple requests concurrently without blocking or missing any annotations."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188-196, 213-219, 221-223, 329-332, 334, 336-340, 342, 344, 350-351, 353-354, 356-359, 361-362, 367-368, 370, 376-379, 381"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021999639999989995,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Mocked annotator failed to annotate the text.', 'expected_exception': 'annotator.exceptions.ConcurrentAnnotationError'}",
          "{'message': 'Mocked annotator returned an error code that indicates a failure.', 'expected_exception': 'annotator.exceptions.ConcurrentAnnotationError'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 116,
          "total_tokens": 245
        },
        "why_needed": "This test is necessary because concurrent annotation can lead to failures if not handled properly."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_concurrent_annotation_handles_failures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0019040090000146392,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider should report progress', 'expected_value': 'progress', 'actual_value': 'report'}",
          "{'name': 'Mock cache should not affect progress reporting', 'expected_value': 'progress', 'actual_value': 'report'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 96,
          "total_tokens": 211
        },
        "why_needed": "To ensure that the annotator is reporting progress correctly."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_progress_reporting",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015020660000004682,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': 'progress message', 'expected_value': 'a progress message'}",
          "{'assertion_type': 'contains', 'pattern': 'percentage complete', 'expected_value': 'the percentage of the annotation task completed'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 101,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the annotator correctly reports progress messages during the annotation process."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_reports_progress_messages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 91,
          "line_ranges": "47, 50-51, 58-59, 65, 67-68, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015288859999884608,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider is called with an empty list', 'expected_result': [], 'actual_result': 1}",
          "{'name': 'mock_cache is not called when opt-out is True', 'expected_result': [], 'actual_result': 0}",
          "{'name': 'mock_assembler is called with the correct arguments when limit is True', 'expected_result': [{'text': '...', 'type': '...'}, {'text': '...', 'type': '...'}], 'actual_result': 1}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
        "token_usage": {
          "completion_tokens": 185,
          "prompt_tokens": 104,
          "total_tokens": 289
        },
        "why_needed": "The test respects the opt-out and limit mechanisms of the annotator."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_opt_out_and_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-257, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0017490179999981592,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider.get_annotated_data() should be called with a valid rate limit key', 'expected_value': 'valid_rate_limit_key'}",
          "{'name': 'mock_provider.get_annotated_data() should be called within the allowed time window', 'expected_value': 'allowed_time_window'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 112,
          "total_tokens": 242
        },
        "why_needed": "To ensure the annotator respects the rate limit for a given scenario."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_respects_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 94,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 12.001923922000003,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'Annotations should be applied sequentially', 'expected_result': 'Annotations should be applied in the expected order'}",
          "{'assertion': 'Annotations without a previous annotation will not affect subsequent annotations', 'expected_result': 'Annotations without a previous annotation should not affect subsequent annotations'}"
        ],
        "scenario": "Sequential annotation",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 98,
          "total_tokens": 209
        },
        "why_needed": "To ensure that annotations are applied in the correct order and to prevent potential errors."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 98,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-87, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264-267, 269-274, 277-279, 281, 283-284, 289-290, 292, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 24.002063970999984,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mock provider should be called with error message', 'expected_result': 'Mock provider was called with an error message', 'actual_result': 'Mock provider did not call with an error message'}",
          "{'name': 'Mock cache should be cleared after error is reported', 'expected_result': 'Mock cache should be cleared after error is reported', 'actual_result': 'Mock cache was not cleared after error was reported'}",
          "{'name': 'Mock assembler should not report any errors in sequential annotation', 'expected_result': 'Mock assembler did not report any errors in sequential annotation', 'actual_result': 'Mock assembler reported an error in sequential annotation'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
        "token_usage": {
          "completion_tokens": 212,
          "prompt_tokens": 105,
          "total_tokens": 317
        },
        "why_needed": "Error tracking in sequential annotation is necessary to ensure that errors are properly reported and handled."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_sequential_annotation_error_tracking",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 2,
          "line_ranges": "47-48"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000849352000003023,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': 'does nothing'}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 108,
          "total_tokens": 181
        },
        "why_needed": "LLM is disabled, so the test should do nothing."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 7,
          "line_ranges": "47, 50-54, 56"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009016499999887628,
      "file_path": "tests/test_annotator.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_provider.is_available', 'expected_result': {'is_true': False}}"
        ],
        "scenario": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 101,
          "total_tokens": 185
        },
        "why_needed": "The annotator should skip the annotation process if the provider is unavailable."
      },
      "nodeid": "tests/test_annotator.py::TestAnnotateTests::test_skips_if_provider_unavailable",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 359-360"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007962109999937184,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Error Code', 'expected_code': 400, 'actual_code': 404}",
          "{'assertion_type': 'Message', 'expected_message': 'Failed to parse LLM response as JSON'}"
        ],
        "scenario": "Test Base Parse Response Malformed JSON After Extract",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 152,
          "total_tokens": 265
        },
        "why_needed": "To ensure that the `extract_json_from_response` function correctly handles malformed JSON responses and raises a meaningful error."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_malformed_json_after_extract",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342-346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008420269999760421,
      "file_path": "tests/test_base_coverage_v2.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should be able to correctly identify and extract the expected key from the `response_data` dictionary.",
          "The function should be able to handle scenarios with multiple keys in the response data.",
          "The function should be able to ignore non-string values when extracting keys.",
          "The function should raise an error if it encounters a non-string value that is not recognized as a valid key.",
          "The function should correctly handle nested dictionaries or lists within the `response_data` dictionary.",
          "The function should preserve the original structure of the input data when parsing non-string fields.",
          "The function should be able to handle edge cases such as empty response data or null values."
        ],
        "scenario": "Test that the `test_base_parse_response_non_string_fields` function correctly handles non-string fields in the response data.",
        "token_usage": {
          "completion_tokens": 209,
          "prompt_tokens": 269,
          "total_tokens": 478
        },
        "why_needed": "This test prevents a potential bug where the function fails to parse non-string fields, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_base_coverage_v2.py::test_base_parse_response_non_string_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007751720000044315,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider_type', 'expected_value': 'GeminiProvider'}",
          "{'name': 'provider_name', 'expected_value': 'gemini'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 104,
          "total_tokens": 214
        },
        "why_needed": "The test is necessary because the `get_provider` function returns a `GeminiProvider` object which needs to be asserted as correct."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_gemini_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002014221999985466,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config is not None', 'expected': 'None', 'actual': \"Config(provider='invalid')\"}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 106,
          "total_tokens": 203
        },
        "why_needed": "The test is necessary because it checks for the expected error when a valid configuration is passed to the get_provider function with an invalid provider."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000771253999999999,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider', 'expected_type': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 109,
          "total_tokens": 184
        },
        "why_needed": "To test the functionality of getting a LitEllM provider."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_litellm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007461569999804851,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider is an instance of NoopProvider', 'expected_type': 'NoopProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 104,
          "total_tokens": 188
        },
        "why_needed": "To test the functionality of a NoopProvider in the base maximization algorithm."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_noop_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007554839999954766,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider type', 'expected_type': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 108,
          "total_tokens": 195
        },
        "why_needed": "To ensure the `get_ollama_provider` function returns an instance of `OllamaProvider` correctly."
      },
      "nodeid": "tests/test_base_maximal.py::TestGetProvider::test_get_ollama_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 134-135, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008162389999881725,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `is_available()` method should return True for both instances of the provider.",
          "The `is_available()` method should return True when called multiple times in quick succession.",
          "The `checks` attribute should increment correctly each time the `_check_availability()` method is called."
        ],
        "scenario": "Verify that the LLM provider returns a boolean indicating availability.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 280,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a potential regression where the LLM provider does not return a boolean indicating availability."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_available_caches_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007574990000023263,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"provider.get_model_name() == 'test-model'\", 'expected_result': 'test-model'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestLlmProviderDefaults",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 114,
          "total_tokens": 203
        },
        "why_needed": "To ensure that the `get_model_name` method of `ConcreteProvider` returns the default model name specified in the configuration."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_model_name_defaults_to_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 155"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007452650000061567,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.get_rate_limits() should return None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_base_maximal.py",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 108,
          "total_tokens": 184
        },
        "why_needed": "To ensure that the rate limits are set to None by default when no specific rate limit is specified."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_get_rate_limits_defaults_to_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 174"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00074721000001432,
      "file_path": "tests/test_base_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_local() is False', 'expected_value': False, 'message': 'Expected provider.is_local() to return False'}"
        ],
        "scenario": "tests/test_base_maximal.py::TestLlmProviderDefaults",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 105,
          "total_tokens": 195
        },
        "why_needed": "To verify that the `is_local` method returns False for default LLM providers."
      },
      "nodeid": "tests/test_base_maximal.py::TestLlmProviderDefaults::test_is_local_defaults_to_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 35,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 192-194, 196-200, 203-206, 209-210, 213-214, 216-218, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007680979999804549,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'src/module.py' file should be present in the prompt.",
          "The 'def helper()' function should be included in the prompt."
        ],
        "scenario": "Verify that context files are included in the batch prompt.",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 261,
          "total_tokens": 349
        },
        "why_needed": "This test prevents a potential issue where context files are not added to the prompt, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_context_files_included",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "34, 39-40, 156-157, 160, 162, 164-168, 170-177, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007489719999966837,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Test Group: test.py::test_add[*]",
          "Parameterizations (2 variants)",
          "[1+1=2]",
          "[0+0=0]",
          "ONE annotation"
        ],
        "scenario": "Test that the parametrized batch prompt includes all required information.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 330,
          "total_tokens": 427
        },
        "why_needed": "This test prevents a potential regression where the batch prompt is missing or incorrect due to incomplete parameterizations."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_parametrized_batch_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 15,
          "line_ranges": "34, 39, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00079186199999981,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Test: test.py::test_foo should be included in the prompt.",
          "```python"
        ],
        "scenario": "The test verifies that a single test generates a normal batch prompt.",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 269,
          "total_tokens": 345
        },
        "why_needed": "This test prevents a potential regression where the batch prompt is missing for single tests."
      },
      "nodeid": "tests/test_batching.py::TestBuildBatchPrompt::test_single_test_prompt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007191569999918102,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The source code of the function should be the same across all hash computations.",
          "The length of each hash computation should be equal (32 bytes) for the given function.",
          "Different versions of the function should produce different source hashes.",
          "The source code should not change between hash computations for the same function.",
          "_compute_source_hash(source) should return a consistent hash value for the given source code.",
          "The length of the returned hash value should be equal (32 bytes) for the given source code."
        ],
        "scenario": "Test the consistency of source hashes for a given function.",
        "token_usage": {
          "completion_tokens": 165,
          "prompt_tokens": 220,
          "total_tokens": 385
        },
        "why_needed": "This test prevents regression where different versions of the same function produce different source hashes, potentially leading to inconsistencies in batch processing."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67, 70"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007404459999804658,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'hash1 != hash2', 'description': 'The two computed hashes are not equal.'}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 127,
          "total_tokens": 220
        },
        "why_needed": "To ensure that different sources produce different hashes, which is a key aspect of batching and caching in the Compute Source Hash system."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "67-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007227429999829837,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _compute_source_hash() returns an empty string for an empty source', 'expected_result': '', 'actual_result': ''}"
        ],
        "scenario": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 94,
          "total_tokens": 183
        },
        "why_needed": "Because an empty source is a valid input for the ComputeSourceHash function."
      },
      "nodeid": "tests/test_batching.py::TestComputeSourceHash::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007738589999917167,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.validate() returns errors', 'message': \"Any error messages returned by the validate method should contain the string 'batch_max_tests'.\"}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 126,
          "total_tokens": 223
        },
        "why_needed": "The 'batch_max_tests' configuration option must be at least 1 to prevent invalid batch configurations."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_batch_max_tests_minimum",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000743131999996649,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'value': 0, 'type': 'asserts', 'message': 'context_line_padding must be >= 0'}"
        ],
        "scenario": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 126,
          "total_tokens": 207
        },
        "why_needed": "Context line padding must be non-negative."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_context_line_padding_non_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007811430000117525,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"Context compression mode should be one of 'none', 'fast', or 'fastest'.\", 'expected_value': 'none'}"
        ],
        "scenario": "Test invalid context compression",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 122,
          "total_tokens": 202
        },
        "why_needed": "To ensure that the context compression mode is valid and does not cause any issues."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007538920000058624,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'not equal to', 'expected_value': 'context_compression', 'actual_value': 'none'}",
          "{'assertion_type': 'not in', 'expected_value': ['lines'], 'actual_value': ['none']}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 133,
          "total_tokens": 229
        },
        "why_needed": "Valid compression modes should pass."
      },
      "nodeid": "tests/test_batching.py::TestConfigValidation::test_valid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007154790000072353,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert stripped base node ID', 'value': \"_get_base_nodeid('test.py::test[a-b-c]') == 'test.py::test'\", 'expected_result': 'test.py::test'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 109,
          "total_tokens": 215
        },
        "why_needed": "This test ensures that complex parameters are fully stripped from the base node ID."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_nested_params",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53-54"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007618960000002062,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"_get_base_nodeid('tests/test_foo.py::test_add[1+1=2]') == 'tests/test_foo.py::test_add'\", 'expected_result': 'tests/test_foo.py::test_add'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 133,
          "total_tokens": 256
        },
        "why_needed": "The test is necessary because it checks the behavior of _get_base_nodeid when a parameterized node id is used."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_parametrized_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 2,
          "line_ranges": "53, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007250380000130008,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'tests/test_foo.py::test_bar', 'actual_value': 'tests/test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 123,
          "total_tokens": 206
        },
        "why_needed": "To test the functionality of getting a base node ID without any parameters."
      },
      "nodeid": "tests/test_batching.py::TestGetBaseNodeid::test_simple_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 24,
          "line_ranges": "53-54, 67-68, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007830570000066928,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of each batch is correct.",
          "Each group has exactly two tests in it.",
          "Only one group has only one test in it."
        ],
        "scenario": "Large groups should be split by batch_max_tests.",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 364,
          "total_tokens": 445
        },
        "why_needed": "This test prevents regression when large groups of tests are added to the batch."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batch_max_size_respected",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 6,
          "line_ranges": "92-93, 95, 97-99"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007653229999959876,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert len(batches) == 2', 'expected': 2, 'actual': 1}"
        ],
        "scenario": "Test case for TestGroupTestsForBatching",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 170,
          "total_tokens": 246
        },
        "why_needed": "This test is needed because the batching feature is disabled."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_batching_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 27,
          "line_ranges": "34, 39-40, 53-54, 67, 70, 92-93, 95, 103-106, 108-110, 122-123, 126-132, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007790690000035738,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of batches is equal to 1.",
          "Each batch has exactly 3 tests.",
          "Each batch is a parameterized test group.",
          "The base node ID of each batch is 'test.py::test_add'.",
          "All tests in the first batch are parametrized."
        ],
        "scenario": "Test 'Parametrized tests should be grouped together' verifies that parametrized tests are correctly grouped.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 346,
          "total_tokens": 472
        },
        "why_needed": "This test prevents regression by ensuring that parametrized tests are not scattered across multiple batches."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_parametrized_tests_grouped",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 18,
          "line_ranges": "53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007658650000053058,
      "file_path": "tests/test_batching.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Each test should be its own batch.",
          "There should be two batches with the same number of tests.",
          "The first batch should have exactly one test.",
          "The second batch should also have exactly one test."
        ],
        "scenario": "The test verifies that single tests are grouped into batches without any grouping.",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 278,
          "total_tokens": 377
        },
        "why_needed": "This test prevents a regression where single tests are not properly grouped in batches."
      },
      "nodeid": "tests/test_batching.py::TestGroupTestsForBatching::test_single_tests_no_grouping",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007561059999829922,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'hash_source', 'type': 'function', 'expected_result': 'hash_source(source)'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_consistent_hash",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 107,
          "total_tokens": 180
        },
        "why_needed": "To ensure that the hash function is consistent across different inputs."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_consistent_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000738102000013896,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'different', 'actual': 'same'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 108,
          "total_tokens": 182
        },
        "why_needed": "To ensure that the hash function is working correctly and producing different hashes for different source code."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_different_source_different_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 1,
          "line_ranges": "153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007627779999950235,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The hash value should be 16 characters long.'}"
        ],
        "scenario": "tests/test_cache.py::TestHashSource::test_hash_length",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 100,
          "total_tokens": 167
        },
        "why_needed": "To ensure the hash function is producing a fixed length output."
      },
      "nodeid": "tests/test_cache.py::TestHashSource::test_hash_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 26,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 129, 132-136, 141"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012284899999883692,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of cache entries should be reduced to 2 after clearing.",
          "The annotation 'test::a' with hash1 should be removed from the cache.",
          "The annotation 'test::b' with hash2 should also be removed from the cache.",
          "All annotations in the cache should have been cleared."
        ],
        "scenario": "Test that clearing the cache removes all entries.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 283,
          "total_tokens": 405
        },
        "why_needed": "To prevent a regression where multiple annotations are stored in the cache and then cleared, preventing them from being re-added."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_clear",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 11,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008543599999768503,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'NoneType', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 157,
          "total_tokens": 230
        },
        "why_needed": "To ensure that LLM annotations with errors are not cached."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_does_not_cache_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 9,
          "line_ranges": "39-41, 53, 55-56, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000970327000004545,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The result is None', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_cache.py::TestLlmCache::test_get_missing",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 128,
          "total_tokens": 202
        },
        "why_needed": "To test that the get method returns None for missing entries in the cache."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_get_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 28,
          "line_ranges": "39-41, 53, 55, 58, 60-62, 68-73, 86, 90, 92, 94, 97-101, 103, 118-119, 121"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010783989999936239,
      "file_path": "tests/test_cache.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Check that the annotation is set correctly with the correct scenario.",
          "Check that the confidence of the annotation remains unchanged after retrieval.",
          "Check that the annotation's original key matches its retrieved value.",
          "Verify that no other annotations are stored in the cache without a corresponding user input.",
          "Ensure that the annotation's status (in this case, 'Tests login') is preserved across multiple retrievals."
        ],
        "scenario": "Test that setting and getting an annotation from the cache preserves its original value.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 286,
          "total_tokens": 429
        },
        "why_needed": "Prevents bypass by ensuring annotations are stored in the cache, preventing them from being overwritten without user input."
      },
      "nodeid": "tests/test_cache.py::TestLlmCache::test_set_and_get",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007763430000125027,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'nodeid', 'expected_value': 'test_bad.py'}",
          "{'name': 'message', 'expected_value': 'SyntaxError'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 124,
          "total_tokens": 233
        },
        "why_needed": "The current implementation of the CollectionError class does not validate its structure, which can lead to errors when collecting and processing these types of errors."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_collection_error_structure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007512070000075255,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert get_collection_errors() == []', 'expected_result': [], 'actual_result': []}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 114,
          "total_tokens": 205
        },
        "why_needed": "To ensure the `get_collection_errors` method returns an empty list when the collection is initially empty."
      },
      "nodeid": "tests/test_collector.py::TestCollectorCollectionErrors::test_get_collection_errors_initially_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007647619999886501,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'llm_context_override', 'value': 'None'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 136,
          "total_tokens": 201
        },
        "why_needed": "Default llm_context_override should be None."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_context_override_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007474389999799769,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert llm_opt_out is False', 'expected_value': False, 'message': 'Expected llm_opt_out to be False'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorMarkerExtraction",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 136,
          "total_tokens": 218
        },
        "why_needed": "Default llm_opt_out should be False."
      },
      "nodeid": "tests/test_collector.py::TestCollectorMarkerExtraction::test_llm_opt_out_default_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007732579999810696,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert config.capture_failed_output is True', 'expected_result': True}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 104,
          "total_tokens": 181
        },
        "why_needed": "This test is needed because the collector is not enabled by default."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007434219999993275,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert capture_output_max_chars is equal to 4000', 'expected_value': 4000, 'message': 'The default value of `capture_output_max_chars` is 4000.'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 108,
          "total_tokens": 235
        },
        "why_needed": "The default value of `capture_output_max_chars` is 4000. This is necessary to ensure that the output does not exceed this limit, which could cause issues with downstream processing."
      },
      "nodeid": "tests/test_collector.py::TestCollectorOutputCapture::test_capture_max_chars_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212, 216, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007554139999967902,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': 'xfailed'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 206,
          "total_tokens": 284
        },
        "why_needed": "To ensure that xfail failures are correctly recorded as xfailed."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_failed_is_xfailed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 26,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 212-214"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007613350000212904,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result.outcome', 'expected_value': 'xpassed'}"
        ],
        "scenario": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 205,
          "total_tokens": 280
        },
        "why_needed": "xfail passes should be recorded as xpassed."
      },
      "nodeid": "tests/test_collector.py::TestCollectorXfailHandling::test_xfail_passed_is_xpassed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007455760000141254,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "collector.results should be an empty dictionary.",
          "collector.collection_errors should be an empty list.",
          "collector.collected_count should be 0."
        ],
        "scenario": "Test the creation of a TestCollector instance.",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 205,
          "total_tokens": 293
        },
        "why_needed": "This test prevents a potential bug where the collector is not initialized with any results, leading to incorrect assertions in subsequent tests."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_create_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 15,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007521480000036718,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': ['a_test.py::test_a', 'z_test.py::test_z'], 'actual': ['a_test.py::test_a', 'z_test.py::test_z']}"
        ],
        "scenario": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 227,
          "total_tokens": 322
        },
        "why_needed": "To ensure that the collector correctly sorts test results by nodeid."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_get_results_sorted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "78-79, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007856910000043626,
      "file_path": "tests/test_collector.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `collected_count` attribute should be set to 3 (number of collected items).",
          "The `deselected_count` attribute should be set to 1 (number of deselected items)."
        ],
        "scenario": "Test the `handle_collection_finish` method to ensure it correctly tracks collected and deselected counts.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 256,
          "total_tokens": 368
        },
        "why_needed": "This test prevents a potential bug where the count of collected items is not updated correctly after calling `handle_collection_finish`."
      },
      "nodeid": "tests/test_collector.py::TestTestCollector::test_handle_collection_finish",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014662149999935536,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result.captured_stdout is None', 'expected': {'type': 'NoneType', 'message': 'Should not capture if config disabled'}}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 211,
          "total_tokens": 308
        },
        "why_needed": "Capturing output via handle_runtest_logreport is disabled in this scenario."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_disabled_via_handle_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008897959999956129,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'captured_stderr', 'expected_value': 'Some error'}"
        ],
        "scenario": "TestCollectorInternals::test_capture_output_stderr",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 157,
          "total_tokens": 234
        },
        "why_needed": "To verify that the `collector._capture_output` method correctly captures stderr and returns it to the test case."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008851679999963835,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert captured stdout is correct', 'expected_value': 'Some output'}"
        ],
        "scenario": "TestCollectorInternals::test_capture_output_stdout",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 157,
          "total_tokens": 223
        },
        "why_needed": "To test that the collector captures stdout correctly."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 18,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 261, 264-265, 268"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009923580000190668,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 174,
          "total_tokens": 268
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_capture_output_truncated",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 35,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 155-159, 163-164, 167-169, 171, 181-182, 185-189, 198-200, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0020674909999911506,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `param_id` attribute of the created result is set to 'param1'.",
          "The `llm_opt_out` attribute of the created result is set to True.",
          "The `llm_context_override` attribute of the created result is set to 'complete'.",
          "All required requirements are included in the `requirements` list of the created result."
        ],
        "scenario": "Test creates a result with item markers and verifies the expected behavior.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 382,
          "total_tokens": 513
        },
        "why_needed": "This test prevents regression in case of incorrect marker extraction or incorrect usage of item markers."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_create_result_with_item_markers",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013166349999949034,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'Crash report', 'actual': 'Crash report'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 165,
          "total_tokens": 248
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_repr_crash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 22,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009362930000236247,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _extract_error returns correct longrepr', 'expected_value': 'Some error occurred'}"
        ],
        "scenario": "test_collector_internals",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 130,
          "total_tokens": 205
        },
        "why_needed": "To ensure that the `_extract_error` method returns the correct string representation of an error."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_error_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250, 252"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008880129999795372,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert _extract_skip_reason returns None for no longrepr', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 130,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the `_extract_skip_reason` method returns `None` when no longrepr is provided."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008926809999820762,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert _extract_skip_reason returns 'Just skipped'\", 'expected_value': 'Just skipped'}"
        ],
        "scenario": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 133,
          "total_tokens": 218
        },
        "why_needed": "To ensure the `_extract_skip_reason` method returns a string as expected."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 250-251"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008973099999991518,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 164,
          "total_tokens": 279
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorInternals::test_extract_skip_reason_tuple",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 21,
          "line_ranges": "58, 60-65, 90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008791859999917051,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `collector.collection_errors` is set to 1.",
          "The value of `collector.collection_errors[0].nodeid` is set to `"
        ],
        "scenario": "When the `handle_collection_report` method is called with a report that indicates a collection error, it should record this error in the `collection_errors` list.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 273,
          "total_tokens": 392
        },
        "why_needed": "This test prevents a potential bug where a collection report fails and does not trigger any errors, potentially leading to missed issues or incorrect reporting."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_collection_report_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 42,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-118, 124, 127-128, 130, 140-141, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238, 261, 264-265, 268-269"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002179722000022366,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.rerun_count should be equal to 1 (expected)",
          "res.final_outcome should be 'failed' (expected)",
          "collector.results['t::r'] should contain the expected data"
        ],
        "scenario": "Test 'handle_runtest_rerun' verifies that the TestCollector handles rerun attribute correctly.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 281,
          "total_tokens": 394
        },
        "why_needed": "This test prevents a potential regression where the TestCollector might not handle reruns correctly, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 36,
          "line_ranges": "90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009105050000073334,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "res.outcome == 'error'",
          "res.phase == 'setup'",
          "res.error_message == 'Setup failed'"
        ],
        "scenario": "When the `handle_runtest_setup_failure` test function is called with a setup log report that fails, then it should record an error in the collector's report.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 300,
          "total_tokens": 411
        },
        "why_needed": "This test prevents a potential bug where the setup log report is not properly recorded when it fails, potentially causing incorrect reporting of the test result."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_setup_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 38,
          "line_ranges": "90, 93-94, 96, 99, 110-112, 114-115, 124, 127-128, 130, 132-133, 135-137, 140, 155-159, 163, 167, 171, 209-210, 227-228, 230-234, 238"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012540380000132245,
      "file_path": "tests/test_collector_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert res.outcome == 'error', assert res.phase == 'teardown', assert res.error_message == 'Cleanup failed'",
          "assert not res.wasxfail",
          "assert res.duration is None"
        ],
        "scenario": "TestCollectorReportHandling test handle_runtest_teardown_failure: Should record error if teardown fails after pass.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 391,
          "total_tokens": 502
        },
        "why_needed": "Prevents regression by ensuring that the collector logs an error when a teardown failure occurs after a successful run."
      },
      "nodeid": "tests/test_collector_maximal.py::TestCollectorReportHandling::test_handle_runtest_teardown_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007746009999891612,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Context compression is not one of the allowed values: 'gzip', 'deflate', 'lz4', 'snappy'.\"}"
        ],
        "scenario": "Test invalid compression mode",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 124,
          "total_tokens": 202
        },
        "why_needed": "To ensure that the Context Compression validation correctly identifies and reports invalid compression modes."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_invalid_compression_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007564560000048459,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding is not a valid value for the context_line_padding parameter.', 'expected_value': -1}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 121,
          "total_tokens": 198
        },
        "why_needed": "Negative padding should fail validation."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_negative_padding_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007704429999932927,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'type', 'expected_type': 'str', 'actual_type': 'None'}"
        ],
        "scenario": "TestConfigValidation",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 135,
          "total_tokens": 205
        },
        "why_needed": "To ensure that valid compression modes can pass validation without raising any errors."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_valid_compression_modes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007600629999728881,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'context_line_padding is not present in any error message', 'expected_value': 0}"
        ],
        "scenario": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 122,
          "total_tokens": 195
        },
        "why_needed": "Zero padding should be valid."
      },
      "nodeid": "tests/test_context_compression.py::TestConfigValidation::test_zero_padding_valid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008260669999913262,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_compression', 'expected_value': 'lines'}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 119,
          "total_tokens": 193
        },
        "why_needed": "Context compression should be enabled by default ('lines')."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007411480000030224,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.context_compression', 'expected_value': 'lines'}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_compression_mode_lines",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 113,
          "total_tokens": 182
        },
        "why_needed": "Lines compression mode should be available."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_compression_mode_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007483910000019023,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Line padding should default to 2.', 'expected_value': 2, 'actual_value': 0}"
        ],
        "scenario": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 106,
          "total_tokens": 191
        },
        "why_needed": "To ensure that line padding is set correctly for default context compression."
      },
      "nodeid": "tests/test_context_compression.py::TestContextCompression::test_line_padding_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0007906799999943814,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The count of '#' characters should be zero for contiguous lines without gaps.",
          "The string '# L3:' should be present in the result.",
          "The string '# L4:' should be present in the result.",
          "The string '# L5:' should be present in the result.",
          "The line numbers (L3, L4, and L5) should not be missing or appear out of order."
        ],
        "scenario": "Test that contiguous covered lines do not have gap indicators.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 293,
          "total_tokens": 428
        },
        "why_needed": "Prevents regression where contiguous lines without gaps are incorrectly identified as uncovered."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_contiguous_lines_no_gap",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 216-217"
        }
      ],
      "duration": 0.0007422600000097646,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 130,
          "total_tokens": 212
        },
        "why_needed": "The test is needed because it checks the behavior of the `_extract_covered_lines` method when there are no lines to extract."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_empty_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 24,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242-247, 249"
        }
      ],
      "duration": 0.0008110889999954907,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `_extract_covered_lines` should find the correct range for each covered line and add a gap indicator between them.",
          "The function `_extract_covered_lines` should extract two separate ranges, one starting from L3 and another from L15.",
          "The function `_extract_covered_lines` should not miss any gaps in the extracted ranges.",
          "The function `_extract_covered_lines` should correctly identify the start line of each range (L3 and L15).",
          "The function `_extract_covered_lines` should handle overlapping covered lines correctly.",
          "The function `_extract_covered_lines` should preserve the original order of covered lines in the input list."
        ],
        "scenario": "Test Extract Covered Lines: Multiple covered ranges should be extracted with gap indicators.",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 274,
          "total_tokens": 470
        },
        "why_needed": "This test prevents a regression where multiple covered lines are not correctly identified with gap indicators."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_multiple_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0008309449999899243,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_covered_lines() includes lines 2, 3, and 4 in the result with 1 line padding.",
          "The function _extract_covered_lines() includes lines 2, 3, and 4 in the result without any padding.",
          "The function _extract_covered_lines() correctly handles cases where there are multiple covered lines."
        ],
        "scenario": "Single covered line should be extracted with padding.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 302,
          "total_tokens": 431
        },
        "why_needed": "This test prevents a regression where the single line is not extracted correctly due to incorrect padding."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_extract_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 23,
          "line_ranges": "33, 216, 219-220, 223-228, 231-232, 235-237, 239-240, 242, 244-247, 249"
        }
      ],
      "duration": 0.0007927740000184258,
      "file_path": "tests/test_context_compression.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The extracted covered lines should have line numbers within the range [1, 3].",
          "The uncovered lines should not have negative line numbers (0).",
          "The uncovered lines should be at least as many as the number of lines in the source code minus one.",
          "The uncovered lines should be at most three."
        ],
        "scenario": "Verify that padding does not extend beyond file boundaries.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 288,
          "total_tokens": 412
        },
        "why_needed": "This test prevents a bug where padding exceeds the number of lines in the source code, causing incorrect coverage reports."
      },
      "nodeid": "tests/test_context_compression.py::TestExtractCoveredLines::test_padding_boundary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008872109999913391,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The prompt should not contain 'truncated'.\", 'expected_result': 'short content'}"
        ],
        "scenario": "tests/test_context_limits.py::test_no_truncation_needed",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 158,
          "total_tokens": 249
        },
        "why_needed": "This test is necessary because the current implementation truncates context when it's too long. This can cause issues with the expected output."
      },
      "nodeid": "tests/test_context_limits.py::test_no_truncation_needed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 25,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016999329999975998,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "F1 should get full allocation (40 tokens)",
          "F2 should get truncated allocation (~480 tokens)",
          "Total used tokens should be equal to total allocated tokens",
          "Wasted tokens should be less than total allocated tokens",
          "F2 content in prompt should contain 'f2' and have more characters than F1 content",
          "F2 content in prompt should not exceed 800 characters"
        ],
        "scenario": "test_smart_distribution verifies that the smart distribution algorithm allocates budget to F1 and F2 correctly, resulting in zero waste.",
        "token_usage": {
          "completion_tokens": 163,
          "prompt_tokens": 773,
          "total_tokens": 936
        },
        "why_needed": "This test prevents regression by ensuring that the smart distribution algorithm does not allocate too much budget to F1 or F2, leading to wasted tokens."
      },
      "nodeid": "tests/test_context_limits.py::test_smart_distribution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 24,
          "line_ranges": "243, 245, 264, 266, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307, 310, 312, 314"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009047339999881387,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'f1' is present in the prompt.",
          "The file 'f2' is present in the prompt.",
          "The string 'truncated' is present in the prompt."
        ],
        "scenario": "Verify that the splitting logic correctly identifies files with large content and truncates them according to the budget.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 317,
          "total_tokens": 426
        },
        "why_needed": "This test prevents a potential regression where the splitting logic fails to truncate large files, leading to incorrect output or unexpected behavior."
      },
      "nodeid": "tests/test_context_limits.py::test_splitting_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "243, 245, 264, 266, 270-272, 274-275"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009334179999882508,
      "file_path": "tests/test_context_limits.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the prompt should be less than 5 times the limit (100 tokens) minus some overhead.",
          "Either '[... truncated]' or 'Relevant context' should be present in the prompt if it's heavily truncated or skipped."
        ],
        "scenario": "When the test_truncation_logic function is called with a large context file, it should truncate the prompt to fit within the 100 token limit.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 397,
          "total_tokens": 522
        },
        "why_needed": "This test prevents a potential bug where the prompt exceeds the 100 token limit and is truncated or skipped."
      },
      "nodeid": "tests/test_context_limits.py::test_truncation_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007097289999933309,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The result should be the same as the expected output.', 'expected_output': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 128,
          "total_tokens": 214
        },
        "why_needed": "To test the functionality of collapsing empty lines in a context."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_collapse_three_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007291560000055597,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The collapsed string should contain only one blank line.', 'expected_result': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 127,
          "total_tokens": 212
        },
        "why_needed": "To test the functionality of collapsing empty lines in a multi-line string."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_many_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007461779999857754,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'line1\\n\\nline2', 'actual_value': 'line1\\n\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 125,
          "total_tokens": 215
        },
        "why_needed": "To ensure that the `collapse_empty_lines` function preserves up to 2 consecutive newlines."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_preserve_two_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 1,
          "line_ranges": "108"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000745856000008871,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'line1\\nline2\\nline3', 'actual_result': 'line1\\nline2\\nline3'}"
        ],
        "scenario": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 121,
          "total_tokens": 203
        },
        "why_needed": "Preserves single newlines in collapsed text."
      },
      "nodeid": "tests/test_context_util.py::TestCollapseEmptyLines::test_single_newline",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008635570000024018,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The output of the function is a string containing only non-empty lines.', 'expected_output': 'line1\\nline2'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 137,
          "total_tokens": 249
        },
        "why_needed": "This test ensures that the `optimize_context` function always collapses empty lines, regardless of whether the `strip_docs` and/or `strip_comms` flags are set."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_always_collapses_empty_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 45,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000987017999989348,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'message': 'Module docstring should be removed'}, 'actual': {'message': 'Module docstring remains unchanged'}}",
          "{'expected': {'message': 'Namespace object should be created and populated with data'}, 'actual': {'message': 'Namespace object is empty'}}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 96,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the combined optimization process is applied correctly and optimizes the code without any unnecessary changes."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_combined_optimization",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 36,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008279399999935322,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring stripping', 'expected': '', 'actual': 'def foo():'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 100,
          "total_tokens": 189
        },
        "why_needed": "The default behavior of the `optimize_context` function should strip all docstrings, but leave comments intact."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_default_strips_docs_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007536009999853377,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is an empty string', 'expected': '', 'actual': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 95,
          "total_tokens": 169
        },
        "why_needed": "To ensure that the context optimizer handles empty sources correctly."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_empty_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007777259999954822,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return a string with one or more newline characters and/or tabs, but instead returns a single newline character."
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 115,
          "total_tokens": 199
        },
        "why_needed": "This test is necessary because the current implementation of optimize_context does not handle source lines with only whitespace correctly."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_source_with_only_whitespace",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 44,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69, 81-82, 86, 88-90, 93, 108, 124, 126-127, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009089219999793841,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'docstring stripping', 'reason': 'The function docstring is not necessary and can be removed.'}",
          "{'assertion_type': 'comment stripping', 'reason': 'Comments are also unnecessary and can be removed.'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 95,
          "total_tokens": 204
        },
        "why_needed": "To optimize context by removing unnecessary documentation from the code."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_both",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 14,
          "line_ranges": "81-82, 86, 88-90, 93, 108, 124, 126, 129-130, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008652500000039254,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'strip_comments', 'expected_result': 'The function foo() is defined, but with no docstring.'}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 95,
          "total_tokens": 179
        },
        "why_needed": "To remove unnecessary comments from the code without affecting its functionality."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_comments_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 6,
          "line_ranges": "108, 124, 126, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007432720000224435,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The optimizer should be able to keep both `foo()` and its explicit call.', 'expected_result': {'code': 'def foo():', 'context': ''}, 'actual_result': {'code': '', 'context': ''}}"
        ],
        "scenario": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 94,
          "total_tokens": 205
        },
        "why_needed": "To ensure that the optimizer can correctly strip out unnecessary code in certain scenarios."
      },
      "nodeid": "tests/test_context_util.py::TestOptimizeContext::test_strip_neither",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007855810000023666,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Source string should contain a comment after the string with hash (#).', 'expected_result': 'url = \"http://example.com#anchor\"'}",
          "{'description': 'Comment should be stripped from the source string.', 'expected_result': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 134,
          "total_tokens": 258
        },
        "why_needed": "To ensure that comments are stripped from strings containing hash (#) symbols, regardless of their position."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_comment_after_string_with_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007614849999981743,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Basic behavior', 'expected': '# comment', 'actual': '# comment'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 133,
          "total_tokens": 209
        },
        "why_needed": "To handle escaped quotes in strings without modifying the original string."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_escaped_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008865899999932481,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '\"don\\'t # worry\"', 'actual': \"'don't # worry'\"}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 101,
          "total_tokens": 182
        },
        "why_needed": "To strip quotes from comments in Python code, which can be useful for parsing and processing scripts."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_mixed_quotes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000782184000001962,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test should be able to successfully remove all comments from the source code.",
          "The output of the stripped source code should match the expected output."
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_no_comments",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 91,
          "total_tokens": 172
        },
        "why_needed": "To strip comments from the source code without affecting the functionality of the code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_no_comments",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007852710000122443,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': '\\'url = \"http://example.com#anchor\"\\'', 'actual_result': '\\'url = \"http://example.com#anchor\"\\''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 135,
          "total_tokens": 225
        },
        "why_needed": "Preserves # inside double-quoted strings."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_double_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000797463000026255,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Expected result', 'expected_value': \"url = 'http://example.com#anchor'\", 'actual_value': 'result'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 135,
          "total_tokens": 226
        },
        "why_needed": "Preserves # inside single-quoted strings in source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_preserve_hash_in_single_quoted_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007576680000056513,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equals', 'expected_value': 'x = 1'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 119,
          "total_tokens": 204
        },
        "why_needed": "To test the functionality of the `strip_comments` function, which removes simple end-of-line comments from source code."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_simple_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 7,
          "line_ranges": "81-82, 86, 88-90, 93"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008232209999903262,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'strip_comments', 'expected_result': ['This is a comment']}"
        ],
        "scenario": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 99,
          "total_tokens": 179
        },
        "why_needed": "To strip standalone comments from the code, which can improve readability and reduce noise."
      },
      "nodeid": "tests/test_context_util.py::TestStripComments::test_strip_standalone_comment",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 4,
          "line_ranges": "27, 29-31"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000821558000012601,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'source': 'def foo( unclosed paren'}, 'actual': {'source': 'def foo( unclosed parentheses'}}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 119,
          "total_tokens": 212
        },
        "why_needed": "The test is checking if the `strip_docstrings` function handles syntax errors correctly."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_handles_syntax_error_gracefully",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 30,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008659020000152395,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function should return an empty list for the given source code.', 'expected_output': []}",
          "{'assertion': 'The function should not modify the original source code.', 'expected_output': ''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 95,
          "total_tokens": 198
        },
        "why_needed": "To strip unnecessary docstrings from a module."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_multiple_docstrings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008139840000183085,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The triple-quoted string is preserved.', 'expected_result': 'The triple-quoted string is preserved.'}",
          "{'assertion': 'The variable foo() does not contain the preserved string.', 'expected_result': 'The variable foo() does not contain the preserved string.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 103,
          "total_tokens": 228
        },
        "why_needed": "Preserve multiline data strings in docstrings for consistency and readability."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_multiline_data_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 25,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008106070000053478,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The string \\'x = \"hello world\".\\' is preserved in the test output.', 'expected_output': '\\'x = \"hello world\".\\''}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 102,
          "total_tokens": 191
        },
        "why_needed": "Preserve regular strings in test output."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_regular_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 27,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58, 61, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008328190000099767,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should preserve the original string literals.', 'expected_result': 'Should preserve strings in lists/dicts.'}",
          "{'description': 'The function should not modify the original string literals.', 'expected_result': 'Does not modify the original string literals.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 146,
          "total_tokens": 272
        },
        "why_needed": "Preserving strings in structures is important for maintaining code readability and avoiding potential issues with string literals."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_preserves_strings_in_structures",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009421240000051512,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function should return a JSON object with the expected structure.', 'expected_result': {'scenario': 'tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring', 'why_needed': 'To strip multiline docstrings from Python code.', 'key_assertions': ['...']}, 'actual_result': \"{'scenario': 'tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring', 'why_needed': 'To strip multiline docstrings from Python code.', 'key_assertions': ['...']}\"}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
        "token_usage": {
          "completion_tokens": 173,
          "prompt_tokens": 97,
          "total_tokens": 270
        },
        "why_needed": "To strip multiline docstrings from Python code."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_multiline_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008818109999992885,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring removal', 'description': 'The function should remove all docstrings, including triple double-quoted ones.'}",
          "{'name': 'no exception raised', 'description': 'If a docstring is found, the function should not raise an exception.'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 106,
          "total_tokens": 237
        },
        "why_needed": "To ensure that the context manager strips all types of docstrings, not just triple double-quoted ones."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_double_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 29,
          "line_ranges": "27, 29, 33, 35-36, 38-45, 47-49, 51-52, 55-56, 58-59, 61-62, 64, 66-69"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008308760000090842,
      "file_path": "tests/test_context_util.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'docstring removal', 'expected': 'def foo():\\n    pass'}"
        ],
        "scenario": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 106,
          "total_tokens": 195
        },
        "why_needed": "The test is necessary because the current implementation does not correctly handle triple single-quoted docstrings."
      },
      "nodeid": "tests/test_context_util.py::TestStripDocstrings::test_strip_triple_single_quoted_docstring",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 19,
          "line_ranges": "134-135, 137-141, 143-144, 476, 478, 524-531"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008017909999864514,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'm1' in models",
          "assert 'm2' in models",
          "assert provider._parse_preferred_models() == [] when config.model == None",
          "assert provider._parse_preferred_models() == [] when config.model == 'All'"
        ],
        "scenario": "Test the parsing of edge cases for Gemini models with different configurations.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 273,
          "total_tokens": 394
        },
        "why_needed": "This test prevents a regression where the gemini model parser fails to parse models when the 'm1, m2' configuration is used."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_model_parsing_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 35,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008226200000081008,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The next_available_in method should return a value greater than 0 if there are available tokens.",
          "The next_available_in method should return 0 if both limits have been reached.",
          "The record_tokens method should not increment the request count."
        ],
        "scenario": "Verify that the rate limiter prevents over and under token limits when recording tokens but not requests.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 273,
          "total_tokens": 387
        },
        "why_needed": "This test prevents a potential bug where the rate limiter allows excessive token usage without penalizing requests."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_gemini_rate_limiter_edge_math",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 47,
          "line_ranges": "96-103, 130-133, 135, 137-139, 141, 143, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007952990000035243,
      "file_path": "tests/test_coverage_boosters.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `coverage_percent` field of `SourceCoverageEntry` is equal to 50.0 when converted to a dictionary key.",
          "The `error` field of `LlmAnnotation` is equal to 'timeout' when converted to a dictionary key.",
          "The `duration` field of `RunMeta` is equal to 1.0 when converted to a dictionary key."
        ],
        "scenario": "Verify that the `to_dict()` method of `SourceCoverageEntry` and `RunMeta` objects returns the expected values.",
        "token_usage": {
          "completion_tokens": 156,
          "prompt_tokens": 318,
          "total_tokens": 474
        },
        "why_needed": "This test prevents regression in coverage booster models where the `coverage_percent` or `duration` fields are not being correctly converted to dictionary keys."
      },
      "nodeid": "tests/test_coverage_boosters.py::TestCoverageBoosters::test_models_to_dict_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007616960000120798,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mapper.config', 'expected_type': 'Config', 'actual_type': 'Config', 'message': ''}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 109,
          "total_tokens": 185
        },
        "why_needed": "To test the initialization of the Mapper with a given configuration."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_create_mapper",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 3,
          "line_ranges": "44-45, 308"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007615959999895949,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert isinstance(warnings, list)', 'expected_result': [1, 2, 3], \"# Expected to be a list of warnings (e.g., from a file or variable with an error message) but actually returns something else (e.g., an empty list or a different type of value). This test ensures that the `get_warnings` method correctly identifies and returns warnings as expected in this scenario. If it doesn't, the test will fail and provide additional context for where the issue is occurring.\": 'message_type', 'Test failed: TestCoverageMapper::test_get_warnings': 'This test has failed because the `get_warnings()` method of the CoverageMapper class does not return a list of warnings as expected. The actual output may vary depending on the configuration and environment, but it should contain at least one warning or an empty list.'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
        "token_usage": {
          "completion_tokens": 239,
          "prompt_tokens": 110,
          "total_tokens": 349
        },
        "why_needed": "To ensure that the `get_warnings` method returns a list of warnings as expected."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_get_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0013799040000037621,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `mapper.map_coverage()` function should return an empty dictionary.",
          "The returned dictionary should be empty.",
          "There should be at least one warning in the `mapper.warnings` list."
        ],
        "scenario": "Test that the `map_coverage` method returns an empty dictionary when no coverage file is found.",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 277,
          "total_tokens": 381
        },
        "why_needed": "Prevents a regression where the test fails to report any coverage information when there are no coverage files."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapper::test_map_coverage_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007871340000065175,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "When including phase 'all', the mapper should extract node IDs for all specified phases.",
          "The mapper should return the same node ID for each phase when included in 'all'.",
          "If `include_phase=all`, the mapper should not include any phase-specific node IDs in the coverage map."
        ],
        "scenario": "The test verifies that the `CoverageMapper` correctly extracts node IDs for all phases when `include_phase=all`.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 279,
          "total_tokens": 410
        },
        "why_needed": "This test prevents a regression where the coverage map might not include all phases if `include_phase=all` is used."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_all_phases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007596430000091914,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'assert mapper._extract_nodeid([]) == None', 'expected_result': 'None'}",
          "{'assertion': 'assert mapper._extract_nodeid(None) == None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 128,
          "total_tokens": 246
        },
        "why_needed": "To handle cases where the context is empty or None, allowing for proper coverage extraction."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_empty_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 216, 220, 224-225, 228-230"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007769650000000183,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"nodeid extraction should be performed for nodeid in 'setup' phase\", 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 139,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the test does not filter out setup phase when including a run phase."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_filters_setup",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007906699999864486,
      "file_path": "tests/test_coverage_map.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'test.py::test_foo', 'actual_value': 'test.py::test_foo|run'}"
        ],
        "scenario": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 145,
          "total_tokens": 233
        },
        "why_needed": "To extract the correct node ID from the run phase context."
      },
      "nodeid": "tests/test_coverage_map.py::TestCoverageMapperContextExtraction::test_extract_nodeid_with_run_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 29,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152, 156, 160-162, 167-170, 199, 202"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.00119894499999873,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should not raise an exception when called with mock_data that has contexts.",
          "The function should return an empty dictionary when called with mock_data that does not have contexts.",
          "The function should increment the call count correctly when calling it on mock_data that raises an exception.",
          "The function should only be called once when calling it on mock_data that has contexts.",
          "The function should handle the exception gracefully and return a valid result.",
          "The function's context side effect should not affect its behavior when called with mock_data that does not have contexts."
        ],
        "scenario": "Test 'test_contexts_by_lineno_exception' verifies that the test_contexts_by_lineno function handles exceptions correctly when calling it on mock data.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 332,
          "total_tokens": 525
        },
        "why_needed": "This test prevents a regression where the test_contexts_by_lineno function fails to handle an exception raised by its context side effect."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_contexts_by_lineno_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 7,
          "line_ranges": "44-45, 118, 121-122, 127-128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009961250000003474,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': {}, 'actual_value': {'__len__': 0}}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 136,
          "total_tokens": 216
        },
        "why_needed": "To test that the function returns an empty dictionary when no measured files are found."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_no_measured_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012105170000040744,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"mock_data.measured_files.return_value == ['file.txt', 'data.json']\", 'expected_result': ['file.txt', 'data.json'], 'message': \"Expected mock_data.measured_files.return_value to be equal to ['file.txt', 'data.json']\"}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 154,
          "total_tokens": 274
        },
        "why_needed": "To ensure that non-Python files are skipped from coverage reports."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestExtractContexts::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 2,
          "line_ranges": "44-45"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007534709999958977,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'coverage.py is installed', 'description': 'The coverage.py module should be present in the environment.'}",
          "{'name': 'coverage.py is not installed', 'description': 'The coverage.py module should not be present in the environment.'}"
        ],
        "scenario": "Test Load Coverage Data",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 166,
          "total_tokens": 264
        },
        "why_needed": "Coverage.py is required for this test."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_coverage_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010343259999956445,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should return None.', 'expected_result': 'None'}",
          "{'description': \"The warnings should have a message containing 'W001'.\", 'expected_result': ['W001']}"
        ],
        "scenario": "TestLoadCoverageData",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 153,
          "total_tokens": 245
        },
        "why_needed": "To test the scenario when no .coverage file exists."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestLoadCoverageData::test_no_coverage_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.001334678999995731,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `map_source_coverage` function should return an empty list when analyzing an exception.",
          "A warning with the message 'COVERAGE_ANALYSIS_FAILED' should be added to the coverage report."
        ],
        "scenario": "Test the exception handling feature when analyzing an exception.",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 286,
          "total_tokens": 388
        },
        "why_needed": "This test prevents regression by ensuring that analysis2 raises an exception and is properly handled, preventing potential warnings from being added to the coverage report."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_analysis_exception_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 18,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259-261, 273-274, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013058149999949364,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Equal to', 'expected_value': [], 'actual_value': []}"
        ],
        "scenario": "tests/test_coverage_map_coverage",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 178,
          "total_tokens": 245
        },
        "why_needed": "To test the coverage map when a file has no statements."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_empty_statements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.001592902999988155,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `map_source_coverage` method returns a list with exactly one element.",
          "The `covered` attribute of the first element in the list is set to 2 (indicating that two lines were covered).",
          "The `missed` attribute of the first element in the list is set to 1 (indicating that one line was missed)."
        ],
        "scenario": "Test that test files are included when omit_tests_from_coverage is False.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 322,
          "total_tokens": 458
        },
        "why_needed": "This test prevents a regression where test coverage is not reported for test files when omitting tests from coverage is enabled."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_include_test_files_when_not_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 10,
          "line_ranges": "44-45, 243-244, 246-249, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0018221429999982774,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'expected_result', 'type': 'list', 'description': 'Expected result of the test'}"
        ],
        "scenario": "{'id': 'tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_non_python_files', 'description': 'Test case for test_skip_non_python_files'}",
        "token_usage": {
          "completion_tokens": 173,
          "prompt_tokens": 154,
          "total_tokens": 327
        },
        "why_needed": "{'reason': 'Ensure that non-Python files are skipped from coverage reports.', 'description': 'This test is necessary to ensure that non-Python files are excluded from coverage reports.'}"
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_non_python_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-255, 257, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0012456219999990026,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': [], 'actual_result': []}"
        ],
        "scenario": "Test Map Source Coverage",
        "token_usage": {
          "completion_tokens": 59,
          "prompt_tokens": 182,
          "total_tokens": 241
        },
        "why_needed": "To ensure that test files are skipped when omit_tests_from_coverage is True."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestMapSourceCoverage::test_skip_test_files_when_configured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 11,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008287919999929727,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') == 'test_foo.py::test_bar'"
        ],
        "scenario": "Test that all phases are accepted when configured.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 305,
          "total_tokens": 439
        },
        "why_needed": "Prevents regression in case of phase filtering, where only specific phases should be covered."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_all_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007596720000151436,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 115,
          "total_tokens": 189
        },
        "why_needed": "To handle empty string inputs and ensure correct output."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 4,
          "line_ranges": "44-45, 216-217"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007850799999857827,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert mapper._extract_nodeid(None) == None', 'description': 'The `_extract_nodeid` method should return None for a None input.'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 114,
          "total_tokens": 217
        },
        "why_needed": "To ensure that the `extract_nodeid` method returns None when an invalid input (None) is provided."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007867230000044856,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_nodeid should return the nodeid when the phase matches 'run'.",
          "The function _extract_nodeid should return None when the phase doesn't match 'run' or 'setup'.",
          "The function _extract_nodeid should return None when the phase doesn't match 'teardown'."
        ],
        "scenario": "Test that run phase is the default filter.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 297,
          "total_tokens": 413
        },
        "why_needed": "Prevents a regression where the test would fail due to an incorrect default configuration."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_run_phase_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008036149999952613,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') is None"
        ],
        "scenario": "Test that setup phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 293,
          "total_tokens": 418
        },
        "why_needed": "Prevents a bug where the test would incorrectly filter out nodes in the setup phase due to incorrect configuration."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_setup_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231, 233-234, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000760744999979579,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mapper._extract_nodeid('test_foo.py::test_bar|teardown') == 'test_foo.py::test_bar'",
          "mapper._extract_nodeid('test_foo.py::test_bar|run') is None",
          "mapper._extract_nodeid('test_foo.py::test_bar|setup') is None"
        ],
        "scenario": "Test that teardown phase is correctly filtered when configured.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 296,
          "total_tokens": 425
        },
        "why_needed": "Prevents a regression where the 'teardown' phase is not properly filtered, potentially leading to false positives in coverage reports."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_teardown_phase_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 6,
          "line_ranges": "44-45, 216, 220, 224, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000744254000011324,
      "file_path": "tests/test_coverage_map_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'equality', 'expected_value': 'test_foo.py::test_bar'}"
        ],
        "scenario": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 136,
          "total_tokens": 220
        },
        "why_needed": "To test the scenario where a node id does not contain a pipe delimiter."
      },
      "nodeid": "tests/test_coverage_map_coverage.py::TestPhaseFiltering::test_extract_nodeid_without_pipe",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 57,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 137-140, 144, 148, 150, 152-153, 156, 160-163, 165, 167-168, 173, 176, 178-184, 187-189, 191-194, 196, 199-200, 202, 216, 220, 224-225, 228-229, 231, 233, 236"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 13,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65-67"
        }
      ],
      "duration": 0.0015259669999920789,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `test_one` context should be present in the coverage report for `app.py`.",
          "The number of lines in `test_one` should match the expected count (2).",
          "Each line in `test_one` should have a unique file path that includes `app.py`.",
          "If `test_two` is not present, then `test_one` should be included in the coverage report.",
          "The context for `test_two` should not include any lines from `app.py`.",
          "Each line in `test_two` should have a unique file path that includes `app.py`.",
          "If `test_one` is not present, then `test_two` should be included in the coverage report.",
          "The context for `test_two` should include all lines from `app.py`.",
          "Each line in `test_two` should have a unique file path that includes `app.py`.",
          "If `test_one` is not present, then `test_two` should be included in the coverage report.",
          "The context for `test_two` should include all lines from `app.py`.",
          "Each line in `test_two` should have a unique file path that includes `app.py`.",
          "]"
        ],
        "scenario": "Test that the `extract_contexts` method of `CoverageMapper` correctly extracts contexts for all paths in `_extract_contexts` when coverage is enabled.",
        "token_usage": {
          "completion_tokens": 345,
          "prompt_tokens": 413,
          "total_tokens": 758
        },
        "why_needed": "This test prevents a regression where the coverage map does not include all contexts for a specific file, potentially leading to incomplete or inaccurate coverage analysis."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_full_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 118, 121-122, 127, 131-135, 144-146"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012355630000229212,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is', 'expected_value': {}, 'actual_value': 'result'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 174,
          "total_tokens": 261
        },
        "why_needed": "To test the coverage mapper's behavior when there are no test contexts."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_contexts_no_contexts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 14,
          "line_ranges": "44-45, 216, 220, 224-225, 228-229, 231-234, 236, 239"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007703829999741174,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _extract_nodeid returns the correct node ID for a given phase.",
          "The function _extract_nodeid ignores the 'run' phase and its corresponding node ID.",
          "The function _extract_nodeid correctly handles coverage reports without any phases specified.",
          "The function _extract_nodeid ignores context files that do not match any phase.",
          "The function _extract_nodeid returns the correct node ID for a given phase, even when no phase is included in the report."
        ],
        "scenario": "Test the ability to extract node IDs from coverage reports when no phases are specified.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 323,
          "total_tokens": 480
        },
        "why_needed": "This test prevents regression in coverage reporting when certain phases are not included in the report."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_extract_nodeid_variants",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 9,
          "line_ranges": "44-45, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000976628000017854,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return None for _load_coverage_data() when no .coverage files exist.",
          "The number of warnings should be exactly 1.",
          "The first warning code should be 'W001'."
        ],
        "scenario": "Test that the test_load_coverage_data_no_files function correctly handles cases where no coverage files exist.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 276,
          "total_tokens": 387
        },
        "why_needed": "This test prevents a potential bug or regression by ensuring that the function does not silently fail when there are no coverage files."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_no_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 17,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94-96, 107-111, 114"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014606750000041302,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `mapper._load_coverage_data()` returns None if an error occurs while reading the coverage file.",
          "Any warnings generated by the mapper are marked as indicating that the coverage data failed to be read.",
          "The test verifies that any messages containing 'Failed to read coverage data' are present in the warnings list.",
          "The test ensures that the coverage module's `CoverageData` class is mocked with an exception when it raises an error during reading.",
          "The test verifies that the `CoverageMapper` instance returns None after attempting to load coverage data from a corrupted file."
        ],
        "scenario": "Test should handle errors reading coverage files and prevent regression.",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 343,
          "total_tokens": 521
        },
        "why_needed": "This test prevents a potential regression where the CoverageMapper class does not properly handle errors when loading coverage data from corrupted files."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_read_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 15,
          "line_ranges": "44-45, 72-73, 83, 86, 88, 92, 94, 98, 101-104, 106"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002394643000002361,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock CoverageData instance returned by _load_coverage_data() should have been updated at least twice.",
          "The mock CoverageData instances should not be None after calling update.",
          "The mock CoverageData instances should be of the same type (CoverageData) before and after calling update.",
          "The mock CoverageData instances should have different attributes (update_count, data) before and after calling update.",
          "The _load_coverage_data() function should return a new instance of CoverageData or None if no coverage files are available.",
          "The _load_coverage_data() function should call the update method on the mock CoverageData instances with at least two calls.",
          "The _load_coverage_data() function should not raise an exception when called multiple times without any changes to the CoverageData instances."
        ],
        "scenario": "Test should handle parallel coverage files from xdist and verify that it correctly updates the CoverageData instances.",
        "token_usage": {
          "completion_tokens": 236,
          "prompt_tokens": 378,
          "total_tokens": 614
        },
        "why_needed": "This test prevents regression in handling parallel coverage files, ensuring that the CoverageMapperMaximal class can correctly update its internal data structures when dealing with such scenarios."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_load_coverage_data_with_parallel_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 5,
          "line_ranges": "44-45, 58-60"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010252000000150474,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_load_coverage_data` method should return `None` when called without any arguments.",
          "The `map_coverage` method should return an empty dictionary when the input is `None` or an empty list.",
          "The `map_coverage` method should not raise an exception when given a non-empty list of coverage data.",
          "The `map_coverage` method should preserve the original order of metrics in case of duplicate values.",
          "The `_load_coverage_data` method should be able to handle cases where the input is a dictionary with no keys.",
          "The `map_coverage` method should not throw an exception when given a non-list value for coverage data.",
          "The `map_coverage` method should return the original list of metrics if it's already present in the result."
        ],
        "scenario": "Test the `map_coverage` method with _load_coverage_data returning None.",
        "token_usage": {
          "completion_tokens": 217,
          "prompt_tokens": 228,
          "total_tokens": 445
        },
        "why_needed": "Prevents a potential bug where an empty dictionary is returned when there's no coverage data."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_coverage_no_data",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 22,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252-254, 259, 261, 263-268, 271, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0013588350000190985,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The mock_cov.analysis2 should be called with an Exception exception.",
          "mock_data.measured_files.return_value should return ['app.py']",
          "mock_cov.get_data.return_value should raise a MagicMock exception.",
          "entries should not contain any files that have errors.",
          "len(entries) should equal 0 after skipping error files."
        ],
        "scenario": "Test that the CoverageMapper handles analysis errors correctly.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 274,
          "total_tokens": 399
        },
        "why_needed": "This test prevents a potential regression where an error during coverage analysis would prevent the mapper from working as expected."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_analysis_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 32,
          "line_ranges": "44-45, 243-244, 246-248, 250, 252, 259-261, 273, 276-279, 281-283, 285-293, 295, 299-300"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 17,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64, 100, 103, 111-112, 116, 123"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.001519784999999274,
      "file_path": "tests/test_coverage_map_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file path of the mapped entry should be 'app.py'.",
          "The number of statements in the mapped entry should be 3.",
          "The coverage percentage of the mapped entry should be 66.67%.",
          "All files in the source directory should be covered by the test.",
          "At least one file in the source directory should not be missed by the test.",
          "The coverage percentage for each file in the source directory should be calculated correctly."
        ],
        "scenario": "Verify that the test maps all source files in map_source_coverage with a comprehensive coverage percentage.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 345,
          "total_tokens": 504
        },
        "why_needed": "This test prevents regression by ensuring that all source files are covered, even if analysis2 is not."
      },
      "nodeid": "tests/test_coverage_map_maximal.py::TestCoverageMapperMaximal::test_map_source_coverage_comprehensive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007716750000099637,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The returned warning code is correct (WarningCode.W001_NO_COVERAGE).",
          "The message of the warning is 'No .coverage file found'.",
          "The detail of the warning is 'test-detail'."
        ],
        "scenario": "Test the make_warning factory function to ensure it correctly returns a WarningCode.W001_NO_COVERAGE instance with the expected message and detail.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 236,
          "total_tokens": 365
        },
        "why_needed": "To prevent unexpected warnings when no coverage files are found, this test verifies that the make_warning factory function returns an appropriate WarningCode.W001_NO_COVERAGE instance."
      },
      "nodeid": "tests/test_errors.py::test_make_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007568979999916792,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'assert WarningCode.W001_NO_COVERAGE.value == \"W001\"', 'description': 'Correct value for W001_NO_COVERAGE'}",
          "{'message': 'assert WarningCode.W101_LLM_ENABLED.value == \"W101\"', 'description': 'Correct value for W101_LLM_ENABLED'}",
          "{'message': 'assert WarningCode.W201_OUTPUT_PATH_INVALID.value == \"W201\"', 'description': 'Correct value for W201'}",
          "{'message': 'assert WarningCode.W301_INVALID_CONFIG.value == \"W301\"', 'description': 'Correct value for W301'}",
          "{'message': 'assert WarningCode.W401_AGGREGATE_DIR_MISSING.value == \"W401\"', 'description': 'Correct value for W401'}"
        ],
        "scenario": "Test that warning codes have correct values.",
        "token_usage": {
          "completion_tokens": 206,
          "prompt_tokens": 240,
          "total_tokens": 446
        },
        "why_needed": "This test prevents a regression where the warning code values are incorrect, potentially causing issues with the application's functionality."
      },
      "nodeid": "tests/test_errors.py::test_warning_code_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007594019999999091,
      "file_path": "tests/test_errors.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'detail' key is present in the dictionary and its value matches the expected string.",
          "The 'code' key matches the expected string.",
          "The 'message' key matches the expected string.",
          "The 'no_detail' key is not present in the dictionary or its value does not match the expected string.",
          "The 'W001_NO_COVERAGE' code matches the expected string.",
          "The 'W101_LLM_ENABLED' code matches the expected string."
        ],
        "scenario": "Test ReportWarning.to_dict() method.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 276,
          "total_tokens": 419
        },
        "why_needed": "Prevents a potential warning that could be misleading or incorrect."
      },
      "nodeid": "tests/test_errors.py::test_warning_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007525289999819051,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `make_warning` returns an instance of `WarningCode.W101_LLM_ENABLED`.",
          "The message associated with the warning is set to `WARNING_MESSAGES[WarningCode.W101_LLM_ENABLED]`.",
          "The detail attribute of the warning is None."
        ],
        "scenario": "Test the `make_warning` function with known code.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 222,
          "total_tokens": 331
        },
        "why_needed": "Prevents a warning from being generated for known code that should not be warned about."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_known_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007420389999879262,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'missing_code', 'expected_value': 'WarningCode.W001_NO_COVERAGE'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 202,
          "total_tokens": 286
        },
        "why_needed": "To test the functionality of using a fallback message for unknown code (if enum allowed it)"
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_unknown_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007353370000089399,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'code', 'value': 'WarningCode.W301_INVALID_CONFIG'}",
          "{'name': 'detail', 'value': 'Bad value'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 127,
          "total_tokens": 213
        },
        "why_needed": "To test that a warning is created with the correct code and detail."
      },
      "nodeid": "tests/test_errors_maximal.py::TestMakeWarning::test_make_warning_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007929040000078658,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert isinstance(code.value, str)', 'expected_result': 'True'}",
          "{'name': \"assert code.value.startswith('W')\", 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 109,
          "total_tokens": 210
        },
        "why_needed": "The test is checking if the WarningCode enum values are strings."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningCodes::test_codes_are_strings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007482210000091527,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'data', 'expected_value': {'code': 'W001', 'message': 'No coverage'}, 'actual_value': {'code': 'W001', 'message': 'No coverage'}}",
          "{'name': 'type', 'expected_value': 'dict', 'actual_value': 'dict'}"
        ],
        "scenario": "Tests for error handling and serialization of Warning objects.",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 147,
          "total_tokens": 280
        },
        "why_needed": "The test is necessary to ensure that the warning object can be serialized correctly without any additional details."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_no_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007453550000207088,
      "file_path": "tests/test_errors_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"data == {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}\", 'expected_result': {'code': 'W001', 'message': 'No coverage', 'detail': 'Check setup'}, 'actual_result': 'data'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 161,
          "total_tokens": 285
        },
        "why_needed": "To test the serialization of warning data to a dictionary with detail."
      },
      "nodeid": "tests/test_errors_maximal.py::TestWarningDataClass::test_warning_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007474699999932,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result for foo/bar.txt', 'type': 'assertion', 'value': 'False'}",
          "{'name': 'Expected result for foo/bar.pyc', 'type': 'assertion', 'value': 'False'}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 115,
          "total_tokens": 234
        },
        "why_needed": "This test ensures that the `is_python_file` function correctly identifies non-.py files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_non_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007272209999769075,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected the function to return True for .py files', 'type': 'assertion'}",
          "{'message': 'The function is returning False for foo/bar.py', 'type': 'expected_result'}"
        ],
        "scenario": "tests/test_fs.py::TestIsPythonFile::test_python_file",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 98,
          "total_tokens": 197
        },
        "why_needed": "The function `is_python_file` should be able to identify `.py` files."
      },
      "nodeid": "tests/test_fs.py::TestIsPythonFile::test_python_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.0010753030000216768,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'path', 'expected_result': '/subdir/file.py', 'actual_result': 'subdir/file.py'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 144,
          "total_tokens": 236
        },
        "why_needed": "To ensure that the `make_relative` function correctly makes a path relative to the test directory."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_makes_path_relative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.000732170999981463,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 107,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the `make_relative` function returns a normalized path when no base is provided."
      },
      "nodeid": "tests/test_fs.py::TestMakeRelative::test_returns_normalized_with_no_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007271419999881346,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert normalize path returns original path for already normalized paths', 'expected_value': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 96,
          "total_tokens": 177
        },
        "why_needed": "To ensure that the `normalize_path` function correctly handles already-normalized paths."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_already_normalized",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007605549999993855,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'foo\\\\bar', 'expected': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 100,
          "total_tokens": 174
        },
        "why_needed": "To ensure that the `normalize_path` function correctly handles paths with forward slashes."
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_forward_slashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007648719999906461,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'foo/bar/', 'expected': 'foo/bar'}"
        ],
        "scenario": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
        "token_usage": {
          "completion_tokens": 67,
          "prompt_tokens": 102,
          "total_tokens": 169
        },
        "why_needed": "strips trailing slash from path"
      },
      "nodeid": "tests/test_fs.py::TestNormalizePath::test_strips_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.0007858520000070257,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should_skip_path', 'expected_result': True}",
          "{'name': 'assert', 'message': \"Should return True for should_skip_path('tests/conftest.py', exclude_patterns=['test*'])\"}",
          "{'name': 'assert', 'message': \"Should return False for should_skip_path('src/module.py', exclude_patterns=['test*'])\"}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 126,
          "total_tokens": 269
        },
        "why_needed": "To ensure that custom patterns are correctly excluded from the test directory."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_custom_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007493130000000292,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert should_skip_path returns False for src/module.py', 'expected_value': False, 'actual_value': 'tests/test_fs.py::TestShouldSkipPath::test_normal_path'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 96,
          "total_tokens": 199
        },
        "why_needed": "The test is checking if the `should_skip_path` function correctly handles normal file system paths."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_normal_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007248969999977817,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert should_skip_path returns True for .git/objects/foo', 'expected': True, 'actual': 'True'}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 99,
          "total_tokens": 188
        },
        "why_needed": "The current implementation of `should_skip_path` does not correctly handle Git directories."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007414079999819023,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert should_skip_path returns True for __pycache__/bar.pyc', 'expected_result': True}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 109,
          "total_tokens": 203
        },
        "why_needed": "This test case is needed because the `should_skip_path` function does not currently handle __pycache__ directories."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007781570000133797,
      "file_path": "tests/test_fs.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert should_skip_path returns True for venv directory', 'description': 'Expected the `should_skip_path` function to return True when given a path that is a venv directory.', 'value': True}",
          "{'name': 'assert should_skip_path returns True for .venv directory', 'description': 'Expected the `should_skip_path` function to return True when given a path that is a .venv directory.', 'value': True}"
        ],
        "scenario": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
        "token_usage": {
          "completion_tokens": 171,
          "prompt_tokens": 121,
          "total_tokens": 292
        },
        "why_needed": "The test is checking if the `should_skip_path` function correctly identifies venv directories as being to be skipped."
      },
      "nodeid": "tests/test_fs.py::TestShouldSkipPath::test_skips_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007515180000154942,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert is_python_file('module.txt') is False",
          "assert is_python_file('module.pyc') is False",
          "assert is_python_file('module') is False"
        ],
        "scenario": "Testing that non-.py files do not return True.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 210,
          "total_tokens": 307
        },
        "why_needed": "Prevents a false positive assertion in the `is_python_file` method where it incorrectly returns True for non-python files."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_false",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 1,
          "line_ranges": "79"
        }
      ],
      "duration": 0.0007385419999934584,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert is_python_file('module.py') is True",
          "assert is_python_file('path/to/module.py') is True",
          "assert is_python_file(Path('module.py')) is True"
        ],
        "scenario": "Verifies that a module file (.py) returns True.",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 212,
          "total_tokens": 303
        },
        "why_needed": "Prevents the test from incorrectly identifying non-python files as Python files."
      },
      "nodeid": "tests/test_fs_coverage.py::TestIsPythonFile::test_is_python_file_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 12,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63, 65, 67"
        }
      ],
      "duration": 0.0011705820000145195,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The test verifies that make_relative returns a normalized absolute path even if the input path is not relative to the base.",
          "The test verifies that the 'project1' and 'file.py' strings are present in the returned path.",
          "The test verifies that the result of make_relative does not contain any '..' or '..\\",
          "The test verifies that the result of make_relative is a string (not a file object) as expected.",
          "The test verifies that the parent directory of the input paths is preserved in the output path.",
          "The test verifies that the relative_to parameter is not used to resolve the issue.",
          "The test verifies that the make_relative function behaves correctly when given unrelated paths."
        ],
        "scenario": "Test makes a relative path not under the base directory.",
        "token_usage": {
          "completion_tokens": 203,
          "prompt_tokens": 301,
          "total_tokens": 504
        },
        "why_needed": "Prevents regression when make_relative is called with paths that are not relative to the base directory."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_path_not_under_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 55, 58-60, 63-64"
        }
      ],
      "duration": 0.00105778099998588,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_result': 'subdir/file.py', 'actual_result': 'subdir/file.py'}"
        ],
        "scenario": "Test Make Relative",
        "token_usage": {
          "completion_tokens": 63,
          "prompt_tokens": 147,
          "total_tokens": 210
        },
        "why_needed": "To test the functionality of making relative paths correctly."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 7,
          "line_ranges": "30, 33, 36, 39, 42, 55-56"
        }
      ],
      "duration": 0.000744733999994196,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': \"Expected result to be 'path/to/file.py'\", 'expected_result': 'path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 116,
          "total_tokens": 209
        },
        "why_needed": "The test is necessary because the `make_relative` function does not handle cases where the base path is None correctly."
      },
      "nodeid": "tests/test_fs_coverage.py::TestMakeRelative::test_make_relative_with_none_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007501250000245818,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The normalized path contains a single forward slash.', 'expected_result': '/path/to/file.py'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 114,
          "total_tokens": 198
        },
        "why_needed": "To ensure that backslashes are correctly converted to forward slashes in file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_backslashes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0008396519999962493,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'path/to/file.py', 'actual_value': \"normalize_path(Path('path/to/file.py'))\"}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 110,
          "total_tokens": 196
        },
        "why_needed": "Normalization of a file path object is necessary to ensure correct behavior in certain scenarios."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_path_object",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 5,
          "line_ranges": "30, 33, 36, 39, 42"
        }
      ],
      "duration": 0.0007389030000126695,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result is equal to expected value', 'expected_value': 'path/to/dir', 'actual_value': 'path/to/dir'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 111,
          "total_tokens": 206
        },
        "why_needed": "To ensure that the `normalize_path` function correctly removes trailing slashes from file paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestNormalizePath::test_normalize_path_trailing_slash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 11,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116, 123"
        }
      ],
      "duration": 0.0007375309999986257,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'is False', 'expected_value': False}",
          "{'assertion_type': 'is False', 'expected_value': False}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 120,
          "total_tokens": 211
        },
        "why_needed": "Regular paths are not skipped by default."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_not_skip_regular_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007310789999905865,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'should be True', 'description': 'The function should return True when a .git directory is found.'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 102,
          "total_tokens": 193
        },
        "why_needed": "The test should skip .git directories because they contain Git hooks that can interfere with the file system coverage."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_git",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007483509999985927,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'should be True', 'expected_value': True}",
          "{'message': '.venv', 'expected_value': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 124,
          "total_tokens": 212
        },
        "why_needed": "To ensure that the function correctly handles paths starting with a skip directory name."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_path_starting_with_skip_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007627180000042699,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'src/__pycache__/module.cpython-312.pyc'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 116,
          "total_tokens": 191
        },
        "why_needed": "Because the test module __pycache__ is being tested."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_pycache",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007470890000149666,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"should_skip_path('/usr/lib/python3.12/site-packages/pkg/mod.py')\", 'expected_result': 'True'}"
        ],
        "scenario": "/usr/lib/python3.12/site-packages/pkg/mod.py",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 111,
          "total_tokens": 202
        },
        "why_needed": "The test is needed because it checks for site-packages directories, which are not covered by the current implementation."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_site_packages",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 10,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-113"
        }
      ],
      "duration": 0.0007461769999963508,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'path': 'venv/lib/python3.12/site.py', 'expected_result': True}",
          "{'path': '.venv/lib/python3.12/site.py', 'expected_result': True}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 130,
          "total_tokens": 243
        },
        "why_needed": "Because the 'venv' directory contains a Python package (site.py) which should be skipped."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_venv",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/fs.py",
          "line_count": 15,
          "line_ranges": "30, 33, 36, 39, 42, 100, 103, 111-112, 116-117, 119-121, 123"
        }
      ],
      "duration": 0.0016247719999853416,
      "file_path": "tests/test_fs_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'should_skip_path is called with the correct arguments', 'expected_result': 'True'}",
          "{'assertion': 'should_skip_path returns False for a path that matches an exclude pattern', 'expected_result': 'False'}"
        ],
        "scenario": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 132,
          "total_tokens": 243
        },
        "why_needed": "Custom exclude patterns are needed to skip certain paths."
      },
      "nodeid": "tests/test_fs_coverage.py::TestShouldSkipPath::test_should_skip_with_exclude_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 50,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-227, 232-233, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0031155270000056134,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'Gemini requests-per-day limit reached' in res.error",
          "assert mock_limiter.next_available_in.return_value == None",
          "assert provider._rate_limiters['m1'].next_available_in.return_value == None",
          "assert mock_limiter.next_available_in.return_value == None",
          "assert provider._rate_limiters['m1'].next_available_in.return_value == None",
          "assert mock_limiter.next_available_in.return_value is None",
          "assert mock_limiter.next_available_in.return_value == None"
        ],
        "scenario": "Test that the test_annotate_loop_daily_limit_hit function prevents a regression where the daily limit is hit and the Gemini API returns an error.",
        "token_usage": {
          "completion_tokens": 197,
          "prompt_tokens": 367,
          "total_tokens": 564
        },
        "why_needed": "This test prevents the regression where the daily limit is hit, causing the Gemini API to return an error when trying to annotate internal nodes with the 'src' nodeid."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotate_loop_daily_limit_hit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 100,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-236, 239-244, 263-265, 268, 293, 295, 299-303, 318-320, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002742378000021972,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Mocking the _GeminiRateLimiter to prevent RateLimitExceeded RPD (Line 300)",
          "_CallGemini to raise _GeminiRateLimitExceeded when RateLimitExceeded RPD is encountered",
          "Mocking the _ModelExhaustedAt dictionary to track model exhaustion and return an error message",
          "Asserting that exceptions are correctly propagated through the provider's methods",
          "Verifying that the correct exception messages are returned in the error object"
        ],
        "scenario": "Tests the coverage of exceptions when using GeminiProvider with a mock configuration.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 730,
          "total_tokens": 896
        },
        "why_needed": "This test prevents regression that occurs when using GeminiProvider with a mock configuration and encountering exceptions, such as generation failures or rate limit exceeded errors."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_annotation_exceptions_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 173,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181-182, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 254-255, 259, 340, 343, 346, 348-356, 358-361, 363-364, 366-367, 435, 437-439, 441-442, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-498, 502-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-564, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.1673053909999851,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The _rate_limiters dictionary should contain 'm1' with a value of 100.",
          "_parse_rate_limits method returns a dictionary with the correct requests per day limit (100).",
          "The provider._models attribute contains the expected list of fallback models ('fallback')."
        ],
        "scenario": "Prevents regression in coverage gaps by ensuring proper rate limiting and annotation logic.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 821,
          "total_tokens": 946
        },
        "why_needed": "This test prevents a potential regression where the GeminiProvider's rate limiting and annotation logic may not be properly implemented, leading to uncovered coverage gaps."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_coverage_gaps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 13,
          "line_ranges": "134-135, 137-141, 143-144, 524-527"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008221909999974741,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 156,
          "total_tokens": 354
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_parse_preferred_models_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 10,
          "line_ranges": "39-42, 81-82, 84, 87-89"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007953699999916353,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert len(_GeminiRateLimiter._daily_requests) == 0', 'description': 'The length of _GeminiRateLimiter._daily_requests should be 0 after pruning.'}"
        ],
        "scenario": "TestGeminiProvider",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 157,
          "total_tokens": 247
        },
        "why_needed": "To test the coverage gaps of Gemini provider after pruning daily requests."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_prune_daily_requests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 4,
          "line_ranges": "39-42"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0016512939999984155,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total tokens used should be less than or equal to the limit (100).",
          "If `request_tokens` exceeds the remaining tokens, the loop should not return.",
          "The `remaining` variable should decrease as the tokens are used.",
          "The GeminiRateLimiter should not allow a huge token usage that implies it cannot fit all requests.",
          "If `request_tokens` is massive and `limit=100`, the test should pass without returning 0.0 at line 106/108."
        ],
        "scenario": "Test verifies that the GeminiRateLimiter does not allow excessive token usage to prevent TPM fallback.",
        "token_usage": {
          "completion_tokens": 186,
          "prompt_tokens": 524,
          "total_tokens": 710
        },
        "why_needed": "This test prevents regression in case of extremely high token usage, causing the GeminiRateLimiter to return 0.0 at line 106/108 when a huge request is made."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiCoverageGaps::test_tpm_available_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 14,
          "line_ranges": "134-135, 137-141, 143-144, 164-165, 167-169"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008605520000060096,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation should contain an error message indicating that google-generativeai was not installed.",
          "The error message should include the exact string 'google-generativeai not installed'.",
          "The annotation should be able to distinguish between a forced import error and a normal import operation.",
          "The annotation should not report any other errors when the dependency is installed.",
          "The annotation should provide a clear indication of why the annotation failed (in this case, due to missing dependencies).",
          "The annotation should include all necessary information about the test environment (e.g., module flagging, test result)."
        ],
        "scenario": "Test that a forced import error is annotated correctly when google-generativeai is not installed.",
        "token_usage": {
          "completion_tokens": 186,
          "prompt_tokens": 259,
          "total_tokens": 445
        },
        "why_needed": "This test prevents a potential regression where the GeminiProvider does not correctly handle import errors due to missing dependencies."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_import_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0027502490000017588,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'GEMINI_API_TOKEN' key should be present in the annotation error message.",
          "The 'GEMINI_API_TOKEN' key should contain the value 'None'.",
          "The 'GEMINI_API_TOKEN' key should not be empty.",
          "The 'GEMINI_API_TOKEN' key should have a non-empty string value.",
          "The 'GEMINI_API_TOKEN' key should not be None.",
          "The 'GEMINI_API_TOKEN' key should contain the correct type (str).",
          "The 'GEMINI_API_TOKEN' key should not be an empty string.",
          "The 'GEMINI_API_TOKEN' key should have a non-empty string value.",
          "</key_assertions>"
        ],
        "scenario": "Test annotation when token is missing.",
        "token_usage": {
          "completion_tokens": 203,
          "prompt_tokens": 313,
          "total_tokens": 516
        },
        "why_needed": "To prevent a potential bug where the Gemini API token is not set, and thus the annotation fails."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_no_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 214,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-237, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004511686999990161,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by the provider matches the expected scenario of 'Recovered Scenario'.",
          "The mock post call count is correct, indicating that the retry mechanism was successful.",
          "The _parse_response method returns a Mock object with the correct scenario and error.",
          "The provider's internal annotation logic does not modify the test result nodeid.",
          "The provider's internal annotation logic does not change the expected outcome of the test.",
          "The provider's internal annotation logic does not add any new assertions or checks.",
          "The provider's internal annotation logic does not remove any existing assertions or checks.",
          "The provider's internal annotation logic does not modify the mock post call count."
        ],
        "scenario": "Test that the GeminiProvider annotates a rate limit retry scenario correctly.",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 636,
          "total_tokens": 832
        },
        "why_needed": "This test prevents regression in the GeminiProvider's ability to handle rate limit retries."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_rate_limit_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 19,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 208,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-568, 574"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.3919080089999909,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by _annotate_internal has the expected scenario.",
          "The annotation does not have any errors.",
          "The annotation correctly extracts the 'text' part from the response.",
          "The annotation correctly extracts the 'tokens' part from the response.",
          "The annotation returns a valid LlmAnnotation object with the correct key assertions.",
          "The annotation does not return an error when the response is in the expected format.",
          "The annotation calls _parse_response correctly to extract the scenario and tokens.",
          "</key_assertions>"
        ],
        "scenario": "Test that the GeminiProvider correctly annotates a success scenario with the correct key assertions.",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 649,
          "total_tokens": 819
        },
        "why_needed": "This test prevents regression where the provider incorrectly returns an error when annotating a successful response."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_annotate_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 12,
          "line_ranges": "134-135, 137-141, 143-144, 332-333, 335"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0024879980000207524,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should return False indicating that the environment is not available.",
          "The provider should not throw any exceptions if the environment is already available.",
          "The provider's _check_availability method should be able to detect when the environment is not available.",
          "The provider's _check_availability method should not make any assumptions about the availability of the environment based on the API token.",
          "The provider's _check_availability method should only return False if the environment is truly unavailable.",
          "The provider's _check_availability method should be able to handle cases where the environment has been previously checked and found available.",
          "The provider's _check_availability method should not interfere with other tests that rely on the environment being available."
        ],
        "scenario": "Tests the availability of the Gemini provider when no API token is provided.",
        "token_usage": {
          "completion_tokens": 203,
          "prompt_tokens": 235,
          "total_tokens": 438
        },
        "why_needed": "Prevents a bug where the provider tries to access an unavailable environment."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProvider::test_availability",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 111,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 221-224, 228-230, 232-233, 235-237, 239-244, 263-265, 268, 272-276, 279-281, 283-286, 288-292, 318-320, 322-323, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 60.00317919399998,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider._model_exhausted_at` dictionary is updated correctly with the model ID 'm1' after an exception occurs.",
          "The `provider._cooldowns` dictionary contains the expected cooldown value for model 'm1'.",
          "The cooldown value for model 'm1' exceeds 5.5 seconds as expected."
        ],
        "scenario": "Verify that the GeminiProvider class correctly annotates retry exceptions and updates model exhaustion states accordingly.",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 651,
          "total_tokens": 790
        },
        "why_needed": "This test prevents regression where the GeminiProvider class fails to properly handle retry exceptions and update model exhaustion states when encountering a resource exhausted error."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_exceptions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 27,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 97,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-94, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 254, 259, 340, 343, 471-473"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0031921270000054847,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `_model_exhausted_at` attribute is updated correctly after a retry loop.",
          "The `GeminiProvider` ensures that models are exhausted within the specified time frame (24h+).",
          "The test maintains coverage by ensuring that the model's exhaustion check passes even when the Gemini API token limit exceeds the daily limit."
        ],
        "scenario": "Test that the `GeminiProvider` ensures models are exhausted after a retry loop and coverage is maintained.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 482,
          "total_tokens": 626
        },
        "why_needed": "This test prevents regression where the Gemini API token limit exceeds the daily limit of 24 hours ago, causing the model to be exhausted without sufficient retries."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_annotate_retry_loop_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-356, 358-361, 363-364, 366-367"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010914229999912095,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocked config has rpm=10', 'expected_value': 10, 'actual_value': 0}",
          "{'name': 'limits.requests_per_minute == 10', 'expected_value': 10, 'actual_value': 0}"
        ],
        "scenario": "Test GeminiProvider::test_ensure_rate_limits_error",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 156,
          "total_tokens": 281
        },
        "why_needed": "This test ensures that the `GeminiProvider` raises an error when rate limiting is attempted with a non-integer value."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_ensure_rate_limits_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009666909999737072,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'models are empty', 'description': 'The list of available models should be empty after a network error.', 'expected_result': [], 'actual_result': 'models == []'}",
          "{'name': 'limit_map is empty', 'description': 'The limit map should be empty after a network error.', 'expected_result': {}, 'actual_result': 'limit_map == {}'}"
        ],
        "scenario": "Test fetch available models with an exception",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 132,
          "total_tokens": 271
        },
        "why_needed": "To test the error handling of GeminiProvider when a network error occurs"
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 476-477, 537, 539-543, 547-548, 550-559, 562-563, 567, 569, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001490020000005643,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "models containing non-list supportedGenerationMethods",
          "models not containing inputTokenLimit",
          "model m3 in limit_map",
          "model m1 and model m2 not in models",
          "model m3 in limit_map"
        ],
        "scenario": "Test that a fetch of available models with invalid JSON returns the expected result.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 340,
          "total_tokens": 449
        },
        "why_needed": "Prevents a potential bug where an incorrect or malformed JSON is returned, potentially causing unexpected behavior or errors."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_fetch_available_models_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 3,
          "line_ranges": "65-66, 163"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 15,
          "line_ranges": "134-135, 137-141, 143-144, 486, 488-491, 493"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002013811000011856,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_ensure is called once', 'description': 'The `mock_ensure` function should be called exactly once during the test.', 'condition': 'mock_ensure.call_count == 1'}"
        ],
        "scenario": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 144,
          "total_tokens": 268
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method of the GeminiProvider class calls the mock function `mock_ensure` when necessary."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_get_max_context_tokens_calls_ensure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "134-135, 137-141, 143-144, 449-457, 459-460, 463-466"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007907509999824924,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.requests_per_minute is None', 'expected_value': 0, 'actual_value': 'None'}",
          "{'name': 'config.tokens_per_minute == 100', 'expected_value': 100, 'actual_value': 100}"
        ],
        "scenario": "TestGeminiProviderDetailed::test_parse_rate_limits_types",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 156,
          "total_tokens": 273
        },
        "why_needed": "To ensure that the GeminiProvider can correctly parse rate limits and return valid configuration values."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiProviderDetailed::test_parse_rate_limits_types",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 11,
          "line_ranges": "39-42, 81-85, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007648320000157582,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of _request_times should be 1 after pruning.",
          "The length of _token_usage should be 1 after pruning.",
          "_request_times[0] should equal now - 10.0 after pruning."
        ],
        "scenario": "Verify that the _prune method of _GeminiRateLimiter removes all old requests and updates token usage accordingly.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 323,
          "total_tokens": 442
        },
        "why_needed": "This test prevents a potential regression where the rate limiter would not prune older requests, leading to incorrect usage of tokens."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_prune_logic",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 6,
          "line_ranges": "39-42, 66-67"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007325320000006741,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Limiter should not store any tokens when record_tokens is called with a negative number', 'expected_result': 0, 'actual_result': {'scenario': 'Tests for the Gemini Rate Limiter', 'why_needed': 'The test is necessary to ensure that the rate limiter correctly handles invalid token records.', 'key_assertions': ['...']}}"
        ],
        "scenario": "Tests for the Gemini Rate Limiter",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 127,
          "total_tokens": 256
        },
        "why_needed": "The test is necessary to ensure that the rate limiter correctly handles invalid token records."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_record_tokens_invalid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 45-46, 48-50, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007612360000166518,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Limiter is initialized with correct rate limit', 'description': 'The limiter should be initialized with a rate limit of 1 requests per day', 'expected_value': 1, 'actual_value': 0}",
          "{'name': 'Rate limiting feature works as expected for single request per day', 'description': 'The rate limiting feature should not allow more than one request per day', 'expected_value': 0, 'actual_value': 100}"
        ],
        "scenario": "Test the rate limiting feature with a single request per day",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 129,
          "total_tokens": 293
        },
        "why_needed": "The test ensures that the rate limiting feature correctly limits requests to 1 per day."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpd_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 27,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-97, 100-102"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008072020000042812,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "limiter.next_available_in(100) == 0.0",
          "limiter.record_request()",
          "assert limiter.next_available_in(100) == 0.0",
          "limiter.record_request()",
          "wait > 0 and wait <= 60.0"
        ],
        "scenario": "Verify that the RPM limit is correctly enforced for the first two requests and that it's not exceeded.",
        "token_usage": {
          "completion_tokens": 140,
          "prompt_tokens": 280,
          "total_tokens": 420
        },
        "why_needed": "This test prevents a potential issue where the rate limiter exceeds the allowed number of requests per minute, potentially leading to unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_rpm_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 100-101, 103-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007491629999947236,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _seconds_until_tpm_available should return 0.0 when no tokens are requested.",
          "The function _seconds_until_tpm_available should return 0.0 when more than limit tokens are requested but empty usage.",
          "The function _seconds_until_tpm_available should return 0.0 for normal usage within the limit.",
          "The function _seconds_until_tpm_available should return a value less than or equal to 60 seconds when usage exceeds the limit by 1 second."
        ],
        "scenario": "Verify that the rate limiter returns 0 seconds when no tokens are requested and within the limit.",
        "token_usage": {
          "completion_tokens": 170,
          "prompt_tokens": 377,
          "total_tokens": 547
        },
        "why_needed": "This test prevents a potential regression where the rate limiter does not return 0 seconds for requests with no tokens."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_seconds_until_tpm_available_branches",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 24,
          "line_ranges": "32-34, 39-42, 45-46, 48-50, 58-60, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008149359999833905,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `wait_for_slot` method raises `_GeminiRateLimitExceeded` with a `limit_type` of 'requests_per_day'.",
          "The `limit_type` attribute of the raised exception matches the expected value.",
          "_GeminiRateLimitExceeded has a `limit_type` attribute that is set to 'requests_per_day' when it is raised.",
          "The `value` attribute of the raised exception contains an object with a `limit_type` attribute set to 'requests_per_day'.",
          "The `limit_type` value matches the expected value for requests per day.",
          "_GeminiRateLimitExceeded raises an exception when the daily limit is exceeded, which prevents unexpected behavior in downstream applications."
        ],
        "scenario": "Test that waiting for a daily limit exceeded throws an exception when the rate limit is set to one request per day.",
        "token_usage": {
          "completion_tokens": 228,
          "prompt_tokens": 263,
          "total_tokens": 491
        },
        "why_needed": "This test prevents a potential regression where the limiter does not raise an exception when the daily limit is exceeded, potentially leading to unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_daily_limit_exceeded",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 18,
          "line_ranges": "39-42, 58-59, 61-63, 73, 76-78, 81-82, 84, 87-88"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001436729999994668,
      "file_path": "tests/test_gemini_provider.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The wait_for_slot function sleeps for at least 10 seconds.",
          "The next_available_in method is called with an argument of 10.0.",
          "The mock_sleep assertion is called once with a value of 10.0."
        ],
        "scenario": "Test that the wait_for_slot function sleeps for a sufficient amount of time when the next available slot is not immediately available.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 325,
          "total_tokens": 451
        },
        "why_needed": "This test prevents a potential regression where the rate limiter does not sleep long enough between requests, potentially leading to performance issues or unexpected behavior."
      },
      "nodeid": "tests/test_gemini_provider.py::TestGeminiRateLimiter::test_wait_for_slot_sleeps",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0008451230000048326,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config1 and config2 are different', 'expected': 'different', 'actual': 'not equal'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 119,
          "total_tokens": 213
        },
        "why_needed": "To ensure that different configurations of the Compute API produce different hashes, which can be used to identify and fix potential issues."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_different_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "96-101, 103-104"
        }
      ],
      "duration": 0.0007565780000220457,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'h should have length 16', 'expected_result': 16}",
          "{'assertion': 'h should not contain any non-ASCII characters', 'expected_result': 'All characters in h are ASCII'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 109,
          "total_tokens": 226
        },
        "why_needed": "To ensure the computed hash is short and can be stored efficiently in a database or other storage system."
      },
      "nodeid": "tests/test_hashing.py::TestComputeConfigHash::test_returns_short_hash",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 6,
          "line_ranges": "32, 44-48"
        }
      ],
      "duration": 0.0008569649999969897,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': \"b'test content'\", 'actual_value': \"b'test content'\"}"
        ],
        "scenario": "ComputeFileSha256 test",
        "token_usage": {
          "completion_tokens": 71,
          "prompt_tokens": 144,
          "total_tokens": 215
        },
        "why_needed": "To ensure the computed SHA-256 hash of a file matches its content hash."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_consistent_with_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "44-48"
        }
      ],
      "duration": 0.0008732359999896744,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': \"A hash of length 64 for a file with contents 'hello world'.\"}"
        ],
        "scenario": "Test Compute File Sha256",
        "token_usage": {
          "completion_tokens": 64,
          "prompt_tokens": 124,
          "total_tokens": 188
        },
        "why_needed": "To verify the correctness of the file hashing algorithm."
      },
      "nodeid": "tests/test_hashing.py::TestComputeFileSha256::test_hashes_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007505760000015016,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'sig1', 'expected_type': 'bytes', 'actual_type': 'bytes'}",
          "{'name': 'sig2', 'expected_type': 'bytes', 'actual_type': 'bytes'}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_different_key",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 125,
          "total_tokens": 216
        },
        "why_needed": "To ensure that different keys produce different signatures."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_different_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "61"
        }
      ],
      "duration": 0.0007869840000012118,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_length': 64, 'actual_length': 0}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeHmac::test_with_key",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 108,
          "total_tokens": 180
        },
        "why_needed": "To verify that the HMAC computation is correct and produces the expected signature."
      },
      "nodeid": "tests/test_hashing.py::TestComputeHmac::test_with_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007986960000039289,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'hash': 'd41d8cd98f00b804d0a131e86038e95'}, 'actual': {'hash': 'd41d8cd98f00b804d0a131e86038e95'}}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_consistent",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 115,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the hash function produces consistent results for the same input."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_consistent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 1,
          "line_ranges": "32"
        }
      ],
      "duration": 0.0007414899999957925,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected the hash length to be 64 hex chars.', 'expected_value': 64}"
        ],
        "scenario": "tests/test_hashing.py::TestComputeSha256::test_length",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 103,
          "total_tokens": 181
        },
        "why_needed": "To ensure the hash length is correct and consistent across different inputs."
      },
      "nodeid": "tests/test_hashing.py::TestComputeSha256::test_length",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.07868803799999569,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"snapshot contains 'pytest'\", 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 102,
          "total_tokens": 180
        },
        "why_needed": "To ensure that the 'pytest' package is included in the dependency snapshot."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_includes_pytest",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 8,
          "line_ranges": "113-114, 116-121"
        }
      ],
      "duration": 0.08073570499999505,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "snapshot is an instance of dict"
        ],
        "scenario": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
        "token_usage": {
          "completion_tokens": 55,
          "prompt_tokens": 98,
          "total_tokens": 153
        },
        "why_needed": "The function `get_dependency_snapshot()` should return a dictionary."
      },
      "nodeid": "tests/test_hashing.py::TestGetDependencySnapshot::test_returns_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 5,
          "line_ranges": "73, 76-77, 80-81"
        }
      ],
      "duration": 0.0008805200000097102,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The loaded key should match the expected value.', 'expected_value': 'my-secret-key'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 145,
          "total_tokens": 226
        },
        "why_needed": "To test the functionality of loading a HMAC key from a file."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_loads_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 4,
          "line_ranges": "73, 76-78"
        }
      ],
      "duration": 0.0008357060000037109,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected result', 'type': 'NoneType', 'expected_value': 'None'}"
        ],
        "scenario": "Test Load HMAC Key with Missing Key File",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 126,
          "total_tokens": 198
        },
        "why_needed": "To test the case where a key file does not exist."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_missing_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/hashing.py",
          "line_count": 2,
          "line_ranges": "73-74"
        }
      ],
      "duration": 0.0007663450000165994,
      "file_path": "tests/test_hashing.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert key is None', 'expected_result': 'None'}"
        ],
        "scenario": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 110,
          "total_tokens": 180
        },
        "why_needed": "Because the HMAC key is not loaded."
      },
      "nodeid": "tests/test_hashing.py::TestLoadHmacKey::test_no_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000754482999980155,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.aggregate_dir is None",
          "config.aggregate_policy == 'latest'",
          "config.aggregate_include_history is False"
        ],
        "scenario": "Test aggregation default configuration.",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 201,
          "total_tokens": 274
        },
        "why_needed": "This test prevents regression where the default aggregation policy is set to 'latest' without including history."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_aggregation_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008064809999837053,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.capture_failed_output', 'expected_value': 'True'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 107,
          "total_tokens": 177
        },
        "why_needed": "The test captures failed output by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_capture_failed_output_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007643330000064452,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'config.llm_context_mode', 'expected_value': 'minimal'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 107,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the context mode is set to 'minimal' by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_context_mode_default_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 4,
          "line_ranges": "123, 171, 284, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007698720000064441,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'is_llm_enabled() returns False for default config', 'expected_value': False, 'actual_value': 'get_default_config().is_llm_enabled()'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 109,
          "total_tokens": 200
        },
        "why_needed": "LLM is not enabled by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_llm_not_enabled_by_default",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000800600000019358,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "config.omit_tests_from_coverage"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 109,
          "total_tokens": 175
        },
        "why_needed": "The test is necessary because the 'omit_tests_from_coverage' configuration option is set to True by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_omit_tests_default_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008057389999862608,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"config.provider == 'none'\", 'expected_result': 'none'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 101,
          "total_tokens": 180
        },
        "why_needed": "To ensure that the provider is set to 'none' when it's not explicitly specified."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_provider_default_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007867540000177087,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Excluding secret files', 'expected_result': ['secret'], 'actual_result': [False]}",
          "{'name': 'Including .env files', 'expected_result': ['.env'], 'actual_result': [True]}"
        ],
        "scenario": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 132,
          "total_tokens": 249
        },
        "why_needed": "This test is necessary because the default configuration does not exclude secret files by default."
      },
      "nodeid": "tests/test_integration_gate.py::TestConfigDefaults::test_secret_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.006897533000000067,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "nodeids should be equal to the sorted list of nodeids from the report.json file.",
          "nodeids should contain only the nodeids mentioned in the tests section of the report.json file.",
          "nodeids should not contain any duplicates.",
          "nodeids should be in ascending order (i.e., no nodes are skipped or duplicated)",
          "nodeids should not include any non-nodeid strings from the test results",
          "nodeids should only contain node ids that were actually tested and reported as passed",
          "nodeids should not be empty"
        ],
        "scenario": "The test verifies that the output of the deterministic pipeline is sorted by nodeid.",
        "token_usage": {
          "completion_tokens": 177,
          "prompt_tokens": 313,
          "total_tokens": 490
        },
        "why_needed": "This test prevents a regression where the sorted nodeids are not as expected due to changes in the underlying data structure."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_deterministic_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.007298523999992312,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.summary.total` property is set to 0.",
          "The `data` dictionary has a 'summary' key with a 'total' value of 0.",
          "The `json.loads()` function correctly parses the JSON data from the report file."
        ],
        "scenario": "Test that an empty test suite produces a valid report.",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 240,
          "total_tokens": 346
        },
        "why_needed": "This test prevents regression where an empty test suite causes the report to be invalid."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_empty_test_suite",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 118,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.038987696999981836,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'report.html' exists at the specified path.",
          "The content of the 'report.html' file contains the expected string '<html'.",
          "The content of the 'report.html' file contains the expected string 'test_pass'."
        ],
        "scenario": "Test that the full pipeline generates an HTML report.",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 270,
          "total_tokens": 370
        },
        "why_needed": "This test prevents a regression where the HTML report is not generated correctly."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_html_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/_git_info.py",
          "line_count": 2,
          "line_ranges": "2-3"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 138,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06353959599999826,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'schema_version' key in the JSON output should be set to SCHEMA_VERSION.",
          "The 'summary' section of the JSON output should contain the correct total, passed, failed, and skipped counts.",
          "The 'passed', 'failed', and 'skipped' keys within the 'summary' section should have the expected values (1, 1, and 1 respectively)."
        ],
        "scenario": "The test verifies that the full pipeline generates a valid JSON report with the expected structure and content.",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 419,
          "total_tokens": 567
        },
        "why_needed": "This test prevents regression when the pipeline fails to generate a JSON report due to an issue with the report writer configuration."
      },
      "nodeid": "tests/test_integration_gate.py::TestFullPipeline::test_json_report_generation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008006400000226677,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "ReportRoot has required fields: schema_version, run_meta, summary, and tests.",
          "Required fields are present in the data: 'schema_version', 'run_meta', 'summary', and 'tests'.",
          "The ReportRoot object contains all required fields.",
          "No report root is missing required fields.",
          "Missing required field would cause a validation error."
        ],
        "scenario": "Test verifies that the ReportRoot object has required fields.",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 250,
          "total_tokens": 376
        },
        "why_needed": "The test prevents a potential bug where the report root is missing required fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_report_root_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008074919999785379,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 136,
          "total_tokens": 247
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_aggregation_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007683790000214685,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field is present in the data.",
          "The 'interrupted' field is present in the data.",
          "The 'collect_only' field is present in the data.",
          "The 'collected_count' field is present in the data.",
          "The 'selected_count' field is present in the data."
        ],
        "scenario": "Test 'test_run_meta_has_status_fields' verifies that RunMeta has run status fields.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 237,
          "total_tokens": 368
        },
        "why_needed": "This test prevents a potential regression where the RunMeta object does not have all required status fields."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_run_meta_has_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007876660000079028,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'SCHEMA_VERSION', 'type': 'string'}",
          "{'name': '.', 'type': 'boolean'}"
        ],
        "scenario": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 103,
          "total_tokens": 191
        },
        "why_needed": "The schema version must be defined to ensure compatibility with the API."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_schema_version_defined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007484829999953035,
      "file_path": "tests/test_integration_gate.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "nodeid",
          "outcome",
          "duration"
        ],
        "scenario": "The test verifies that the TestCaseResult object has the required fields.",
        "token_usage": {
          "completion_tokens": 76,
          "prompt_tokens": 223,
          "total_tokens": 299
        },
        "why_needed": "This test prevents a potential bug where the TestCaseResult object is missing one or more required fields, potentially leading to incorrect results or errors."
      },
      "nodeid": "tests/test_integration_gate.py::TestSchemaCompatibility::test_test_case_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.001512448999989,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider.annotate` method should raise an error with a meaningful message.",
          "The annotation should contain an `error` key with a non-None value.",
          "The `error` value should be a string indicating the cause of the failure.",
          "The `error` value should not be None.",
          "The `provider.annotate` method should not return any result if all retries are exhausted.",
          "The annotation should contain an `error` key with a non-None value.",
          "The `provider.annotate` method should raise an exception when the API call fails.",
          "The `provider.annotate` method should log an error message when the API call fails."
        ],
        "scenario": "Test that all retries are exhausted when API calls fail.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 346,
          "total_tokens": 539
        },
        "why_needed": "Prevents the test from passing if all retries are exhausted due to an API call failure."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_all_retries_exhausted",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 38,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141, 144-145, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011387340000226231,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The API call should return an annotation with an error message.",
          "The error status code should be 500 (Internal server error).",
          "The test source file should not have any annotations.",
          "Any context files used by the test should also not have any annotations.",
          "If no force refresh is requested, the provider should not call LiteLLMProvider's annotate method.",
          "If a non-401 error occurs and no force refresh is requested, the result of annotate should be None.",
          "The annotation returned from annotate should contain an error message."
        ],
        "scenario": "Test that non-401 errors don't force token refresh.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 367,
          "total_tokens": 534
        },
        "why_needed": "Prevents regression in case of non-401 error and no force refresh."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_non_401_error_no_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 47,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.0028165949999845,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `test_foo()` function should be annotated with a successful result even though it was marked as transient.",
          "The API call to fail twice before succeeding should not raise an exception.",
          "The LLM token refresh attempt should eventually succeed after the transient error."
        ],
        "scenario": "Test that retry succeeds after transient error when API call fails twice before succeeding.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 433,
          "total_tokens": 550
        },
        "why_needed": "This test prevents regression in case of transient errors, where the LLM token refresh attempt fails but subsequent attempts succeed."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_retry_succeeds_after_transient_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 54,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-188, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 6.179057300999972,
      "file_path": "tests/test_litellm_retry_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `provider.annotate()` should have been called with an additional argument (2 or more).",
          "The error status code should be 401 for the first call to `mock_completion`.",
          "The error message should contain '{",
          "The response choices should include a new token.",
          "The retry count should be greater than or equal to 2 after a 401 error occurs.",
          "The annotation result should not be None when a 401 error occurs."
        ],
        "scenario": "Test that 401 error triggers token refresh (lines 123-126) when API call fails first, then succeeds.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 473,
          "total_tokens": 640
        },
        "why_needed": "To ensure the test catches and reports a retry of the token refresh after a 401 error occurs."
      },
      "nodeid": "tests/test_litellm_retry_coverage.py::TestLiteLLMTokenRefreshRetry::test_token_refresh_on_401",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 10,
          "line_ranges": "65-66, 384, 386, 388, 391, 396, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007973740000011276,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'provider.__class__.__name__ == \"GeminiProvider\"', 'expected_result': 'GeminiProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
        "token_usage": {
          "completion_tokens": 91,
          "prompt_tokens": 131,
          "total_tokens": 222
        },
        "why_needed": "To ensure that the GeminiProvider class is correctly instantiated when the 'gemini' provider is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_gemini_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007708650000211037,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected': 'LiteLLMProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 140,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the LiteLLMProvider class is correctly instantiated when a specific provider ('litellm') is used."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_litellm_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000772116999996797,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider should be NoneType', 'expected_value': 'None'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 115,
          "total_tokens": 195
        },
        "why_needed": "To ensure that the GetProvider function returns a NoopProvider when the provider is None."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_none_returns_noop",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 384, 386, 388, 391-392, 394"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007527209999977913,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.__class__.__name__', 'expected': 'OllamaProvider'}"
        ],
        "scenario": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 154,
          "total_tokens": 240
        },
        "why_needed": "To ensure that the OllamaProvider class is correctly created and returned from the get_provider function."
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_ollama_returns_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "384, 386, 388, 391, 396, 401, 406"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007619680000061635,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 125,
          "total_tokens": 193
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm.py::TestGetProvider::test_unknown_raises",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007534009999972113,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "hasattr(provider, 'annotate')",
          "hasattr(provider, 'is_available')",
          "hasattr(provider, 'get_model_name')",
          "hasattr(provider, 'config')"
        ],
        "scenario": "Test that NoopProvider implements LlmProvider interface.",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 232,
          "total_tokens": 327
        },
        "why_needed": "Prevents a potential bug where NoopProvider is not implementing required methods of LlmProvider."
      },
      "nodeid": "tests/test_llm.py::TestLlmProviderContract::test_noop_implements_interface",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007801829999891652,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `annotation` variable will be of type `LlmAnnotation`.",
          "The `scenario` attribute of the `annotation` object will be an empty string.",
          "The `why_needed` attribute of the `annotation` object will be an empty string.",
          "All `key_assertions` in the `annotation` object will be empty lists."
        ],
        "scenario": "The test verifies that the `annotate` method of the `NoopProvider` class returns an empty `LlmAnnotation` object when no annotation is provided.",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 249,
          "total_tokens": 411
        },
        "why_needed": "This test prevents a regression where the `annotate` method does not return an error or warning message when no annotation is given, but instead returns an empty annotation."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_annotate_returns_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 67"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007327639999914481,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert get_model_name() == ''\", 'description': 'The `get_model_name` method should return an empty string.'}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 114,
          "total_tokens": 221
        },
        "why_needed": "The test is failing because the `get_model_name` method of the `NoopProvider` class returns an empty string when the model name is not specified."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_get_model_name_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 59"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007963030000155413,
      "file_path": "tests/test_llm.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider.is_available() should be a boolean value', 'expected_type': 'bool'}"
        ],
        "scenario": "tests/test_llm.py::TestNoopProvider::test_is_available",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 108,
          "total_tokens": 192
        },
        "why_needed": "To ensure the NoopProvider class is always available and does not raise any exceptions when instantiated."
      },
      "nodeid": "tests/test_llm.py::TestNoopProvider::test_is_available",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007596639999860599,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 115,
          "total_tokens": 262
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000766277000025184,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "checks password",
          "checks username"
        ],
        "scenario": "The test verifies that the AnnotationSchema can parse a dictionary into a valid annotation.",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 274,
          "total_tokens": 340
        },
        "why_needed": "This test prevents potential bugs where the AnnotationSchema does not correctly handle malformed input data."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_from_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007523399999911362,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'schema.scenario', 'value': '', 'expected_value': ''}",
          "{'name': 'schema.why_needed', 'value': '', 'expected_value': ''}"
        ],
        "scenario": "This test checks if the AnnotationSchema can handle an empty input.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 109,
          "total_tokens": 216
        },
        "why_needed": "The test is necessary because it ensures that the AnnotationSchema can process and validate valid inputs, including empty ones."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007678290000114885,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 306,
          "prompt_tokens": 119,
          "total_tokens": 425
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_handles_partial",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007723580000060792,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert 'scenario' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert 'why_needed' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert 'key_assertions' in ANNOTATION_JSON_SCHEMA['properties']",
          "assert isinstance(ANNOTATION_JSON_SCHEMA, dict)",
          "assert len(ANNOTATION_JSON_SCHEMA) > 0",
          "assert all(key in ANNOTATION_JSON_SCHEMA for key in ['scenario', 'why_needed', 'key_assertions'])"
        ],
        "scenario": "The test verifies that the schema has required fields.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 215,
          "total_tokens": 379
        },
        "why_needed": "This test prevents a potential bug where the schema is not properly defined with required fields, potentially leading to errors or inconsistencies in the data."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_has_required_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "90-92, 94-96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007628400000214697,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert data['scenario'] == 'Tests feature X'",
          "assert data['why_needed'] == 'Prevents bug Y'",
          "assert 'key_assertions' in data",
          "# Verify the presence of key_assertions"
        ],
        "scenario": "The test verifies that the `AnnotationSchema` instance correctly serializes to a dictionary.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 247,
          "total_tokens": 394
        },
        "why_needed": "This test prevents regression by ensuring that the `AnnotationSchema` class handles scenario and why_needed keys correctly when converting to a dictionary."
      },
      "nodeid": "tests/test_llm_contract.py::TestAnnotationSchema::test_schema_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 6,
          "line_ranges": "65-66, 384, 386, 388-389"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007440239999993992,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_type': 'NoopProvider', 'actual_type': 'get_provider(config)'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 118,
          "total_tokens": 198
        },
        "why_needed": "To test that the factory returns a NoopProvider for a specific provider."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_from_factory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007817449999834025,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'type_of_provider', 'expected': 'LlmProvider', 'actual': 'NoopProvider'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 117,
          "total_tokens": 207
        },
        "why_needed": "To ensure that the NoopProvider class correctly implements the LlmProvider interface."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_is_llm_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007729479999909472,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result.scenario == \"\"",
          "assert result.why_needed == \"\"",
          "assert result.key_assertions == []"
        ],
        "scenario": "The NoopProvider should return an empty annotation when the test function does not have any dependencies.",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 253,
          "total_tokens": 346
        },
        "why_needed": "This test prevents a regression where the NoopProvider returns an incorrect annotation for tests with no dependencies."
      },
      "nodeid": "tests/test_llm_contract.py::TestNoopProvider::test_noop_returns_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007645029999991948,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `scenario` attribute is present and has the correct value.",
          "The `why_needed` attribute is present and has the correct value.",
          "The `key_assertions` list contains all the required assertions to verify the expected attributes of the returned object."
        ],
        "scenario": "The test verifies that the `annotate` method of the `ProviderContract` class returns an instance of `TestCaseResult` with the expected attributes.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 263,
          "total_tokens": 406
        },
        "why_needed": "This test prevents a potential regression where the `annotate` method does not return an instance of `TestCaseResult` with the expected attributes, potentially causing issues downstream in the testing framework."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_annotate_returns_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007403380000141624,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The provider should not return None for an empty test.', 'expected_result': 'Not None'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 145,
          "total_tokens": 232
        },
        "why_needed": "To ensure the NoopProvider class handles empty code gracefully and returns a valid TestCaseResult."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_empty_code",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 8,
          "line_ranges": "65-66, 87-89, 97-98, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 2,
          "line_ranges": "32, 51"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008009409999942818,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'None', 'actual_value': 'not None'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 148,
          "total_tokens": 225
        },
        "why_needed": "To ensure the NoopProvider class can handle None context without raising an error."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_handles_none_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 15,
          "line_ranges": "65-66, 384, 386, 388-389, 391-392, 394, 396-397, 399, 401-402, 404"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 9,
          "line_ranges": "134-135, 137-141, 143-144"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/noop.py",
          "line_count": 1,
          "line_ranges": "32"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000805079999992131,
      "file_path": "tests/test_llm_contract.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"has attribute 'annotate'\", 'value': 'True'}",
          "{'name': 'is callable annotate method', 'value': 'True'}"
        ],
        "scenario": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 145,
          "total_tokens": 238
        },
        "why_needed": "To ensure that all providers have an annotate method."
      },
      "nodeid": "tests/test_llm_contract.py::TestProviderContract::test_provider_has_annotate_method",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 187,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 263-265, 299, 311-312, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 435, 437-439, 441-444, 449-452, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524-525, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009358029999759765,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'expected', 'expected_value': 1000, 'actual_value': 15000}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 98,
          "total_tokens": 212
        },
        "why_needed": "The current implementation of annotate_handles_context can handle contexts up to 1000 tokens. However, with the increasing size of the input data, it may exceed this limit and cause performance issues."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_handles_context_too_large",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 34,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-195, 471-473, 497-498, 502-503, 537"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000879247000000305,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "the annotation contains an error message indicating that 'litellm' is not installed.",
          "the annotation includes the correct installation instructions for 'litellm'.",
          "the annotation provides clear and concise feedback to the user about what they need to do to resolve the issue."
        ],
        "scenario": "The LiteLLMProvider annotates the missing dependency 'litellm' in a test case.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 270,
          "total_tokens": 394
        },
        "why_needed": "This test prevents the provider from silently failing when a required dependency is not installed, potentially masking bugs or regressions."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 21,
          "line_ranges": "134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-188"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008192249999865453,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'GEMINI_API_TOKEN is not set'",
          "assert provider._config.api_token is None",
          "assert fake_genai.configure.called_with(None) == True",
          "assert fake_google.generativeai.__path__.endswith[:] == []",
          "assert fake_google.generativeai._config.api_key is None",
          "assert provider._config._api_key is None",
          "assert fake_api_core.exceptions.ResourceExhausted.called_once() == True"
        ],
        "scenario": "Test that the `GeminiProvider` requires an API token and prevents a missing token error.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 440,
          "total_tokens": 620
        },
        "why_needed": "The current implementation does not verify if the required API token is set before annotating the test. This can lead to unexpected behavior or errors when trying to annotate tests with missing tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_missing_token",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 220,
          "line_ranges": "39-42, 45-46, 48, 52-54, 66, 68-70, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246-247, 249-252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-430, 432, 435, 437-439, 441-444, 449-455, 457, 459-460, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009566720000009354,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the correct number of tokens are recorded on the limiter for the specified model.",
          "Check if the rate limits logic ran without error.",
          "Verify that the correct user is logged in based on the annotated test function.",
          "Ensure that the correct usage metadata is provided to the API.",
          "Verify that the total token count matches the expected value.",
          "Confirm that the annotation was successful and no errors occurred."
        ],
        "scenario": "Test that annotate records tokens function works correctly by annotating a login test.",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 783,
          "total_tokens": 921
        },
        "why_needed": "Prevents regressions and ensures accurate token usage tracking."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_records_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "32-34, 39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 263-265, 299-300, 304-306, 308-309, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413-416, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010110540000027868,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The annotation of retries should match the expected key', 'expected_value': 'annotation_retries', 'actual_value': 'annotation_retries'}",
          "{'name': 'The annotation of retries should be a dictionary with the correct keys', 'expected_value': {'key1': 'value1', 'key2': 'value2'}, 'actual_value': {}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 98,
          "total_tokens": 235
        },
        "why_needed": "To ensure that the LLM provider can annotate retries correctly when rate limiting is in place."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_retries_on_rate_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 210,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526-527, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010745319999898584,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'models are rotated', 'description': 'The model should be rotated after a certain number of iterations (e.g. 1000).'}",
          "{'name': 'no more than one model per day', 'description': 'No more than one LLM model should be trained on the system at any given time.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
        "token_usage": {
          "completion_tokens": 139,
          "prompt_tokens": 100,
          "total_tokens": 239
        },
        "why_needed": "Rotating models on the daily limit is necessary because it prevents LLMs from being trained indefinitely and causing performance issues."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_rotates_models_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 216,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000985254999989138,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected error message', 'description': 'The test expects an error message indicating that the daily limit has been reached.'}",
          "{'name': 'Expected exception type', 'description': 'The test expects a `LimitExceededError` to be raised when the daily limit is exceeded.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 98,
          "total_tokens": 232
        },
        "why_needed": "To ensure that the LLM provider skips annotating tasks when the daily limit is exceeded."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_skips_on_daily_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 209,
          "line_ranges": "39-42, 45-46, 48-49, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-101, 103, 105, 107-109, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-466, 471-473, 476-478, 497-498, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0010345080000035978,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "status ok",
          "redirect"
        ],
        "scenario": "Test that LiteLLM provider annotates a successful response with the correct information.",
        "token_usage": {
          "completion_tokens": 60,
          "prompt_tokens": 474,
          "total_tokens": 534
        },
        "why_needed": "Prevents regression by ensuring the provider correctly handles successful responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 47,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 222,
          "line_ranges": "39-42, 45-46, 48-50, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-210, 212-213, 215-216, 218, 222-230, 232-233, 235-236, 239-244, 246, 249-250, 252, 261, 318-320, 340-343, 346-349, 352-356, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457, 459, 461-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011000709999962055,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'model recovered', 'description': 'The model should be able to recover and return correct results after 24 hours.'}",
          "{'name': 'provider returns incorrect results', 'description': 'The provider should return incorrect results for a certain period after the model has recovered.'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 104,
          "total_tokens": 256
        },
        "why_needed": "The test is needed because the model may not recover from an exhausted state after 24 hours. This could lead to a situation where the provider returns incorrect results for a certain period."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_exhausted_model_recovers_after_24h",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 68,
          "line_ranges": "134-135, 137-141, 143-144, 346, 348-349, 352-356, 358-361, 363-364, 366-367, 435, 437-439, 441-444, 449-452, 463-466, 476, 478, 497-498, 502-508, 511, 514-516, 518-521, 524-525, 537, 539-541, 544-545"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008230119999836916,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'method call', 'expected': 'fetch_available_models', 'actual': 'raise ValueError'}",
          "{'name': 'error message', 'expected': 'No models available.', 'actual': 'None'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 92,
          "total_tokens": 198
        },
        "why_needed": "To ensure that the `fetch_available_models` method raises an error when no models are available."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_fetch_available_models_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/gemini.py",
          "line_count": 201,
          "line_ranges": "39-42, 45-46, 48, 52-54, 73, 76-78, 81-82, 84, 87-88, 92-93, 95-96, 100-102, 134-135, 137-141, 143-144, 164-166, 173-175, 178, 181, 184, 186-187, 189, 191-192, 198-206, 208-209, 222-224, 228-230, 232, 235-236, 239-244, 246, 249-250, 252, 261, 340-343, 346-349, 352, 358-361, 363-364, 366-367, 383, 385-388, 390-403, 406, 410-411, 413, 418-422, 424-425, 432, 435, 437-439, 441-444, 449-455, 457-458, 463-466, 471-473, 476-478, 497-499, 502-505, 507-508, 511, 514-516, 518-521, 524, 526, 528-531, 537, 539-543, 547-548, 550-552, 554-555, 557-559, 562-563, 567, 569-571, 574"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0011209700000165412,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Model list is updated', 'description': 'The model list should be updated every time the test runs.'}",
          "{'name': 'Refresh interval is respected', 'description': \"The LLM provider's refresh interval should be respected and not cause any issues with the test.\"}"
        ],
        "scenario": "The model list is refreshed after an interval.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 96,
          "total_tokens": 216
        },
        "why_needed": "To ensure the model list is updated correctly and consistently with the LLM provider's refresh interval."
      },
      "nodeid": "tests/test_llm_providers.py::TestGeminiProvider::test_model_list_refreshes_after_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 50,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009441600000172912,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify the correct API key is captured before and after token refresh.",
          "Verify the response data contains the expected scenario, why needed, and key assertions.",
          "Verify that the first call to fake_completion raises a FakeAuthError (401 Unauthorized).",
          "Verify that the second call to fake_completion returns a successful response with the correct API key.",
          "Verify that the captured keys are in the correct order (token-1 before token-2)."
        ],
        "scenario": "Test that LiteLLM provider retries on 401 after refreshing token.",
        "token_usage": {
          "completion_tokens": 142,
          "prompt_tokens": 580,
          "total_tokens": 722
        },
        "why_needed": "Reason for retrying with token refresh."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_401_retry_with_token_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008062210000048253,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `error` attribute of the annotation is set to `boom` when a completion error occurs.",
          "The `boom` string is present within the `error` attribute.",
          "The test case asserts that the `error` attribute is not `None`",
          "The `boom` string is found in the `error` attribute"
        ],
        "scenario": "The test verifies that the LiteLLMProvider annotates completion errors correctly.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 307,
          "total_tokens": 435
        },
        "why_needed": "This test prevents a regression where LiteLLM providers do not surface completion errors in annotations."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_handles_completion_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 43,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 206, 211"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008383009999874957,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "response_data must be a dictionary",
          "response_data must contain 'key_assertions'",
          "response_data must not be empty",
          "response_data must have exactly one key_assertion",
          "response_data.key_assertions should be a list"
        ],
        "scenario": "Test that LiteLLMProvider rejects invalid key_assertions payloads.",
        "token_usage": {
          "completion_tokens": 112,
          "prompt_tokens": 346,
          "total_tokens": 458
        },
        "why_needed": "To prevent regression where the provider incorrectly handles invalid key_assertions payloads, making it harder to identify and fix issues."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 8,
          "line_ranges": "37-38, 41, 82-86"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000808476000003111,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by the annotate method should include an error message indicating that the required dependency 'litellm' is missing and how to install it.",
          "The error message should be in the format 'required package <name> not installed. Install with: pip install <name>'",
          "The error message should provide a clear indication of what needs to be done to resolve the issue.",
          "The annotation should include the exact name of the required package, which is 'litellm' in this case.",
          "The annotation should include the correct installation command for the required package.",
          "The annotation should not silently fail or produce incorrect results if the required package is not installed."
        ],
        "scenario": "The LiteLLMProvider should report a missing dependency error when the required package is not installed.",
        "token_usage": {
          "completion_tokens": 217,
          "prompt_tokens": 271,
          "total_tokens": 488
        },
        "why_needed": "This test prevents a potential bug where the LiteLLMProvider does not correctly handle cases where the required package is not installed, potentially leading to silent failures or incorrect results."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_missing_dependency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008809210000038092,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation contains the expected scenario, why needed, and key assertions.",
          "The annotation has a non-zero confidence level.",
          "The captured model matches the one used in the test.",
          "The 'tests/test_auth.py::test_login' message is present in the response.",
          "The 'def test_login()' message is present in the response.",
          "The system role of the message is 'system'.",
          "The status OK assertion is present in the response."
        ],
        "scenario": "Test that the LiteLLM provider annotates a successful response correctly.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 475,
          "total_tokens": 625
        },
        "why_needed": "Prevents regression by ensuring the annotation is correct for a valid response."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_success_with_mock_response",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 34,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008351050000214855,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The annotation returned by _annotate_internal method does not contain any messages.",
          "The content of the annotation's error message is 'CUSTOM PROMPT'.",
          "The annotation contains a custom prompt as expected.",
          "The key 'why_needed' in the annotation matches the provided reason.",
          "The key 'key_assertions' in the annotation matches the expected list of assertions.",
          "The value of the content key in the captured messages is 'CUSTOM PROMPT'.",
          "The type of the error message in the annotation is None."
        ],
        "scenario": "Test that LiteLLMProvider overrides the prompt when provided.",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 373,
          "total_tokens": 534
        },
        "why_needed": "To ensure that the LiteLLM provider correctly handles prompt override scenarios."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 39,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196-201, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00100526300002457,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token_usage` attribute of the annotation returned by `provider.annotate(test, 'src')` is not None.",
          "The value of `prompt_tokens` in `annotation.token_usage` is set to `100`.",
          "The value of `completion_tokens` in `annotation.token_usage` is set to `50`.",
          "The value of `total_tokens` in `annotation.token_usage` is set to `150`.",
          "The `token_usage` attribute is correctly populated with the expected values even when there are no token usage data available.",
          "The test verifies that the `prompt_tokens`, `completion_tokens`, and `total_tokens` attributes have the correct values for a given test case.",
          "The test also verifies that the `token_usage` attribute has a value of `None` when it should be `None` (i.e., no token usage data is available).",
          "The test ensures that the `token_usage` attribute is correctly populated with the expected values even in cases where there are multiple choices or no completion tokens."
        ],
        "scenario": "Test: tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_token_usage",
        "token_usage": {
          "completion_tokens": 278,
          "prompt_tokens": 426,
          "total_tokens": 704
        },
        "why_needed": "Prevents regression in token usage extraction for LiteLLM providers."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182-183, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008515760000022965,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `api_base` in the response data should be 'https://proxy.corp.com/v1'.",
          "The value of `litellm_api_base` in the config should be 'https://proxy.corp.com/v1'.",
          "The value of `api_base` in the completion call should match the one provided in the config.",
          "The response data should contain a key named `scenario`.",
          "The response data should contain a key named `why_needed`.",
          "The response data should contain a key named `key_assertions`.",
          "The value of `api_base` in the completion call should be 'https://proxy.corp.com/v1'."
        ],
        "scenario": "Test: tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough verifies that the LiteLLM provider passes api_base to completion call.",
        "token_usage": {
          "completion_tokens": 221,
          "prompt_tokens": 387,
          "total_tokens": 608
        },
        "why_needed": "This test prevents regression in case when API base is not provided in the config."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_base_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 35,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008533299999839983,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The API key should be captured and stored in the `captured` dictionary.",
          "The response data from the fake completion function should include the expected 'api_key' key.",
          "The 'api_key' value in the response data should match the static API key provided by the environment (TEST_KEY).",
          "The 'key_assertions' list should contain the expected 'api_key' assertion.",
          "The captured dictionary should have an 'api_key' key with the correct value.",
          "The fake completion function should return a response data object with the expected 'response_data' key and 'api_key' value.",
          "The 'why_needed' assertion should indicate that this test prevents a regression in API key passing to the completion call."
        ],
        "scenario": "The liteLLM provider should pass the static API key to the completion call.",
        "token_usage": {
          "completion_tokens": 222,
          "prompt_tokens": 384,
          "total_tokens": 606
        },
        "why_needed": "This test prevents a regression where the API key is not passed through to the completion call, potentially causing issues with the model's behavior or performance."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_api_key_passthrough",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 36,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 132-133, 170-174, 176-178, 182, 186-187, 190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008171510000067883,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `provider.annotate(test, 'src')` annotation should include an error message indicating that authentication failed.",
          "The `annotation.error` attribute should contain the string 'Authentication failed'.",
          "The `annotation.error` attribute should not be `None` when the test passes.",
          "The `annotation.error` attribute should contain the exact phrase 'Authentication failed' to ensure it matches the expected error message.",
          "The `annotation.error` attribute should be a string, not a list or other data structure.",
          "The `annotation.error` attribute should have the correct type (str) to ensure it's a string value."
        ],
        "scenario": "Test that the LiteLLM provider returns an auth error when no refresher is configured.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 338,
          "total_tokens": 536
        },
        "why_needed": "This test prevents a bug where the provider returns an authentication error without refreshing the token, potentially causing unexpected behavior or errors in downstream applications."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_error_without_refresher",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 51,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 122, 124-127, 129-130, 132-133, 141-142, 170-174, 176-178, 182, 186-188, 190"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 31,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 2.0013481969999987,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `litellm_token_refresh_command` is set to 'get-token'.",
          "The `llm_max_retries` is set to 2. If this value is exceeded, the provider will retry after a second failure.",
          "The `AuthenticationError` raised by the fake completion function is not caught by the provider's authentication error handler.",
          "The `litellm_token_refresh_command` is called with an argument 'token-new' when the retry attempt fails. This should trigger the authentication error handler.",
          "The `litellm_token_refresh_command` is called without any arguments when the retry attempt succeeds. The authentication error handler should not be triggered in this case.",
          "If the provider's authentication error handler catches the `FakeAuthError`, it should not report an authentication error on subsequent retries.",
          "The `litellm_token_refresh_command` is set to 'get-token' with a different argument when the retry attempt fails. This should trigger the authentication error handler and prevent the provider from reporting an authentication error on subsequent retries.",
          "If the provider's authentication error handler raises an exception, it should not be caught by the `fake_run` function and thus prevent the provider from reporting an authentication error on subsequent retries."
        ],
        "scenario": "Test that the LiteLLM provider reports an authentication error when retrying after a second failure.",
        "token_usage": {
          "completion_tokens": 317,
          "prompt_tokens": 419,
          "total_tokens": 736
        },
        "why_needed": "To prevent the provider from reporting an authentication error on subsequent retries, which could mask legitimate errors."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_auth_retry_fails_on_second_attempt",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 3,
          "line_ranges": "37-38, 41"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007710440000039398,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The response is not None, indicating that the function does not throw an exception",
          "The response contains the expected JSON structure",
          "The 'error' key in the response matches the expected value",
          "The 'why_needed' key in the response is empty, as it's not necessary for this test",
          "The 'key_assertions' list in the response is empty, indicating that the function does not throw an exception"
        ],
        "scenario": "Test 'LiteLLMProvider::test_context_too_long_error' verifies that the LiteLLM provider handles context too long error correctly.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 370,
          "total_tokens": 536
        },
        "why_needed": "The test prevents a potential bug or regression where the LiteLLM provider throws an error when handling responses with invalid contexts."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_context_too_long_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227-228, 230-231"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007913330000235419,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': 16384, 'message': 'Expected result to be 16384'}"
        ],
        "scenario": "test_get_max_context_tokens_dict_format",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 218,
          "total_tokens": 297
        },
        "why_needed": "To ensure the correct dictionary format is returned when calling get_max_context_tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_dict_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 10,
          "line_ranges": "37-38, 41, 221-222, 224, 227, 232-234"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007497949999901721,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'get_max_context_tokens returns an error when no context is provided', 'expected_value': 0, 'actual_value': 1}",
          "{'name': 'get_max_context_tokens returns a fallback value for max context tokens when an error occurs', 'expected_value': 10, 'actual_value': 20}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 101,
          "total_tokens": 231
        },
        "why_needed": "To ensure the LLMProvider can handle errors and return a fallback value for max context tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 9,
          "line_ranges": "37-38, 41, 221-222, 224, 227-229"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008051979999947889,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The function should return 8192 as the maximum context tokens.', 'expected_value': 8192}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 213,
          "total_tokens": 302
        },
        "why_needed": "To ensure the LiteLLM provider correctly returns the maximum context tokens."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_get_max_context_tokens_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 5,
          "line_ranges": "65-66, 134, 137-138"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 6,
          "line_ranges": "37-38, 41, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008027940000090439,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Mocking sys.modules', 'expected': {'litellm': 'fake_litellm'}, 'actual': 'fake_litellm'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 160,
          "total_tokens": 269
        },
        "why_needed": "To test the `is_available` method of the `LiteLLMProvider` class, which checks if a required module is installed."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_is_available_with_module",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 41,
          "line_ranges": "37-38, 41-42, 44-48, 60-61, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-188, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008910900000103084,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `litellm_token_refresh_command` is set to 'get-token'.",
          "Check if the `litellm_token_refresh_interval` is set to 3600 seconds (1 hour).",
          "Assert that the `api_key` variable in the captured output matches the expected value.",
          "Verify that the provider's `LiteLLMProvider` instance has been updated with the new configuration.",
          "Check if the `test_case()` method of the `LiteLLMProvider` instance returns a `CaseResult` object with an outcome of 'passed'.",
          "Assert that the captured output contains the expected API key value."
        ],
        "scenario": "Test the LiteLLM provider's token refresh integration.",
        "token_usage": {
          "completion_tokens": 194,
          "prompt_tokens": 442,
          "total_tokens": 636
        },
        "why_needed": "The test prevents a bug where the provider does not refresh tokens in a timely manner, potentially causing issues with dynamic token usage."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_token_refresh_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 42,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95, 98, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 139, 141-142, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008803290000116704,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The provider should raise a ConnectionError on the first attempt (0 calls).",
          "The provider should raise a ConnectionError on the second attempt (1 call).",
          "The provider should raise a ConnectionError on the third attempt (2 calls) and pass the test.",
          "The provider should not retry transient errors if there are less than 3 attempts left.",
          "The provider should not retry transient errors if there are more than 3 attempts left."
        ],
        "scenario": "Test that the LiteLLMProvider retries transient errors and that the test passes when there are more than 3 retry attempts.",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 426,
          "total_tokens": 595
        },
        "why_needed": "This test prevents a regression where the provider does not retry transient errors, potentially causing the test to fail with a ConnectionError."
      },
      "nodeid": "tests/test_llm_providers.py::TestLiteLLMProvider::test_transient_error_retry",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 70,
          "line_ranges": "65-66, 87-89, 97-99, 101, 103, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 243, 245, 264, 266-267, 270-272, 274, 277, 279-280, 283, 286, 290-291, 294-295, 298-299, 305, 307-308, 312, 314, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 27,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 85-86, 92, 138, 140, 142-144, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008955680000042321,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"Expected a dictionary with 'scenario', 'why_needed', and 'key_assertions' keys\", 'expected_value': {'scenario': 'tests/test_llm_providers.py', 'why_needed': 'To ensure that the LLM provider can handle context length errors and still return valid JSON.', 'key_assertions': ['...']}}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 103,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the LLM provider can handle context length errors and still return valid JSON."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_fallbacks_on_context_length_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 29,
          "line_ranges": "65-66, 87-89, 97-99, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 18,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 97-98, 100-101, 103-104"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008242140000049858,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "error == 'Failed after 2 retries. Last error: boom'",
          "annotation.error == 'Failed after 2 retries. Last error: boom'",
          "provider._call_ollama().__name__ == 'boom'",
          "test_case.__name__ == 'test_case'",
          "test_case.__doc__ is None"
        ],
        "scenario": "The test verifies that the annotate method returns an error message when a call to Ollama raises a call error.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 347,
          "total_tokens": 483
        },
        "why_needed": "This test prevents regression where the annotation fails to detect call errors in Ollama providers."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_handles_call_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 9,
          "line_ranges": "65-66, 87-89, 97-99, 105"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "42-46"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008091960000058407,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert annotation.error == 'httpx not installed. Install with: pip install httpx'",
          "provider.annotate(test, 'def test_case(): assert True')",
          "test_case()",
          "assert test_case().__name__ == 'test_case'"
        ],
        "scenario": "The Ollama provider reports missing httpx dependency when annotating a test case.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 268,
          "total_tokens": 389
        },
        "why_needed": "This test prevents a bug where the provider incorrectly assumes that httpx is installed and reports an error instead of suggesting to install it."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_missing_httpx",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 22,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 13,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-65, 94, 96"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008019120000142266,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'OllamaProvider should annotate runtime error', 'expected_value': 'True'}",
          "{'name': 'OllamaProvider should return immediate annotation result', 'expected_value': 'Immediate annotation result'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_runtime_error_immediate_fail",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 99,
          "total_tokens": 218
        },
        "why_needed": "The test is failing because the OLLAMA provider is not annotating runtime errors immediately."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_runtime_error_immediate_fail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008370369999965988,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "check status",
          "validate token",
          "assert True"
        ],
        "scenario": "Test that the annotate method correctly annotates a full flow with mocked HTTP responses and returns a CaseResult object.",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 414,
          "total_tokens": 496
        },
        "why_needed": "Prevents authentication bugs by ensuring that the Ollama provider returns an error when the login process fails."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_success_full_flow",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 34,
          "line_ranges": "42-43, 49, 52-53, 58, 60-61, 63-67, 71-72, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008662220000132947,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'Verify that the provider sets the correct error message.', 'condition': 'annotation.error is None', 'assertion': \"captured_messages[0][1]['content'] == 'CUSTOM PROMPT'\"}",
          "{'description': 'Verify that the custom prompt is used for annotation.', 'condition': \"captured_messages[0][1]['content'] != 'default prompt'\", 'assertion': 'False'}"
        ],
        "scenario": "Test that LiteLLMProvider overrides the prompt when provided with a custom prompt.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 373,
          "total_tokens": 522
        },
        "why_needed": "This test prevents regression in case providers override the default prompt."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_prompt_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 46,
          "line_ranges": "65-66, 87-89, 97-98, 105, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 264, 266-267, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 40,
          "line_ranges": "42-43, 49, 52, 55, 58, 60-61, 63-67, 71, 74-80, 83, 92, 190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008560040000133995,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of prompt tokens should be equal to 100.",
          "The number of completion tokens should be equal to 50.",
          "The total number of tokens should be equal to 150.",
          "The `token_usage` attribute should not be `None`.",
          "The value of `prompt_tokens` should match the expected value of 100.",
          "The value of `completion_tokens` should match the expected value of 50.",
          "The value of `total_tokens` should match the expected value of 150."
        ],
        "scenario": "The test verifies that the `annotate` method of `LiteLLMProvider` correctly extracts token usage from a response.",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 426,
          "total_tokens": 601
        },
        "why_needed": "This test prevents regression in handling cases where token usage is not provided in the response."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_annotate_with_token_usage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008349240000029567,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function _call_ollama() returns a dictionary with the expected 'response', 'model', 'prompt', and 'system' values.",
          "The captured URL and JSON data match the expected values from the API call.",
          "The timeout value is set to the expected 60 seconds.",
          "The response is not empty or None, as expected for a successful API call.",
          "The model and prompt are correctly retrieved from the captured JSON data.",
          "The system prompt is used in the generated response, as expected.",
          "The stream parameter is False, indicating no streaming of the response.",
          "The timeout value is respected during the API call."
        ],
        "scenario": "Test Ollama provider makes correct API call to generate response.",
        "token_usage": {
          "completion_tokens": 187,
          "prompt_tokens": 470,
          "total_tokens": 657
        },
        "why_needed": "Prevents regression in API call functionality of the Ollama provider."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 17,
          "line_ranges": "190, 192-200, 204-207, 209, 211-212"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008162290000086614,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The captured JSON response contains the default model 'llama3.2'.",
          "The captured JSON response does not contain any custom model specified by the user.",
          "The captured JSON response has the correct key 'model' with value 'llama3.2'."
        ],
        "scenario": "Test that the default model is used when not specified for Ollama provider.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 344,
          "total_tokens": 458
        },
        "why_needed": "This test prevents a regression where the default model is not used when it should be."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_call_ollama_uses_default_model",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 6,
          "line_ranges": "113-114, 116-117, 119-120"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007644519999985278,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider._check_availability() is False', 'expected_value': False, 'actual_value': 'False'}"
        ],
        "scenario": "Ollama provider returns False when server is unavailable",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 183,
          "total_tokens": 264
        },
        "why_needed": "The Ollama provider should return False when the server is unavailable."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008088250000071184,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'provider._check_availability() should return False for a 500 status code', 'expected_value': False}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 197,
          "total_tokens": 294
        },
        "why_needed": "To test the availability of an Ollama provider when it returns a non-200 status code."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_non_200",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 5,
          "line_ranges": "113-114, 116-118"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007905909999976757,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The URL '/api/tags' is included in the provided URL.",
          "A response with a status code of 200 is returned from the /api/tags endpoint.",
          "The OllamaProvider instance's _check_availability method returns True after calling this test."
        ],
        "scenario": "The test verifies that the Ollama provider checks for availability via the /api/tags endpoint successfully.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 296,
          "total_tokens": 417
        },
        "why_needed": "This test prevents a potential regression where the provider fails to check availability due to a misconfigured or outdated API."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_check_availability_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008282709999889448,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context Length', 'expected_value': 100, 'actual_value': 0}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 99,
          "total_tokens": 178
        },
        "why_needed": "To ensure that the `get_max_context_tokens_context_length` method returns the correct key for the context length in the JSON response."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_context_length_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 11,
          "line_ranges": "138, 140, 142-147, 175-176, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007696510000130274,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected max_context_tokens to be greater than 0', 'description': 'The `max_context_tokens` parameter should have a value greater than 0.'}",
          "{'name': \"Expected error message to contain 'maximum context tokens exceeded'\", 'description': \"The error message should contain the phrase 'maximum context tokens exceeded'.\"}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_fallback_on_error",
        "token_usage": {
          "completion_tokens": 141,
          "prompt_tokens": 101,
          "total_tokens": 242
        },
        "why_needed": "To handle cases where the maximum context token count is exceeded during training."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_fallback_on_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 16,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 165-167, 172-173"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008110899999849153,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'value': 10}",
          "{'name': 'context_token_count', 'value': 20}"
        ],
        "scenario": "tests/test_llm_providers.py",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 99,
          "total_tokens": 183
        },
        "why_needed": "To ensure that the `get_max_context_tokens` method returns the correct maximum context tokens for a given model info."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_model_info",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 15,
          "line_ranges": "138, 140, 142-147, 149-150, 156, 158, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014293780000116385,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'max_context_tokens', 'expected_value': 100, 'actual_value': 64}",
          "{'name': 'context_token_length', 'expected_value': 128, 'actual_value': 96}"
        ],
        "scenario": "Tests for OLLAMA provider",
        "token_usage": {
          "completion_tokens": 102,
          "prompt_tokens": 97,
          "total_tokens": 199
        },
        "why_needed": "To ensure that the maximum context tokens can be retrieved from parameters correctly."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_from_parameters",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 10,
          "line_ranges": "138, 140, 142-147, 149, 178"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007821659999933672,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Status code', 'value': 200}",
          "{'name': 'Response content type', 'value': 'application/json'}",
          "{'name': 'Content length', 'value': 0}"
        ],
        "scenario": "Tests for LLM providers",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 101,
          "total_tokens": 198
        },
        "why_needed": "To ensure the Ollama provider returns an error when the input is too large to be processed in a single context."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_get_max_context_tokens_non_200_status",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 2,
          "line_ranges": "65-66"
        },
        {
          "file_path": "src/pytest_llm_report/llm/ollama.py",
          "line_count": 1,
          "line_ranges": "128"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007894990000067992,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'provider.is_local() is True', 'expected_result': True}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
        "token_usage": {
          "completion_tokens": 82,
          "prompt_tokens": 123,
          "total_tokens": 205
        },
        "why_needed": "To ensure the Ollama provider always returns `is_local=True`."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_is_local_returns_true",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 7,
          "line_ranges": "65-66, 325-326, 329-331"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-52, 55"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00077295799999888,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Error message', 'expected': 'Failed to parse LLM response as JSON'}"
        ],
        "scenario": "Test Ollama Provider: test_parse_response_invalid_json",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 138,
          "total_tokens": 216
        },
        "why_needed": "To ensure the OllamaProvider correctly handles invalid JSON responses and reports an error."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 16,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346-348"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008114499999862801,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Invalid response: key_assertions must be a list', 'code': 400}"
        ],
        "scenario": "This test checks that the Ollama provider rejects invalid key_assertions payloads.",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 174,
          "total_tokens": 309
        },
        "why_needed": "The Ollama provider should reject responses with invalid key_assertions payloads."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_invalid_key_assertions",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008127920000049471,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected JSON format', 'expected': '{\"scenario\": \"tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence\", \"why_needed\": \"To ensure that the Ollama provider correctly extracts JSON from markdown code fences.\"}', 'actual': '{\"scenario\": \"tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence\", \"why_needed\": \"To ensure that the Ollama provider correctly extracts JSON from markdown code fences.\"}'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 127,
          "total_tokens": 305
        },
        "why_needed": "To ensure that the Ollama provider correctly extracts JSON from markdown code fences."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_code_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 6,
          "line_ranges": "38, 42-44, 46-47"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007538520000025528,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'response is a string', 'expected_value': 'string', 'actual_value': '<pre><code></code></pre>'}"
        ],
        "scenario": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 128,
          "total_tokens": 225
        },
        "why_needed": "To test the functionality of extracting JSON from plain markdown fences (no language)."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_json_in_plain_fence",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 20,
          "line_ranges": "65-66, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008554220000007717,
      "file_path": "tests/test_llm_providers.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert a",
          "assert b"
        ],
        "scenario": "Test Ollama provider parses valid JSON responses with correct scenario, why needed, and key assertions.",
        "token_usage": {
          "completion_tokens": 69,
          "prompt_tokens": 292,
          "total_tokens": 361
        },
        "why_needed": "Prevents bugs by ensuring the Ollama provider correctly identifies scenarios and key assertions in valid JSON responses."
      },
      "nodeid": "tests/test_llm_providers.py::TestOllamaProvider::test_parse_response_success",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 32,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008227619999843228,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `distribute_token_budget` should allocate a total of 10 tokens to `small.py` and no more than 45 tokens to `large.py` when distributing the token budget.",
          "The allocation for `small.py` should be at least 10 tokens, but not exceed 30 tokens.",
          "The allocation for `large.py` should be between 30 tokens and 45 tokens inclusive.",
          "The total allocated tokens should sum up to 60 (budget) - 16 (tokens needed by small.py) = 44 tokens for large.py.",
          "The remaining budget after allocating to small.py should be sufficient for large.py, allowing it to get at least 38 content tokens (44 - 6 overhead)."
        ],
        "scenario": "Test verify water-fill algorithm satisfies smaller files first with constrained token budget.",
        "token_usage": {
          "completion_tokens": 223,
          "prompt_tokens": 396,
          "total_tokens": 619
        },
        "why_needed": "This test prevents regression in the water-fill algorithm when distributing tokens to files, ensuring that smaller files are given priority and have sufficient content to fill their allocated budget."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_constrained",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 2,
          "line_ranges": "42-43"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007440939999980856,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': {'message': 'Expected distribute_token_budget to return {}'}, 'description': 'Test case for empty input'}",
          "{'assertion': {'message': 'Expected distribute_token_budget to return {}'}, 'description': 'Test case for no budget'}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 115,
          "total_tokens": 239
        },
        "why_needed": "This test ensures that the `distribute_token_budget` function behaves correctly when given an empty input or no budget."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_empty",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 30,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 90-91, 93-94, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007651029999919956,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The contents of `l1.py` and `l2.py` should be roughly equal.",
          "The allocation for `l1.py` should be between 35% and 50% of its total content (44 tokens).",
          "The allocation for `l2.py` should also be between 35% and 50% of its total content (44 tokens)."
        ],
        "scenario": "Verify fair sharing when neither fits.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 327,
          "total_tokens": 456
        },
        "why_needed": "Prevents regression where both files are large and the budget is insufficient to cover their content."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_fair_share",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007758130000183883,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'length of allocations should be equal to 3', 'expected_value': 3, 'actual_value': 1, '# This test is failing because only one file was allocated. Verify why this happened and correct it if necessary.': \"reason_for_failure': 'Only one file was allocated instead of three.'\"}"
        ],
        "scenario": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 133,
          "total_tokens": 267
        },
        "why_needed": "Verify the limit of max_files in the distribute_token_budget function to prevent over-allocation of files."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_max_files",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 28,
          "line_ranges": "20, 42, 46-47, 51-53, 55-60, 66-67, 70-71, 73, 75, 77, 79, 81-82, 84, 86-87, 96, 98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007862729999885687,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of allocations should be equal to 2.",
          "Each file's allocation should match its required amount (10 tokens for f1.py and 10 tokens for f2.py).",
          "All files should have their allocated tokens within the total budget (32 tokens in this case)."
        ],
        "scenario": "Verify that the function `distribute_token_budget` correctly allocates tokens to files when the budget is sufficient.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 332,
          "total_tokens": 462
        },
        "why_needed": "This test prevents a potential bug where the function does not allocate enough tokens to all files, resulting in incomplete content."
      },
      "nodeid": "tests/test_llm_utils.py::test_distribute_token_budget_sufficient",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/utils.py",
          "line_count": 1,
          "line_ranges": "20"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007594820000065283,
      "file_path": "tests/test_llm_utils.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert estimate_tokens([]) == 1",
          "assert estimate_tokens('') == 1",
          "assert estimate_tokens('a') == 1",
          "assert estimate_tokens('aaaa') == 1",
          "assert estimate_tokens('aaaa' * 10) == 10"
        ],
        "scenario": "Verify rough token estimation (chars / 4) for an empty string.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 217,
          "total_tokens": 336
        },
        "why_needed": "Prevents a potential division by zero error when estimating tokens in the absence of any input."
      },
      "nodeid": "tests/test_llm_utils.py::test_estimate_tokens",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "263-266"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000797664000003806,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the expected value.",
          "The 'line_ranges' key in the dictionary should match the expected value.",
          "The 'line_count' key in the dictionary should match the expected value.",
          "All values in the dictionary should be strings or integers, as they represent coverage data.",
          "Any non-string or integer values should be ignored during serialization."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes the test data.",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 255,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a potential bug where coverage data is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestArtifactEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 3,
          "line_ranges": "241-243"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007660050000026786,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the original file path.",
          "The 'line_ranges' key in the dictionary should match the original line ranges.",
          "The 'line_count' key in the dictionary should match the original line count."
        ],
        "scenario": "Test that `CoverageEntry.to_dict()` correctly serializes the test entry.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 255,
          "total_tokens": 372
        },
        "why_needed": "This test prevents a potential bug where the serialized `CoverageEntry` is not as expected, potentially leading to incorrect coverage data."
      },
      "nodeid": "tests/test_models.py::TestCollectionError::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 4,
          "line_ranges": "65-68"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007381620000046496,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary matches the expected value.",
          "The 'line_ranges' key in the dictionary matches the expected value.",
          "The 'line_count' key in the dictionary matches the expected value.",
          "All assertions pass for a single CoverageEntry instance."
        ],
        "scenario": "Tests CoverageEntry serialization correctly.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 255,
          "total_tokens": 358
        },
        "why_needed": "CoverageEntry may not serialize correctly if line ranges are invalid or missing."
      },
      "nodeid": "tests/test_models.py::TestCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007864629999971839,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "annotation.scenario == \"\" (empty string)",
          "annotation.why_needed == \"Empty annotation should have default values.\"",
          "annotation.key_assertions == [] (no key-value pairs or default values)",
          "assert annotation.confidence is None (default confidence value)",
          "assert annotation.error is None (default error message)"
        ],
        "scenario": "An empty annotation should be created with default values.",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 212,
          "total_tokens": 336
        },
        "why_needed": "This test prevents a potential bug where an empty annotation does not have any key-value pairs or default values."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_empty_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 9,
          "line_ranges": "130-133, 135, 137, 139, 141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007718150000073365,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "required_fields = ['scenario', 'why_needed', 'key_assertions']",
          "confidence field is optional and not included when None",
          "asserts that 'confidence' is not present in the dictionary"
        ],
        "scenario": "The test verifies that the `to_dict` method of `LlmAnnotation` returns a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 230,
          "total_tokens": 337
        },
        "why_needed": "This test prevents regression where the minimal annotation is missing some required fields."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "130-133, 135-137, 139-141, 143"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007570980000082272,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts that the 'scenario' field is correctly set to 'Tests user login'.",
          "Asserts that the 'confidence' field is correctly set to 0.95.",
          "Asserts that the 'context_summary' field has the expected mode and bytes value.",
          "Asserts that the 'error' field is None (correct behavior for LlmAnnotation).",
          "Asserts that the 'key_assertions' list contains all required assertions.",
          "Asserts that the dictionary d has all the correct keys and values."
        ],
        "scenario": "Test to dictionary with all fields",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 284,
          "total_tokens": 441
        },
        "why_needed": "Prevents incorrect output when not providing all required fields for LlmAnnotation."
      },
      "nodeid": "tests/test_models.py::TestLlmAnnotation::test_to_dict_with_all_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007900900000095135,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "d['schema_version'] == SCHEMA_VERSION",
          "d['tests'] == []",
          "not in d ['warnings', 'collection_errors']",
          "d['schema_version'] not in d"
        ],
        "scenario": "Test default report schema version and empty lists.",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 231,
          "total_tokens": 336
        },
        "why_needed": "Prevents a potential bug where the default report does not have a valid schema version or contains non-empty lists of tests, collection errors."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_default_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007601950000264424,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'collection_errors' key in the report dictionary is present and has exactly one item.",
          "The value of the 'nodeid' field within the first collection error object is set to 'test_bad.py'.",
          "The 'message' field within the first collection error object contains the specified message 'SyntaxError'."
        ],
        "scenario": "Test Report with Collection Errors should include them.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 237,
          "total_tokens": 354
        },
        "why_needed": "This test prevents a regression where the report does not include collection errors when they exist."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007815839999807395,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The length of the 'warnings' list in the report is 1.\", 'expected_value': 1, 'actual_value': 0}",
          "{'assertion': \"The code in the first warning is 'W001'.\", 'expected_value': 'W001', 'actual_value': 'No coverage'}"
        ],
        "scenario": "Test Report Root",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 144,
          "total_tokens": 260
        },
        "why_needed": "To ensure the test report includes warnings."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_report_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 73,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007841190000021925,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"nodeids == ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z']\", 'expected_result': ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z'], 'actual_result': ['a_test.py::test_a', 'm_test.py::test_m', 'z_test.py::test_z']}"
        ],
        "scenario": "Tests should be sorted by nodeid in output.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 215,
          "total_tokens": 374
        },
        "why_needed": "Because the current implementation does not sort tests by nodeid, it may cause unexpected behavior when sorting or filtering test results."
      },
      "nodeid": "tests/test_models.py::TestReportRoot::test_tests_sorted_by_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 8,
          "line_ranges": "70-71, 73-75, 77-79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007392039999842837,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'detail', 'expected_value': '/path/to/file'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 131,
          "total_tokens": 204
        },
        "why_needed": "The `to_dict` method of the `ReportWarning` class should include its detail information."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_with_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007677879999903325,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'detail' key is expected to be present in the dictionary.",
          "The value of 'detail' is not provided when it's absent.",
          "The test verifies that the 'detail' key does not exist."
        ],
        "scenario": "Test to dictionary without detail should exclude it.",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 223,
          "total_tokens": 311
        },
        "why_needed": "Prevents a warning about missing detailed information."
      },
      "nodeid": "tests/test_models.py::TestReportWarning::test_to_dict_without_detail",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 39,
          "line_ranges": "286-288, 290-292, 376-392, 394, 397, 399, 402, 405, 407, 409, 411-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008051890000047024,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'run_id' field should be present and equal to 'run-123'.",
          "The 'run_group_id' field should be present and equal to 'group-456'.",
          "The 'is_aggregated' field should be True.",
          "The 'aggregation_policy' field should be 'merge'.",
          "The 'run_count' field should be 3.",
          "The 'source_reports' list should have exactly two elements."
        ],
        "scenario": "Verify that RunMeta has aggregation fields present in the test data.",
        "token_usage": {
          "completion_tokens": 147,
          "prompt_tokens": 343,
          "total_tokens": 490
        },
        "why_needed": "This test prevents regression where RunMeta is missing or incorrectly configured aggregation fields."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_aggregation_fields_present",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007541729999900326,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_annotations_enabled' key is present in the data.",
          "The 'llm_provider' key is not present in the data.",
          "The 'llm_model' key is not present in the data."
        ],
        "scenario": "Test LLM fields are excluded when annotations are disabled.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 232,
          "total_tokens": 353
        },
        "why_needed": "This test prevents a regression where the LLM fields (llm_annotations_enabled, llm_provider, and llm_model) are included in the RunMeta object even when annotations are not enabled."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_fields_excluded_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 43,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419-431, 433, 435, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007859319999852232,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of 'llm_annotations_enabled' in the data dictionary should be True.",
          "The value of 'llm_provider' in the data dictionary should match the provided 'ollama' string.",
          "The value of 'llm_model' in the data dictionary should match the provided 'llama3.2:1b' string.",
          "The value of 'llm_context_mode' in the data dictionary should match the provided 'complete' string.",
          "The value of 'llm_annotations_count' in the data dictionary should be 10.",
          "The value of 'llm_annotations_errors' in the data dictionary should be 2."
        ],
        "scenario": "Verify that LLM traceability fields are included when enabled for the provided RunMeta instance.",
        "token_usage": {
          "completion_tokens": 197,
          "prompt_tokens": 327,
          "total_tokens": 524
        },
        "why_needed": "This test prevents regression where the presence of annotations is not properly tracked or reported by the model."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_llm_traceability_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007450059999882797,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': {'source_reports': []}, 'actual_value': {'source_reports': []}}",
          "{'expected_value': {'is_aggregated': False}, 'actual_value': {'is_aggregated': True}}"
        ],
        "scenario": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 130,
          "total_tokens": 246
        },
        "why_needed": "This test ensures that non-aggregated reports do not include source reports."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_non_aggregated_excludes_source_reports",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 49,
          "line_ranges": "286-288, 290-292, 376-392, 394-417, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007988060000059249,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'git_sha' field is correctly set to 'abc1234'.",
          "The 'git_dirty' field is correctly set to True.",
          "The 'repo_version' field is correctly set to '1.0.0'.",
          "The 'repo_git_sha' field is correctly set to 'abc1234'.",
          "The 'repo_git_dirty' field is correctly set to True.",
          "The 'plugin_git_sha' field is correctly set to 'def5678'.",
          "The 'plugin_git_dirty' field is correctly set to False.",
          "The 'config_hash' field is correctly set to 'def5678'.",
          "The length of the `source_reports` list is correctly set to 1."
        ],
        "scenario": "Test RunMeta to dict with all optional fields.",
        "token_usage": {
          "completion_tokens": 207,
          "prompt_tokens": 483,
          "total_tokens": 690
        },
        "why_needed": "Prevents regression in handling of legacy and new fields in the `RunMeta` class."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_meta_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 29,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007690909999951145,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'exit_code' field in the RunMeta object should be equal to 1.",
          "The 'interrupted' field in the RunMeta object should be True.",
          "The 'collect_only' field in the RunMeta object should be True.",
          "The 'collected_count' field in the RunMeta object should be equal to 10.",
          "The 'selected_count' field in the RunMeta object should be equal to 8.",
          "The 'deselected_count' field in the RunMeta object should be equal to 2."
        ],
        "scenario": "Test RunMeta to include run status fields.",
        "token_usage": {
          "completion_tokens": 175,
          "prompt_tokens": 285,
          "total_tokens": 460
        },
        "why_needed": "This test prevents a regression where the RunMeta object does not contain all necessary run status fields, potentially leading to incorrect analysis results."
      },
      "nodeid": "tests/test_models.py::TestRunMeta::test_run_status_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007644920000018374,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"schema_version.split('.').should.have.length.equal.to(3)\", 'message': 'Schema version should be in semver format.'}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 115,
          "total_tokens": 198
        },
        "why_needed": "The schema version should be in semver format."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 54,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009597279999979946,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'ReportRoot.schema_version', 'expected_value': 'SCHEMA_VERSION'}",
          "{'name': 'report.to_dict().schema_version', 'expected_value': 'SCHEMA_VERSION'}"
        ],
        "scenario": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 119,
          "total_tokens": 228
        },
        "why_needed": "The test is necessary to ensure that the ReportRoot object includes the schema version in its JSON representation."
      },
      "nodeid": "tests/test_models.py::TestSchemaVersion::test_schema_version_in_report_root",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 8,
          "line_ranges": "96-103"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000787876000003962,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key in the dictionary should match the expected value.",
          "The 'line_ranges' key in the dictionary should match the expected format (e.g., '1-3, 5, 10-15').",
          "The 'line_count' key in the dictionary should match the expected value (10 in this case)."
        ],
        "scenario": "CoverageEntry serialization test.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 256,
          "total_tokens": 375
        },
        "why_needed": "This test prevents a potential bug where the coverage entry is not properly serialized to JSON."
      },
      "nodeid": "tests/test_models.py::TestSourceCoverageEntry::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 5,
          "line_ranges": "286-288, 290, 292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007377819999874191,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "error": "Failed to parse LLM response as JSON",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_minimal",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 6,
          "line_ranges": "286-288, 290-292"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009248829999819463,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"Expected value of 'run_id' key\", 'value': 'run-1'}"
        ],
        "scenario": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 134,
          "total_tokens": 219
        },
        "why_needed": "To ensure that the SourceReport object's run_id attribute is correctly included in its dictionary representation."
      },
      "nodeid": "tests/test_models.py::TestSourceReport::test_to_dict_with_run_id",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007967219999898134,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'file_path' key is set to the correct file path.",
          "The 'line_ranges' key is set to the expected line ranges.",
          "The 'line_count' key is set to the correct line count."
        ],
        "scenario": "Test that the `CoverageEntry` class correctly serializes a coverage summary.",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 254,
          "total_tokens": 349
        },
        "why_needed": "This test prevents regression in coverage reporting functionality."
      },
      "nodeid": "tests/test_models.py::TestSummary::test_to_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007580700000175966,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'nodeid' field should match the node ID of the test.",
          "The 'outcome' field should be set to 'passed'.",
          "The 'duration' field should be set to 0.0 (or any other default value).",
          "The 'phase' field should be set to 'call'."
        ],
        "scenario": "Test that a minimal result has the required fields.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 244,
          "total_tokens": 363
        },
        "why_needed": "This test prevents regression where a minimal result is not provided with all necessary information."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_minimal_result",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 24,
          "line_ranges": "65-68, 190, 194-199, 201, 203, 205, 207, 210-212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007789679999916643,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the 'coverage' key in the result dictionary should be equal to 1.",
          "The value of the 'file_path' key within the 'coverage' list should match the provided file path.",
          "Each item in the 'coverage' list should have a 'file_path' attribute matching the provided file path."
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage verifies that the test result includes a coverage list.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 256,
          "total_tokens": 387
        },
        "why_needed": "This test prevents regression by ensuring that the coverage report is always present and accurate."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007392349999975067,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert llm_opt_out is True', 'expected_value': True, 'message': '', 'type': 'assertion'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 145,
          "total_tokens": 238
        },
        "why_needed": "To ensure that the LLM opt-out flag is correctly set in the result."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_llm_opt_out",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203, 205, 207-210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007999780000034207,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'rerun_count', 'expected_value': 2, 'message': 'Rerun count is not equal to 2'}",
          "{'name': 'final_outcome', 'expected_value': 'passed', 'message': 'Final outcome is not passed'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 162,
          "total_tokens": 280
        },
        "why_needed": "The test case result should include rerun fields."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_with_rerun",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 19,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007887680000067121,
      "file_path": "tests/test_models.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'field': 'final_outcome', 'expected_value': 'passed', 'actual_value': 'passed'}"
        ],
        "scenario": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
        "token_usage": {
          "completion_tokens": 148,
          "prompt_tokens": 152,
          "total_tokens": 300
        },
        "why_needed": "This test is needed because it ensures that the `result` dictionary does not include 'rerun_count' and 'final_outcome' fields when a result without reruns is returned."
      },
      "nodeid": "tests/test_models.py::TestTestCaseResult::test_result_without_rerun_excludes_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "96-103, 241-243, 263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008079929999951219,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result['param_id'] == 'a-b-c',",
          "assert result['param_summary'] == 'a=1, b=2, c=3',",
          "assert result['captured_stdout'] == 'stdout content',",
          "assert result['captured_stderr'] == 'stderr content',",
          "assert result['requirements'] == ['REQ-100'],",
          "assert result['llm_opt_out'] is True,",
          "assert result['llm_context_override'] == 'complete',",
          "assert len(result['coverage']) == 1,",
          "assert result['llm_annotation']['scenario'] == 'Tests foo'"
        ],
        "scenario": "Test to_dict includes all optional fields when set.",
        "token_usage": {
          "completion_tokens": 182,
          "prompt_tokens": 454,
          "total_tokens": 636
        },
        "why_needed": "Prevents bar regression in coverage report generation."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 59,
          "line_ranges": "263-266, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530-532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007890979999842784,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of 'artifacts' in the result dictionary is 2.",
          "The path of the first artifact entry in 'result' is 'report.html'.",
          "All required artifacts are included in the 'artifacts' list in 'result'."
        ],
        "scenario": "Test to_dict includes artifacts when set.",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 264,
          "total_tokens": 373
        },
        "why_needed": "This test prevents a potential bug where the 'to_dict' method does not include all required artifacts in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_artifacts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 58,
          "line_ranges": "241-243, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526-528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007770550000145704,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `result['collection_errors']` is 1.",
          "The value of `result['collection_errors'][0]['nodeid']` is 'broken_test.py'.",
          "The node id of the first collection error matches the expected node id."
        ],
        "scenario": "Test to_dict includes collection_errors when set.",
        "token_usage": {
          "completion_tokens": 108,
          "prompt_tokens": 243,
          "total_tokens": 351
        },
        "why_needed": "Prevents a potential bug where the test fails due to missing or incorrect collection errors in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_collection_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534-536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007680490000154805,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'custom_metadata' key should be present in the result dictionary with the correct values.",
          "The 'custom_metadata' key should contain the expected project, environment, and build_number values.",
          "The custom metadata should not be overridden by default settings (e.g., 'project', 'environment').",
          "Custom metadata should be included when the 'to_dict' method is called with a ReportRoot object that has it set.",
          "The custom metadata value should match the expected value for each key.",
          "If custom_metadata is not provided, the report dictionary should still contain default values (e.g., 'project': None, 'environment': None).",
          "Custom metadata should be included in the report even if it's empty or contains only default values."
        ],
        "scenario": "Test to_dict includes custom_metadata when set.",
        "token_usage": {
          "completion_tokens": 211,
          "prompt_tokens": 264,
          "total_tokens": 475
        },
        "why_needed": "Prevents a potential bug where custom metadata is not included in the report dictionary even if it's set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_custom_metadata",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538-540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007684289999758676,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'hmac_signature', 'actual': 'signature123'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 128,
          "total_tokens": 205
        },
        "why_needed": "HMAC signature is included in the report when it's set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_hmac_signature",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536-538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007888179999895328,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'sha256 in report.to_dict()', 'expected': 'abcdef1234567890', 'actual': 'None'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 131,
          "total_tokens": 236
        },
        "why_needed": "The test is necessary to ensure that the ReportRoot class correctly converts its internal state into a dictionary, including any SHA-256 hashes."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_sha256",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 63,
          "line_ranges": "96-103, 376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532-534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007803419999845573,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of `source_coverage` should be 1.",
          "The file path of the first element in `source_coverage` should match 'src/mod.py'.",
          "The source_coverage entry for 'src/mod.py' should have a 'file_path' key matching 'src/mod.py'."
        ],
        "scenario": "Test to_dict includes source_coverage when set.",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 282,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a regression where the coverage information is not included in the report."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 55,
          "line_ranges": "376-392, 394, 397, 399, 402, 405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007779670000047645,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"len(result['warnings']) == 1\", 'expected_value': 1, 'message': \"Expected len(result['warnings']) to be 1, but got {}\", 'actual_value': '1'}",
          "{'name': \"result['warnings'][0]['code'] == 'W001'\", 'expected_value': 'W001', 'message': \"Expected result['warnings'][0]['code'] to be 'W001', but got {}\", 'actual_value': \"'W001'\"}"
        ],
        "scenario": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
        "token_usage": {
          "completion_tokens": 182,
          "prompt_tokens": 151,
          "total_tokens": 333
        },
        "why_needed": "This test is needed to ensure that the `to_dict` method of `ReportRoot` includes warnings when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestReportRootToDict::test_to_dict_with_warnings",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 12,
          "line_ranges": "467-475, 477-479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007494539999868266,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"result['coverage_total_percent'] == 85.5\", 'expected_result': 85.5}"
        ],
        "scenario": "...",
        "token_usage": {
          "completion_tokens": 57,
          "prompt_tokens": 153,
          "total_tokens": 210
        },
        "why_needed": "..."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_with_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 11,
          "line_ranges": "467-475, 477, 479"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008315569999979289,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': 'None', 'actual_value': 'coverage_total_percent'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 131,
          "total_tokens": 226
        },
        "why_needed": "The test is needed because it checks that the `to_dict` method excludes coverage_total_percent when None."
      },
      "nodeid": "tests/test_models_coverage.py::TestSummaryToDict::test_to_dict_without_coverage_total_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 42,
          "line_ranges": "65-68, 130-133, 135, 137, 139, 141, 143, 190, 194-199, 201-207, 210-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007870340000124543,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert result['param_id'] == 'a-b-c',",
          "assert result['param_summary'] == 'a=1, b=2, c=3',",
          "assert result['captured_stdout'] == 'stdout content',",
          "assert result['captured_stderr'] == 'stderr content',",
          "assert result['requirements'] == ['REQ-100'],",
          "assert result['llm_opt_out'] is True,"
        ],
        "scenario": "Test to_dict includes all optional fields when set.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 454,
          "total_tokens": 590
        },
        "why_needed": "Prevents bar regression in coverage reporting."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_all_optional_fields",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220-222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007559150000133741,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected_value': {'captured_stderr': 'Error output here'}, 'actual_value': 'Error output here'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 149,
          "total_tokens": 247
        },
        "why_needed": "to include captured stderr in the result dictionary when to_dict is called on a TestCaseResult object"
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218-220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007618269999909444,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'captured_stdout', 'expected_value': 'Debug output here'}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 149,
          "total_tokens": 227
        },
        "why_needed": "The `to_dict` method includes captured stdout when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_captured_stdout",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 21,
          "line_ranges": "190, 194-199, 201, 203-207, 210, 212, 214, 216, 218, 220, 222, 224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008024429999977656,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'id': 'param_summary', 'expected': 'x=1, y=2'}"
        ],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 163,
          "total_tokens": 332
        },
        "why_needed": "The test to_dict method includes param_summary when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_param_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 20,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007819040000072164,
      "file_path": "tests/test_models_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'expected_value': ['REQ-001', 'REQ-002'], 'actual_value': ['REQ-001', 'REQ-002']}"
        ],
        "scenario": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 151,
          "total_tokens": 247
        },
        "why_needed": "To include requirements in the test case result dictionary when set."
      },
      "nodeid": "tests/test_models_coverage.py::TestTestCaseResultToDict::test_to_dict_with_requirements",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007584010000130093,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `defaults` contains the expected globs: `*.pyc`, `__pycache__/*`, and `*secret*`.",
          "The function `defaults` does not contain the expected glob `*password*`."
        ],
        "scenario": "Test the default exclude globs for the LLM context.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 222,
          "total_tokens": 325
        },
        "why_needed": "This test prevents a potential bug where the default exclude globs are not correctly set."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000811679999998205,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `--password` and `--token` patterns should match any occurrences in the provided redact patterns.",
          "The `--api[_-]?key` pattern should match any occurrences in the provided redact patterns."
        ],
        "scenario": "Test the default redact patterns of the Config class.",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 228,
          "total_tokens": 331
        },
        "why_needed": "Prevents a potential issue where sensitive information like API keys might be exposed if they are not properly redacted."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008362760000011349,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.provider == 'none'",
          "cfg.llm_context_mode == 'minimal'",
          "cfg.llm_max_tests == 0",
          "cfg.llm_max_retries == 10",
          "cfg.llm_context_bytes == 32000",
          "cfg.llm_context_file_limit == 10",
          "cfg.llm_requests_per_minute == 5",
          "cfg.llm_timeout_seconds == 30",
          "cfg.llm_cache_ttl_seconds == 86400",
          "cfg.include_phase == 'run'",
          "cfg.aggregate_policy == 'latest'",
          "not cfg.is_llm_enabled() is True"
        ],
        "scenario": "Test that default values are set correctly for the test_default_values scenario.",
        "token_usage": {
          "completion_tokens": 184,
          "prompt_tokens": 318,
          "total_tokens": 502
        },
        "why_needed": "This test prevents a regression where the default configuration settings are not properly initialized."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_default_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 293"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007479200000091168,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg', 'expected_type': 'Config'}",
          "{'name': 'cfg.provider', 'expected_value': 'none'}"
        ],
        "scenario": "tests/test_options.py",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 104,
          "total_tokens": 185
        },
        "why_needed": "To ensure that the default configuration is correctly set to 'none'."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_get_default_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008183219999864377,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `is_llm_enabled` should return False for provider 'none'.",
          "The function `is_llm_enabled` should return True for provider 'ollama'.",
          "The function `is_llm_enabled` should return True for provider 'litellm'.",
          "The function `is_llm_enabled` should return True for provider 'gemini'."
        ],
        "scenario": "Test that is_llm_enabled check is enabled for different providers.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 263,
          "total_tokens": 391
        },
        "why_needed": "Prevents regression in LLM configuration when switching between providers."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_is_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-221, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007810129999938908,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'length of errors list', 'value': 1, 'expected_value': 1}",
          "{'name': 'error message in first error', 'value': \"Invalid aggregate_policy 'random'\", 'expected_value': \"Invalid aggregate_policy 'random'\"}"
        ],
        "scenario": "test_validate_invalid_aggregate_policy",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 128,
          "total_tokens": 246
        },
        "why_needed": "To ensure that the `aggregate_policy` parameter is validated correctly and raises an error when it's invalid."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_aggregate_policy",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-213, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007801120000010542,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'value': \"Invalid llm_context_mode 'mega_max'\", 'expected_value': \"Invalid llm_context_mode 'mega_max'\"}"
        ],
        "scenario": "test_validate_invalid_context_mode",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 131,
          "total_tokens": 223
        },
        "why_needed": "To ensure that the `llm_context_mode` is valid and raises an error when it's invalid."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_context_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-229, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007910220000155732,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The error message should indicate an invalid include phase 'lunch_break'.\", 'expected_value': \"Invalid include_phase 'lunch_break'\", 'actual_value': \"Invalid include_phase 'lunch_break'\"}"
        ],
        "scenario": "test_validate_invalid_include_phase",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 129,
          "total_tokens": 230
        },
        "why_needed": "To ensure that the `include_phase` parameter is valid and does not cause any issues during validation."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 28,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007850699999778499,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'length of errors list', 'value': 1, 'expected_value': 1}",
          "{'name': 'error message in errors list', 'value': \"Invalid provider 'invalid_provider'\", 'expected_value': \"Invalid provider 'invalid_provider'\"}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 122,
          "total_tokens": 238
        },
        "why_needed": "To ensure that the Config class correctly handles and reports invalid providers."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_invalid_provider",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 31,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245-254, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007503749999955289,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.validate() should return at least 5 error messages.",
          "The 'llm_context_bytes' value must be at least 1000.",
          "The 'llm_max_tests' value must be 0 (no limit) or positive.",
          "The 'llm_requests_per_minute' value must be at least 1.",
          "The 'llm_timeout_seconds' value must be at least 1.",
          "The 'llm_max_retries' value must be 0 or positive.",
          "All error messages should contain the specified constraint strings."
        ],
        "scenario": "Test validation of numeric constraints for TestConfig.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 329,
          "total_tokens": 495
        },
        "why_needed": "Prevents regression due to invalid configuration values, ensuring correct behavior under different scenarios."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_numeric_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007648330000051828,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Configuration validation should be successful for a valid config', 'description': 'The function should return an empty list of errors when given a well-formed, non-empty configuration.'}"
        ],
        "scenario": "tests/test_options.py::TestConfig::test_validate_valid_config",
        "token_usage": {
          "completion_tokens": 99,
          "prompt_tokens": 100,
          "total_tokens": 199
        },
        "why_needed": "To ensure that the configuration is properly validated and no errors are raised when a valid configuration is provided."
      },
      "nodeid": "tests/test_options.py::TestConfig::test_validate_valid_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0037613249999992604,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `aggregate_dir` attribute of the configuration object should be set to 'aggr_dir'.",
          "The `aggregate_policy` attribute of the configuration object should be set to 'merge'.",
          "The `aggregate_run_id` attribute of the configuration object should be set to 'run-123'.",
          "The `aggregate_group_id` attribute of the configuration object should be set to 'group-abc'."
        ],
        "scenario": "Test the `load_aggregation_options` function to ensure it correctly loads aggregation options.",
        "token_usage": {
          "completion_tokens": 154,
          "prompt_tokens": 295,
          "total_tokens": 449
        },
        "why_needed": "This test prevents a bug where the aggregation policy is not being loaded correctly, potentially leading to incorrect results or errors in downstream processing."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_aggregation_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.003645449000003964,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg.batch_parametrized_tests is True', 'expected_value': True, 'message': 'Expected cfg.batch_parametrized_tests to be True'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 138,
          "total_tokens": 238
        },
        "why_needed": "To test that the batch flag is disabled by default and correctly handled when it's set to None."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_batch_flag_conflict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004103495999999041,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_max_retries` attribute in the config should be set to 10 by default.",
          "The `rootpath` attribute of the mock Pytest configuration should point to a temporary directory.",
          "The `llm_report_html`, etc. attributes in the config should not have any values assigned.",
          "The `llm_provider` and `llm_model` attributes should be set to their default values (e.g., 'default', 'default')",
          "The `llm_context_mode` attribute should be set to its default value ('strict')",
          "The `llm_context_compression` attribute should be set to its default value ('off')",
          "No other attributes in the config should have any values assigned."
        ],
        "scenario": "Test handling when pyproject.toml doesn't exist.",
        "token_usage": {
          "completion_tokens": 214,
          "prompt_tokens": 413,
          "total_tokens": 627
        },
        "why_needed": "This test prevents a potential bug where the LLM configuration is not properly loaded due to missing pyproject.toml file."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_config_missing_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607-608, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004257965999983071,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_pytest_config.option.llm_coverage_source', 'expected_value': 'cov_dir'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 126,
          "total_tokens": 198
        },
        "why_needed": "To test the coverage source option."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_coverage_source",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 85,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0038410150000061094,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"cfg.provider == 'none'\", 'expected': 'None'}",
          "{'name': 'cfg.report_html is None', 'expected': 'None'}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_defaults",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 116,
          "total_tokens": 210
        },
        "why_needed": "To test the functionality of the default configuration when no options are set."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 132,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492-494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004363193000017418,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'CLI options override pyproject.toml options', 'expected_value': 'True'}"
        ],
        "scenario": "test_load_from_cli_overrides_pyproject",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 134,
          "total_tokens": 204
        },
        "why_needed": "To test that CLI options override pyproject.toml options."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 133,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004895369999985633,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'override pyproject.toml', 'expected_value': 'pyproject.toml'}"
        ],
        "scenario": "load config from cli provider override",
        "token_usage": {
          "completion_tokens": 65,
          "prompt_tokens": 130,
          "total_tokens": 195
        },
        "why_needed": "CLI provider option overrides pyproject.toml"
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_provider_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 86,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494-495, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0036057639999853563,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_pytest_config.option.llm_max_retries', 'expected_value': 2, 'actual_value': 2}"
        ],
        "scenario": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 130,
          "total_tokens": 215
        },
        "why_needed": "To test the functionality of loading retries from CLI."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_cli_retries",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 134,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386-388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005105604000021913,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The file is created with the correct name and path.', 'expected_result': 'pyproject.toml', 'actual_result': \"tmp_path / 'pyproject.toml'\"}",
          "{'name': 'The file has the correct content.', 'expected_result': \"The file contains the following lines:\\n```\\ntmp_path / 'pyproject.toml'\\n\\n[tool.pyprojecttools]\\n    tools = ['pyproject']\\n```\", 'actual_result': \"tmp_path / 'pyproject.toml'\\n\\n[tool.pyprojecttools]\\n    tools = ['pyproject']\\n```\\n\"}"
        ],
        "scenario": "test_load_from_pyproject",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 119,
          "total_tokens": 312
        },
        "why_needed": "To test the functionality of the `load_config` function in `tests/test_options.py`."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 88,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460, 463, 466, 470-474, 476-477, 479, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0035362940000140952,
      "file_path": "tests/test_options.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "cfg.prompt_tier == 'minimal'",
          "cfg.batch_parametrized_tests is False",
          "cfg.context_compression == 'none'"
        ],
        "scenario": "Test loading token optimization options from CLI.",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 264,
          "total_tokens": 368
        },
        "why_needed": "Prevents a bug where the 'llm_prompt_tier' option is set to 'minimal' and the 'batch_parametrized_tests' option is True, causing an error when running tests in parallel."
      },
      "nodeid": "tests/test_options.py::TestLoadConfig::test_load_token_optimization_options",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488, 490-492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005052644000016926,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of `llm_dependency_snapshot` is set to `deps.json` as expected.",
          "The configuration file `deps.json` is loaded and its contents are compared with the expected snapshot.",
          "The `report_dependency_snapshot` field in the configuration is updated correctly based on the specified snapshot file.",
          "No dependency snapshots are missing or incorrect when using the correct `llm_dependency_snapshot` option.",
          "Dependency snapshots are generated correctly for all dependencies when using the correct `llm_dependency_snapshot` option."
        ],
        "scenario": "Verify that the `llm_dependency_snapshot` option is used to specify a dependency snapshot file.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 213,
          "total_tokens": 393
        },
        "why_needed": "This test prevents a potential regression where the `llm_dependency_snapshot` option is not used correctly, leading to incorrect or missing dependency snapshots in the report."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_dependency_snapshot",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486, 488-490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.005622141000003467,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The value of 'llm_evidence_bundle' in the configuration file should be 'bundle.zip'.",
          "The value of 'llm_evidence_bundle' in the configuration file should match the expected value when CLI override is enabled.",
          "The mock object's option.llm_evidence_bundle attribute should have been set to 'bundle.zip'."
        ],
        "scenario": "Verify that the 'llm_evidence_bundle' option is correctly set to 'bundle.zip' when CLI override is enabled.",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 217,
          "total_tokens": 368
        },
        "why_needed": "This test prevents a potential bug where the 'llm_evidence_bundle' option is not set to the correct value even though CLI override is enabled."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_evidence_bundle",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484-486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0053057179999882464,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `llm_report_json` option is set to 'output.json' in the mock configuration.",
          "The `report_json` value in the loaded configuration matches 'output.json'.",
          "The expected value for `report_json` is not being overridden by any other options.",
          "The test ensures that the correct value is used for report JSON.",
          "No other values are overriding or setting the `report_json` option.",
          "The mock configuration does not contain any other override for `report_json`.",
          "The loaded configuration has a valid and expected value for `report_json`."
        ],
        "scenario": "Verify that the `test_cli_report_json` test sets the expected value for `report_json` in the configuration.",
        "token_usage": {
          "completion_tokens": 193,
          "prompt_tokens": 212,
          "total_tokens": 405
        },
        "why_needed": "This test prevents a potential bug where the report JSON is not set correctly, potentially leading to incorrect or missing reports."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 92,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470-474, 476-477, 479, 482, 484, 486-488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004992180999977336,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_pdf` option is set to 'output.pdf'.",
          "The `llm_report_pdf` option is set to 'output.pdf'.",
          "The output of `cfg.report_pdf` is 'output.pdf'."
        ],
        "scenario": "Verify that the `test_cli_report_pdf` test function sets the `report_pdf` option to 'output.pdf' correctly.",
        "token_usage": {
          "completion_tokens": 131,
          "prompt_tokens": 212,
          "total_tokens": 343
        },
        "why_needed": "This test prevents a potential bug where the `llm_report_pdf` option is not set to the expected value, causing the CLI report PDF to be generated incorrectly."
      },
      "nodeid": "tests/test_options_coverage.py::TestCliOverrides::test_cli_report_pdf",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-237, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007849999999791635,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'pattern': 'litellm_token_output_format', 'value': 'xml'}"
        ],
        "scenario": "test_validate_invalid_token_output_format",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 130,
          "total_tokens": 208
        },
        "why_needed": "To ensure that the token output format is valid and does not cause coverage issues."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_invalid_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241-242, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007830670000146256,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The token refresh interval must be at least 60 seconds', 'expected_value': 60, 'actual_value': 30}"
        ],
        "scenario": "Test validation when token refresh interval is too short",
        "token_usage": {
          "completion_tokens": 94,
          "prompt_tokens": 146,
          "total_tokens": 240
        },
        "why_needed": "Because the current token refresh interval (30 seconds) is too short, which may lead to unexpected behavior and security vulnerabilities."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_token_refresh_interval_too_short",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 26,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007787790000008954,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The validate method returns an empty list of errors.', 'expected_result': [], 'message': 'Expected the validate method to return an empty list of errors.'}"
        ],
        "scenario": "Test validation of valid LiteLLM config.",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 142,
          "total_tokens": 232
        },
        "why_needed": "To ensure the validation process works correctly with a valid LiteLLM configuration."
      },
      "nodeid": "tests/test_options_coverage.py::TestConfigValidationCoverage::test_validate_valid_litellm_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438-440, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014750530000071649,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'include_history=true', 'actual': 'include_history=true'}",
          "{'name': 'aggregate_include_history file path', 'expected': '/path/to/aggregate_include_history.py', 'actual': '/path/to/aggregate_include_history.py'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
        "token_usage": {
          "completion_tokens": 133,
          "prompt_tokens": 118,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the aggregate_include_history feature is loaded correctly and includes all necessary history."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_include_history",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436-438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001449043999997457,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has an aggregate_policy section', 'expected_value': 'pyproject.toml should exist and have an aggregate_policy section'}",
          "{'name': 'aggregate_policy is present in pyproject.toml', 'expected_value': 'aggregate_policy should be present in pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 121,
          "total_tokens": 258
        },
        "why_needed": "To ensure that the aggregate policy is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_aggregate_policy_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 150,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340-346, 348-350, 352-354, 356-357, 360-369, 372-375, 378-392, 396, 400, 402, 404, 408-410, 412-413, 416-422, 426-428, 430-432, 436-440, 444-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0021522520000019085,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'All config keys should be present in pyproject.toml', 'expected': ['...'], 'actual': {'scenario': 'tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined', 'why_needed': '...', 'key_assertions': ['...']}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 120,
          "total_tokens": 245
        },
        "why_needed": "To ensure that all configuration keys are loaded when loading the PyProject."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_all_config_keys_combined",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390-392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014378139999848827,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True'}",
          "{'name': 'cache_dir path exists in pyproject.toml', 'expected': '/path/to/cache/dir'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 113,
          "total_tokens": 218
        },
        "why_needed": "To ensure the cache directory is loaded correctly from the PyProject file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_dir",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388-390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00148843799999554,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and is not empty', 'value': 'True'}",
          "{'name': 'pyproject.toml contents match expected format', 'value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 116,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the cache TTL seconds are loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_cache_ttl_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418-420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014635819999853084,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml', 'value': \"Missing 'capture_failed_output' key in pyproject.toml\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
        "token_usage": {
          "completion_tokens": 96,
          "prompt_tokens": 116,
          "total_tokens": 212
        },
        "why_needed": "The test is failing because the `capture_failed_output` key is not present in the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_failed_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420-422, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015015029999858598,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'max_chars = 1024', 'actual': 'max_chars = 2048'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 119,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the `capture_output.max_chars` option is correctly loaded and applied when running tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_capture_output_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362-364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014819859999875007,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context bytes are present in the PyPI repository', 'expected_value': 'True'}",
          "{'name': 'Context bytes are correctly extracted and returned', 'expected_value': 'The `context_bytes` feature is properly loaded from the PyPI repository.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 113,
          "total_tokens": 236
        },
        "why_needed": "To ensure that the `context_bytes` feature is properly loaded from the PyPI repository."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368-369, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015107899999975416,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is readable', 'description': 'The pyproject.toml file should exist and be readable.'}",
          "{'name': 'context_exclude_globs setting is defined in pyproject.toml', 'description': 'The `context_exclude_globs` setting should be defined in the `pyproject.toml` file.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
        "token_usage": {
          "completion_tokens": 152,
          "prompt_tokens": 119,
          "total_tokens": 271
        },
        "why_needed": "To ensure that the `context_exclude_globs` setting in pyproject.toml is properly loaded and excluded from the test coverage."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_exclude_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364-366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001562086000006957,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True'}",
          "{'name': 'pyproject.toml content', 'expected': 'The contents of pyproject.toml are correct.'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
        "token_usage": {
          "completion_tokens": 111,
          "prompt_tokens": 116,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the context file limit is correctly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_file_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366-368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014715770000179873,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"['*.txt', '*.py']\", '# Expected contents of pyproject.toml file\\n': '', 'actual': \"['*.txt', '*.py']\\n\"}",
          "{'name': 'context_include_globs option value', 'expected': '[\"*.txt\", \"*.py\"]', '# Expected value of context_include_globs option\\n': '', 'actual': '[\"*.txt\", \"*.py\"]\\n'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 119,
          "total_tokens": 298
        },
        "why_needed": "To ensure that the `context_include_globs` option is properly loaded from the PyProject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_context_include_globs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446-447, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001478639999987763,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'pyproject.toml exists in the current working directory', 'actual': 'pyproject.toml was created at path /tmp/pyproject.toml'}",
          "{'name': 'pyproject.toml is not empty', 'expected': 'pyproject.toml is not empty', 'actual': 'pyproject.toml contents were read successfully'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
        "token_usage": {
          "completion_tokens": 153,
          "prompt_tokens": 118,
          "total_tokens": 271
        },
        "why_needed": "To ensure that the hmac key file is loaded correctly and used for HMAC operations."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_hmac_key_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372-374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001531318999980158,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': '```\\ntoml\\ninclude_param_values = [\"path/to/file1\", \"path/to/file2\"]\\n```', 'actual': '```\\ntoml\\ninclude_param_values = []\\n```'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 116,
          "total_tokens": 248
        },
        "why_needed": "To ensure that the include_param_values function in options.py is correctly loading parameter values from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_param_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014287770000009914,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has an includes section', 'expected': 'True', 'actual': 'False'}",
          "{'name': 'includes section in pyproject.toml is not empty', 'expected': 'True', 'actual': ''}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
        "token_usage": {
          "completion_tokens": 123,
          "prompt_tokens": 113,
          "total_tokens": 236
        },
        "why_needed": "To ensure that the include phase is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_phase",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426-428, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001473790999995117,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `include_pytest_invocation` option is present in `pyproject.toml`', 'value': 'True'}",
          "{'name': 'The `include_pytest_invocation` option has the correct path', 'value': '/path/to/include_pytest_invocation'}"
        ],
        "scenario": "Tests for `tests/test_options_coverage`",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 122,
          "total_tokens": 248
        },
        "why_needed": "To ensure that the `include_pytest_invocation` option is correctly loaded from `pyproject.toml`."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_include_pytest_invocation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430-432, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014757540000118752,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'file existence', 'expected_value': 'invocation_redact_patterns.toml', 'actual_value': 'pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_invocation_redact_patterns",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 121,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the invocation_redact_patterns are correctly loaded from the pyproject.toml file and used during testing."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_invocation_redact_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340-342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001549331999996184,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base",
        "token_usage": {
          "completion_tokens": 54,
          "prompt_tokens": 122,
          "total_tokens": 176
        },
        "why_needed": "To ensure that the litellm_api_base is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_base",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342-344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015008019999811495,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'litellm_api_key', 'expected_value': '<API_KEY>', 'actual_value': '<-loaded_api_key>'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 122,
          "total_tokens": 214
        },
        "why_needed": "To ensure that the litellm API key is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_api_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356-357, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014765560000000733,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"litellm_token_json_key = 'path/to/litellm_token.json'\", 'actual': 'pyproject.toml contents'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 125,
          "total_tokens": 238
        },
        "why_needed": "To ensure that the litellm token JSON key is correctly loaded from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352-354, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014988869999967847,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': \"The contents of the pyproject.toml file should contain a section named 'litellm.token.output_format'.\"}",
          "{'name': 'litellm.token.output_format value', 'expected': \"The value of the 'litellm.token.output_format' setting in the pyproject.toml file should be 'litellm_token_output_format'.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
        "token_usage": {
          "completion_tokens": 161,
          "prompt_tokens": 125,
          "total_tokens": 286
        },
        "why_needed": "To ensure that the litellm_token_output_format is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_output_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344-346, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0015078650000077687,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The `litellm_token_refresh_command` should be present in the `pyproject.toml` file', 'expected_value': 'litellm_token_refresh_command'}",
          "{'name': 'The `litellm_token_refresh_command` should have the correct path to the token', 'expected_value': '/path/to/token'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 125,
          "total_tokens": 275
        },
        "why_needed": "To ensure that the `litellm_token_refresh_command` is properly loaded from the `pyproject.toml` file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 111,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348-350, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0022153590000186796,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The correct token refresh interval is loaded', 'description': \"The value of 'litellm.token_refresh_interval' in the .pyproject.toml file should be equal to the expected value.\"}"
        ],
        "scenario": "Loading a specific configuration file from the .pyproject.toml file.",
        "token_usage": {
          "completion_tokens": 101,
          "prompt_tokens": 125,
          "total_tokens": 226
        },
        "why_needed": "To ensure that the correct token refresh interval is loaded for litellm."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_litellm_token_refresh_interval",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 73,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 449, 451, 453-456, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014443749999770716,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 158,
          "total_tokens": 286
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_malformed_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380-382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014520189999984723,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `max_concurrency` value in `pyproject.toml` is read correctly from disk.",
          "The `max_concurrency` value in `pyproject.toml` is not overridden by any environment variables."
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 116,
          "total_tokens": 225
        },
        "why_needed": "To ensure that the `max_concurrency` setting in `pyproject.toml` is correctly applied when loading dependencies."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_concurrency",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378-380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014644929999860778,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"pyproject.toml should contain a 'max_tests' setting\", 'value': 'True'}",
          "{'name': \"pyproject.toml should have a value for 'max_tests'\", 'value': 10}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests",
        "token_usage": {
          "completion_tokens": 126,
          "prompt_tokens": 113,
          "total_tokens": 239
        },
        "why_needed": "To ensure that the 'max_tests' setting in pyproject.toml is correctly loaded and used to determine the number of tests to run."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444-446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00146031599999219,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': 'True'}",
          "{'name': 'metadata_file_path', 'expected': '/path/to/pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 113,
          "total_tokens": 217
        },
        "why_needed": "To ensure that the metadata file is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_metadata_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336-337, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014468310000097517,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'The ollama_host is present in the pyproject.toml file', 'value': 'True'}",
          "{'name': 'The ollama_host is a valid Python module', 'value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 119,
          "total_tokens": 238
        },
        "why_needed": "To ensure that the ollama_host is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_ollama_host",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 110,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.001485611999981984,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is not empty', 'expected': {'path': '/tmp/pyproject.toml', 'status': 0}, 'actual': {'path': '/tmp/pyproject.toml', 'status': 0}}",
          "{'name': 'pyproject.toml contains omit_tests_from_coverage section', 'expected': {'key': 'omit_tests_from_coverage', 'value': 'True'}, 'actual': {'key': 'omit_tests_from_coverage', 'value': 'True'}}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
        "token_usage": {
          "completion_tokens": 183,
          "prompt_tokens": 121,
          "total_tokens": 304
        },
        "why_needed": "To ensure that omit_tests_from_coverage is correctly loaded from pyproject.toml, even when tests are omitted."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_omit_tests_from_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374-375, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014585519999741337,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'max_chars = 100', 'actual': 'max_chars = 100'}",
          "{'name': 'param_value_max_chars value is an integer', 'expected': 100, 'actual': 100}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 119,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the `param_value_max_chars` option is correctly loaded and used in the build process."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_param_value_max_chars",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416-418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014540939999960756,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'Collect keyword is present in pyproject.toml', 'actual': 'Collect keyword is not present in pyproject.toml'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 116,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the 'collect' option is loaded correctly from pyproject.toml."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_report_collect_only",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 109,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0014620379999996658,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'value': 'True'}",
          "{'name': 'timeout_seconds setting exists', 'value': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 113,
          "total_tokens": 217
        },
        "why_needed": "To ensure that the timeout seconds setting in pyproject.toml is properly loaded and used when loading dependencies."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectLoadingCoverage::test_load_timeout_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400-402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.00412575799998649,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists and is not empty', 'expected_value': 'True'}",
          "{'name': 'pyproject.toml file has a correct `optimizations` section', 'expected_value': \"{'batch_max_tests': True}\"}"
        ],
        "scenario": "test_load_batch_max_tests",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 117,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the `batch_max_tests` optimization is correctly applied to batched tests in Pytest."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_max_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 131,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396-398, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004979507000001604,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'value': 'batch_parametrized_tests'}"
        ],
        "scenario": "load_batch_parametrized_tests",
        "token_usage": {
          "completion_tokens": 66,
          "prompt_tokens": 123,
          "total_tokens": 189
        },
        "why_needed": "Optimize PyProject token optimization for parametrized tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_batch_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402-404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004160762999987355,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml file exists', 'expected': 'True'}",
          "{'name': 'context_compression directory exists', 'expected': 'True'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
        "token_usage": {
          "completion_tokens": 103,
          "prompt_tokens": 117,
          "total_tokens": 220
        },
        "why_needed": "To ensure that the context compression feature is correctly loaded and used in the project."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404-405, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004889058000003388,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Context line padding is applied correctly', 'expected_value': 'True'}",
          "{'name': 'Context line padding is not applied to empty lines', 'expected_value': 'False'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 117,
          "total_tokens": 227
        },
        "why_needed": "To ensure that the context line padding is correctly loaded and applied to the test files."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_context_line_padding",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 130,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332, 334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392-393, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 502, 504-505, 507, 511-512, 514, 516-517, 519, 521-522, 524, 528-529, 531, 534-535, 537-538, 540, 542-543, 545, 547-548, 550-551, 554-555, 557-558, 561-562, 564, 566-567, 569, 572-573, 575-576, 578, 581-584, 588-589, 591, 593-594, 596, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.004312067000000752,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists and has a prompt_tier section', 'expected_value': 'True'}",
          "{'name': \"pyproject.toml has a 'prompt_tier' section with the correct type\", 'expected_value': 'prompt_tier: string'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 117,
          "total_tokens": 247
        },
        "why_needed": "To ensure that the 'prompt_tier' is loaded correctly from the pyproject.toml file."
      },
      "nodeid": "tests/test_options_coverage.py::TestPyprojectTokenOptimization::test_load_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271-273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007537209999952665,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'batch_max_tests must be at least 1', 'description': \"The 'batch_max_tests' field in the configuration should be set to a value greater than or equal to 1.\"}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 135,
          "total_tokens": 251
        },
        "why_needed": "The test validation with batch_max_tests < 1 is necessary because it checks if the configuration can be validated without any tests."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_batch_max_tests_too_small",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 27,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273-274, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008012199999996028,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The context_line_padding must be 0 or positive', 'expected_value': '0'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 129,
          "total_tokens": 206
        },
        "why_needed": "Negative context_line_padding is not allowed."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_context_line_padding_negative",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-269, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007718560000000707,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'value': 'Invalid context_compression'}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 124,
          "total_tokens": 197
        },
        "why_needed": "To ensure that the test suite covers all possible error messages when validating with invalid context compression."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_context_compression",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 29,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-261, 265-266, 271, 273, 276"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007879059999993387,
      "file_path": "tests/test_options_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Test case 1: Invalid prompt_tier value', 'description': 'The test case checks if the validation function returns an error message for an invalid `prompt_tier` value.', 'expected_result': 'Invalid prompt_tier'}",
          "{'name': 'Test case 2: Multiple errors for multiple invalid prompt_tier values', 'description': 'The test case checks if the validation function correctly reports multiple errors for different invalid `prompt_tier` values.', 'expected_result': ['Invalid prompt_tier']}"
        ],
        "scenario": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
        "token_usage": {
          "completion_tokens": 179,
          "prompt_tokens": 125,
          "total_tokens": 304
        },
        "why_needed": "To ensure that the validation function correctly identifies and reports invalid `prompt_tier` values."
      },
      "nodeid": "tests/test_options_coverage.py::TestValidationCoverageExtended::test_validate_invalid_prompt_tier",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 124,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-337, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378-380, 382, 384-386, 388, 390, 392, 396, 400, 402, 404, 408-410, 412-413, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603-605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002660503000015524,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'cfg is an instance of Config', 'expected_type': 'Config'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
        "token_usage": {
          "completion_tokens": 72,
          "prompt_tokens": 119,
          "total_tokens": 191
        },
        "why_needed": "To ensure that the plugin configuration has safe defaults."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_config_defaults",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007621880000101555,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "pytestconfig is not None"
        ],
        "scenario": "tests/test_plugin_integration.py",
        "token_usage": {
          "completion_tokens": 55,
          "prompt_tokens": 108,
          "total_tokens": 163
        },
        "why_needed": "Because the `TestPluginConfigLoading` test relies on the plugin's configuration being available."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginConfigLoading::test_markers_exist_in_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 91,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09355913999999643,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should exist at the specified path.",
          "The `report.html` file should exist at the specified path.",
          "Both `report.json` and `report.html` files should be present in the report directory.",
          "The `report.json` file should contain valid JSON data.",
          "The `report.html` file should not contain any syntax errors or warnings.",
          "The test function `test_simple()` should return a successful assertion result."
        ],
        "scenario": "Test generates both JSON and HTML reports for a simple test.",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 279,
          "total_tokens": 430
        },
        "why_needed": "This test prevents regression in cases where the report generation is not compatible with different output formats."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_both_json_and_html_outputs",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05938679100000854,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Expected pytest_collection_finish counts items to return the correct count', 'description': 'The test should run successfully and report the correct number of tests.', 'expected_result': 3, 'actual_result': 0}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 198,
          "total_tokens": 305
        },
        "why_needed": "pytest_collection_finish counts items (line 378)"
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_collection_finish_counts_items",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 11,
          "line_ranges": "70-71, 73-75, 77, 79, 142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05730061299999534,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `nested` directory should be created in the report.json file.",
          "The `report.json` file should exist.",
          "The `nested` directory should have been created using `pytester.makepyfile()`.",
          "The `report.json` file should contain a nested directory structure.",
          "The `test_pass()` function should not be executed inside the nested directory.",
          "The `report.json` file should not be empty after running `pytester.runpytest()`."
        ],
        "scenario": "Test that output directories are created if missing.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 247,
          "total_tokens": 396
        },
        "why_needed": "This test prevents a regression where the plugin does not create nested directories for reports."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_creates_nested_directory",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 50,
          "line_ranges": "78-79, 90, 93-94, 96, 99-103, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330, 332, 334-335, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06210264699998902,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the report contains an error code of 1.",
          "The 'error' value in the 'summary' key is set to True.",
          "The fixture name 'broken_fixture' is used in the test.",
          "The pytester.runpytest command captures and logs fixture errors.",
          "The captured error message is included in the report JSON file."
        ],
        "scenario": "Test that fixture errors are captured in report.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 286,
          "total_tokens": 408
        },
        "why_needed": "Fixture errors should be reported and logged for debugging purposes."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_fixture_error_captured",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 59,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 114,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.1696460310000134,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'passed' outcome should be present in the report.",
          "The 'failed' outcome should be present in the report.",
          "The 'skipped' outcome should be present in the report.",
          "All three types of outcomes ('passed', 'failed', and 'skipped') should be included in the report.",
          "The report path is correctly set to capture the output of pytest_runtest_makereport.",
          "The report format (json) is used as expected."
        ],
        "scenario": "Test pytest_runtest_makereport captures all outcomes to ensure accurate reporting.",
        "token_usage": {
          "completion_tokens": 164,
          "prompt_tokens": 335,
          "total_tokens": 499
        },
        "why_needed": "This test prevents a potential issue where the report may not capture all outcomes, leading to inaccurate reporting and debugging difficulties."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_makereport_captures_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 250,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403-404, 558-559, 562-563, 566-568, 579, 583, 602-603, 619-620"
        }
      ],
      "duration": 0.054035345999977835,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'not exists', 'expected_result': 'True'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 150,
          "total_tokens": 228
        },
        "why_needed": "To ensure that the plugin correctly handles cases where no output is specified."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_no_report_when_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486-488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.594513639000013,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytester.makepyfile` function should be called with a valid test file.",
          "The `test_pass()` function should be defined and return True.",
          "The `result.ret` attribute of the `pytester.runpytest` function should be equal to 0 (success) or warning.",
          "The `assert result.ret == 0` statement should not throw an error if the plugin is disabled.",
          "If the plugin was enabled, it should exit with code 0 (success) or warning, but definitely run.",
          "The `pytester.makepyfile` function should be called with a valid test file that includes the `--llm-pdf=report.pdf` option."
        ],
        "scenario": "Test that --llm-pdf option enables the plugin.",
        "token_usage": {
          "completion_tokens": 195,
          "prompt_tokens": 435,
          "total_tokens": 630
        },
        "why_needed": "To prevent a regression where the plugin is disabled due to missing Playwright configuration."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_pdf_option_enables_plugin",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 75,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05925473300001727,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'start_time' key is present in the run meta data.",
          "The value of the 'start_time' key matches the actual start time recorded by pytest.",
          "The 'start_time' key is correctly formatted as a timestamp.",
          "The test function `test_pass()` does not affect the start time recording.",
          "The report generated by pytester includes the correct start time data.",
          "The start time is reported in seconds since the epoch.",
          "The start time is recorded at the beginning of each session."
        ],
        "scenario": "Test that the `pytest_sessionstart` hook records start time correctly when running with pytester.",
        "token_usage": {
          "completion_tokens": 176,
          "prompt_tokens": 276,
          "total_tokens": 452
        },
        "why_needed": "This test prevents a potential bug where the `pytest_sessionstart` hook does not record the start time of the session."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginHooksWithPytester::test_session_start_records_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007512880000035693,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "error": "Failed after 10 retries. Last error: timed out",
        "key_assertions": [],
        "scenario": "",
        "why_needed": ""
      },
      "llm_context_override": "balanced",
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_context_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007562060000054771,
      "file_path": "tests/test_plugin_integration.py",
      "llm_opt_out": true,
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000777295999995431,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'TypeError', 'message': 'requirement marker should not cause errors.'}"
        ],
        "scenario": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 90,
          "total_tokens": 170
        },
        "why_needed": "Because the requirement marker is causing a TypeError when it's used as a function."
      },
      "nodeid": "tests/test_plugin_integration.py::TestPluginIntegration::test_requirement_marker",
      "outcome": "passed",
      "phase": "call",
      "requirements": [
        "REQ-001",
        "REQ-002"
      ]
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 136,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.04132402599998386,
      "file_path": "tests/test_plugin_integration.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the report writer writes a valid JSON file.",
          "Verify that the report writer writes a valid HTML file containing all test names.",
          "Check if there are any assertion errors in the report.",
          "Verify that the total number of tests is correct (2 in this case).",
          "Verify that only 'test_a.py' and 'test_b.py' are included in the HTML report.",
          "Ensure the report writer can handle test failures with error messages."
        ],
        "scenario": "Test the integration of report writer with pytest_llm_report.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 417,
          "total_tokens": 566
        },
        "why_needed": "This test prevents regression that may occur when using the report writer to generate reports."
      },
      "nodeid": "tests/test_plugin_integration.py::TestReportGeneration::test_report_writer_integration",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586-587"
        }
      ],
      "duration": 0.0013826699999981429,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 151,
          "total_tokens": 237
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 12,
          "line_ranges": "558-559, 562, 566-568, 579-580, 586, 590-592"
        }
      ],
      "duration": 0.0016255939999894053,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_report was called once with mock_report as argument', 'expected': 1, 'actual': 0}"
        ],
        "scenario": "TestPluginCollectReport",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 204,
          "total_tokens": 285
        },
        "why_needed": "To test that the collectreport function calls the collector when enable is True."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.0008849870000062765,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pytest_collectreport() was called with no arguments', 'description': 'The pytest_collectreport() function was called with no arguments. This indicates that the function did not receive any arguments, which is unexpected behavior.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 138,
          "total_tokens": 257
        },
        "why_needed": "This test is needed because the `pytest_collectreport` function should not raise an exception when a session is not available."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_no_session",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 579, 583"
        }
      ],
      "duration": 0.000894837000004145,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_report.session is None', 'expected': 'None'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 134,
          "total_tokens": 212
        },
        "why_needed": "To ensure the plugin correctly handles a null session in Pytest."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginCollectReport::test_pytest_collectreport_session_none",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002937082000016744,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'LLM model compatibility', 'description': 'The LLM model used by the plugin is not compatible with PyTorch. This raises a warning.'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
        "token_usage": {
          "completion_tokens": 104,
          "prompt_tokens": 143,
          "total_tokens": 247
        },
        "why_needed": "Llama model is not compatible with PyTorch, and this warning is raised to inform the user."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_llm_enabled_warning",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 135,
          "line_ranges": "123, 171, 199, 202-205, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 25,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-358, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.00261820399998669,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml validation error', 'message': 'Invalid pyproject.toml file'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 134,
          "total_tokens": 214
        },
        "why_needed": "Validation errors are raised when the pytest configuration is invalid."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_validation_errors",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 17,
          "line_ranges": "328-330, 332-334, 336-338, 342-343, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012054069999862804,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'mocking', 'expected_result': 'mock_config.addinivalue_line.called', 'actual_result': 'True'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 170,
          "total_tokens": 262
        },
        "why_needed": "To ensure that the configure function skips on xdist workers as expected."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigure::test_pytest_configure_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 30,
          "line_ranges": "328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0030557039999905555,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The Config.validate() method should return an empty list.",
          "The load_config() method of pytest_llm_report.options.Config should be called once.",
          "The mock_load.assert_called_once() assertion should pass with the correct arguments."
        ],
        "scenario": "Test that fallback to load_config is triggered when Config.load is missing.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 747,
          "total_tokens": 844
        },
        "why_needed": "To prevent a crash due to missing Config.load method."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginConfigureFallback::test_pytest_configure_fallback_load",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 122,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-334, 336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599-607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.002297593999998071,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml contents', 'expected': 'pyproject.toml should contain some overrides'}",
          "{'name': 'load_config function call', 'expected': 'load_config function should be called with pyproject.toml as argument'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 140,
          "total_tokens": 264
        },
        "why_needed": "To test the functionality of CLI options overriding pyproject.toml options in plugin load configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_cli_overrides_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 112,
          "line_ranges": "123, 171, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360-362, 364, 366, 368, 372, 374, 378, 380, 382-384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.09107045800001856,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'pyproject.toml exists', 'expected': True, 'message': 'PyProject.toml should exist in the test directory'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 136,
          "total_tokens": 231
        },
        "why_needed": "To ensure that the plugin can load configuration from a PyProject.toml file."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginLoadConfig::test_load_config_from_pyproject",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 9,
          "line_ranges": "399, 403-404, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0012816509999993286,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_terminal_summary` function should return None when called with an invalid stash value (in this case, False).",
          "The `stash.get` method of the mock configuration object was called once with a False argument.",
          "The `enabled_key` variable in the mock configuration object is not set to True."
        ],
        "scenario": "Test that terminal summary skips when plugin is disabled.",
        "token_usage": {
          "completion_tokens": 121,
          "prompt_tokens": 281,
          "total_tokens": 402
        },
        "why_needed": "This test prevents a regression where the plugin's terminal summary function does not skip when the plugin is disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "399-400, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.001005292999991525,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [],
        "scenario": "",
        "token_usage": {
          "completion_tokens": 80,
          "prompt_tokens": 164,
          "total_tokens": 244
        },
        "why_needed": ""
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::test_terminal_summary_worker_skip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 69,
          "line_ranges": "123, 171, 308, 311-312, 320-322, 460-461, 463-464, 466-467, 470, 472-473, 476-477, 482-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0038284009999927093,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option of the loaded config is set to 'out.html'.",
          "The `llm_report_json` and `llm_report_pdf` options are not set in the loaded config.",
          "The `llm_evidence_bundle`, `llm_dependency_snapshot`, `llm_requests_per_minute`, `llm_aggregate_dir`, `llm_aggregate_policy`, `llm_aggregate_run_id`, `llm_aggregate_group_id`, and `llm_max_retries` options are not set in the loaded config.",
          "The `llm_coverage_source`, `llm_prompt_tier`, `llm_batch_parametrized`, `llm_context_compression`, `llm_context_bytes`, `llm_context_file_limit`, `llm_max_tests`, and `llm_max_concurrency` options are not set in the loaded config.",
          "The `llm_timeout_seconds` option is set to a non-zero value.",
          "The `llm_capture_failed` option is not set in the loaded config.",
          "The `llm_ollama_host`, `llm_litellm_api_base`, and `llm_litellm_token_refresh_command` options are not set in the loaded config.",
          "The `llm_litellm_api_key` option is not set in the loaded config.",
          "The `llm_litellm_token_output_format` option is not set in the loaded config.",
          "The `llm_litellm_token_json_key` option is not set in the loaded config.",
          "The `llm_cache_dir` and `llm_cache_ttl` options are not set in the loaded config.",
          "The `llm_metadata_file` option is not set in the loaded config.",
          "The `llm_hmac_key_file` option is not set in the loaded config.",
          "The `include_params` option is not set in the loaded config.",
          "The `strip_docstrings` option is not set in the loaded config."
        ],
        "scenario": "Verify that the `load_config` function correctly loads config options from pytest objects (CLI) into a mock configuration object.",
        "token_usage": {
          "completion_tokens": 498,
          "prompt_tokens": 639,
          "total_tokens": 1137
        },
        "why_needed": "This test prevents regression in the `load_config` function, which is used to load configuration options from pytest objects (CLI), potentially causing issues with plugin functionality."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginMaximal::testload_config",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 7,
          "line_ranges": "558-559, 562-563, 566-568"
        }
      ],
      "duration": 0.001609965000000102,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_item.config.stash.get returns False', 'expected_value': False}",
          "{'name': 'gen.send returns StopIteration', 'expected_value': True}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
        "token_usage": {
          "completion_tokens": 105,
          "prompt_tokens": 220,
          "total_tokens": 325
        },
        "why_needed": "The test is necessary to ensure that makereport skips when disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0018645319999848198,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_runtest_makereport` function should be able to find and stash the `mock_collector` instance.",
          "The `stash_get` method of the mock item's config should return True when `_enabled_key` is present.",
          "The `stash_get` method of the mock item's config should return `mock_collector` when `_collector_key` is present.",
          "The `handle_runtest_logreport` method of the mock collector instance should be called with `mock_report` and `mock_item` as arguments.",
          "The `get_result` method of the mock report object should return an empty result if no outcome was yielded.",
          "No exceptions should be raised when calling `send` on the mock outcome.",
          "The mock item's stash get method should not raise an exception when `_enabled_key` is present."
        ],
        "scenario": "Test makereport calls collector when enabled.",
        "token_usage": {
          "completion_tokens": 230,
          "prompt_tokens": 371,
          "total_tokens": 601
        },
        "why_needed": "Prevents a potential bug where the plugin does not call the collector even if makereport is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginRuntest::test_runtest_makereport_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 602-603"
        }
      ],
      "duration": 0.0012610219999942274,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_session.config.stash.get.return_value', 'expected': {'False': {}}}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
        "token_usage": {
          "completion_tokens": 88,
          "prompt_tokens": 149,
          "total_tokens": 237
        },
        "why_needed": "The test is necessary to verify that the `pytest_collection_finish` hook skips collection when disabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 10,
          "line_ranges": "558-559, 562, 566-568, 602, 606-608"
        }
      ],
      "duration": 0.001667472999997699,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'mock_collector.handle_collection_finish was called once with mock_session.items', 'expected_call_count': 1}"
        ],
        "scenario": "TestPluginSessionHooks",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 219,
          "total_tokens": 292
        },
        "why_needed": "To ensure that the collector is called when collection_finish is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_collection_finish_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 8,
          "line_ranges": "558-559, 562, 566-568, 619-620"
        }
      ],
      "duration": 0.0013086310000005597,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'pytest_sessionstart', 'actual_value': 'pytest_sessionstart'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 157,
          "total_tokens": 238
        },
        "why_needed": "The test is currently disabled and needs to be enabled for it to run."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_disabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 11,
          "line_ranges": "558-559, 562, 566-568, 619, 623, 626, 628-629"
        }
      ],
      "duration": 0.0010664179999935186,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert _collector_key in mock_stash",
          "assert _start_time_key in mock_stash"
        ],
        "scenario": "Test that sessionstart initializes collector when enabled and stash is properly populated.",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 335,
          "total_tokens": 422
        },
        "why_needed": "This test prevents a potential regression where the collector might not be created or initialized correctly when pytest_sessionstart is called with an enabled configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginSessionHooks::test_pytest_sessionstart_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002441223000005266,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "parser.getgroup.assert_called_with('llm-report', 'LLM-enhanced test reports')",
          "group.addoption.call_args_list[0][0].startswith('--llm-report')",
          "group.addoption.call_args_list[1][0].startswith('--llm-coverage-source')"
        ],
        "scenario": "Test pytest_addoption adds expected arguments to the parser.",
        "token_usage": {
          "completion_tokens": 125,
          "prompt_tokens": 293,
          "total_tokens": 418
        },
        "why_needed": "This test prevents a potential bug where pytest_addoption does not add all required arguments to the parser, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 220,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.002473833999999897,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'parser.addini was not called', 'expected_result': [], 'actual_result': 'parser.addini was called'}"
        ],
        "scenario": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
        "token_usage": {
          "completion_tokens": 95,
          "prompt_tokens": 140,
          "total_tokens": 235
        },
        "why_needed": "pytest_addoption no longer adds INI options in pytest 7.0 and later"
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_pytest_addoption_no_ini",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 53,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 468, 470-473, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0030668640000044434,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report_html` option is set to 'out.html' and the `CoverageMapper` class is instantiated with this value.",
          "The `Coverage` instance is created with a mock `report` method that returns a percentage of 85.5.",
          "The `load` method of the `Coverage` instance is called once, indicating successful loading of the coverage data.",
          "The `report` method of the `Coverage` instance is called once, indicating successful calculation of the coverage percentage.",
          "The `stash` dictionary passed to the `pytest_terminal_summary` function contains a key-value pair with `_enabled_key` and `_config_key` set to `True` and `cfg`, respectively.",
          "The mock configuration object `mock_config` has its `workerinput` attribute deleted, ensuring that it is not used in the test.",
          "The mock stash dictionary passed to the `pytest_terminal_summary` function contains a key-value pair with `_enabled_key` set to `True` and `_config_key` set to `cfg`, indicating correct configuration of the plugin."
        ],
        "scenario": "Test coverage percentage calculation logic for `pytest_llm_report.plugin.pytest_terminal_summary`.",
        "token_usage": {
          "completion_tokens": 285,
          "prompt_tokens": 395,
          "total_tokens": 680
        },
        "why_needed": "This test prevents regression in the coverage percentage calculation logic, ensuring that the plugin correctly calculates the terminal summary coverage."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_coverage_calculation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 66,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-486, 491-494, 497, 499, 502-504, 512-514, 516, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0028103949999831457,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the `pytest_terminal_summary_llm_enabled` test passes without any errors.",
          "Check if the `mock_annotate` function is called once with the correct configuration.",
          "Verify that the `stash` dictionary contains the expected keys and values.",
          "Confirm that the `mock_config.stash` dictionary matches the original stash.",
          "Test that the `pytest_terminal_summary` function does not raise any exceptions.",
          "Ensure that the `mock_writer_cls.return_value` is set to the mock writer instance.",
          "Verify that the `mock_provider.get_model_name.return_value` and `mock_get_provider.return_value` functions return the expected values.",
          "Check if the `pytest_terminal_summary` function does not raise any exceptions when called with a valid configuration."
        ],
        "scenario": "Test terminal summary with LLM enabled runs annotations.",
        "token_usage": {
          "completion_tokens": 204,
          "prompt_tokens": 477,
          "total_tokens": 681
        },
        "why_needed": "Prevents regression in plugin behavior when LLM is enabled."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_llm_enabled",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 45,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.0019163799999830644,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "mock_terminalreporter.call_args_list[0][1] should return 'Collector' as expected.",
          "stash._enabled_key should be True.",
          "stash._config_key should have the correct value (pytest_llm_report.plugin.Config).",
          "mock_config.stash.mock_stash._enabled_key should be True.",
          "mock_config.stash.mock_stash._config_key should have the correct value (pytest_llm_report.plugin.Config).",
          "mock_terminalreporter.call_args_list[0][1] should return 'Collector' as expected.",
          "stash._enabled_key should be False after calling pytest_terminal_summary() with mock_config."
        ],
        "scenario": "Test terminal summary creates collector if missing.",
        "token_usage": {
          "completion_tokens": 198,
          "prompt_tokens": 391,
          "total_tokens": 589
        },
        "why_needed": "This test prevents a potential regression where the plugin does not create a collector even when it is supposed to be present in the configuration."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_no_collector",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 21,
          "line_ranges": "399, 403, 407, 410-411, 413-414, 417-418, 420, 422-426, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.003026568999985102,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The aggregated report should be available in both `out.html` and `out.json` files.",
          "The aggregation function should return a report object.",
          "The write_json method of the ReportWriter class should be called with the aggregated report.",
          "The write_html method of the ReportWriter class should be called with the aggregated report.",
          "The aggregation function should not raise an error if there are no terminals being reported.",
          "The write_json and write_html methods should only be called once each, even if multiple terminals are being reported.",
          "The stash object should contain both _enabled_key and _config_key keys with the correct values.",
          "The stash object should not have a workerinput key.",
          "</key_assertions>"
        ],
        "scenario": "Test terminal summary with aggregation enabled.",
        "token_usage": {
          "completion_tokens": 200,
          "prompt_tokens": 441,
          "total_tokens": 641
        },
        "why_needed": "This test prevents regression in the case where aggregation is enabled and there are multiple terminals being reported."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummary::test_terminal_summary_with_aggregation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 16,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 3,
          "line_ranges": "123, 171, 284"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 52,
          "line_ranges": "399, 403, 407, 410, 429-430, 432, 434, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-466, 476-479, 485-486, 491-492, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.004844603999998753,
      "file_path": "tests/test_plugin_maximal.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pytest_terminal_summary` function should not raise a UserWarning when computing coverage percentage.",
          "The `coverage.Coverage.load()` method should be called before attempting to compute coverage percentage.",
          "The `coverage.Coverage` object should have been loaded successfully without raising an OSError.",
          "The `pytest_terminal_summary` function should not log the error message 'Failed to compute coverage percentage' as a warning.",
          "The `pytest_terminal_summary` function should correctly handle cases where coverage data is not available (e.g., due to disk full)",
          "The `pytest_terminal_summary` function should not raise an exception when computing coverage percentage for non-existent files or directories",
          "The `pytest_terminal_summary` function should log the error message 'Failed to compute coverage percentage' in a meaningful way, including the file path and line number where the error occurred.",
          "The `pytest_terminal_summary` function should correctly handle cases where the coverage data is not available for all lines (e.g., due to disk full)"
        ],
        "scenario": "Test coverage calculation error during terminal summary generation.",
        "token_usage": {
          "completion_tokens": 261,
          "prompt_tokens": 389,
          "total_tokens": 650
        },
        "why_needed": "This test prevents a regression where the coverage calculation fails due to an OSError caused by disk full."
      },
      "nodeid": "tests/test_plugin_maximal.py::TestPluginTerminalSummaryErrors::test_terminal_summary_coverage_error",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 63,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.006717481999999109,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'utils.py' file should be present in the assembled context.",
          "The 'def util()' function should be found in the 'utils.py' file within the assembled context.",
          "The assembly result should include a coverage entry for the 'utils.py' file with line ranges and count.",
          "The test result nodeid should match the one specified in the test file.",
          "The outcome of the test should be 'passed'.",
          "The assembler should be able to assemble a balanced context without any issues.",
          "The assembler should be able to find dependencies like 'utils.py' within the assembled context."
        ],
        "scenario": "Assembling a balanced context for test_a.py with utils.py dependency",
        "token_usage": {
          "completion_tokens": 187,
          "prompt_tokens": 331,
          "total_tokens": 518
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler is unable to assemble a balanced context due to missing dependencies."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_balanced_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116, 139-140, 268-272"
        }
      ],
      "duration": 0.0009602880000159075,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'contains', 'value': 'test_1'}"
        ],
        "scenario": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 176,
          "total_tokens": 253
        },
        "why_needed": "To ensure that the ContextAssembler can assemble a complete context for a test file."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_complete_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 30,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0009981900000184396,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'test_1' function is present in the source code of the test file.",
          "The context object returned by the ContextAssembler does not contain any information about the test file.",
          "The test result nodeid matches the expected location of the test function.",
          "The test file's source code contains an assert statement that passes when run with minimal context mode."
        ],
        "scenario": "Test the ContextAssembler with minimal context mode and a test file.",
        "token_usage": {
          "completion_tokens": 129,
          "prompt_tokens": 267,
          "total_tokens": 396
        },
        "why_needed": "This test prevents regression when using minimal context mode for tests that require minimal setup."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_assemble_minimal_context",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 46,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0010543150000046353,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'f1.py' file in the context is truncated to 40 bytes or less, as expected.",
          "A message indicating truncation ('... truncated') is present in the 'f1.py' file.",
          "The length of the 'f1.py' file is within the allowed limit (20 bytes + truncation message)."
        ],
        "scenario": "Test the ContextAssembler with balanced context limits to ensure it correctly truncates long content when exceeding the limit.",
        "token_usage": {
          "completion_tokens": 143,
          "prompt_tokens": 335,
          "total_tokens": 478
        },
        "why_needed": "This test prevents regression by ensuring that the ContextAssembler does not silently truncate long code snippets without providing a clear indication of what was truncated."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_balanced_context_limits",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 50,
          "line_ranges": "33, 49, 52, 55, 58, 60, 63, 65, 78-79, 82-84, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0010627000000056341,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'f1.py' node should be present in the assembled context.",
          "The content of 'f1.py' should not be truncated despite exceeding the 20 byte context limit.",
          "No 'truncated' key should be present in the context of 'f1.py'."
        ],
        "scenario": "Tests the ContextAssembler with complete mode and large context file to ensure it does not truncate files exceeding a certain size.",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 361,
          "total_tokens": 489
        },
        "why_needed": "This test prevents regression where the ContextAssembler truncates files larger than the specified context limit in complete mode."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_complete_context_limits_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 26,
          "line_ranges": "33, 78-79, 82-84, 86-87, 92, 94-95, 98-101, 103-112, 116"
        }
      ],
      "duration": 0.0009656189999986964,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The method `_get_test_source` returns an empty string when given a file path that does not exist.",
          "The method `_get_test_source` correctly extracts the test name and parameters from a nested test file with multiple levels of nesting."
        ],
        "scenario": "Test the ContextAssembler with edge cases, specifically non-existent files and nested test names with parameters.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 275,
          "total_tokens": 391
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler fails to extract test sources from non-existent or deeply nested test files."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_get_test_source_edge_cases",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 5,
          "line_ranges": "33, 284-287"
        }
      ],
      "duration": 0.0014736799999752748,
      "file_path": "tests/test_prompts.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'test.pyc' is expected to be excluded because it has a .pyc extension.",
          "The file 'secret/key.txt' is expected to be excluded because of the presence of '*'.",
          "The file 'public/readme.md' should not be excluded as it does not contain any sensitive information."
        ],
        "scenario": "The test verifies that the ContextAssembler should exclude certain files from being processed.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 227,
          "total_tokens": 349
        },
        "why_needed": "This test prevents a potential bug where some important files are inadvertently excluded during processing."
      },
      "nodeid": "tests/test_prompts.py::TestContextAssembler::test_should_exclude",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        }
      ],
      "duration": 0.000925472999995236,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context_files is an empty list when config llm_context_mode is set to 'minimal'.",
          "test_source contains the expected function definition.",
          "Context files are generated only if test_foo.py has a __file__ attribute and it's not empty.",
          "The assemble function does not modify or create any context files in this case."
        ],
        "scenario": "Test assemble minimal mode returns no context files.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 298,
          "total_tokens": 415
        },
        "why_needed": "Prevents regression where assemble function does not generate any context files."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_minimal_mode",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 62,
          "line_ranges": "33, 49, 52, 55, 58, 60-61, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0011352459999898201,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The assembler should use balanced mode due to the override.",
          "The module file path should be included in the context files.",
          "The coverage entry for the module file should have a line range of '1' and a line count of '1'.",
          "The test source code should be included in the context files.",
          "The context files should not contain any other test files or modules.",
          "The coverage entry for the module file should not include any other lines than specified (i.e., only '1').",
          "The LLM context override should be balanced, i.e., neither minimal nor default mode should be used."
        ],
        "scenario": "Test assemble respects llm_context_override from test.",
        "token_usage": {
          "completion_tokens": 189,
          "prompt_tokens": 362,
          "total_tokens": 551
        },
        "why_needed": "This test prevents regression by ensuring that the ContextAssembler uses the correct LLM context mode even when an override is specified."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_assemble_with_context_override",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 1,
          "line_ranges": "171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 20,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163-164, 201, 284-286"
        }
      ],
      "duration": 0.0009106260000066868,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file \"secret_config.py\" should be excluded from the balanced context.",
          "The file 'api_key = ",
          "The line count of the excluded file should be 1.",
          "The line ranges for the excluded file should start at line 1 and end at line 1.",
          "The coverage entry for the excluded file should have a line range of '1' and a line count of 1.",
          "The test should fail if the LLM context mode is set to 'balanced' without excluding any files.",
          "The test should pass if the LLM context mode is set to 'balanced' with the specified exclude patterns in place."
        ],
        "scenario": "Test the test_balanced_context_excludes_patterns function to ensure it excludes files matching exclude patterns in a balanced context.",
        "token_usage": {
          "completion_tokens": 213,
          "prompt_tokens": 331,
          "total_tokens": 544
        },
        "why_needed": "This test prevents regression where the LLM context mode is set to 'balanced' and the assembler does not exclude files that match the specified patterns."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_excludes_patterns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 16,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-161, 201"
        }
      ],
      "duration": 0.0008262569999999414,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'message': 'Expected an empty context to be returned when a non-existent file is provided.', 'expected_result': {}, 'actual_result': {}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 201,
          "total_tokens": 294
        },
        "why_needed": "To test the scenario where a balanced context file does not exist."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 34,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.013587049999983947,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The content of the source file is truncated to prevent excessive memory usage.",
          "A message indicating that the content was truncated is included in the context.",
          "The source file's length does not exceed the specified limit when assembling it into the context.",
          "The test verifies that the source file's content is truncated, preventing potential issues with coverage and memory consumption.",
          "The test ensures a clear indication of truncation in the context, facilitating debugging and maintenance efforts."
        ],
        "scenario": "Test that balanced context respects max bytes limit.",
        "token_usage": {
          "completion_tokens": 151,
          "prompt_tokens": 405,
          "total_tokens": 556
        },
        "why_needed": "This test prevents a potential memory leak or incorrect coverage due to an overly large source file being assembled into the context."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_max_bytes_limit",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 3,
          "line_ranges": "33, 139-140"
        }
      ],
      "duration": 0.000780031999994435,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'assert context is empty', 'expected_result': '{}', 'actual_result': '{}'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 162,
          "total_tokens": 251
        },
        "why_needed": "To ensure that the ContextAssembler can correctly assemble a balanced context with no coverage."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_no_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 35,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-157, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-194, 196-197, 201, 284-285, 287"
        }
      ],
      "duration": 0.0010392670000101134,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "context should have only one node (either 'file1.py' or 'file2.py')",
          "should not exceed the maximum allowed bytes of 5 for 'file1.py'",
          "should be empty if no files are processed",
          "should contain both 'content1 = 1' and 'content2 = 2' from both files",
          "should have only one line range (1) with a single line count (1)",
          "should not exceed the maximum allowed bytes of 5 for 'file2.py'",
          "should be empty if no files are processed"
        ],
        "scenario": "Test that loop exits when max bytes is reached before processing file.",
        "token_usage": {
          "completion_tokens": 183,
          "prompt_tokens": 409,
          "total_tokens": 592
        },
        "why_needed": "This test prevents a potential bug where the ContextAssembler exceeds the maximum allowed bytes in a balanced context without encountering any files."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_balanced_context_reaches_max_bytes_before_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/context_util.py",
          "line_count": 17,
          "line_ranges": "27, 29, 33, 35-36, 64, 66-69, 108, 124, 126-127, 129, 133, 135"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 38,
          "line_ranges": "33, 139, 142-145, 147-148, 152-153, 155-156, 159-160, 163, 166-167, 170-171, 173-174, 177, 181-182, 189, 192-193, 196-197, 201, 268-272, 284-285, 287"
        }
      ],
      "duration": 0.0009969170000090344,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'inclusion', 'expected_inclusion': ['module.py'], 'actual_inclusion': ['module.py']}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
        "token_usage": {
          "completion_tokens": 90,
          "prompt_tokens": 211,
          "total_tokens": 301
        },
        "why_needed": "To ensure that the complete context delegates to balanced correctly."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_complete_context_delegates_to_balanced",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 9,
          "line_ranges": "33, 78-79, 82-83, 86-89"
        }
      ],
      "duration": 0.0008556619999922077,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result == \"\"', 'expected_value': '', 'message': 'Expected _get_test_source with empty nodeid to return an empty string'}"
        ],
        "scenario": "Test _get_test_source with empty nodeid returns empty string",
        "token_usage": {
          "completion_tokens": 92,
          "prompt_tokens": 148,
          "total_tokens": 240
        },
        "why_needed": "To ensure that the function correctly handles cases where the input nodeid is empty."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_empty_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.000930081999996446,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'Source code extraction should stop at the next function definition', 'expected_result': 'The source code for the current test file should be extracted until the next function definition.', 'actual_result': 'The source code for the entire test file is extracted.'}"
        ],
        "scenario": "Tests that source extraction stops at next function definition.",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 129,
          "total_tokens": 243
        },
        "why_needed": "To ensure correct source code is extracted and to prevent errors due to incomplete or incorrect source code."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_extraction_stops_at_next_def",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 6,
          "line_ranges": "33, 78-79, 82-84"
        }
      ],
      "duration": 0.0008181019999824457,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result is empty', 'expected_result': '', 'message': 'Expected result to be an empty string'}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_file_not_exists",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 142,
          "total_tokens": 240
        },
        "why_needed": "To test the _get_test_source method's behavior when a non-existent file is provided as an argument."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_file_not_exists",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 25,
          "line_ranges": "33, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-112, 114, 116"
        }
      ],
      "duration": 0.000935373000004347,
      "file_path": "tests/test_prompts_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': {'indentation': 4, 'expected_lines': ['def add(a, b):', '    return a + b']}, 'actual': {'indentation': 2, 'expected_lines': ['def add(a, b):', '    return a + b']}}"
        ],
        "scenario": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
        "token_usage": {
          "completion_tokens": 128,
          "prompt_tokens": 118,
          "total_tokens": 246
        },
        "why_needed": "The test is necessary to ensure that the _get_test_source function correctly extracts functions with proper indentation."
      },
      "nodeid": "tests/test_prompts_coverage.py::TestContextAssemblerEdgeCases::test_get_test_source_with_class",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000783797999986291,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3', 'actual': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
        "token_usage": {
          "completion_tokens": 68,
          "prompt_tokens": 106,
          "total_tokens": 174
        },
        "why_needed": "To ensure consecutive lines are compressed correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.0007919829999991634,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': 'The output should contain a single range string.', 'expected_value': '1-3'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 107,
          "total_tokens": 184
        },
        "why_needed": "To test the handling of duplicate ranges in the compress_ranges function."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_duplicates",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "29-30"
        }
      ],
      "duration": 0.0007695310000030986,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '', 'actual_value': ''}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 92,
          "total_tokens": 162
        },
        "why_needed": "The current implementation of `compress_ranges` does not handle an empty input list correctly."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_empty_list",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.0007456470000022364,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'value': '1-3, 5, 10-12, 15'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 130,
          "total_tokens": 209
        },
        "why_needed": "To test the functionality of compressing mixed ranges (single numbers and overlapping ranges)."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_mixed_ranges",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 14,
          "line_ranges": "29, 33, 35-37, 39-40, 45-47, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007502849999809769,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1, 3, 5', 'actual': '1, 3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 113,
          "total_tokens": 191
        },
        "why_needed": "Non-consecutive lines should be comma-separated."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_non_consecutive_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "29, 33, 35-37, 39, 50, 52, 65-66"
        }
      ],
      "duration": 0.0007581200000004173,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '5', 'actual_value': '5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_single_line",
        "token_usage": {
          "completion_tokens": 70,
          "prompt_tokens": 96,
          "total_tokens": 166
        },
        "why_needed": "To ensure that the single line case does not use range notation."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_single_line",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 12,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 50, 52, 65, 67"
        }
      ],
      "duration": 0.000739955999989661,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-2', 'actual': '1-2'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
        "token_usage": {
          "completion_tokens": 73,
          "prompt_tokens": 103,
          "total_tokens": 176
        },
        "why_needed": "To ensure that two consecutive lines are compressed to a single range."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_two_consecutive",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 16,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67"
        }
      ],
      "duration": 0.000780931999997847,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1-3, 5', 'actual': '1-3, 5'}"
        ],
        "scenario": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 110,
          "total_tokens": 195
        },
        "why_needed": "The test is necessary because the current implementation of `compress_ranges` only works with sorted input."
      },
      "nodeid": "tests/test_ranges.py::TestCompressRanges::test_unsorted_input",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 2,
          "line_ranges": "81-82"
        }
      ],
      "duration": 0.0007595529999946393,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"expand_ranges('') == []\", 'expected_result': [], 'actual_result': []}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 90,
          "total_tokens": 171
        },
        "why_needed": "The function `expand_ranges` is expected to handle an empty input string correctly."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 11,
          "line_ranges": "81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007793599999956768,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3, 5, 10, 11, 12], 'actual': ['1', '2', '3', '5', '10', '11', '12']}",
          "{'expected': [], 'actual': []}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_mixed",
        "token_usage": {
          "completion_tokens": 107,
          "prompt_tokens": 121,
          "total_tokens": 228
        },
        "why_needed": "To handle mixed ranges and singles in the input string."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_mixed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 10,
          "line_ranges": "81, 84-91, 95"
        }
      ],
      "duration": 0.0007522900000083155,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [1, 2, 3], 'actual': ['1', '2', '3']}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_range",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 99,
          "total_tokens": 173
        },
        "why_needed": "The range function should expand to a list."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_range",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 27,
          "line_ranges": "29, 33, 35-37, 39-40, 42, 45-47, 50, 52, 65-67, 81, 84-91, 93, 95"
        }
      ],
      "duration": 0.0007614170000067588,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': [1, 2, 3, 5, 10, 11, 12, 15], 'actual_value': [1, 2, 3, 5, 10, 11, 12, 15]}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
        "token_usage": {
          "completion_tokens": 114,
          "prompt_tokens": 134,
          "total_tokens": 248
        },
        "why_needed": "To ensure that `compress_ranges` and `expand_ranges` are inverses."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_roundtrip",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/ranges.py",
          "line_count": 7,
          "line_ranges": "81, 84-87, 93, 95"
        }
      ],
      "duration": 0.0007442350000133047,
      "file_path": "tests/test_ranges.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': [5], 'actual': ['5']}"
        ],
        "scenario": "tests/test_ranges.py::TestExpandRanges::test_single_number",
        "token_usage": {
          "completion_tokens": 79,
          "prompt_tokens": 95,
          "total_tokens": 174
        },
        "why_needed": "The function `expand_ranges` is expected to return a list containing the input value when given a single number as an argument."
      },
      "nodeid": "tests/test_ranges.py::TestExpandRanges::test_single_number",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65, 67"
        }
      ],
      "duration": 0.0007557559999895602,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function should return '500ms' when given a duration of 0.5 seconds.",
          "The function should return '1ms' when given a duration of 0.001 seconds.",
          "The function should return '0ms' when given a duration of 0.0 seconds."
        ],
        "scenario": "Test the format_duration function with duration values less than 1s.",
        "token_usage": {
          "completion_tokens": 120,
          "prompt_tokens": 211,
          "total_tokens": 331
        },
        "why_needed": "This test prevents a regression where the function does not correctly format durations for values < 1s."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 2,
          "line_ranges": "65-66"
        }
      ],
      "duration": 0.0007739100000208055,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1.23s', 'actual': '1.23s'}",
          "{'expected': '60.00s', 'actual': '60.00s'}"
        ],
        "scenario": "tests/test_render.py::TestFormatDuration::test_seconds",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 116,
          "total_tokens": 213
        },
        "why_needed": "To ensure the `format_duration` function correctly formats durations in seconds for values greater than or equal to 1 second."
      },
      "nodeid": "tests/test_render.py::TestFormatDuration::test_seconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0007607149999842022,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "outcome_to_css_class('passed') == 'outcome-passed'",
          "outcome_to_css_class('failed') == 'outcome-failed'",
          "outcome_to_css_class('skipped') == 'outcome-skipped'",
          "outcome_to_css_class('xfailed') == 'outcome-xfailed'",
          "outcome_to_css_class('xpassed') == 'outcome-xpassed'"
        ],
        "scenario": "All outcomes should map to CSS classes.",
        "token_usage": {
          "completion_tokens": 134,
          "prompt_tokens": 263,
          "total_tokens": 397
        },
        "why_needed": "This test prevents a potential CSS class mapping issue where 'xfailed' and 'xpassed' are mapped to different classes."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 8,
          "line_ranges": "79-85, 87"
        }
      ],
      "duration": 0.0007727280000153769,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': 'outcome-unknown', 'actual': 'outcome-unknown'}"
        ],
        "scenario": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 102,
          "total_tokens": 183
        },
        "why_needed": "The test is necessary because the `outcome_to_css_class` function does not handle unknown outcomes correctly."
      },
      "nodeid": "tests/test_render.py::TestOutcomeToCssClass::test_unknown_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65-67, 79-85, 87, 121-124, 126-127, 131-132, 155-157, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008379399999967063,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The presence of the '<!DOCTYPE html>' header in the rendered HTML document.",
          "The inclusion of 'Test Report' in the rendered HTML document.",
          "The presence of 'PASSED' and 'FAILED' text in the rendered HTML document.",
          "The correct display of plugin and repository metadata in the rendered HTML document."
        ],
        "scenario": "Test renders basic report with fallback HTML.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 426,
          "total_tokens": 545
        },
        "why_needed": "Prevents rendering of incomplete or malformed reports due to missing or incorrect plugin and repository metadata."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_basic_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 57,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-129, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0007873050000171133,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report includes the source file 'src/foo.py' as part of the rendered HTML.",
          "The assertion checks that at least 5 lines were included in the rendered HTML.",
          "The test verifies that the 'src/foo.py' file is present in the rendered HTML.",
          "The test ensures that exactly 5 lines are included in the rendered HTML.",
          "The report includes coverage information for the specified files.",
          "The assertion checks that the coverage information is accurate and complete."
        ],
        "scenario": "Test renders coverage for fallback HTML generation.",
        "token_usage": {
          "completion_tokens": 144,
          "prompt_tokens": 288,
          "total_tokens": 432
        },
        "why_needed": "Prevents regression in rendering coverage information when fallback is used."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 64,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-134, 136-137, 140-142, 144, 147, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0007983950000038931,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report includes the specified LLM annotations with confidence scores.",
          "The 'Tests login flow' scenario is included in the rendered HTML.",
          "The 'Prevents auth bypass' scenario is included in the rendered HTML.",
          "The confidence score for each annotation is displayed correctly (85%)."
        ],
        "scenario": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
        "token_usage": {
          "completion_tokens": 132,
          "prompt_tokens": 317,
          "total_tokens": 449
        },
        "why_needed": "This test prevents LLM annotation rendering issues that could cause the 'Tests login flow' and 'Prevents auth bypass' scenarios to be missed."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_llm_annotation",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 68,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-178, 180-186, 191, 206, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008157380000000103,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'Source Coverage' section should be present in the rendered HTML report.",
          "The 'src/foo.py' file path should be included in the 'Source Coverage' section.",
          "The percentage of covered source code (80.0%) should be displayed in the 'Source Coverage' section.",
          "The range of missed source code (5-10) should be included in the 'Missed Source Code' section.",
          "The ranges of covered and missed source code should match the corresponding coverage percentages.",
          "The 'covered_ranges' and 'missed_ranges' values should contain the expected ranges for each file."
        ],
        "scenario": "Verifies that the source coverage summary is included in the rendered HTML report.",
        "token_usage": {
          "completion_tokens": 189,
          "prompt_tokens": 331,
          "total_tokens": 520
        },
        "why_needed": "This test prevents a regression where the source coverage summary was not displayed correctly due to missing or incomplete code coverage data."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 55,
          "line_ranges": "65, 67, 79-85, 87, 121-124, 126-127, 131-132, 155-156, 159-167, 172-174, 210-211, 224, 257-264, 267, 269, 271-277, 280-281, 285"
        }
      ],
      "duration": 0.0008802990000162936,
      "file_path": "tests/test_render.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'XFailed' and 'XPassed' tags should be present in the rendered HTML.",
          "Both 'XFailed' and 'XPassed' tags should contain the corresponding text.",
          "If either tag is missing, the test should fail with an error message indicating the missing tag.",
          "The 'xfailed' and 'xpassed' values should match the actual counts from the report.",
          "The 'xpass' value should be present but not exceed the total count.",
          "The 'xfail' value should be absent or equal to 0 if it's supposed to be missing.",
          "The HTML structure should maintain a consistent layout and formatting."
        ],
        "scenario": "Test renders xpass summary with fallback HTML.",
        "token_usage": {
          "completion_tokens": 181,
          "prompt_tokens": 283,
          "total_tokens": 464
        },
        "why_needed": "Prevents regression when rendering summary without fallback."
      },
      "nodeid": "tests/test_render.py::TestRenderFallbackHtml::test_renders_xpass_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007780180000054315,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Different Content', 'expected_result': {'hash1': '...', 'hash2': '...'}}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 115,
          "total_tokens": 192
        },
        "why_needed": "Because different content should produce different hashes."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_different_content",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 1,
          "line_ranges": "55"
        }
      ],
      "duration": 0.0007691010000030474,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '', 'actual': ''}"
        ],
        "scenario": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
        "token_usage": {
          "completion_tokens": 63,
          "prompt_tokens": 129,
          "total_tokens": 192
        },
        "why_needed": "To ensure that an empty bytes object produces consistent hash."
      },
      "nodeid": "tests/test_report_writer.py::TestComputeSha256::test_empty_bytes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 72,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307"
        }
      ],
      "duration": 0.005100714999997535,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert meta.duration == 60.0",
          "assert meta.pytest_version",
          "assert meta.plugin_version == __version__",
          "assert meta.python_version"
        ],
        "scenario": "Test that the build_run_meta method includes version info and other relevant metadata.",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 318,
          "total_tokens": 416
        },
        "why_needed": "This test prevents regression where the report writer does not include version information or plugin versions in the run meta."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_run_meta",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 19,
          "line_ranges": "156-158, 319, 321-322, 324-335, 337"
        }
      ],
      "duration": 0.0008039759999860507,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total number of outcomes should be 6 (1 passed, 1 failed, 1 skipped, 1 xfailed, 1 xpassed, 1 error).",
          "The number of passed outcomes should be 1.",
          "The number of failed outcomes should be 1.",
          "The number of skipped outcomes should be 1.",
          "The number of xfailed and xpassed outcomes should be 1 each."
        ],
        "scenario": "Test verifies that the `build_summary` method counts all outcome types correctly.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 336,
          "total_tokens": 485
        },
        "why_needed": "This test prevents a regression where the total count of outcomes is incorrect when there are multiple skipped tests."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_all_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 319, 321-322, 324-329, 337"
        }
      ],
      "duration": 0.0007784080000021731,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total count should be equal to the sum of passed, failed and skipped counts.",
          "The passed count should be equal to the number of tests with outcome 'passed'.",
          "The failed count should be equal to the number of tests with outcome 'failed'.",
          "The skipped count should be equal to the number of tests with outcome 'skipped'."
        ],
        "scenario": "The test verifies that the `total` attribute of a `Summary` object is correctly calculated by summing up all outcomes.",
        "token_usage": {
          "completion_tokens": 150,
          "prompt_tokens": 283,
          "total_tokens": 433
        },
        "why_needed": "This test prevents regression where the total count of passed, failed and skipped tests might not match the actual number of tests run."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_build_summary_counts",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "156-158"
        }
      ],
      "duration": 0.000731639999997924,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `config` attribute of the `writer` should be set to the provided `Config` object.",
          "The `warnings` list of the `writer` should be empty.",
          "The `artifacts` list of the `writer` should be empty."
        ],
        "scenario": "Test that the `ReportWriter` class initializes correctly.",
        "token_usage": {
          "completion_tokens": 113,
          "prompt_tokens": 199,
          "total_tokens": 312
        },
        "why_needed": "This test prevents a potential bug where the `ReportWriter` instance does not have access to its configuration and warnings."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_create_writer",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337"
        }
      ],
      "duration": 0.005196126000015511,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the report.tests list should be equal to 2.",
          "The total number of tests in the summary should be 2.",
          "All tests should have a 'passed' outcome.",
          "Each test should have a unique nodeid.",
          "The ReportWriter class should not write any files if no output paths are specified.",
          "The report.summary.total property should return 2 even if there are less than 2 tests."
        ],
        "scenario": "Test 'ReportWriter::test_write_report_assembles_tests' verifies that the ReportWriter class writes a report with all tests.",
        "token_usage": {
          "completion_tokens": 167,
          "prompt_tokens": 255,
          "total_tokens": 422
        },
        "why_needed": "This test prevents regression in cases where the output paths of the tests are not specified, causing the report to be incomplete or missing."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_assembles_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 98,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-199, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.0058155440000007275,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 85.5, 'actual_value': 'report.summary.coverage_total_percent'}"
        ],
        "scenario": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
        "token_usage": {
          "completion_tokens": 89,
          "prompt_tokens": 132,
          "total_tokens": 221
        },
        "why_needed": "The test is necessary to ensure that the ReportWriter class correctly includes the total coverage percentage in its report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_coverage_percent",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 97,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337"
        }
      ],
      "duration": 0.005104300999988709,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The length of the `source_coverage` list in the report should be equal to 1.",
          "The file path of the first element in the `source_coverage` list should match 'src/foo.py'.",
          "Each element in the `source_coverage` list should have the following attributes: `file_path`, `statements`, `missed`, `covered`, `coverage_percent`, `covered_ranges`, and `missed_ranges`."
        ],
        "scenario": "Test ReportWriter::test_write_report_includes_source_coverage verifies that the report includes source coverage summary.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 291,
          "total_tokens": 457
        },
        "why_needed": "This test prevents a bug where the report does not include source coverage information, which can make it difficult to understand the code's performance and identify areas for improvement."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_includes_source_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 99,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186-189, 192-193, 197-198, 202, 211-218, 222, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337"
        }
      ],
      "duration": 0.005124989999984564,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The first test should have a single coverage entry.",
          "The file path of the first test's coverage entry should match 'src/foo.py'.",
          "Only one coverage entry should be included in the merged report."
        ],
        "scenario": "Test ReportWriter::test_write_report_merges_coverage verifies that the test writes a merged coverage report.",
        "token_usage": {
          "completion_tokens": 116,
          "prompt_tokens": 285,
          "total_tokens": 401
        },
        "why_needed": "This test prevents regression in case of multiple tests with different coverage, where only one test's coverage is being written to the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriter::test_write_report_merges_coverage",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 62,
          "line_ranges": "376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202-206, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513-514, 516-519, 522-523"
        }
      ],
      "duration": 0.006167011999991701,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file 'report.json' should exist in the test directory.",
          "At least one warning message with code 'W203' should be present in the report."
        ],
        "scenario": "Test the fallback behavior of atomic write when it fails.",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 276,
          "total_tokens": 363
        },
        "why_needed": "To prevent a regression where an atomic write attempt fails and does not trigger a direct write."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 7,
          "line_ranges": "70-71, 73-75, 77, 79"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 81,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528-530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 128,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-484, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.006160749000002852,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Directory should be created', 'description': 'The directory should be created at the expected path.'}",
          "{'name': 'Report JSON file should be present', 'description': 'A report.json file should be present in the expected location.'}"
        ],
        "scenario": "Test case: creates directory if missing",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 171,
          "total_tokens": 286
        },
        "why_needed": "Because the test requires a report to be written, which will create a new directory if it doesn't exist."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_creates_directory_if_missing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 12,
          "line_ranges": "156-158, 477-480, 487-491"
        }
      ],
      "duration": 0.0011385820000100466,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `writer._ensure_dir(json_path)` raises an exception with code 'W201' when directory creation fails.",
          "Any warnings raised during directory creation are captured and stored in the `writer.warnings` list.",
          "The `writer.warnings` list contains at least one warning with code 'W201'.",
          "The function does not silently ignore errors or return an error code, but instead raises an exception.",
          "The test verifies that directory creation fails and captures warnings to ensure the report writer behaves as expected.",
          "The test uses a mock `mkdir` function to simulate a permission denied error, which is raised by the `writer._ensure_dir(json_path)` method.",
          "The test ensures that the `writer.warnings` list contains at least one warning with code 'W201' for each directory creation attempt."
        ],
        "scenario": "Test ensures that directory creation fails and captures warnings.",
        "token_usage": {
          "completion_tokens": 239,
          "prompt_tokens": 278,
          "total_tokens": 517
        },
        "why_needed": "This test prevents a bug where the report writer does not handle directory creation failures correctly, potentially leading to silent errors or incorrect reporting."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_ensure_dir_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.001098597999998674,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_git_info` function should not raise an exception when it encounters a git command failure.",
          "The `sha` variable should be set to `None` in this case.",
          "The `dirty` variable should also be set to `None` in this case.",
          "When running subprocess.check_output with an exception, the test should still pass without raising an error.",
          "The function's return values should not be affected by external git command failures.",
          "The function should handle git commands that fail silently (e.g., due to permissions issues) correctly.",
          "The `get_git_info` function should not raise a `SubprocessError` exception when it encounters a git command failure."
        ],
        "scenario": "Test 'test_git_info_failure' verifies that the `get_git_info` function handles git command failures gracefully by returning `None` for both SHA and dirty flag values.",
        "token_usage": {
          "completion_tokens": 229,
          "prompt_tokens": 231,
          "total_tokens": 460
        },
        "why_needed": "This test prevents a potential regression where the `get_git_info` function fails to return expected results when running git commands with errors."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_git_info_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 120,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.03957078899998123,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The report.html file should exist at the specified path.",
          "The report.html file should contain the expected content (test1, test2, PASSED, FAILED, Skipped, XFailed, XPassed, Errors).",
          "All nodes with nodeid 'test1' and 'test2' should be found in the HTML content.",
          "Node 'Skipped' should not be found in the HTML content.",
          "Node 'XFailed' should not be found in the HTML content.",
          "Node 'XPassed' should not be found in the HTML content.",
          "All error messages should be included in the HTML content (AssertionError)."
        ],
        "scenario": "Test verifies that the report writer creates an HTML file and includes expected content.",
        "token_usage": {
          "completion_tokens": 200,
          "prompt_tokens": 366,
          "total_tokens": 566
        },
        "why_needed": "This test prevents a bug where the report writer does not create an HTML file or does not include expected content in the report."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 123,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-333, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.038585439000002,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Asserts that 'XFAILED' and 'XFailed' keywords are present in the HTML string.",
          "Checks if 'XPASSED' and 'XPassed' keywords are also present in the HTML string.",
          "Verifies that all xfail outcomes have a corresponding xpassed outcome in the report.",
          "Ensures that the report includes both xfailed and xpassed outcomes for each test.",
          "Guarantees that the report does not exclude any tests from being included in the summary."
        ],
        "scenario": "The test verifies that the report includes xfail outcomes in the HTML summary.",
        "token_usage": {
          "completion_tokens": 159,
          "prompt_tokens": 308,
          "total_tokens": 467
        },
        "why_needed": "This test prevents regression by ensuring that xfail outcomes are included in the HTML summary."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_html_includes_xfail_summary",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.00603518400001235,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `report.json` file should be created in the specified path.",
          "The artifact should be tracked for this test.",
          "The number of artifacts should be at least one.",
          "The file should exist as expected.",
          "The report writer should create a new JSON file with each test."
        ],
        "scenario": "Test verifies that a JSON file is created with the report.",
        "token_usage": {
          "completion_tokens": 110,
          "prompt_tokens": 265,
          "total_tokens": 375
        },
        "why_needed": "Prevents regression where test reports are not written to files."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_json_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 130,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408, 417, 419, 421-430, 441-442, 444-450, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.042158956999998054,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `write_pdf` method of the `ReportWriter` class should create a new PDF file with the specified path.",
          "The `report.pdf` attribute of the `Config` object should be set to the created PDF file path.",
          "All artifacts in the report should have paths that match the created PDF file path.",
          "The `write_report` method of the `ReportWriter` class should create a new PDF file with the specified path and add it to the report artifacts."
        ],
        "scenario": "Test writes PDF file when Playwright is available.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 478,
          "total_tokens": 627
        },
        "why_needed": "Prevents regression where the test fails due to missing or corrupted PDF files."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_creates_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 103,
          "line_ranges": "67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226, 230-231, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 408-412, 415"
        }
      ],
      "duration": 0.006728193000014926,
      "file_path": "tests/test_report_writer.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `pdf_path` does not exist after the test is run.",
          "Any warnings raised are of type WarningCode.W204_PDF_PLAYWRIGHT_MISSING.",
          "The warning message includes the string 'Playwright is missing'."
        ],
        "scenario": "Test that a warning is raised when Playwright is missing for PDF output.",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 311,
          "total_tokens": 426
        },
        "why_needed": "To prevent the test from failing and providing incorrect results, this test verifies that it correctly handles the absence of Playwright for PDF output."
      },
      "nodeid": "tests/test_report_writer.py::TestReportWriterWithFiles::test_write_pdf_missing_playwright_warns",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "67-73, 85-86"
        }
      ],
      "duration": 0.0029822759999831305,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': 'None', 'actual_value': 'sha'}",
          "{'expected_value': 'None', 'actual_value': 'dirty'}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 123,
          "total_tokens": 221
        },
        "why_needed": "To test that the function correctly returns None when given a non-git directory."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_nonexistent_path",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 16,
          "line_ranges": "67-74, 76-81, 83-84"
        }
      ],
      "duration": 0.006263573000012457,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is None or isinstance(sha, str)', 'expected_result': {'type': 'NoneType', 'message': 'Test should return None if not in git repo'}, 'actual_result': {'type': 'str', 'message': 'Expected to get a string from the test_get_git_info function'}}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
        "token_usage": {
          "completion_tokens": 135,
          "prompt_tokens": 159,
          "total_tokens": 294
        },
        "why_needed": "To ensure that the test_get_git_info function correctly returns git info from a valid git repository."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetGitInfo::test_git_info_from_valid_repo",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0009001660000080847,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_plugin_git_info()` should return either None or a string.",
          "The function `get_plugin_git_info()` should still work via git runtime fallback even when `_git_info` is not available.",
          "The value of `sha` in the returned tuple should be None if `_git_info` import fails, otherwise it should be a string.",
          "The type of `sha` in the returned tuple should be either None or str.",
          "If `_git_info` import fails, the test should still pass without raising an AssertionError."
        ],
        "scenario": "Test falls back to git runtime when _git_info import fails.",
        "token_usage": {
          "completion_tokens": 169,
          "prompt_tokens": 253,
          "total_tokens": 422
        },
        "why_needed": "Prevents a potential bug where the test fails due to an import error in _git_info."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 3,
          "line_ranges": "127-128, 130"
        }
      ],
      "duration": 0.0007801309999990735,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert sha is None or isinstance(sha, str)', 'expected_result': ['None', 'str'], 'actual_result': [1, 2]}"
        ],
        "scenario": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo",
        "token_usage": {
          "completion_tokens": 84,
          "prompt_tokens": 141,
          "total_tokens": 225
        },
        "why_needed": "To ensure the plugin git info function returns some values."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestGetPluginGitInfo::test_plugin_git_info_returns_values",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 80,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 122,
          "line_ranges": "55, 67-74, 76-81, 83-84, 98-99, 102, 105-108, 110, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.00659031500001106,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion_type': 'File existence', 'expected_result': 'report.json exists'}"
        ],
        "scenario": "Test atomic write falls back to direct write on error.",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 181,
          "total_tokens": 266
        },
        "why_needed": "To ensure the report writer can handle cases where an atomic write operation fails and needs to fall back to a non-atomic write method."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterAtomicWrite::test_atomic_write_fallback",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 28,
          "line_ranges": "156-158, 408, 417, 419, 421-423, 431-436, 439, 441-442, 455, 460, 462, 465-469, 477-478"
        }
      ],
      "duration": 0.11663618599999381,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `writer.write_pdf` method should raise a `FileNotFoundError` if the PDF file cannot be written to the specified path.",
          "The `writer.warnings` list should contain warnings with code 'W201' when an exception is raised during report generation.",
          "The `report.pdf` attribute of the `writer` object should not be set to a non-existent file path.",
          "Any exceptions raised by `writer.write_pdf` or `writer.warnings` should be caught and handled correctly.",
          "The `config.report_pdf` parameter should be set to a valid file path for the report PDF.",
          "The `report.pdf` attribute of the `ReportWriter` object should not be set to an empty string.",
          "Any errors that occur during report generation should be logged or reported in some way."
        ],
        "scenario": "Test PDF generation when playwright raises exception.",
        "token_usage": {
          "completion_tokens": 218,
          "prompt_tokens": 356,
          "total_tokens": 574
        },
        "why_needed": "Prevents regression where playwright raises an exception and report generation fails without a clear error message."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_exception",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 9,
          "line_ranges": "156-158, 408-412, 415"
        }
      ],
      "duration": 0.001175791999997955,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'W204' warning should be raised indicating that playwright is missing.",
          "The PDF file 'report.pdf' should not exist.",
          "Any other warnings or errors related to playwright should be suppressed."
        ],
        "scenario": "Test PDF generation when playwright is not installed.",
        "token_usage": {
          "completion_tokens": 97,
          "prompt_tokens": 293,
          "total_tokens": 390
        },
        "why_needed": "Prevents a potential bug where the report writer fails to create a PDF file without the required playwright installation."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_pdf_playwright_not_installed",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 11,
          "line_ranges": "156-158, 455, 460, 462, 465-469"
        }
      ],
      "duration": 0.056798992000011594,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path returned by _resolve_pdf_html_source should exist and have a suffix of '.html'.",
          "The path returned by _resolve_pdf_html_source should not be empty.",
          "The suffix of the returned path should be '.html'."
        ],
        "scenario": "Test _resolve_pdf_html_source creates temp file when no HTML source is provided.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 265,
          "total_tokens": 383
        },
        "why_needed": "Prevents a potential bug where the test fails if no HTML source is provided, causing the report writer to create an empty temporary file."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_creates_temp",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 26,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65-67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 13,
          "line_ranges": "156-158, 455-457, 460, 462, 465-469"
        }
      ],
      "duration": 0.033818897999992714,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path to the resolved HTML file exists.",
          "The path to the resolved HTML file is different from the original non-existent HTML file.",
          "The report writer does not fall back to using a temporary PDF file when an HTML source is missing.",
          "The test passes even if the actual HTML source is not found but the configuration specifies it should exist.",
          "The test ensures that the path to the resolved HTML file is correct and unique.",
          "The test verifies that the report writer does not use the original non-existent HTML file as a fallback.",
          "The test checks for any potential issues with the temporary PDF file created during the resolution process."
        ],
        "scenario": "Verify that the test resolves an HTML source when a non-existent HTML file exists.",
        "token_usage": {
          "completion_tokens": 196,
          "prompt_tokens": 270,
          "total_tokens": 466
        },
        "why_needed": "Prevents a bug where the report writer falls back to using a temporary PDF file instead of the actual HTML source."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_missing_html_file",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 2,
          "line_ranges": "123, 171"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 7,
          "line_ranges": "156-158, 455-458"
        }
      ],
      "duration": 0.0009510110000121585,
      "file_path": "tests/test_report_writer_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The path of the resolved PDF file is set to the existing HTML file.",
          "The temporary flag is set to False, indicating that the report was generated from a non-temporary source.",
          "The report writer uses the existing HTML file as its source.",
          "The configuration string contains the path to the existing HTML file.",
          "The resolved PDF file has the same name as the original HTML file.",
          "The resolved PDF file is not temporary, indicating that it was generated from a non-temporary source."
        ],
        "scenario": "Test _resolve_pdf_html_source uses existing HTML file.",
        "token_usage": {
          "completion_tokens": 157,
          "prompt_tokens": 266,
          "total_tokens": 423
        },
        "why_needed": "This test prevents a regression where the report writer does not use an existing HTML file."
      },
      "nodeid": "tests/test_report_writer_coverage.py::TestReportWriterPDF::test_resolve_html_source_uses_existing",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 5,
          "line_ranges": "77-81"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000794998999992913,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "asserts the correct value for `scenario` field",
          "asserts the correct value for `why_needed` field",
          "asserts the correct values for `key_assertions` field",
          "sets the expected confidence level"
        ],
        "scenario": "Test that `from_dict` can handle a full dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 276,
          "total_tokens": 376
        },
        "why_needed": "Prevents regression in case of missing or invalid data."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_from_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 8,
          "line_ranges": "90-92, 94-98"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007961710000188305,
      "file_path": "tests/test_schemas.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "assert data['scenario'] == 'Verify login'",
          "assert data['why_needed'] == 'Catch auth bugs'",
          "assert data['key_assertions'] == ['assert 200', 'assert token']",
          "# Correctly assert all required fields"
        ],
        "scenario": "Test that the `to_dict` method correctly converts the schema to a dictionary with all required fields.",
        "token_usage": {
          "completion_tokens": 119,
          "prompt_tokens": 273,
          "total_tokens": 392
        },
        "why_needed": "This test prevents regression bugs where the `to_dict` method is not correctly converting the schema to a dictionary."
      },
      "nodeid": "tests/test_schemas.py::TestAnnotationSchema::test_to_dict_full",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09254808599999365,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The file path of the report should exist after running the test with the `--llm-report` option.",
          "The content of the report should contain the string '<html>'.",
          "The string 'test_simple' should be present in the report's content."
        ],
        "scenario": "The test verifies that an HTML report is created when the `--llm-report` option is used.",
        "token_usage": {
          "completion_tokens": 122,
          "prompt_tokens": 264,
          "total_tokens": 386
        },
        "why_needed": "This test prevents a regression where the HTML report is not generated correctly due to changes in the PyTest configuration."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 69,
          "line_ranges": "78-79, 90, 93-94, 96, 99-104, 106-107, 109-112, 114-119, 121-122, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212-214, 216, 227-228, 230-236, 250-251, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 116,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-335, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.13076042299999813,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `assert_summary(labels: list[str], expected: int)` should be able to correctly identify and report on each status in the list.",
          "For the 'Passed' label, it should match the expected count of 1.",
          "For the 'Failed' label, it should also match the expected count of 1.",
          "For the 'Skipped' label, it should not raise an assertion error but instead continue to the next test.",
          "For the 'XFailed' and 'XPassed' labels, they should also match their respective expected counts.",
          "The function `assert_summary(labels: list[str], expected: int)` should be able to handle cases where there are multiple statuses in a single line (e.g., 'Errors', 'Error').",
          "It should raise an assertion error if the number of labels does not match the expected count.",
          "For each label, it should check that the count is equal to the expected value and return without raising an assertion error if this condition is met."
        ],
        "scenario": "test_html_summary_counts_all_statuses verifies that the HTML summary counts include all statuses.",
        "token_usage": {
          "completion_tokens": 265,
          "prompt_tokens": 621,
          "total_tokens": 886
        },
        "why_needed": "This test prevents regression where some statuses are missing from the HTML summary."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_html_summary_counts_all_statuses",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 55,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-118, 124, 127, 132-133, 140-141, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 227-228, 230-236, 261, 264, 268, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-327, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06599388700001896,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `schema_version` key in the report data should be equal to '1.0'.",
          "The `summary` key in the report data should contain an object with keys `total`, `passed`, and `failed`.",
          "The `summary` object's `total` value should be equal to 2.",
          "The `summary` object's `passed` value should be equal to 1.",
          "The `summary` object's `failed` value should be equal to 1.",
          "The `test_pass()` function is called with no assertions.",
          "The `test_fail()` function is called with a non-zero assertion count."
        ],
        "scenario": "The JSON report is created and contains the expected schema version, summary statistics, and test counts.",
        "token_usage": {
          "completion_tokens": 202,
          "prompt_tokens": 295,
          "total_tokens": 497
        },
        "why_needed": "This test prevents a regression where the report generation process fails to create a valid JSON file with the required metadata."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_json_report_created",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 20,
          "line_ranges": "39-41, 53, 55-56, 86, 90, 92, 94, 97-101, 103, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 96,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221, 223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 55,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 325-326, 329-330, 333-334, 337-339, 342, 344, 346, 351, 353-357, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 43,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 170-174, 176-178, 182, 186-187, 190, 192-193, 196, 204, 213, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/llm/schemas.py",
          "line_count": 7,
          "line_ranges": "38, 42-43, 50-53"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 103,
          "line_ranges": "130-133, 135-137, 139, 141, 143, 190, 194-199, 201, 203, 205, 207, 210, 212-214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419-437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-506, 509, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 115,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05842702199998939,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_pass()` returns True.",
          "The value of `litellm.completion` is set to `mock_completion` before the test runs.",
          "The `pytest_configure` function imports `litellm` and sets its completion function to `mock_completion`.",
          "The `pyproject.toml` file includes `[tool.pytest_llm_report]` configuration with `litellm.completion = mock_completion`.",
          "The `makefile` creates a `pyproject.toml` file with the specified configuration.",
          "The `pytester.makeconftest` function patches `litellm.completion` before importing it in the test.",
          "The `mock_completion` function returns a `SimpleNamespace` with the expected annotations.",
          "The `asserts True` assertion passes when the `test_pass()` function is called."
        ],
        "scenario": "Verify that LLM annotations are included in the report when a provider is enabled.",
        "token_usage": {
          "completion_tokens": 233,
          "prompt_tokens": 385,
          "total_tokens": 618
        },
        "why_needed": "Prevents regressions by ensuring LLM annotations are present in reports."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_annotations_in_report",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/cache.py",
          "line_count": 12,
          "line_ranges": "39-41, 53, 55-56, 86, 88, 118-119, 121, 153"
        },
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 39,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/llm/annotator.py",
          "line_count": 100,
          "line_ranges": "47, 50-51, 58-59, 65, 67, 70, 73-74, 76, 84, 86-89, 95-96, 98-99, 106-108, 112-113, 116, 121-122, 132, 134, 137-141, 144-151, 181-182, 184, 186, 188, 199-206, 213-219, 221-223, 249-252, 254-255, 257-258, 260, 262, 264, 269-274, 277-279, 281, 283-284, 289-290, 292-295, 298-301, 303"
        },
        {
          "file_path": "src/pytest_llm_report/llm/base.py",
          "line_count": 37,
          "line_ranges": "65-66, 87-89, 97, 105, 134, 137-138, 155, 163, 174, 185, 188, 191-198, 200, 212, 214, 216, 219-221, 384, 386, 388, 391, 396-397, 399"
        },
        {
          "file_path": "src/pytest_llm_report/llm/batching.py",
          "line_count": 33,
          "line_ranges": "34, 39, 53, 55, 92-93, 95, 103-106, 108-110, 112-116, 136, 156-157, 160, 162, 181-185, 187-188, 190, 224"
        },
        {
          "file_path": "src/pytest_llm_report/llm/litellm_provider.py",
          "line_count": 44,
          "line_ranges": "37-38, 41, 60, 62, 82-83, 89, 92, 95-96, 100-101, 104, 106-107, 112, 114, 116-117, 120, 135, 137, 170-174, 176-178, 182, 186-187, 190, 221-222, 224, 227-229, 242-243, 245"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 136,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-325, 327-328, 332-336, 340, 342, 344, 348, 352, 356, 360, 362, 364, 366, 368, 372, 374, 378, 380, 382, 384, 386, 388, 390, 392, 396, 400, 402, 404, 408, 412, 416, 418, 420, 426, 430, 436, 438, 444, 446, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 316,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362-364, 366-367, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-494, 497, 499, 502-507, 512-514, 516-517, 523-531, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/prompts.py",
          "line_count": 29,
          "line_ranges": "33, 49, 52, 55, 58-59, 65, 78-79, 82-83, 86-87, 92, 94, 98-101, 103-109, 111-112, 116"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 111,
          "line_ranges": "55, 67-73, 85-86, 98-99, 102, 105-108, 113, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-296, 298-299, 301-302, 304-305, 307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09404757400000108,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "Verify that the error is reported in the HTML output.",
          "Check if the error message is displayed correctly.",
          "Ensure the error is included in the report.",
          "Verify that the error is not silently ignored.",
          "Test that the error is surfaced even when it's a litellm.completion error.",
          "Check for any additional error messages or details.",
          "Verify that the error is reported with the correct severity level."
        ],
        "scenario": "Test that LLM errors are surfaced in HTML output.",
        "token_usage": {
          "completion_tokens": 138,
          "prompt_tokens": 313,
          "total_tokens": 451
        },
        "why_needed": "This test prevents regression where LLM errors are not reported correctly."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestBasicReportGeneration::test_llm_error_is_reported",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181-182, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214-216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05704744799999162,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'llm_opt_out' marker should be present in the test results.",
          "The 'llm_opt_out' marker should have a value of True for this test.",
          "There should only be one test result with the 'llm_opt_out' marker.",
          "The 'llm_opt_out' marker should not be False for any other tests."
        ],
        "scenario": "The test verifies that the LLM opt-out marker is correctly recorded in the report.",
        "token_usage": {
          "completion_tokens": 137,
          "prompt_tokens": 290,
          "total_tokens": 427
        },
        "why_needed": "This test prevents a regression where the LLM opt-out marker might not be properly recorded in the report."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_llm_opt_out_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-200, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203, 205, 207, 210, 212, 214, 216, 218, 220, 222-224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.05563729700000408,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `@pytest.mark.requirement` decorator should be applied to functions or modules that require specific requirements.",
          "The `requirement` parameter in `@pytest.mark.requirement` should be set to a valid string (e.g., 'REQ-001', 'REQ-002')",
          "The `requirements` key in the test function's metadata should contain a list of strings (e.g., ['REQ-001', 'REQ-002'])",
          "The `tests` key in the JSON report file should contain exactly one object with a single string value (the requirements)",
          "Each requirement in the requirements list should be present in the report",
          "The `reqs` variable should contain a list of strings that includes both 'REQ-001' and 'REQ-002'",
          "The test function should have been run successfully without any errors"
        ],
        "scenario": "Test the requirement marker to ensure it records the correct requirements.",
        "token_usage": {
          "completion_tokens": 239,
          "prompt_tokens": 307,
          "total_tokens": 546
        },
        "why_needed": "This test prevents a potential bug where the requirement marker is not recorded correctly, causing tests to fail with an incorrect list of requirements."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestMarkers::test_requirement_marker",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.062189760999984856,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of xfailed tests is correctly reported as 2.",
          "All xfailed tests are included in the 'xfailed' key in the report.",
          "Each xfailed test has a corresponding outcome ('xfailed') in the list of outcomes.",
          "The test does not fail due to an assertion error or other non-xfailed test."
        ],
        "scenario": "The test verifies that multiple xfailed tests are recorded in the report.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 317,
          "total_tokens": 444
        },
        "why_needed": "This test prevents regression by ensuring that all xfailed tests are properly reported and counted."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_multiple_xfail_outcomes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 43,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 106-107, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 112,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328-329, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.055950782999985904,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The number of skipped tests should be exactly 1 according to the report.",
          "The 'skip' marker should not be present in the report.",
          "The report path should contain a JSON file with the correct data.",
          "The 'summary' key in the report data should have a value of 'skipped'.",
          "The 'skipped' count in the report data should be 1.",
          "The test skipped function should not be executed during the run.",
          "The pytester.runpytest command should produce an error message indicating that skipping tests was prevented."
        ],
        "scenario": "Test that skipping tests prevents the 'skip' marker from being recorded in the report.",
        "token_usage": {
          "completion_tokens": 180,
          "prompt_tokens": 264,
          "total_tokens": 444
        },
        "why_needed": "This test ensures that skipping tests is properly handled and reported by pytester, preventing incorrect or misleading results."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_skip_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 47,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-116, 119, 121-122, 124, 127, 132-133, 140, 155-159, 163, 167-169, 171, 181, 185-186, 198-199, 209-210, 212, 216, 250-251, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201-203, 205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 113,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324, 326, 328, 330-331, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06067651500001148,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'summary' key in the JSON report should contain a single integer value equal to 1 for each xfailed test.",
          "Each xfailed test should have a corresponding entry in the 'summary' JSON report with a 'xfailed' counter value of 1.",
          "If a new test with the same name fails, the 'xfailed' counter should be incremented correctly and reflected in the report."
        ],
        "scenario": "Verifies that the 'xfailed' counter is incremented when an xfailed test is run.",
        "token_usage": {
          "completion_tokens": 158,
          "prompt_tokens": 264,
          "total_tokens": 422
        },
        "why_needed": "Prevents regression in the test outcomes report, where the 'xfailed' counter might not be updated correctly if a new test with the same name fails."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestOutcomes::test_xfail_outcome",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 76,
          "line_ranges": "190, 194-199, 201, 203-205, 207, 210, 212, 214, 216, 218, 220, 222, 224, 376-392, 394, 397, 399, 402-405, 407, 409, 411, 413, 415, 419, 437, 467-475, 477, 479, 518, 520-524, 526, 528, 530, 532, 534, 536, 538, 540"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484-486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 110,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222-223, 226, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 347, 350-352, 355-356, 359-361, 364, 367-371, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.06011165800001095,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_param` should be called three times with input values [1, 2, 3].",
          "The total number of tests executed should be 3 (three runs of the test).",
          "At least one test should pass for each input value.",
          "No additional tests should be run beyond what is expected from `test_param`."
        ],
        "scenario": "Test parameterized tests are recorded separately.",
        "token_usage": {
          "completion_tokens": 130,
          "prompt_tokens": 290,
          "total_tokens": 420
        },
        "why_needed": "This test prevents regression in parametrized testing, where the same test is run multiple times with different inputs."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestParametrization::test_parametrized_tests",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.05065113700001689,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The help text should include at least one example.', 'expected_result': ['Example:'], 'actual_result': ['*Example:*--llm-report*', '*Example:*--model-serve']}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
        "token_usage": {
          "completion_tokens": 109,
          "prompt_tokens": 123,
          "total_tokens": 232
        },
        "why_needed": "This test is necessary to ensure that the help text for the CLI tool includes usage examples."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_help_contains_examples",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.045241233000012926,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'markers are found', 'value': ['llm_opt_out', 'llm_context', 'requirement']}",
          "{'name': 'expected stdout lines', 'value': ['*llm_opt_out*', '*llm_context*', '*requirement*']}"
        ],
        "scenario": "test markers registered",
        "token_usage": {
          "completion_tokens": 93,
          "prompt_tokens": 142,
          "total_tokens": 235
        },
        "why_needed": "Test that LLM markers are correctly registered."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_markers_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 89,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482, 484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 240,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.05145284799999672,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'plugin is registered', 'description': 'The plugin should be registered before any tests are run.'}"
        ],
        "scenario": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 118,
          "total_tokens": 205
        },
        "why_needed": "The plugin is not registered, which means it will not be available for use during test execution."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestPluginRegistration::test_plugin_registered",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 40,
          "line_ranges": "78-79, 90, 93-94, 96, 99-100, 104, 109-112, 114-115, 124, 127, 132-133, 140, 155-159, 163-164, 167-169, 171, 181, 185-186, 198-199, 209-210, 277, 285"
        },
        {
          "file_path": "src/pytest_llm_report/coverage_map.py",
          "line_count": 12,
          "line_ranges": "44-45, 58-60, 72-73, 83, 86, 88-90"
        },
        {
          "file_path": "src/pytest_llm_report/errors.py",
          "line_count": 4,
          "line_ranges": "142-145"
        },
        {
          "file_path": "src/pytest_llm_report/models.py",
          "line_count": 1,
          "line_ranges": "190"
        },
        {
          "file_path": "src/pytest_llm_report/options.py",
          "line_count": 90,
          "line_ranges": "123, 171, 199, 202-203, 209-210, 217-218, 225-226, 233-234, 241, 245, 247, 249, 251, 253, 257-258, 265-266, 271, 273, 276, 284, 308, 311-312, 320-322, 460, 463, 466, 470, 472-473, 476-477, 482-484, 486, 488, 490, 492, 494, 499-500, 504-505, 511-512, 516-517, 521-522, 528-529, 534, 537-538, 542-543, 547-548, 554-555, 561-562, 566-567, 572, 575-576, 581, 583, 588-589, 593-594, 599, 601, 603, 605, 607, 611, 613"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 288,
          "line_ranges": "41, 44-48, 50-54, 56-60, 62-66, 68-72, 74-79, 81-86, 90-94, 96-100, 102-106, 108-112, 114-118, 122-126, 128-132, 134-138, 142-146, 148-153, 155-159, 161-165, 169-174, 176-181, 185-190, 192-197, 199-204, 208-213, 215-219, 223-227, 229-233, 235-239, 241-245, 247-252, 254-258, 260-264, 268-272, 274-279, 283-287, 289-293, 297-302, 304-309, 311-315, 328-330, 332-334, 336-338, 342, 346-347, 349, 351, 354-355, 362, 371-373, 399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485, 491-492, 534-544, 558-559, 562, 566-568, 579, 583, 602, 606-608, 619, 623, 626, 628-629"
        },
        {
          "file_path": "src/pytest_llm_report/render.py",
          "line_count": 25,
          "line_ranges": "30-31, 40, 42-46, 50-51, 53, 65, 67, 79-85, 87, 99, 101-102, 107"
        },
        {
          "file_path": "src/pytest_llm_report/report_writer.py",
          "line_count": 106,
          "line_ranges": "55, 67-73, 85-86, 98-100, 127-128, 130, 156-158, 186, 192-193, 197-198, 202, 211-218, 222, 226-227, 230, 233, 254, 256-259, 262-264, 266, 268-275, 277-278, 280-289, 291-294, 296-297, 299-300, 302-303, 305-307, 319, 321-322, 324-325, 337, 383, 385-386, 389, 392, 395, 398-402, 477-478, 502, 504, 506-508, 510, 513"
        }
      ],
      "duration": 0.09415404300000318,
      "file_path": "tests/test_smoke_pytester.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `test_special_chars_in_nodeid` should not crash when given special characters in node IDs.",
          "The HTML file generated by pytester should contain the '<html' tag.",
          "The test assertion `assert s` should pass for all cases where a special character is present in the node ID.",
          "The function `pytester.makepyfile()` should create an HTML report that contains the '<html' tag.",
          "The file path of the generated report should exist and be valid.",
          "The content of the report should contain the '<html' tag."
        ],
        "scenario": "Test that special characters in nodeid are handled correctly by pytester.",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 288,
          "total_tokens": 466
        },
        "why_needed": "This test prevents a potential regression where special characters in node IDs could cause issues with the LLM report generation."
      },
      "nodeid": "tests/test_smoke_pytester.py::TestSpecialCharacters::test_special_chars_in_nodeid",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007341849999988881,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1m 0.0s', 'actual': '1m 0.0s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
        "token_usage": {
          "completion_tokens": 85,
          "prompt_tokens": 106,
          "total_tokens": 191
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats a duration of exactly one minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_boundary_one_minute",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.000719787999997834,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"assert '\u03bcs' in result\", 'expected': '500\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
        "token_usage": {
          "completion_tokens": 81,
          "prompt_tokens": 121,
          "total_tokens": 202
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-millisecond durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_microseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007354880000036701,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '500.0ms', 'actual': '500.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
        "token_usage": {
          "completion_tokens": 78,
          "prompt_tokens": 119,
          "total_tokens": 197
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats sub-second durations as milliseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_milliseconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007553949999987708,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': \"result contains 'm'\", 'expected': '1m 30.5s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_minutes_format",
        "token_usage": {
          "completion_tokens": 83,
          "prompt_tokens": 124,
          "total_tokens": 207
        },
        "why_needed": "To ensure the `format_duration` function correctly formats durations over a minute, including minutes and seconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_minutes_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 6,
          "line_ranges": "39, 41, 43, 46-48"
        }
      ],
      "duration": 0.0007328030000053332,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of `format_duration(185.0)` is '3m 5.0s'."
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 112,
          "total_tokens": 187
        },
        "why_needed": "To ensure the `format_duration` function correctly formats multiple minutes into a human-readable string."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_multiple_minutes",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.000739425000006122,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected_value': '1.00s', 'actual_value': '1.0'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_one_second",
        "token_usage": {
          "completion_tokens": 86,
          "prompt_tokens": 101,
          "total_tokens": 187
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats a duration of exactly one second as '1.00s'."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_one_second",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 4,
          "line_ranges": "39, 41, 43-44"
        }
      ],
      "duration": 0.0007302379999885034,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'description': \"The result should contain 's' (seconds) as part of the assertion.\", 'value': 's'}",
          "{'description': \"The result should be equal to '5.50s'.\", 'value': '5.50s'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_seconds_format",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 110,
          "total_tokens": 225
        },
        "why_needed": "To ensure the function `format_duration` correctly formats seconds under a minute."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_seconds_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 3,
          "line_ranges": "39, 41-42"
        }
      ],
      "duration": 0.0007779569999968317,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'expected': '1.0ms', 'actual': '1.0ms'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 111,
          "total_tokens": 188
        },
        "why_needed": "To ensure that the `format_duration` function correctly formats small millisecond durations."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_small_milliseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 2,
          "line_ranges": "39-40"
        }
      ],
      "duration": 0.0007426109999926211,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result', 'expected': '1\u03bcs'}"
        ],
        "scenario": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
        "token_usage": {
          "completion_tokens": 77,
          "prompt_tokens": 116,
          "total_tokens": 193
        },
        "why_needed": "To ensure that the `format_duration` function can correctly format very small durations as microseconds."
      },
      "nodeid": "tests/test_time.py::TestFormatDuration::test_very_small_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.000753291000023637,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': \"The result of iso_format(dt) is equal to '2024-01-15T10:30:45+00:00'\", 'expected_result': '2024-01-15T10:30:45+00:00'}"
        ],
        "scenario": "Tests time-related functionality with UTC timezone",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 143,
          "total_tokens": 249
        },
        "why_needed": "To ensure that datetime objects can be correctly formatted with the UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_datetime_with_utc",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007448449999856166,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Result format', 'expected': '2024-06-20T14:00:00', 'actual': '2024-06-20T14:00:00', 'message': 'Expected result format, got actual'}"
        ],
        "scenario": "Testing naive datetime formats",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 136,
          "total_tokens": 242
        },
        "why_needed": "To ensure that the `iso_format` function correctly handles naive datetime objects without a timezone."
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_naive_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "27"
        }
      ],
      "duration": 0.0007607149999842022,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Result contains 123456', 'expected_value': '123456', 'actual_value': '<built-in function isoformat> returns a string containing the ISO-formatted time in seconds and microseconds, which is then converted to a datetime object.'}"
        ],
        "scenario": "Tests time module with microseconds",
        "token_usage": {
          "completion_tokens": 98,
          "prompt_tokens": 133,
          "total_tokens": 231
        },
        "why_needed": "To test the format of datetime objects with microseconds"
      },
      "nodeid": "tests/test_time.py::TestIsoFormat::test_formats_with_microseconds",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007540329999926598,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'assert result.tzinfo is not None', 'description': 'The returned datetime object has a valid timezone info.'}",
          "{'name': 'assert result.tzinfo == UTC', 'description': \"The returned datetime object's timezone info is set to UTC.\"}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
        "token_usage": {
          "completion_tokens": 124,
          "prompt_tokens": 109,
          "total_tokens": 233
        },
        "why_needed": "This test ensures that the `utc_now()` function returns a datetime object with an associated UTC timezone."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_has_utc_timezone",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007666859999915232,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'Before and After Time Tolerance', 'description': 'The result should be within 1 second of the before and after times.'}"
        ],
        "scenario": "Test that the function returns a valid UTC now.",
        "token_usage": {
          "completion_tokens": 87,
          "prompt_tokens": 116,
          "total_tokens": 203
        },
        "why_needed": "The test is necessary to ensure the function can return a current time within a reasonable tolerance."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_is_current_time",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        },
        {
          "file_path": "src/pytest_llm_report/util/time.py",
          "line_count": 1,
          "line_ranges": "15"
        }
      ],
      "duration": 0.0007959710000022824,
      "file_path": "tests/test_time.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'name': 'result is an instance of datetime', 'expected_result': 'datetime'}"
        ],
        "scenario": "tests/test_time.py::TestUtcNow::test_returns_datetime",
        "token_usage": {
          "completion_tokens": 75,
          "prompt_tokens": 94,
          "total_tokens": 169
        },
        "why_needed": "This test ensures that the `utc_now` function returns a datetime object."
      },
      "nodeid": "tests/test_time.py::TestUtcNow::test_returns_datetime",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008293929999751981,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "- The 'run' function of subprocess is called with an incorrect returncode (1) when the get-token command fails.",
          "- The error message returned by the 'run' function contains the string 'Authentication failed'",
          "- The test asserts that the error message includes the string 'exit 1', which indicates a failure in the process."
        ],
        "scenario": "Test TokenRefresher raises error on command failure when get-token command fails.",
        "token_usage": {
          "completion_tokens": 136,
          "prompt_tokens": 310,
          "total_tokens": 446
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher class does not handle command failures properly, potentially leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_command_failure",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-109, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008339219999982106,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "the output of the get_token() method is an empty string",
          "the error message returned by the TokenRefreshError exception includes the phrase 'empty output'",
          "the error message is in lowercase to ensure correct comparison with the expected string"
        ],
        "scenario": "The test verifies that the TokenRefresher raises an error when given an empty output.",
        "token_usage": {
          "completion_tokens": 117,
          "prompt_tokens": 297,
          "total_tokens": 414
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not raise an error when it encounters an empty output."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_empty_output",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008646999999939453,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` returns the expected token value for both `token1` and `token2` when `force=True`.",
          "The function `get_token(force=True)` calls the `fake_run` function with the correct arguments, resulting in a completed process object with an `stdout` attribute containing the expected token value.",
          "The `call_count` variable is incremented correctly to 2 after calling `get_token()` twice with `force=True`.",
          "The `token1` and `token2` variables are assigned the expected values based on their indices in the list of tokens returned by `get_token()`."
        ],
        "scenario": "Verify that `TokenRefresher` forces a cache refresh when `force=True` and the refresh interval is set to 3600 seconds.",
        "token_usage": {
          "completion_tokens": 214,
          "prompt_tokens": 346,
          "total_tokens": 560
        },
        "why_needed": "This test prevents a potential bug where the token refresh does not occur immediately after setting `force=True` due to the cached tokens taking too long to expire."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_force_refresh",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008205769999847234,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `json_key` parameter passed to the `get_token()` method of the `TokenRefresher` class matches the custom JSON key provided in the test.",
          "The output of the `subprocess.run()` function is a JSON object with the correct access token.",
          "The `access_token` field in the JSON response is set to the specified custom key.",
          "The `token` variable is assigned the expected custom key value.",
          "The `assert` statement checks that the `token` variable matches the expected custom key value.",
          "The `subprocess.run()` function returns a CompletedProcess object with a returncode of 0, indicating successful execution."
        ],
        "scenario": "Verify that the `TokenRefresher` class uses a custom JSON key for token refresh.",
        "token_usage": {
          "completion_tokens": 209,
          "prompt_tokens": 303,
          "total_tokens": 512
        },
        "why_needed": "This test prevents a potential bug where the default JSON key used by the `TokenRefresher` class is not compatible with the expected custom key."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_custom_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 29,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132-135, 139, 143-144, 148"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008597309999913705,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the `get-token` command is a JSON object with 'token' and 'expires_in' keys.",
          "The 'token' key contains the value 'json-token-value'.",
          "The 'expires_in' key contains the value 3600 (one hour).",
          "The 'command' attribute of the `TokenRefresher` instance matches the command used to run the `get-token` command.",
          "The `refresh_interval` attribute is set to 3600 seconds.",
          "The `output_format` attribute is set to 'json'."
        ],
        "scenario": "The `TokenRefresher` class extracts the correct JSON format for the obtained token.",
        "token_usage": {
          "completion_tokens": 178,
          "prompt_tokens": 308,
          "total_tokens": 486
        },
        "why_needed": "This test prevents a potential bug where the token is not in the expected JSON format."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_json_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008257769999886477,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "token == 'my-secret-token'",
          "output_format == 'text'",
          "subprocess.run.returncode == 0",
          "# expected return code of 0"
        ],
        "scenario": "The `TokenRefresher` class extracts the correct token from the text output when using the 'text' format.",
        "token_usage": {
          "completion_tokens": 106,
          "prompt_tokens": 298,
          "total_tokens": 404
        },
        "why_needed": "This test prevents a bug where the token is not extracted correctly if the output format is set to 'text'."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_get_token_text_format",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 25,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-134, 149-150"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008379500000046392,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The output of the get_token() method should contain 'json' in its string representation.",
          "The error message should include the word 'json'.",
          "The error message should not be empty.",
          "The error message should not contain any other characters besides 'json'."
        ],
        "scenario": "The test verifies that the TokenRefresher raises a TokenRefreshError when given invalid JSON.",
        "token_usage": {
          "completion_tokens": 118,
          "prompt_tokens": 299,
          "total_tokens": 417
        },
        "why_needed": "This test prevents a bug where the TokenRefresher incorrectly handles invalid JSON input."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalid_json",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156, 160-162"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008198349999872789,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The token returned by get_token() should be different from the initial token.",
          "The call count of the fake_run function should increase by 1 after calling invalidate() on the TokenRefresher instance.",
          "The output of get_token() should contain the token number in the format 'token-X', where X is the call count.",
          "The output of invalidate() should be a CompletedProcessResult with returncode=0, stdout='token-Y', and stderr=''",
          "The value of call_count after calling invalidate() on the TokenRefresher instance should be 1.",
          "The token returned by get_token() should not be equal to the initial token.",
          "The output of get_token() should contain the token number in the format 'token-1' or 'token-2'.",
          "The output of invalidate() should contain the expected error message."
        ],
        "scenario": "Test TokenRefresher.invalidate() clears cache and verifies it is invalidated after a refresh.",
        "token_usage": {
          "completion_tokens": 243,
          "prompt_tokens": 340,
          "total_tokens": 583
        },
        "why_needed": "This test prevents potential regression where the TokenRefresher.invalidate() method does not clear the cache after a successful refresh."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_invalidate",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139-141, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009843739999837453,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `token` key should be present in the response.",
          "A message indicating that the token was not found should be included in the error message.",
          "The error message should include the word 'not found'.",
          "The function should raise a `TokenRefreshError` exception with a meaningful error message.",
          "The function should handle cases where the required JSON key is missing without raising an exception or returning an incorrect result."
        ],
        "scenario": "Test that TokenRefresher raises an error when a JSON key is missing.",
        "token_usage": {
          "completion_tokens": 153,
          "prompt_tokens": 325,
          "total_tokens": 478
        },
        "why_needed": "To prevent the test from passing and to ensure that the function correctly handles cases where a required JSON key is not present."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_missing_json_key",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.051555419999999685,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "all threads should acquire the lock before getting a token (token-1)",
          "each thread should get the same token (token-0, token-1, ..., token-4)",
          "no thread should be left without acquiring the lock",
          "the order of tokens obtained by each thread is consistent (token-1, token-2, ..., token-4)",
          "all threads should finish getting a token within 5 seconds or less"
        ],
        "scenario": "Verify that TokenRefresher is thread-safe by ensuring all threads acquire the lock before getting a token.",
        "token_usage": {
          "completion_tokens": 158,
          "prompt_tokens": 427,
          "total_tokens": 585
        },
        "why_needed": "This test prevents a potential deadlock or starvation of threads when multiple threads are trying to get a token simultaneously."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_thread_safety",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 16,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113-114"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008577670000136095,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `fake_run` raises a `TimeoutExpired` exception with the message 'timed out'.",
          "The test asserts that the error message contains the word 'timed out'.",
          "The test asserts that the error message is case-insensitive (i.e., it matches 'Timed Out' or 'TIMEOUT')."
        ],
        "scenario": "The test verifies that TokenRefresher handles command timeouts correctly by raising a TokenRefreshError when the 'get-token' command times out.",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 279,
          "total_tokens": 441
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher does not raise an error when the 'get-token' command takes too long to complete, potentially leading to unexpected behavior or errors in downstream code."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_timeout_handling",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 28,
          "line_ranges": "59-60, 63-66, 69-72, 83, 85-86, 90, 93-98, 101, 107-108, 111, 132, 153-154, 156"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008205860000032317,
      "file_path": "tests/test_token_refresh.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `token1` should be equal to `"
        ],
        "scenario": "Test that TokenRefresher caches tokens and doesn't call command again.",
        "token_usage": {
          "completion_tokens": 74,
          "prompt_tokens": 353,
          "total_tokens": 427
        },
        "why_needed": "This test prevents a potential issue where the `get-token` command is called multiple times due to caching."
      },
      "nodeid": "tests/test_token_refresh.py::TestTokenRefresher::test_token_caching",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 20,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101-104, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008131939999884708,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` in the `TokenRefresher` class should raise a `TokenRefreshError` with an error message indicating that no stderr output was produced.",
          "The error message should include the string 'exit 1' to indicate that the command failed without producing any stderr output.",
          "The function should also check for the presence of the string 'No error output' in the error message to ensure it is not silently ignoring this information."
        ],
        "scenario": "Test the test_command_failure_no_stderr function to verify that it correctly handles a command failure with no stderr output.",
        "token_usage": {
          "completion_tokens": 165,
          "prompt_tokens": 322,
          "total_tokens": 487
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher class does not handle command failures properly, leading to unexpected behavior or errors."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_command_failure_no_stderr",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008016019999956825,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "{'assertion': 'The function `get_token()` should raise a `TokenRefreshError` when given an empty command string.', 'expected': 'TokenRefreshError', 'actual': 'Exception'}"
        ],
        "scenario": "Test handling of empty command string.",
        "token_usage": {
          "completion_tokens": 100,
          "prompt_tokens": 151,
          "total_tokens": 251
        },
        "why_needed": "To ensure the TokenRefresher class handles empty command strings correctly and raises a TokenRefreshError with the correct message."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_empty_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-88, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008347330000049169,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token()` method of the `TokenRefresher` instance should raise a `TokenRefreshError` with a message indicating that the command string is invalid.",
          "The error message should include the phrase 'Invalid command string'.",
          "When an invalid command string is passed to the `get_token()` method, it should not return any token data.",
          "The error message should be raised as a `pytest.raises(TokenRefreshError)` exception.",
          "The `exc_info.value` attribute of the raised exception should contain a `TokenRefreshError` object with the specified error message.",
          "When an invalid command string is passed to the `get_token()` method, it should not raise any other exceptions (e.g. `ValueError`, `TypeError`)."
        ],
        "scenario": "Test the test_invalid_command_string function to verify it handles an invalid command string (shlex parse error).",
        "token_usage": {
          "completion_tokens": 226,
          "prompt_tokens": 251,
          "total_tokens": 477
        },
        "why_needed": "Prevent a potential bug where the TokenRefresher class incorrectly raises a TokenRefreshError for an invalid command string."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_invalid_command_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 27,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-137, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008465059999878122,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `get_token` method of the `TokenRefresher` instance should raise a `TokenRefreshError` with an error message indicating that the output is not a dictionary.",
          "The error message should include the string 'list' to ensure it's not just a generic JSON parsing error.",
          "The test should verify that the error message includes the expected string, allowing for future changes in JSON formats without breaking the test.",
          "The `json_key` parameter should be set to `"
        ],
        "scenario": "Test verifying that a TokenRefresher raises an error when the output is not a dictionary.",
        "token_usage": {
          "completion_tokens": 162,
          "prompt_tokens": 328,
          "total_tokens": 490
        },
        "why_needed": "This test prevents a potential regression where the TokenRefresher incorrectly handles non-dict JSON outputs."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_not_dict",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000858196999985239,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` of the `TokenRefresher` class should raise a `TokenRefreshError` with the message 'empty or not a string' when given an empty or non-string token value.",
          "The error message returned by `get_token()` should include the string 'empty or not a string' to indicate that the input is invalid.",
          "The function should return a `TokenRefreshError` exception instead of raising it internally, allowing for proper error handling and logging.",
          "The test should verify that the error message includes the specific phrase 'empty or not a string', which indicates the type of input being validated.",
          "The test should also verify that the error is not raised when an empty string is passed as a token value in JSON format, but instead returns an empty or non-string result.",
          "The `json.dumps()` function used to create the expected output should produce a string with only whitespace characters, indicating an empty or invalid input."
        ],
        "scenario": "Test handling when token value is an empty string.",
        "token_usage": {
          "completion_tokens": 251,
          "prompt_tokens": 324,
          "total_tokens": 575
        },
        "why_needed": "Prevents potential bug where the TokenRefresher incorrectly handles empty or non-string token values."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_empty_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 30,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 101, 107-108, 111, 113, 115, 132-135, 139, 143-146, 149"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0009663099999954738,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The `json.dumps` function is called with an integer instead of a string.",
          "An error message indicating 'empty or not a string' is raised when trying to get the token.",
          "The `TokenRefreshError` exception is raised with a meaningful error message."
        ],
        "scenario": "Test that the test_json_token_not_string scenario verifies when token value is not a string, preventing TokenRefreshError.",
        "token_usage": {
          "completion_tokens": 127,
          "prompt_tokens": 326,
          "total_tokens": 453
        },
        "why_needed": "This test prevents the TokenRefreshError by ensuring that the token value is always a string before attempting to refresh it."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_json_token_not_string",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 19,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90, 93-98, 113, 115-118"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0008494410000139396,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The 'get_token' method of the refresher should raise a TokenRefreshError with a message indicating that the command was not found.",
          "The error message should include 'Failed to execute'.",
          "The error message should be raised as an exception, not as a string.",
          "The error message should contain the text 'Command not found'.",
          "The error message should not be empty."
        ],
        "scenario": "Test verifies that a TokenRefresher raises a TokenRefreshError when executing a command that does not exist.",
        "token_usage": {
          "completion_tokens": 149,
          "prompt_tokens": 280,
          "total_tokens": 429
        },
        "why_needed": "This test prevents a potential bug where the TokenRefresher incorrectly handles commands that are not found during execution."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_oserror_on_execution",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 4,
          "line_ranges": "132, 153-155"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.000764932999999246,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The _parse_output method should return a TokenRefreshError if no non-empty lines are found in the output.",
          "The output wrapper should contain whitespace content lines but not any non-whitespace lines.",
          "The parse_output function should raise a TokenRefreshError when given text with only blank lines after the initial strip.",
          "The test should fail when the _parse_output method is called directly with text that has only whitespace lines.",
          "The test should pass if the output wrapper contains non-empty lines and only whitespace content lines."
        ],
        "scenario": "Test handling when text output has only whitespace lines after initial strip.",
        "token_usage": {
          "completion_tokens": 166,
          "prompt_tokens": 376,
          "total_tokens": 542
        },
        "why_needed": "Prevents TokenRefreshError due to incorrect parsing of output with only blank lines after the initial strip."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_text_only_whitespace_lines",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/llm/token_refresh.py",
          "line_count": 11,
          "line_ranges": "59-60, 63, 69, 83, 85-86, 90-91, 113, 115"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 6,
          "line_ranges": "558-559, 562, 566-568"
        }
      ],
      "duration": 0.0007925139999827024,
      "file_path": "tests/test_token_refresh_coverage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The function `get_token()` should raise a `TokenRefreshError` with the message 'empty' when given an empty command string.",
          "The function `get_token()` should return None or handle the case where the command is not empty correctly.",
          "The error message returned by `get_token()` should be 'empty'.",
          "The test should fail if `get_token()` returns a non-empty value instead of raising an exception.",
          "The test should pass if `get_token()` raises a `TokenRefreshError` with the correct message.",
          "The function `get_token()` should handle whitespace-only command strings without any issues or exceptions.",
          "The function `get_token()` should not raise any other types of exceptions for empty commands."
        ],
        "scenario": "Test the test_whitespace_only_command to verify it handles whitespace-only command strings correctly.",
        "token_usage": {
          "completion_tokens": 219,
          "prompt_tokens": 236,
          "total_tokens": 455
        },
        "why_needed": "This test prevents a potential bug where TokenRefresher incorrectly handles empty commands and raises an exception instead of returning a meaningful error message."
      },
      "nodeid": "tests/test_token_refresh_coverage.py::TestTokenRefresherEdgeCases::test_whitespace_only_command",
      "outcome": "passed",
      "phase": "call"
    },
    {
      "coverage": [
        {
          "file_path": "src/pytest_llm_report/collector.py",
          "line_count": 14,
          "line_ranges": "90, 93, 96, 99, 110-112, 114-115, 124, 127, 140, 209-210"
        },
        {
          "file_path": "src/pytest_llm_report/plugin.py",
          "line_count": 73,
          "line_ranges": "399, 403, 407, 410, 429-430, 437-438, 441-442, 444-445, 448-452, 454, 457-458, 460, 463-464, 485-487, 491-494, 497, 499, 502-506, 509, 512-514, 516-521, 523-531, 534-544, 558-559, 562, 566-568"
        }
      ],
      "duration": 0.004507101999990937,
      "file_path": "tests/test_token_usage.py",
      "llm_annotation": {
        "confidence": 0.8,
        "key_assertions": [
          "The total input tokens should be 30 (10 + 20)",
          "The total output tokens should be 15 (5 + 10)",
          "The number of annotations should be 2 (LLM annotation for test1 and test3)",
          "The total tokens should be 45 (15 + 30)"
        ],
        "scenario": "Verify token usage aggregation for multiple test cases",
        "token_usage": {
          "completion_tokens": 115,
          "prompt_tokens": 775,
          "total_tokens": 890
        },
        "why_needed": "Prevent regression in token usage reporting when aggregating results from multiple tests"
      },
      "nodeid": "tests/test_token_usage.py::test_token_usage_aggregation",
      "outcome": "passed",
      "phase": "call"
    }
  ]
}